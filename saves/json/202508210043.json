[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2508.13935v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13935v1",
                "updated": "2025-08-19T15:26:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    15,
                    26,
                    36,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T15:26:36Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    15,
                    26,
                    36,
                    1,
                    231,
                    0
                ],
                "title": "Scavenger+: Revisiting Space-Time Tradeoffs in Key-Value Separated\n  LSM-trees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scavenger+: Revisiting Space-Time Tradeoffs in Key-Value Separated\n  LSM-trees"
                },
                "summary": "Key-Value Stores (KVS) based on log-structured merge-trees (LSM-trees) are\nwidely used in storage systems but face significant challenges, such as high\nwrite amplification caused by compaction. KV-separated LSM-trees address write\namplification but introduce significant space amplification, a critical concern\nin cost-sensitive scenarios. Garbage collection (GC) can reduce space\namplification, but existing strategies are often inefficient and fail to\naccount for workload characteristics. Moreover, current key-value (KV)\nseparated LSM-trees overlook the space amplification caused by the index\nLSM-tree. In this paper, we systematically analyze the sources of space\namplification in KV-separated LSM-trees and propose Scavenger+, which achieves\na better performance-space trade-off. Scavenger+ introduces (1) an\nI/O-efficient garbage collection scheme to reduce I/O overhead, (2) a\nspace-aware compaction strategy based on compensated size to mitigate\nindex-induced space amplification, and (3) a dynamic GC scheduler that adapts\nto system load to make better use of CPU and storage resources. Extensive\nexperiments demonstrate that Scavenger+ significantly improves write\nperformance and reduces space amplification compared to state-of-the-art\nKV-separated LSM-trees, including BlobDB, Titan, and TerarkDB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value Stores (KVS) based on log-structured merge-trees (LSM-trees) are\nwidely used in storage systems but face significant challenges, such as high\nwrite amplification caused by compaction. KV-separated LSM-trees address write\namplification but introduce significant space amplification, a critical concern\nin cost-sensitive scenarios. Garbage collection (GC) can reduce space\namplification, but existing strategies are often inefficient and fail to\naccount for workload characteristics. Moreover, current key-value (KV)\nseparated LSM-trees overlook the space amplification caused by the index\nLSM-tree. In this paper, we systematically analyze the sources of space\namplification in KV-separated LSM-trees and propose Scavenger+, which achieves\na better performance-space trade-off. Scavenger+ introduces (1) an\nI/O-efficient garbage collection scheme to reduce I/O overhead, (2) a\nspace-aware compaction strategy based on compensated size to mitigate\nindex-induced space amplification, and (3) a dynamic GC scheduler that adapts\nto system load to make better use of CPU and storage resources. Extensive\nexperiments demonstrate that Scavenger+ significantly improves write\nperformance and reduces space amplification compared to state-of-the-art\nKV-separated LSM-trees, including BlobDB, Titan, and TerarkDB."
                },
                "authors": [
                    {
                        "name": "Jianshun Zhang"
                    },
                    {
                        "name": "Fang Wang"
                    },
                    {
                        "name": "Jiaxin Ou"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Ming Zhao"
                    },
                    {
                        "name": "Sheng Qiu"
                    },
                    {
                        "name": "Junxun Huang"
                    },
                    {
                        "name": "Baoquan Li"
                    },
                    {
                        "name": "Peng Fang"
                    },
                    {
                        "name": "Dan Feng"
                    }
                ],
                "author_detail": {
                    "name": "Dan Feng"
                },
                "author": "Dan Feng",
                "arxiv_doi": "10.1109/TC.2025.3587513",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TC.2025.3587513",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.13935v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13935v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by IEEE Transactions on Computers",
                "arxiv_journal_ref": "Year 2025, pp. 1-14,",
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13909v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13909v1",
                "updated": "2025-08-19T15:08:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    15,
                    8,
                    39,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T15:08:39Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    15,
                    8,
                    39,
                    1,
                    231,
                    0
                ],
                "title": "Scavenger: Better Space-Time Trade-Offs for Key-Value Separated\n  LSM-trees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scavenger: Better Space-Time Trade-Offs for Key-Value Separated\n  LSM-trees"
                },
                "summary": "Key-Value Stores (KVS) implemented with log-structured merge-tree (LSM-tree)\nhave gained widespread acceptance in storage systems. Nonetheless, a\nsignificant challenge arises in the form of high write amplification due to the\ncompaction process. While KV-separated LSM-trees successfully tackle this\nissue, they also bring about substantial space amplification problems, a\nconcern that cannot be overlooked in cost-sensitive scenarios. Garbage\ncollection (GC) holds significant promise for space amplification reduction,\nyet existing GC strategies often fall short in optimization performance,\nlacking thorough consideration of workload characteristics. Additionally,\ncurrent KV-separated LSM-trees also ignore the adverse effect of the space\namplification in the index LSM-tree. In this paper, we systematically analyze\nthe sources of space amplification of KV-separated LSM-trees and introduce\nScavenger, which achieves a better trade-off between performance and space\namplification. Scavenger initially proposes an I/O-efficient garbage collection\nscheme to reduce I/O overhead and incorporates a space-aware compaction\nstrategy based on compensated size to minimize the space amplification of index\nLSM-trees. Extensive experiments show that Scavenger significantly improves\nwrite performance and achieves lower space amplification than other\nKV-separated LSM-trees (including BlobDB, Titan, and TerarkDB).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value Stores (KVS) implemented with log-structured merge-tree (LSM-tree)\nhave gained widespread acceptance in storage systems. Nonetheless, a\nsignificant challenge arises in the form of high write amplification due to the\ncompaction process. While KV-separated LSM-trees successfully tackle this\nissue, they also bring about substantial space amplification problems, a\nconcern that cannot be overlooked in cost-sensitive scenarios. Garbage\ncollection (GC) holds significant promise for space amplification reduction,\nyet existing GC strategies often fall short in optimization performance,\nlacking thorough consideration of workload characteristics. Additionally,\ncurrent KV-separated LSM-trees also ignore the adverse effect of the space\namplification in the index LSM-tree. In this paper, we systematically analyze\nthe sources of space amplification of KV-separated LSM-trees and introduce\nScavenger, which achieves a better trade-off between performance and space\namplification. Scavenger initially proposes an I/O-efficient garbage collection\nscheme to reduce I/O overhead and incorporates a space-aware compaction\nstrategy based on compensated size to minimize the space amplification of index\nLSM-trees. Extensive experiments show that Scavenger significantly improves\nwrite performance and achieves lower space amplification than other\nKV-separated LSM-trees (including BlobDB, Titan, and TerarkDB)."
                },
                "authors": [
                    {
                        "name": "Jianshun Zhang"
                    },
                    {
                        "name": "Fang Wang"
                    },
                    {
                        "name": "Sheng Qiu"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Jiaxin Ou"
                    },
                    {
                        "name": "Junxun Huang"
                    },
                    {
                        "name": "Baoquan Li"
                    },
                    {
                        "name": "Peng Fang"
                    },
                    {
                        "name": "Dan Feng"
                    }
                ],
                "author_detail": {
                    "name": "Dan Feng"
                },
                "author": "Dan Feng",
                "arxiv_doi": "10.1109/ICDE60146.2024.00312",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ICDE60146.2024.00312",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.13909v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13909v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14 pages, accepted by 2024 IEEE 40st International Conference on Data\n  Engineering (ICDE)",
                "arxiv_journal_ref": "Year: 2024, Pages: 4072-4085",
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13863v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13863v1",
                "updated": "2025-08-19T14:30:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    14,
                    30,
                    41,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T14:30:41Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    14,
                    30,
                    41,
                    1,
                    231,
                    0
                ],
                "title": "Tight Inter-Core Cache Contention Analysis for WCET Estimation on\n  Multicore Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tight Inter-Core Cache Contention Analysis for WCET Estimation on\n  Multicore Systems"
                },
                "summary": "WCET (Worst-Case Execution Time) estimation on multicore architecture is\nparticularly challenging mainly due to the complex accesses over cache shared\nby multiple cores. Existing analysis identifies possible contentions between\nparallel tasks by leveraging the partial order of the tasks or their program\nregions. Unfortunately, they overestimate the number of cache misses caused by\na remote block access without considering the actual cache state and the number\nof accesses. This paper reports a new analysis for inter-core cache contention.\nBased on the order of program regions in a task, we first identify memory\nreferences that could be affected if a remote access occurs in a region.\nAfterwards, a fine-grained contention analysis is constructed that computes the\nnumber of cache misses based on the access quantity of local and remote blocks.\nWe demonstrate that the overall inter-core cache interference of a task can be\nobtained via dynamic programming. Experiments show that compared to existing\nmethods, the proposed analysis reduces inter-core cache interference and WCET\nestimations by 52.31% and 8.94% on average, without significantly increasing\ncomputation overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WCET (Worst-Case Execution Time) estimation on multicore architecture is\nparticularly challenging mainly due to the complex accesses over cache shared\nby multiple cores. Existing analysis identifies possible contentions between\nparallel tasks by leveraging the partial order of the tasks or their program\nregions. Unfortunately, they overestimate the number of cache misses caused by\na remote block access without considering the actual cache state and the number\nof accesses. This paper reports a new analysis for inter-core cache contention.\nBased on the order of program regions in a task, we first identify memory\nreferences that could be affected if a remote access occurs in a region.\nAfterwards, a fine-grained contention analysis is constructed that computes the\nnumber of cache misses based on the access quantity of local and remote blocks.\nWe demonstrate that the overall inter-core cache interference of a task can be\nobtained via dynamic programming. Experiments show that compared to existing\nmethods, the proposed analysis reduces inter-core cache interference and WCET\nestimations by 52.31% and 8.94% on average, without significantly increasing\ncomputation overhead."
                },
                "authors": [
                    {
                        "name": "Shuai Zhao"
                    },
                    {
                        "name": "Jieyu Jiang"
                    },
                    {
                        "name": "Shenlin Cai"
                    },
                    {
                        "name": "Yaowei Liang"
                    },
                    {
                        "name": "Chen Jie"
                    },
                    {
                        "name": "Yinjie Fang"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Guoquan Zhang"
                    },
                    {
                        "name": "Yaoyao Gu"
                    },
                    {
                        "name": "Xiang Xiao"
                    },
                    {
                        "name": "Wei Qin"
                    },
                    {
                        "name": "Xiangzhen Ouyang"
                    },
                    {
                        "name": "Wanli Chang"
                    }
                ],
                "author_detail": {
                    "name": "Wanli Chang"
                },
                "author": "Wanli Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13863v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13863v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13859v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13859v1",
                "updated": "2025-08-19T14:18:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    14,
                    18,
                    16,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T14:18:16Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    14,
                    18,
                    16,
                    1,
                    231,
                    0
                ],
                "title": "Zobrist Hash-based Duplicate Detection in Symbolic Regression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zobrist Hash-based Duplicate Detection in Symbolic Regression"
                },
                "summary": "Symbolic regression encompasses a family of search algorithms that aim to\ndiscover the best fitting function for a set of data without requiring an a\npriori specification of the model structure. The most successful and commonly\nused technique for symbolic regression is Genetic Programming (GP), an\nevolutionary search method that evolves a population of mathematical\nexpressions through the mechanism of natural selection. In this work we analyze\nthe efficiency of the evolutionary search in GP and show that many points in\nthe search space are re-visited and re-evaluated multiple times by the\nalgorithm, leading to wasted computational effort. We address this issue by\nintroducing a caching mechanism based on the Zobrist hash, a type of hashing\nfrequently used in abstract board games for the efficient construction and\nsubsequent update of transposition tables. We implement our caching approach\nusing the open-source framework Operon and demonstrate its performance on a\nselection of real-world regression problems, where we observe up to 34\\%\nspeedups without any detrimental effects on search quality. The hashing\napproach represents a straightforward way to improve runtime performance while\nalso offering some interesting possibilities for adjusting search strategy\nbased on cached information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symbolic regression encompasses a family of search algorithms that aim to\ndiscover the best fitting function for a set of data without requiring an a\npriori specification of the model structure. The most successful and commonly\nused technique for symbolic regression is Genetic Programming (GP), an\nevolutionary search method that evolves a population of mathematical\nexpressions through the mechanism of natural selection. In this work we analyze\nthe efficiency of the evolutionary search in GP and show that many points in\nthe search space are re-visited and re-evaluated multiple times by the\nalgorithm, leading to wasted computational effort. We address this issue by\nintroducing a caching mechanism based on the Zobrist hash, a type of hashing\nfrequently used in abstract board games for the efficient construction and\nsubsequent update of transposition tables. We implement our caching approach\nusing the open-source framework Operon and demonstrate its performance on a\nselection of real-world regression problems, where we observe up to 34\\%\nspeedups without any detrimental effects on search quality. The hashing\napproach represents a straightforward way to improve runtime performance while\nalso offering some interesting possibilities for adjusting search strategy\nbased on cached information."
                },
                "authors": [
                    {
                        "name": "Bogdan Burlacu"
                    }
                ],
                "author_detail": {
                    "name": "Bogdan Burlacu"
                },
                "author": "Bogdan Burlacu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13859v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13859v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13756v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13756v1",
                "updated": "2025-08-19T11:54:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    11,
                    54,
                    30,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T11:54:30Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    11,
                    54,
                    30,
                    1,
                    231,
                    0
                ],
                "title": "INDS: Incremental Named Data Streaming for Real-Time Point Cloud Video",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "INDS: Incremental Named Data Streaming for Real-Time Point Cloud Video"
                },
                "summary": "Real-time streaming of point cloud video, characterized by massive data\nvolumes and high sensitivity to packet loss, remains a key challenge for\nimmersive applications under dynamic network conditions. While\nconnection-oriented protocols such as TCP and more modern alternatives like\nQUIC alleviate some transport-layer inefficiencies, including head-of-line\nblocking, they still retain a coarse-grained, segment-based delivery model and\na centralized control loop that limit fine-grained adaptation and effective\ncaching. We introduce INDS (Incremental Named Data Streaming), an adaptive\nstreaming framework based on Information-Centric Networking (ICN) that rethinks\ndelivery for hierarchical, layered media. INDS leverages the Octree structure\nof point cloud video and expressive content naming to support progressive,\npartial retrieval of enhancement layers based on consumer bandwidth and\ndecoding capability. By combining time-windows with Group-of-Frames (GoF),\nINDS's naming scheme supports fine-grained in-network caching and facilitates\nefficient multi-user data reuse. INDS can be deployed as an overlay, remaining\ncompatible with QUIC-based transport infrastructure as well as future\nMedia-over-QUIC (MoQ) architectures, without requiring changes to underlying IP\nnetworks. Our prototype implementation shows up to 80% lower delay, 15-50%\nhigher throughput, and 20-30% increased cache hit rates compared to\nstate-of-the-art DASH-style systems. Together, these results establish INDS as\na scalable, cache-friendly solution for real-time point cloud streaming under\nvariable and lossy conditions, while its compatibility with MoQ overlays\nfurther positions it as a practical, forward-compatible architecture for\nemerging immersive media systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time streaming of point cloud video, characterized by massive data\nvolumes and high sensitivity to packet loss, remains a key challenge for\nimmersive applications under dynamic network conditions. While\nconnection-oriented protocols such as TCP and more modern alternatives like\nQUIC alleviate some transport-layer inefficiencies, including head-of-line\nblocking, they still retain a coarse-grained, segment-based delivery model and\na centralized control loop that limit fine-grained adaptation and effective\ncaching. We introduce INDS (Incremental Named Data Streaming), an adaptive\nstreaming framework based on Information-Centric Networking (ICN) that rethinks\ndelivery for hierarchical, layered media. INDS leverages the Octree structure\nof point cloud video and expressive content naming to support progressive,\npartial retrieval of enhancement layers based on consumer bandwidth and\ndecoding capability. By combining time-windows with Group-of-Frames (GoF),\nINDS's naming scheme supports fine-grained in-network caching and facilitates\nefficient multi-user data reuse. INDS can be deployed as an overlay, remaining\ncompatible with QUIC-based transport infrastructure as well as future\nMedia-over-QUIC (MoQ) architectures, without requiring changes to underlying IP\nnetworks. Our prototype implementation shows up to 80% lower delay, 15-50%\nhigher throughput, and 20-30% increased cache hit rates compared to\nstate-of-the-art DASH-style systems. Together, these results establish INDS as\na scalable, cache-friendly solution for real-time point cloud streaming under\nvariable and lossy conditions, while its compatibility with MoQ overlays\nfurther positions it as a practical, forward-compatible architecture for\nemerging immersive media systems."
                },
                "authors": [
                    {
                        "name": "Ruonan Chai"
                    },
                    {
                        "name": "Yixiang Zhu"
                    },
                    {
                        "name": "Xinjiao Li"
                    },
                    {
                        "name": "Jiawei Li"
                    },
                    {
                        "name": "Zili Meng"
                    },
                    {
                        "name": "Dirk Kutscher"
                    }
                ],
                "author_detail": {
                    "name": "Dirk Kutscher"
                },
                "author": "Dirk Kutscher",
                "arxiv_comment": "9 pages, 9 figures, 2 tables. To appear in Proc. of the 33rd ACM\n  International Conference on Multimedia (MM '25), October 27--31, 2025,\n  Dublin, Ireland",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13756v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13756v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.2.1; C.2.4; H.5.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13716v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13716v1",
                "updated": "2025-08-19T10:21:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    10,
                    21,
                    33,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T10:21:33Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    10,
                    21,
                    33,
                    1,
                    231,
                    0
                ],
                "title": "CaPGNN: Optimizing Parallel Graph Neural Network Training with Joint\n  Caching and Resource-Aware Graph Partitioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CaPGNN: Optimizing Parallel Graph Neural Network Training with Joint\n  Caching and Resource-Aware Graph Partitioning"
                },
                "summary": "Graph Neural Networks (GNNs) have shown remarkable capabilities in processing\ngraph-structured data prevalent in various real-world applications. However,\nthe scalability of full-batch GNN training becomes severely limited by high\ncommunication overhead and load imbalance in distributed environments. In this\npaper, we present CaPGNN, a novel framework for efficient parallel full-batch\nGNN training on single-server with multi-GPU, designed specifically to reduce\nredundant inter-GPU communication and balance computational workloads. We\npropose a joint adaptive caching algorithm that leverages both CPU and GPU\nmemory to significantly reduce the repetitive transmission of vertex features\nacross partitions. Additionally, we introduce a resource-aware graph\npartitioning algorithm that adjusts subgraph sizes dynamically according to the\nheterogeneous computational and communication capacities of GPUs. Extensive\nexperiments on large-scale benchmark datasets demonstrate that CaPGNN\neffectively reduces communication costs by up to 96% and accelerates GNN\ntraining by up to 12.7 times compared to state-of-the-art approaches. Our\nresults highlight the potential of adaptive caching and resource-aware\npartitioning to facilitate scalable, efficient, and practical deployment of\nfull-batch GNN training in distributed computing environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) have shown remarkable capabilities in processing\ngraph-structured data prevalent in various real-world applications. However,\nthe scalability of full-batch GNN training becomes severely limited by high\ncommunication overhead and load imbalance in distributed environments. In this\npaper, we present CaPGNN, a novel framework for efficient parallel full-batch\nGNN training on single-server with multi-GPU, designed specifically to reduce\nredundant inter-GPU communication and balance computational workloads. We\npropose a joint adaptive caching algorithm that leverages both CPU and GPU\nmemory to significantly reduce the repetitive transmission of vertex features\nacross partitions. Additionally, we introduce a resource-aware graph\npartitioning algorithm that adjusts subgraph sizes dynamically according to the\nheterogeneous computational and communication capacities of GPUs. Extensive\nexperiments on large-scale benchmark datasets demonstrate that CaPGNN\neffectively reduces communication costs by up to 96% and accelerates GNN\ntraining by up to 12.7 times compared to state-of-the-art approaches. Our\nresults highlight the potential of adaptive caching and resource-aware\npartitioning to facilitate scalable, efficient, and practical deployment of\nfull-batch GNN training in distributed computing environments."
                },
                "authors": [
                    {
                        "name": "Xianfeng Song"
                    },
                    {
                        "name": "Yi Zou"
                    },
                    {
                        "name": "Zheng Shi"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Shi"
                },
                "author": "Zheng Shi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13716v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13716v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23387v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23387v3",
                "updated": "2025-08-19T09:13:13Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    9,
                    13,
                    13,
                    1,
                    231,
                    0
                ],
                "published": "2025-07-31T10:02:26Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    10,
                    2,
                    26,
                    3,
                    212,
                    0
                ],
                "title": "SGEMM-cube: Emulating FP32 GEMM on Ascend NPUs Using FP16 Cube Units\n  with Precision Recovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SGEMM-cube: Emulating FP32 GEMM on Ascend NPUs Using FP16 Cube Units\n  with Precision Recovery"
                },
                "summary": "Low-precision matrix engines, such as FP16 cube, offer high throughput but\nlack support for full-precision computation. In this work, we propose\nSGEMM-cube, a high-performance algorithm for emulating FP32 general\nmatrix-matrix multiplication (GEMM) using only FP16 computation units on a\nrepresentative AI accelerator. The method decomposes each FP32 operand into two\nFP16 values and compensates for numerical errors through a tunable scaling\nstrategy. A detailed analysis of numerical errors, including underflow\nconditions and precision loss, guides the selection of scaling parameters to\npreserve up to 22 bits of mantissa accuracy. We further investigate the effect\nof computation order on accuracy and demonstrate that a term-wise accumulation\nscheme improves numerical stability over conventional FP32 GEMM in low-exponent\nregimes. Finally, a cache-aware blocking strategy and double-buffered pipeline\nare introduced to overlap memory transfers with computation, enabling\nSGEMM-cube to achieve up to 77\\% of the theoretical FP32-equivalent peak\nperformance on Ascend 910A NPU lacking native FP32 support. Extensive numerical\nexperiments confirm that our method not only recovers the accuracy of native\nFP32 GEMM but also exhibits superior numerical stability under certain\nconditions, due to its structured and error-aware computation order.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-precision matrix engines, such as FP16 cube, offer high throughput but\nlack support for full-precision computation. In this work, we propose\nSGEMM-cube, a high-performance algorithm for emulating FP32 general\nmatrix-matrix multiplication (GEMM) using only FP16 computation units on a\nrepresentative AI accelerator. The method decomposes each FP32 operand into two\nFP16 values and compensates for numerical errors through a tunable scaling\nstrategy. A detailed analysis of numerical errors, including underflow\nconditions and precision loss, guides the selection of scaling parameters to\npreserve up to 22 bits of mantissa accuracy. We further investigate the effect\nof computation order on accuracy and demonstrate that a term-wise accumulation\nscheme improves numerical stability over conventional FP32 GEMM in low-exponent\nregimes. Finally, a cache-aware blocking strategy and double-buffered pipeline\nare introduced to overlap memory transfers with computation, enabling\nSGEMM-cube to achieve up to 77\\% of the theoretical FP32-equivalent peak\nperformance on Ascend 910A NPU lacking native FP32 support. Extensive numerical\nexperiments confirm that our method not only recovers the accuracy of native\nFP32 GEMM but also exhibits superior numerical stability under certain\nconditions, due to its structured and error-aware computation order."
                },
                "authors": [
                    {
                        "name": "Weicheng Xue"
                    },
                    {
                        "name": "Baisong Xu"
                    },
                    {
                        "name": "Kai Yang"
                    },
                    {
                        "name": "Yongxiang Liu"
                    },
                    {
                        "name": "Dengdeng Fan"
                    },
                    {
                        "name": "Pengxiang Xu"
                    },
                    {
                        "name": "Yonghong Tian"
                    }
                ],
                "author_detail": {
                    "name": "Yonghong Tian"
                },
                "author": "Yonghong Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23387v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23387v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13523v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13523v1",
                "updated": "2025-08-19T05:27:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    5,
                    27,
                    53,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T05:27:53Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    5,
                    27,
                    53,
                    1,
                    231,
                    0
                ],
                "title": "LAMMPS-KOKKOS: Performance Portable Molecular Dynamics Across Exascale\n  Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LAMMPS-KOKKOS: Performance Portable Molecular Dynamics Across Exascale\n  Architectures"
                },
                "summary": "Since its inception in 1995, LAMMPS has grown to be a world-class molecular\ndynamics code, with thousands of users, over one million lines of code, and\nmulti-scale simulation capabilities. We discuss how LAMMPS has adapted to the\nmodern heterogeneous computing landscape by integrating the Kokkos performance\nportability library into the existing C++ code. We investigate performance\nportability of simple pairwise, many-body reactive, and machine-learned\nforce-field interatomic potentials. We present results on GPUs across different\nvendors and generations, and analyze performance trends, probing FLOPS\nthroughput, memory bandwidths, cache capabilities, and thread-atomic operation\nperformance. Finally, we demonstrate strong scaling on all current US exascale\nmachines -- OLCF Frontier, and ALCF Aurora, and NNSA El Capitan -- for the\nthree potentials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since its inception in 1995, LAMMPS has grown to be a world-class molecular\ndynamics code, with thousands of users, over one million lines of code, and\nmulti-scale simulation capabilities. We discuss how LAMMPS has adapted to the\nmodern heterogeneous computing landscape by integrating the Kokkos performance\nportability library into the existing C++ code. We investigate performance\nportability of simple pairwise, many-body reactive, and machine-learned\nforce-field interatomic potentials. We present results on GPUs across different\nvendors and generations, and analyze performance trends, probing FLOPS\nthroughput, memory bandwidths, cache capabilities, and thread-atomic operation\nperformance. Finally, we demonstrate strong scaling on all current US exascale\nmachines -- OLCF Frontier, and ALCF Aurora, and NNSA El Capitan -- for the\nthree potentials."
                },
                "authors": [
                    {
                        "name": "Anders Johansson"
                    },
                    {
                        "name": "Evan Weinberg"
                    },
                    {
                        "name": "Christian R. Trott"
                    },
                    {
                        "name": "Megan J. McCarthy"
                    },
                    {
                        "name": "Stan G. Moore"
                    }
                ],
                "author_detail": {
                    "name": "Stan G. Moore"
                },
                "author": "Stan G. Moore",
                "arxiv_comment": "14 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13523v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.1.4; C.2.4; C.4; D.1.3; D.3.4; E.1; I.6; I.6.8; J.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08422v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08422v2",
                "updated": "2025-08-19T03:13:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    3,
                    13,
                    39,
                    1,
                    231,
                    0
                ],
                "published": "2025-07-11T09:07:43Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    7,
                    43,
                    4,
                    192,
                    0
                ],
                "title": "Upsample What Matters: Region-Adaptive Latent Sampling for Accelerated\n  Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Upsample What Matters: Region-Adaptive Latent Sampling for Accelerated\n  Diffusion Transformers"
                },
                "summary": "Diffusion transformers have emerged as an alternative to U-net-based\ndiffusion models for high-fidelity image and video generation, offering\nsuperior scalability. However, their heavy computation remains a major obstacle\nto real-world deployment. Existing acceleration methods primarily exploit the\ntemporal dimension such as reusing cached features across diffusion timesteps.\nHere, we propose Region-Adaptive Latent Upsampling (RALU), a training-free\nframework that accelerates inference along spatial dimension. RALU performs\nmixed-resolution sampling across three stages: 1) low-resolution denoising\nlatent diffusion to efficiently capture global semantic structure, 2)\nregion-adaptive upsampling on specific regions prone to artifacts at\nfull-resolution, and 3) all latent upsampling at full-resolution for detail\nrefinement. To stabilize generations across resolution transitions, we leverage\nnoise-timestep rescheduling to adapt the noise level across varying\nresolutions. Our method significantly reduces computation while preserving\nimage quality by achieving up to 7.0$\\times$ speed-up on FLUX and 3.0$\\times$\non Stable Diffusion 3 with minimal degradation. Furthermore, RALU is\ncomplementary to existing temporal accelerations such as caching methods, thus\ncan be seamlessly integrated to further reduce inference latency without\ncompromising generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have emerged as an alternative to U-net-based\ndiffusion models for high-fidelity image and video generation, offering\nsuperior scalability. However, their heavy computation remains a major obstacle\nto real-world deployment. Existing acceleration methods primarily exploit the\ntemporal dimension such as reusing cached features across diffusion timesteps.\nHere, we propose Region-Adaptive Latent Upsampling (RALU), a training-free\nframework that accelerates inference along spatial dimension. RALU performs\nmixed-resolution sampling across three stages: 1) low-resolution denoising\nlatent diffusion to efficiently capture global semantic structure, 2)\nregion-adaptive upsampling on specific regions prone to artifacts at\nfull-resolution, and 3) all latent upsampling at full-resolution for detail\nrefinement. To stabilize generations across resolution transitions, we leverage\nnoise-timestep rescheduling to adapt the noise level across varying\nresolutions. Our method significantly reduces computation while preserving\nimage quality by achieving up to 7.0$\\times$ speed-up on FLUX and 3.0$\\times$\non Stable Diffusion 3 with minimal degradation. Furthermore, RALU is\ncomplementary to existing temporal accelerations such as caching methods, thus\ncan be seamlessly integrated to further reduce inference latency without\ncompromising generation quality."
                },
                "authors": [
                    {
                        "name": "Wongi Jeong"
                    },
                    {
                        "name": "Kyungryeol Lee"
                    },
                    {
                        "name": "Hoigi Seo"
                    },
                    {
                        "name": "Se Young Chun"
                    }
                ],
                "author_detail": {
                    "name": "Se Young Chun"
                },
                "author": "Se Young Chun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08422v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08422v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17995v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17995v2",
                "updated": "2025-08-19T01:38:23Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    1,
                    38,
                    23,
                    1,
                    231,
                    0
                ],
                "published": "2025-04-25T00:41:43Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    0,
                    41,
                    43,
                    4,
                    115,
                    0
                ],
                "title": "Role of On-site and Inter-site Coulomb Interactions in KV$_3$Sb$_5$: A\n  first-principles DFT+$U$+$V$ study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Role of On-site and Inter-site Coulomb Interactions in KV$_3$Sb$_5$: A\n  first-principles DFT+$U$+$V$ study"
                },
                "summary": "Nonlocal Coulomb interactions play a crucial role in stabilizing distinct\nelectronic phases in kagome materials. In this work, we systematically\ninvestigate the effects of on-site ($U$) and inter-site ($V$) Coulomb\ninteractions on the electronic structure and stability of charge-density-wave\n(CDW) phases in the kagome metal KV$_3$Sb$_5$ using density functional theory\n(DFT+$U$+$V$) calculations. We demonstrate that $V$ promotes the formation and\nstability of CDW phases, whereas $U$ suppresses these phases, highlighting a\nfundamental competition between local and nonlocal Coulomb interactions. By\ndirectly comparing our theoretical results with angle-resolved photoemission\nspectroscopy (ARPES) data, we identify realistic values of $U$ and $V$ that\naccurately describe the electronic band structure of KV$_3$Sb$_5$. Our findings\nestablish a detailed $U$-$V$ phase diagram for KV$_3$Sb$_5$, offering valuable\ninsights into the correlated electronic states in kagome metals and serving as\na foundation for future explorations of correlation-driven phenomena in related\nmaterials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nonlocal Coulomb interactions play a crucial role in stabilizing distinct\nelectronic phases in kagome materials. In this work, we systematically\ninvestigate the effects of on-site ($U$) and inter-site ($V$) Coulomb\ninteractions on the electronic structure and stability of charge-density-wave\n(CDW) phases in the kagome metal KV$_3$Sb$_5$ using density functional theory\n(DFT+$U$+$V$) calculations. We demonstrate that $V$ promotes the formation and\nstability of CDW phases, whereas $U$ suppresses these phases, highlighting a\nfundamental competition between local and nonlocal Coulomb interactions. By\ndirectly comparing our theoretical results with angle-resolved photoemission\nspectroscopy (ARPES) data, we identify realistic values of $U$ and $V$ that\naccurately describe the electronic band structure of KV$_3$Sb$_5$. Our findings\nestablish a detailed $U$-$V$ phase diagram for KV$_3$Sb$_5$, offering valuable\ninsights into the correlated electronic states in kagome metals and serving as\na foundation for future explorations of correlation-driven phenomena in related\nmaterials."
                },
                "authors": [
                    {
                        "name": "Indukuru Ramesh Reddy"
                    },
                    {
                        "name": "Sayandeep Ghosh"
                    },
                    {
                        "name": "Bongjae Kim"
                    },
                    {
                        "name": "Chang-Jong Kang"
                    }
                ],
                "author_detail": {
                    "name": "Chang-Jong Kang"
                },
                "author": "Chang-Jong Kang",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17995v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17995v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.str-el",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13382v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13382v1",
                "updated": "2025-08-18T21:58:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    18,
                    21,
                    58,
                    18,
                    0,
                    230,
                    0
                ],
                "published": "2025-08-18T21:58:18Z",
                "published_parsed": [
                    2025,
                    8,
                    18,
                    21,
                    58,
                    18,
                    0,
                    230,
                    0
                ],
                "title": "Datarus-R1: An Adaptive Multi-Step Reasoning LLM for Automated Data\n  Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Datarus-R1: An Adaptive Multi-Step Reasoning LLM for Automated Data\n  Analysis"
                },
                "summary": "We present Datarus-R1-14B, a 14 B-parameter open-weights language model\nfine-tuned from Qwen 2.5-14B-Instruct to act as a virtual data analyst and\ngraduate-level problem solver. Datarus is trained not on isolated\nquestion-answer pairs but on full analytical trajectories including reasoning\nsteps, code execution, error traces, self-corrections, and final conclusions,\nall captured in a ReAct-style notebook format spanning finance, medicine,\nnumerical analysis, and other quantitative domains. Our training pipeline\ncombines (i) a trajectory-centric synthetic data generator that yielded 144 000\ntagged notebook episodes, (ii) a dual-reward framework blending a lightweight\ntag-based structural signal with a Hierarchical Reward Model (HRM) that scores\nboth single-step soundness and end-to-end coherence, and (iii) a\nmemory-optimized implementation of Group Relative Policy Optimization (GRPO)\nfeaturing KV-cache reuse, sequential generation, and reference-model sharding.\nA cosine curriculum smoothly shifts emphasis from structural fidelity to\nsemantic depth, reducing the format collapse and verbosity that often plague\nRL-aligned LLMs. A central design choice in Datarus is it dual reasoning\ninterface. In agentic mode the model produces ReAct-tagged steps that invoke\nPython tools to execute real code; in reflection mode it outputs compact\nChain-of-Thought (CoT) traces delimited by <think> and <answer> tags. On\ndemanding postgraduate-level problems, Datarus exhibits an \"AHA-moment\"\npattern: it sketches hypotheses, revises them once or twice, and converges\navoiding the circular, token-inflating loops common to contemporary systems.\nAcross standard public benchmarks Datarus surpasses similar size models and\neven reaches the level of larger reasoning models such as QwQ-32B achieving up\nto 30% higher accuracy on AIME 2024/2025 and LiveCodeBench while emitting\n18-49% fewer tokens per solution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Datarus-R1-14B, a 14 B-parameter open-weights language model\nfine-tuned from Qwen 2.5-14B-Instruct to act as a virtual data analyst and\ngraduate-level problem solver. Datarus is trained not on isolated\nquestion-answer pairs but on full analytical trajectories including reasoning\nsteps, code execution, error traces, self-corrections, and final conclusions,\nall captured in a ReAct-style notebook format spanning finance, medicine,\nnumerical analysis, and other quantitative domains. Our training pipeline\ncombines (i) a trajectory-centric synthetic data generator that yielded 144 000\ntagged notebook episodes, (ii) a dual-reward framework blending a lightweight\ntag-based structural signal with a Hierarchical Reward Model (HRM) that scores\nboth single-step soundness and end-to-end coherence, and (iii) a\nmemory-optimized implementation of Group Relative Policy Optimization (GRPO)\nfeaturing KV-cache reuse, sequential generation, and reference-model sharding.\nA cosine curriculum smoothly shifts emphasis from structural fidelity to\nsemantic depth, reducing the format collapse and verbosity that often plague\nRL-aligned LLMs. A central design choice in Datarus is it dual reasoning\ninterface. In agentic mode the model produces ReAct-tagged steps that invoke\nPython tools to execute real code; in reflection mode it outputs compact\nChain-of-Thought (CoT) traces delimited by <think> and <answer> tags. On\ndemanding postgraduate-level problems, Datarus exhibits an \"AHA-moment\"\npattern: it sketches hypotheses, revises them once or twice, and converges\navoiding the circular, token-inflating loops common to contemporary systems.\nAcross standard public benchmarks Datarus surpasses similar size models and\neven reaches the level of larger reasoning models such as QwQ-32B achieving up\nto 30% higher accuracy on AIME 2024/2025 and LiveCodeBench while emitting\n18-49% fewer tokens per solution."
                },
                "authors": [
                    {
                        "name": "Ayoub Ben Chaliah"
                    },
                    {
                        "name": "Hela Dellagi"
                    }
                ],
                "author_detail": {
                    "name": "Hela Dellagi"
                },
                "author": "Hela Dellagi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13382v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13382v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24584v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24584v3",
                "updated": "2025-08-18T16:52:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    18,
                    16,
                    52,
                    22,
                    0,
                    230,
                    0
                ],
                "published": "2025-05-30T13:32:00Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    13,
                    32,
                    0,
                    4,
                    150,
                    0
                ],
                "title": "AutoChemSchematic AI: Agentic Physics-Aware Automation for Chemical\n  Manufacturing Scale-Up",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoChemSchematic AI: Agentic Physics-Aware Automation for Chemical\n  Manufacturing Scale-Up"
                },
                "summary": "Recent advances in generative AI have accelerated the discovery of novel\nchemicals and materials. However, scaling these discoveries to industrial\nproduction remains a major bottleneck due to the synthesis gap -- the need to\ndevelop entirely new manufacturing processes. This challenge requires detailed\nengineering blueprints: PFDs for equipment layouts and material/energy flows,\nand PIDs for process plant operations. Current AI systems cannot yet reliably\ngenerate these critical engineering schematics, creating a fundamental obstacle\nto manufacturing scale-up of novel discoveries. We present a closed-loop,\nphysics-aware framework for automated generation of industrially viable PFDs\nand PIDs. The framework integrates three key components: (1) domain-specialized\nsmall language models (SLMs) trained for auto-generation of PFDs and PIDs, (2)\na hierarchical knowledge graph containing process flow and instrumentation\ndescriptions for 1,020+ chemicals for Graph Retrieval-Augmented Generation\n(GRAG), and (3) an open-source chemical process simulator for modeling,\nsimulation, optimization, and analysis of novel chemical processes. The SLMs\nare trained through a multi-stage pipeline on synthetic datasets, with process\nsimulator-in-the-loop validation ensuring feasibility. To enhance computational\nefficiency, the framework implements structural pruning (width and depth)\nguided by importance heuristics to reduce language model size while preserving\naccuracy, followed by advanced inference optimizations including\nFlashAttention, Lookahead Decoding, PagedAttention with KV-cache quantization,\nand Test-Time Inference Scaling. Experimental results demonstrate that our\nframework generates simulator-validated process descriptions with high\nfidelity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in generative AI have accelerated the discovery of novel\nchemicals and materials. However, scaling these discoveries to industrial\nproduction remains a major bottleneck due to the synthesis gap -- the need to\ndevelop entirely new manufacturing processes. This challenge requires detailed\nengineering blueprints: PFDs for equipment layouts and material/energy flows,\nand PIDs for process plant operations. Current AI systems cannot yet reliably\ngenerate these critical engineering schematics, creating a fundamental obstacle\nto manufacturing scale-up of novel discoveries. We present a closed-loop,\nphysics-aware framework for automated generation of industrially viable PFDs\nand PIDs. The framework integrates three key components: (1) domain-specialized\nsmall language models (SLMs) trained for auto-generation of PFDs and PIDs, (2)\na hierarchical knowledge graph containing process flow and instrumentation\ndescriptions for 1,020+ chemicals for Graph Retrieval-Augmented Generation\n(GRAG), and (3) an open-source chemical process simulator for modeling,\nsimulation, optimization, and analysis of novel chemical processes. The SLMs\nare trained through a multi-stage pipeline on synthetic datasets, with process\nsimulator-in-the-loop validation ensuring feasibility. To enhance computational\nefficiency, the framework implements structural pruning (width and depth)\nguided by importance heuristics to reduce language model size while preserving\naccuracy, followed by advanced inference optimizations including\nFlashAttention, Lookahead Decoding, PagedAttention with KV-cache quantization,\nand Test-Time Inference Scaling. Experimental results demonstrate that our\nframework generates simulator-validated process descriptions with high\nfidelity."
                },
                "authors": [
                    {
                        "name": "Sakhinana Sagar Srinivas"
                    },
                    {
                        "name": "Shivam Gupta"
                    },
                    {
                        "name": "Venkataramana Runkana"
                    }
                ],
                "author_detail": {
                    "name": "Venkataramana Runkana"
                },
                "author": "Venkataramana Runkana",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24584v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24584v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04823v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04823v2",
                "updated": "2025-08-18T16:06:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    18,
                    16,
                    6,
                    9,
                    0,
                    230,
                    0
                ],
                "published": "2025-04-07T08:22:45Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    8,
                    22,
                    45,
                    0,
                    97,
                    0
                ],
                "title": "Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning\n  Models"
                },
                "summary": "Recent advancements in reasoning language models have demonstrated remarkable\nperformance in complex tasks, but their extended chain-of-thought reasoning\nprocess increases inference overhead. While quantization has been widely\nadopted to reduce the inference cost of large language models, its impact on\nreasoning models remains understudied. In this paper, we conduct the first\nsystematic study on quantized reasoning models, evaluating the open-sourced\nDeepSeek-R1-Distilled Qwen and LLaMA families ranging from 1.5B to 70B\nparameters, QwQ-32B, and Qwen3-8B. Our investigation covers weight, KV cache,\nand activation quantization using state-of-the-art algorithms at varying\nbit-widths, with extensive evaluation across mathematical (AIME, MATH-500),\nscientific (GPQA), and programming (LiveCodeBench) reasoning benchmarks. Our\nfindings reveal that while lossless quantization can be achieved with W8A8 or\nW4A16 quantization, lower bit-widths introduce significant accuracy risks. We\nfurther identify model size, model origin, and task difficulty as critical\ndeterminants of performance. Contrary to expectations, quantized models do not\nexhibit increased output lengths. In addition, strategically scaling the model\nsizes or reasoning steps can effectively enhance the performance. All quantized\nmodels and codes are open-sourced in\nhttps://github.com/ruikangliu/Quantized-Reasoning-Models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in reasoning language models have demonstrated remarkable\nperformance in complex tasks, but their extended chain-of-thought reasoning\nprocess increases inference overhead. While quantization has been widely\nadopted to reduce the inference cost of large language models, its impact on\nreasoning models remains understudied. In this paper, we conduct the first\nsystematic study on quantized reasoning models, evaluating the open-sourced\nDeepSeek-R1-Distilled Qwen and LLaMA families ranging from 1.5B to 70B\nparameters, QwQ-32B, and Qwen3-8B. Our investigation covers weight, KV cache,\nand activation quantization using state-of-the-art algorithms at varying\nbit-widths, with extensive evaluation across mathematical (AIME, MATH-500),\nscientific (GPQA), and programming (LiveCodeBench) reasoning benchmarks. Our\nfindings reveal that while lossless quantization can be achieved with W8A8 or\nW4A16 quantization, lower bit-widths introduce significant accuracy risks. We\nfurther identify model size, model origin, and task difficulty as critical\ndeterminants of performance. Contrary to expectations, quantized models do not\nexhibit increased output lengths. In addition, strategically scaling the model\nsizes or reasoning steps can effectively enhance the performance. All quantized\nmodels and codes are open-sourced in\nhttps://github.com/ruikangliu/Quantized-Reasoning-Models."
                },
                "authors": [
                    {
                        "name": "Ruikang Liu"
                    },
                    {
                        "name": "Yuxuan Sun"
                    },
                    {
                        "name": "Manyi Zhang"
                    },
                    {
                        "name": "Haoli Bai"
                    },
                    {
                        "name": "Xianzhi Yu"
                    },
                    {
                        "name": "Tiezheng Yu"
                    },
                    {
                        "name": "Chun Yuan"
                    },
                    {
                        "name": "Lu Hou"
                    }
                ],
                "author_detail": {
                    "name": "Lu Hou"
                },
                "author": "Lu Hou",
                "arxiv_comment": "COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04823v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04823v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.12767v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.12767v1",
                "updated": "2025-08-18T09:41:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    18,
                    9,
                    41,
                    28,
                    0,
                    230,
                    0
                ],
                "published": "2025-08-18T09:41:28Z",
                "published_parsed": [
                    2025,
                    8,
                    18,
                    9,
                    41,
                    28,
                    0,
                    230,
                    0
                ],
                "title": "Some optimization possibilities in data plane programming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Some optimization possibilities in data plane programming"
                },
                "summary": "Software-defined networking (SDN) technology aims to create a highly flexible\nnetwork by decoupling control plane and the data plane and programming them\nindependently. There has been a lot of research on improving and optimizing the\ncontrol plane, and data plane programming is a relatively new concept, so study\non it is one of the hot topics for researchers. At the 2019 Dagstuhl Seminar,\nwell-known scientists on computer networking discussed challenges and problems\nin the field of data plane programming that need to be addressed over the next\n10 years. Based on this seminar issues and papers review, we suggested some\npossible solutions which are for optimizing data plane to improve packet\nprocessing performance and link utilization. The suggestions include (i)\nenriching data plane language with asynchronous external function, (ii)\ncompression based on payload size, (iii) in-network caching for fast packet\nprocessing, and (iv) offloading external functions to an additional thread,\nvirtual machine (VM) or server, etc. In addition, we implemented some of these\nin the P4 data plane language to illustrate the practicality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software-defined networking (SDN) technology aims to create a highly flexible\nnetwork by decoupling control plane and the data plane and programming them\nindependently. There has been a lot of research on improving and optimizing the\ncontrol plane, and data plane programming is a relatively new concept, so study\non it is one of the hot topics for researchers. At the 2019 Dagstuhl Seminar,\nwell-known scientists on computer networking discussed challenges and problems\nin the field of data plane programming that need to be addressed over the next\n10 years. Based on this seminar issues and papers review, we suggested some\npossible solutions which are for optimizing data plane to improve packet\nprocessing performance and link utilization. The suggestions include (i)\nenriching data plane language with asynchronous external function, (ii)\ncompression based on payload size, (iii) in-network caching for fast packet\nprocessing, and (iv) offloading external functions to an additional thread,\nvirtual machine (VM) or server, etc. In addition, we implemented some of these\nin the P4 data plane language to illustrate the practicality."
                },
                "authors": [
                    {
                        "name": "Altangerel Gereltsetseg"
                    },
                    {
                        "name": "Tejfel Máté"
                    }
                ],
                "author_detail": {
                    "name": "Tejfel Máté"
                },
                "author": "Tejfel Máté",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.12767v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.12767v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.12743v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.12743v1",
                "updated": "2025-08-18T09:06:49Z",
                "updated_parsed": [
                    2025,
                    8,
                    18,
                    9,
                    6,
                    49,
                    0,
                    230,
                    0
                ],
                "published": "2025-08-18T09:06:49Z",
                "published_parsed": [
                    2025,
                    8,
                    18,
                    9,
                    6,
                    49,
                    0,
                    230,
                    0
                ],
                "title": "Dissecting CPU-GPU Unified Physical Memory on AMD MI300A APUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dissecting CPU-GPU Unified Physical Memory on AMD MI300A APUs"
                },
                "summary": "Discrete GPUs are a cornerstone of HPC and data center systems, requiring\nmanagement of separate CPU and GPU memory spaces. Unified Virtual Memory (UVM)\nhas been proposed to ease the burden of memory management; however, at a high\ncost in performance. The recent introduction of AMD's MI300A Accelerated\nProcessing Units (APUs)--as deployed in the El Capitan supercomputer--enables\nHPC systems featuring integrated CPU and GPU with Unified Physical Memory (UPM)\nfor the first time. This work presents the first comprehensive characterization\nof the UPM architecture on MI300A. We first analyze the UPM system properties,\nincluding memory latency, bandwidth, and coherence overhead. We then assess the\nefficiency of the system software in memory allocation, page fault handling,\nTLB management, and Infinity Cache utilization. We propose a set of porting\nstrategies for transforming applications for the UPM architecture and evaluate\nsix applications on the MI300A APU. Our results show that applications on UPM\nusing the unified memory model can match or outperform those in the explicitly\nmanaged model--while reducing memory costs by up to 44%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discrete GPUs are a cornerstone of HPC and data center systems, requiring\nmanagement of separate CPU and GPU memory spaces. Unified Virtual Memory (UVM)\nhas been proposed to ease the burden of memory management; however, at a high\ncost in performance. The recent introduction of AMD's MI300A Accelerated\nProcessing Units (APUs)--as deployed in the El Capitan supercomputer--enables\nHPC systems featuring integrated CPU and GPU with Unified Physical Memory (UPM)\nfor the first time. This work presents the first comprehensive characterization\nof the UPM architecture on MI300A. We first analyze the UPM system properties,\nincluding memory latency, bandwidth, and coherence overhead. We then assess the\nefficiency of the system software in memory allocation, page fault handling,\nTLB management, and Infinity Cache utilization. We propose a set of porting\nstrategies for transforming applications for the UPM architecture and evaluate\nsix applications on the MI300A APU. Our results show that applications on UPM\nusing the unified memory model can match or outperform those in the explicitly\nmanaged model--while reducing memory costs by up to 44%."
                },
                "authors": [
                    {
                        "name": "Jacob Wahlgren"
                    },
                    {
                        "name": "Gabin Schieffer"
                    },
                    {
                        "name": "Ruimin Shi"
                    },
                    {
                        "name": "Edgar A. León"
                    },
                    {
                        "name": "Roger Pearce"
                    },
                    {
                        "name": "Maya Gokhale"
                    },
                    {
                        "name": "Ivy Peng"
                    }
                ],
                "author_detail": {
                    "name": "Ivy Peng"
                },
                "author": "Ivy Peng",
                "arxiv_comment": "To be published in IISWC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.12743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.12743v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.12691v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.12691v1",
                "updated": "2025-08-18T07:49:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    18,
                    7,
                    49,
                    33,
                    0,
                    230,
                    0
                ],
                "published": "2025-08-18T07:49:33Z",
                "published_parsed": [
                    2025,
                    8,
                    18,
                    7,
                    49,
                    33,
                    0,
                    230,
                    0
                ],
                "title": "MixCache: Mixture-of-Cache for Video Diffusion Transformer Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MixCache: Mixture-of-Cache for Video Diffusion Transformer Acceleration"
                },
                "summary": "Leveraging the Transformer architecture and the diffusion process, video DiT\nmodels have emerged as a dominant approach for high-quality video generation.\nHowever, their multi-step iterative denoising process incurs high computational\ncost and inference latency. Caching, a widely adopted optimization method in\nDiT models, leverages the redundancy in the diffusion process to skip\ncomputations in different granularities (e.g., step, cfg, block). Nevertheless,\nexisting caching methods are limited to single-granularity strategies,\nstruggling to balance generation quality and inference speed in a flexible\nmanner. In this work, we propose MixCache, a training-free caching-based\nframework for efficient video DiT inference. It first distinguishes the\ninterference and boundary between different caching strategies, and then\nintroduces a context-aware cache triggering strategy to determine when caching\nshould be enabled, along with an adaptive hybrid cache decision strategy for\ndynamically selecting the optimal caching granularity. Extensive experiments on\ndiverse models demonstrate that, MixCache can significantly accelerate video\ngeneration (e.g., 1.94$\\times$ speedup on Wan 14B, 1.97$\\times$ speedup on\nHunyuanVideo) while delivering both superior generation quality and inference\nefficiency compared to baseline methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging the Transformer architecture and the diffusion process, video DiT\nmodels have emerged as a dominant approach for high-quality video generation.\nHowever, their multi-step iterative denoising process incurs high computational\ncost and inference latency. Caching, a widely adopted optimization method in\nDiT models, leverages the redundancy in the diffusion process to skip\ncomputations in different granularities (e.g., step, cfg, block). Nevertheless,\nexisting caching methods are limited to single-granularity strategies,\nstruggling to balance generation quality and inference speed in a flexible\nmanner. In this work, we propose MixCache, a training-free caching-based\nframework for efficient video DiT inference. It first distinguishes the\ninterference and boundary between different caching strategies, and then\nintroduces a context-aware cache triggering strategy to determine when caching\nshould be enabled, along with an adaptive hybrid cache decision strategy for\ndynamically selecting the optimal caching granularity. Extensive experiments on\ndiverse models demonstrate that, MixCache can significantly accelerate video\ngeneration (e.g., 1.94$\\times$ speedup on Wan 14B, 1.97$\\times$ speedup on\nHunyuanVideo) while delivering both superior generation quality and inference\nefficiency compared to baseline methods."
                },
                "authors": [
                    {
                        "name": "Yuanxin Wei"
                    },
                    {
                        "name": "Lansong Diao"
                    },
                    {
                        "name": "Bujiao Chen"
                    },
                    {
                        "name": "Shenggan Cheng"
                    },
                    {
                        "name": "Zhengping Qian"
                    },
                    {
                        "name": "Wenyuan Yu"
                    },
                    {
                        "name": "Nong Xiao"
                    },
                    {
                        "name": "Wei Lin"
                    },
                    {
                        "name": "Jiangsu Du"
                    }
                ],
                "author_detail": {
                    "name": "Jiangsu Du"
                },
                "author": "Jiangsu Du",
                "arxiv_comment": "7 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.12691v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.12691v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.12485v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.12485v1",
                "updated": "2025-08-17T20:01:12Z",
                "updated_parsed": [
                    2025,
                    8,
                    17,
                    20,
                    1,
                    12,
                    6,
                    229,
                    0
                ],
                "published": "2025-08-17T20:01:12Z",
                "published_parsed": [
                    2025,
                    8,
                    17,
                    20,
                    1,
                    12,
                    6,
                    229,
                    0
                ],
                "title": "Cold-RL: Learning Cache Eviction with Offline Reinforcement Learning for\n  NGINX",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cold-RL: Learning Cache Eviction with Offline Reinforcement Learning for\n  NGINX"
                },
                "summary": "Web proxies such as NGINX commonly rely on least-recently-used (LRU)\neviction, which is size agnostic and can thrash under periodic bursts and mixed\nobject sizes. We introduce Cold-RL, a learned eviction policy for NGINX that\nreplaces LRU's forced-expire path with a dueling Deep Q-Network served by an\nONNX sidecar within a strict microsecond budget. On each eviction, Cold-RL\nsamples the K least-recently-used objects, extracts six lightweight features\n(age, size, hit count, inter-arrival time, remaining TTL, and last origin RTT),\nand requests a bitmask of victims; a hard timeout of 500 microseconds triggers\nimmediate fallback to native LRU. Policies are trained offline by replaying\nNGINX access logs through a cache simulator with a simple reward: a retained\nobject earns one point if it is hit again before TTL expiry. We compare against\nLRU, LFU, size-based, adaptive LRU, and a hybrid baseline on two adversarial\nworkloads. With a 25 MB cache, Cold-RL raises hit ratio from 0.1436 to 0.3538,\na 146 percent improvement over the best classical baseline; at 100 MB, from\n0.7530 to 0.8675, a 15 percent gain; and at 400 MB it matches classical methods\n(about 0.918). Inference adds less than 2 percent CPU overhead and keeps 95th\npercentile eviction latency within budget. To our knowledge, this is the first\nreinforcement learning eviction policy integrated into NGINX with strict SLOs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Web proxies such as NGINX commonly rely on least-recently-used (LRU)\neviction, which is size agnostic and can thrash under periodic bursts and mixed\nobject sizes. We introduce Cold-RL, a learned eviction policy for NGINX that\nreplaces LRU's forced-expire path with a dueling Deep Q-Network served by an\nONNX sidecar within a strict microsecond budget. On each eviction, Cold-RL\nsamples the K least-recently-used objects, extracts six lightweight features\n(age, size, hit count, inter-arrival time, remaining TTL, and last origin RTT),\nand requests a bitmask of victims; a hard timeout of 500 microseconds triggers\nimmediate fallback to native LRU. Policies are trained offline by replaying\nNGINX access logs through a cache simulator with a simple reward: a retained\nobject earns one point if it is hit again before TTL expiry. We compare against\nLRU, LFU, size-based, adaptive LRU, and a hybrid baseline on two adversarial\nworkloads. With a 25 MB cache, Cold-RL raises hit ratio from 0.1436 to 0.3538,\na 146 percent improvement over the best classical baseline; at 100 MB, from\n0.7530 to 0.8675, a 15 percent gain; and at 400 MB it matches classical methods\n(about 0.918). Inference adds less than 2 percent CPU overhead and keeps 95th\npercentile eviction latency within budget. To our knowledge, this is the first\nreinforcement learning eviction policy integrated into NGINX with strict SLOs."
                },
                "authors": [
                    {
                        "name": "Aayush Gupta"
                    },
                    {
                        "name": "Arpit Bhayani"
                    }
                ],
                "author_detail": {
                    "name": "Arpit Bhayani"
                },
                "author": "Arpit Bhayani",
                "arxiv_comment": "8 pages, 4 figures (system architecture, eviction path, training\n  pipeline, and DQN algorithm), 2 tables. Code available at\n  https://github.com/ayushgupta4897/DRL-Cache",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.12485v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.12485v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.2.4; C.4; D.4.2; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13231v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13231v1",
                "updated": "2025-08-17T19:07:08Z",
                "updated_parsed": [
                    2025,
                    8,
                    17,
                    19,
                    7,
                    8,
                    6,
                    229,
                    0
                ],
                "published": "2025-08-17T19:07:08Z",
                "published_parsed": [
                    2025,
                    8,
                    17,
                    19,
                    7,
                    8,
                    6,
                    229,
                    0
                ],
                "title": "Accelerating LLM Inference via Dynamic KV Cache Placement in\n  Heterogeneous Memory System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating LLM Inference via Dynamic KV Cache Placement in\n  Heterogeneous Memory System"
                },
                "summary": "Large Language Model (LLM) inference is increasingly constrained by memory\nbandwidth, with frequent access to the key-value (KV) cache dominating data\nmovement. While attention sparsity reduces some memory traffic, the relevance\nof past tokens varies over time, requiring the full KV cache to remain\naccessible and sustaining pressure on both bandwidth and capacity. With\nadvances in interconnects such as NVLink and LPDDR5X, modern AI hardware now\nintegrates high-bandwidth memory (HBM) with high-speed off-package DRAM, making\nheterogeneous memory systems a practical solution. This work investigates\ndynamic KV cache placement across such systems to maximize aggregated bandwidth\nutilization under capacity constraints. Rather than proposing a specific\nscheduling policy, we formulate the placement problem mathematically and derive\na theoretical upper bound, revealing substantial headroom for runtime\noptimization. To our knowledge, this is the first formal treatment of dynamic\nKV cache scheduling in heterogeneous memory systems for LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference is increasingly constrained by memory\nbandwidth, with frequent access to the key-value (KV) cache dominating data\nmovement. While attention sparsity reduces some memory traffic, the relevance\nof past tokens varies over time, requiring the full KV cache to remain\naccessible and sustaining pressure on both bandwidth and capacity. With\nadvances in interconnects such as NVLink and LPDDR5X, modern AI hardware now\nintegrates high-bandwidth memory (HBM) with high-speed off-package DRAM, making\nheterogeneous memory systems a practical solution. This work investigates\ndynamic KV cache placement across such systems to maximize aggregated bandwidth\nutilization under capacity constraints. Rather than proposing a specific\nscheduling policy, we formulate the placement problem mathematically and derive\na theoretical upper bound, revealing substantial headroom for runtime\noptimization. To our knowledge, this is the first formal treatment of dynamic\nKV cache scheduling in heterogeneous memory systems for LLM inference."
                },
                "authors": [
                    {
                        "name": "Yunhua Fang"
                    },
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Asad Ul Haq"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Kaoutar El Maghraoui"
                    },
                    {
                        "name": "Naigang Wang"
                    },
                    {
                        "name": "Meng Wang"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13231v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13231v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.12407v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.12407v1",
                "updated": "2025-08-17T15:48:50Z",
                "updated_parsed": [
                    2025,
                    8,
                    17,
                    15,
                    48,
                    50,
                    6,
                    229,
                    0
                ],
                "published": "2025-08-17T15:48:50Z",
                "published_parsed": [
                    2025,
                    8,
                    17,
                    15,
                    48,
                    50,
                    6,
                    229,
                    0
                ],
                "title": "ZigzagAttention: Efficient Long-Context Inference with Exclusive\n  Retrieval and Streaming Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZigzagAttention: Efficient Long-Context Inference with Exclusive\n  Retrieval and Streaming Heads"
                },
                "summary": "With the rapid development of large language models (LLMs), handling long\ncontext has become one of the vital abilities in LLMs. Such long-context\nability is accompanied by difficulties in deployment, especially due to the\nincreased consumption of KV cache. There is certain work aiming to optimize the\nmemory footprint of KV cache, inspired by the observation that attention heads\ncan be categorized into retrieval heads that are of great significance and\nstreaming heads that are of less significance. Typically, identifying the\nstreaming heads and and waiving the KV cache in the streaming heads would\nlargely reduce the overhead without hurting the performance that much. However,\nsince employing both retrieval and streaming heads in one layer decomposes one\nlarge round of attention computation into two small ones, it may unexpectedly\nbring extra latency on accessing and indexing tensors. Based on this intuition,\nwe impose an important improvement to the identification process of retrieval\nand streaming heads, in which we design a criterion that enforces exclusively\nretrieval or streaming heads gathered in one unique layer. In this way, we\nfurther eliminate the extra latency and only incur negligible performance\ndegradation. Our method named \\textsc{ZigzagAttention} is competitive among\nconsidered baselines owing to reduced latency and comparable performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid development of large language models (LLMs), handling long\ncontext has become one of the vital abilities in LLMs. Such long-context\nability is accompanied by difficulties in deployment, especially due to the\nincreased consumption of KV cache. There is certain work aiming to optimize the\nmemory footprint of KV cache, inspired by the observation that attention heads\ncan be categorized into retrieval heads that are of great significance and\nstreaming heads that are of less significance. Typically, identifying the\nstreaming heads and and waiving the KV cache in the streaming heads would\nlargely reduce the overhead without hurting the performance that much. However,\nsince employing both retrieval and streaming heads in one layer decomposes one\nlarge round of attention computation into two small ones, it may unexpectedly\nbring extra latency on accessing and indexing tensors. Based on this intuition,\nwe impose an important improvement to the identification process of retrieval\nand streaming heads, in which we design a criterion that enforces exclusively\nretrieval or streaming heads gathered in one unique layer. In this way, we\nfurther eliminate the extra latency and only incur negligible performance\ndegradation. Our method named \\textsc{ZigzagAttention} is competitive among\nconsidered baselines owing to reduced latency and comparable performance."
                },
                "authors": [
                    {
                        "name": "Zhuorui Liu"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Dawei Song"
                    }
                ],
                "author_detail": {
                    "name": "Dawei Song"
                },
                "author": "Dawei Song",
                "arxiv_comment": "5 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.12407v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.12407v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.12357v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.12357v1",
                "updated": "2025-08-17T13:05:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    17,
                    13,
                    5,
                    52,
                    6,
                    229,
                    0
                ],
                "published": "2025-08-17T13:05:52Z",
                "published_parsed": [
                    2025,
                    8,
                    17,
                    13,
                    5,
                    52,
                    6,
                    229,
                    0
                ],
                "title": "Enhancement of the energy storage and electrocaloric effect performances\n  in 0.4 BCZT 0.6 BSTSn medium entropy ceramic prepared by sol gel method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancement of the energy storage and electrocaloric effect performances\n  in 0.4 BCZT 0.6 BSTSn medium entropy ceramic prepared by sol gel method"
                },
                "summary": "Based on the traditional polycrystalline ferroelectric\nBa0.85Ca0.15Zr0.10Ti0.90O3, the 0.4 Ba0.85Ca0.15Zr0.10Ti0.90O3 0.6\nBa0.9Sr0.1Ti0.9Sn0.1O3 medium entropy material with good energy storage and\nelectrocaloric effect performances is designed and synthesized by the solgel\nmethod. The structural, dielectric, energy storage and electrocaloric effect\nproperties of the prepared sample were studied. The findings demonstrate that\nthe 0.4 Ba0.85Ca0.15Zr0.10Ti0.90O3 0.6 Ba0.9Sr0.1Ti0.9Sn0.1O3 ceramic\nsimultaneously has a significant recoverable energy storage density of 255.4\nmJ/cm3, an efficiency of 67.9%, a large ECE temperature change of 1.36 K, and a\nhigh ECE responsivity of 0.453 K.mm/kV under a low electric field of 30 kV/cm.\nMoreover, excellent temperature stability of Wrec (less than 10%) was achieved\nin the investigated sample 0.4BCZT 0.6BSTSn.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Based on the traditional polycrystalline ferroelectric\nBa0.85Ca0.15Zr0.10Ti0.90O3, the 0.4 Ba0.85Ca0.15Zr0.10Ti0.90O3 0.6\nBa0.9Sr0.1Ti0.9Sn0.1O3 medium entropy material with good energy storage and\nelectrocaloric effect performances is designed and synthesized by the solgel\nmethod. The structural, dielectric, energy storage and electrocaloric effect\nproperties of the prepared sample were studied. The findings demonstrate that\nthe 0.4 Ba0.85Ca0.15Zr0.10Ti0.90O3 0.6 Ba0.9Sr0.1Ti0.9Sn0.1O3 ceramic\nsimultaneously has a significant recoverable energy storage density of 255.4\nmJ/cm3, an efficiency of 67.9%, a large ECE temperature change of 1.36 K, and a\nhigh ECE responsivity of 0.453 K.mm/kV under a low electric field of 30 kV/cm.\nMoreover, excellent temperature stability of Wrec (less than 10%) was achieved\nin the investigated sample 0.4BCZT 0.6BSTSn."
                },
                "authors": [
                    {
                        "name": "S. Khardazi"
                    },
                    {
                        "name": "Z. Gargar"
                    },
                    {
                        "name": "A. Lyubchyk"
                    },
                    {
                        "name": "O. Zakir"
                    },
                    {
                        "name": "D. Mezzane"
                    },
                    {
                        "name": "M. Amjoud"
                    },
                    {
                        "name": "A. Alimoussa"
                    },
                    {
                        "name": "Z. Kutnjak"
                    }
                ],
                "author_detail": {
                    "name": "Z. Kutnjak"
                },
                "author": "Z. Kutnjak",
                "arxiv_doi": "10.1016/j.jssc.2025.125547",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.jssc.2025.125547",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.12357v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.12357v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14051v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14051v5",
                "updated": "2025-08-16T23:41:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    16,
                    23,
                    41,
                    48,
                    5,
                    228,
                    0
                ],
                "published": "2025-04-18T19:46:54Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    19,
                    46,
                    54,
                    4,
                    108,
                    0
                ],
                "title": "CAOTE: KV Cache Selection for LLMs via Attention Output Error-Based\n  Token Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAOTE: KV Cache Selection for LLMs via Attention Output Error-Based\n  Token Eviction"
                },
                "summary": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value tokens on top of\nattention-based eviction scores in closed-form. Additionally, CAOTE can act as\na meta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value tokens on top of\nattention-based eviction scores in closed-form. Additionally, CAOTE can act as\na meta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process."
                },
                "authors": [
                    {
                        "name": "Raghavv Goel"
                    },
                    {
                        "name": "Junyoung Park"
                    },
                    {
                        "name": "Mukul Gagrani"
                    },
                    {
                        "name": "Dalton Jones"
                    },
                    {
                        "name": "Matthew Morse"
                    },
                    {
                        "name": "Harper Langston"
                    },
                    {
                        "name": "Mingu Lee"
                    },
                    {
                        "name": "Chris Lott"
                    }
                ],
                "author_detail": {
                    "name": "Chris Lott"
                },
                "author": "Chris Lott",
                "arxiv_comment": "15 pages, 3 figures, 13 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14051v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14051v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09040v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09040v2",
                "updated": "2025-08-16T18:49:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    16,
                    18,
                    49,
                    41,
                    5,
                    228,
                    0
                ],
                "published": "2025-05-14T00:41:44Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    0,
                    41,
                    44,
                    2,
                    134,
                    0
                ],
                "title": "RT-Cache: Training-Free Retrieval for Real-Time Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RT-Cache: Training-Free Retrieval for Real-Time Manipulation"
                },
                "summary": "Real robots are expected to repeat the same behavior in new environments with\nvery little new data, yet modern controllers either incur heavy per-step\ninference or require deployment-time fine-tuning. We propose RT-Cache, a\ntraining-free retrieval-as-control pipeline that caches diverse image action\ntrajectories in a unified vector memory and, at test time, embeds the current\nframe to retrieve and replay multi-step snippets, replacing per-step model\ncalls. A hierarchical search keeps lookups sub-second at million scale,\nshifting cost from compute to storage and enabling real-time control on modest\nGPUs. Across real-robot tasks and large open logs, RT-Cache achieves higher\nsuccess and lower completion time than strong retrieval baselines\n(approximately x2 higher success and ~30% faster in our settings), and a\nsingle-episode anchoring study shows immediate adaptation to a more complex,\ncontact-rich task without fine-tuning. RT-Cache turns experience into an\nappend-only memory, offering a simple, scalable path to few-shot deployment\ntoday and a foundation for multimodal keys and optional integration with\nhigh-level policies. Project page: https://rt-cache.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real robots are expected to repeat the same behavior in new environments with\nvery little new data, yet modern controllers either incur heavy per-step\ninference or require deployment-time fine-tuning. We propose RT-Cache, a\ntraining-free retrieval-as-control pipeline that caches diverse image action\ntrajectories in a unified vector memory and, at test time, embeds the current\nframe to retrieve and replay multi-step snippets, replacing per-step model\ncalls. A hierarchical search keeps lookups sub-second at million scale,\nshifting cost from compute to storage and enabling real-time control on modest\nGPUs. Across real-robot tasks and large open logs, RT-Cache achieves higher\nsuccess and lower completion time than strong retrieval baselines\n(approximately x2 higher success and ~30% faster in our settings), and a\nsingle-episode anchoring study shows immediate adaptation to a more complex,\ncontact-rich task without fine-tuning. RT-Cache turns experience into an\nappend-only memory, offering a simple, scalable path to few-shot deployment\ntoday and a foundation for multimodal keys and optional integration with\nhigh-level policies. Project page: https://rt-cache.github.io/."
                },
                "authors": [
                    {
                        "name": "Owen Kwon"
                    },
                    {
                        "name": "Abraham George"
                    },
                    {
                        "name": "Alison Bartsch"
                    },
                    {
                        "name": "Amir Barati Farimani"
                    }
                ],
                "author_detail": {
                    "name": "Amir Barati Farimani"
                },
                "author": "Amir Barati Farimani",
                "arxiv_comment": "8 pages, 6 figures. Accepted to the 2025 IEEE-RAS 24th International\n  Conference on Humanoid Robots",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09040v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09040v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10824v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10824v2",
                "updated": "2025-08-16T03:17:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    16,
                    3,
                    17,
                    35,
                    5,
                    228,
                    0
                ],
                "published": "2025-08-14T16:48:38Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    16,
                    48,
                    38,
                    3,
                    226,
                    0
                ],
                "title": "Memory-Augmented Transformers: A Systematic Review from Neuroscience\n  Principles to Enhanced Model Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-Augmented Transformers: A Systematic Review from Neuroscience\n  Principles to Enhanced Model Architectures"
                },
                "summary": "Memory is fundamental to intelligence, enabling learning, reasoning, and\nadaptability across biological and artificial systems. While Transformer\narchitectures excel at sequence modeling, they face critical limitations in\nlong-range context retention, continual learning, and knowledge integration.\nThis review presents a unified framework bridging neuroscience principles,\nincluding dynamic multi-timescale memory, selective attention, and\nconsolidation, with engineering advances in Memory-Augmented Transformers. We\norganize recent progress through three taxonomic dimensions: functional\nobjectives (context extension, reasoning, knowledge integration, adaptation),\nmemory representations (parameter-encoded, state-based, explicit, hybrid), and\nintegration mechanisms (attention fusion, gated control, associative\nretrieval). Our analysis of core memory operations (reading, writing,\nforgetting, and capacity management) reveals a shift from static caches toward\nadaptive, test-time learning systems. We identify persistent challenges in\nscalability and interference, alongside emerging solutions including\nhierarchical buffering and surprise-gated updates. This synthesis provides a\nroadmap toward cognitively-inspired, lifelong-learning Transformer\narchitectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory is fundamental to intelligence, enabling learning, reasoning, and\nadaptability across biological and artificial systems. While Transformer\narchitectures excel at sequence modeling, they face critical limitations in\nlong-range context retention, continual learning, and knowledge integration.\nThis review presents a unified framework bridging neuroscience principles,\nincluding dynamic multi-timescale memory, selective attention, and\nconsolidation, with engineering advances in Memory-Augmented Transformers. We\norganize recent progress through three taxonomic dimensions: functional\nobjectives (context extension, reasoning, knowledge integration, adaptation),\nmemory representations (parameter-encoded, state-based, explicit, hybrid), and\nintegration mechanisms (attention fusion, gated control, associative\nretrieval). Our analysis of core memory operations (reading, writing,\nforgetting, and capacity management) reveals a shift from static caches toward\nadaptive, test-time learning systems. We identify persistent challenges in\nscalability and interference, alongside emerging solutions including\nhierarchical buffering and surprise-gated updates. This synthesis provides a\nroadmap toward cognitively-inspired, lifelong-learning Transformer\narchitectures."
                },
                "authors": [
                    {
                        "name": "Parsa Omidi"
                    },
                    {
                        "name": "Xingshuai Huang"
                    },
                    {
                        "name": "Axel Laborieux"
                    },
                    {
                        "name": "Bahareh Nikpour"
                    },
                    {
                        "name": "Tianyu Shi"
                    },
                    {
                        "name": "Armaghan Eshaghi"
                    }
                ],
                "author_detail": {
                    "name": "Armaghan Eshaghi"
                },
                "author": "Armaghan Eshaghi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10824v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10824v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11495v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11495v1",
                "updated": "2025-08-15T14:17:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    14,
                    17,
                    24,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T14:17:24Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    14,
                    17,
                    24,
                    4,
                    227,
                    0
                ],
                "title": "KV-Auditor: Auditing Local Differential Privacy for Correlated Key-Value\n  Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Auditor: Auditing Local Differential Privacy for Correlated Key-Value\n  Estimation"
                },
                "summary": "To protect privacy for data-collection-based services, local differential\nprivacy (LDP) is widely adopted due to its rigorous theoretical bound on\nprivacy loss. However, mistakes in complex theoretical analysis or subtle\nimplementation errors may undermine its practical guarantee. To address this,\nauditing is crucial to confirm that LDP protocols truly protect user data.\nHowever, existing auditing methods, though, mainly target machine learning and\nfederated learning tasks based on centralized differentially privacy (DP), with\nlimited attention to LDP. Moreover, the few studies on LDP auditing focus\nsolely on simple frequency estimation task for discrete data, leaving\ncorrelated key-value data - which requires both discrete frequency estimation\nfor keys and continuous mean estimation for values - unexplored.\n  To bridge this gap, we propose KV-Auditor, a framework for auditing LDP-based\nkey-value estimation mechanisms by estimating their empirical privacy lower\nbounds. Rather than traditional LDP auditing methods that relies on binary\noutput predictions, KV-Auditor estimates this lower bound by analyzing\nunbounded output distributions, supporting continuous data. Specifically, we\nclassify state-of-the-art LDP key-value mechanisms into interactive and\nnon-interactive types. For non-interactive mechanisms, we propose horizontal\nKV-Auditor for small domains with sufficient samples and vertical KV-Auditor\nfor large domains with limited samples. For interactive mechanisms, we design a\nsegmentation strategy to capture incremental privacy leakage across iterations.\nFinally, we perform extensive experiments to validate the effectiveness of our\napproach, offering insights for optimizing LDP-based key-value estimators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To protect privacy for data-collection-based services, local differential\nprivacy (LDP) is widely adopted due to its rigorous theoretical bound on\nprivacy loss. However, mistakes in complex theoretical analysis or subtle\nimplementation errors may undermine its practical guarantee. To address this,\nauditing is crucial to confirm that LDP protocols truly protect user data.\nHowever, existing auditing methods, though, mainly target machine learning and\nfederated learning tasks based on centralized differentially privacy (DP), with\nlimited attention to LDP. Moreover, the few studies on LDP auditing focus\nsolely on simple frequency estimation task for discrete data, leaving\ncorrelated key-value data - which requires both discrete frequency estimation\nfor keys and continuous mean estimation for values - unexplored.\n  To bridge this gap, we propose KV-Auditor, a framework for auditing LDP-based\nkey-value estimation mechanisms by estimating their empirical privacy lower\nbounds. Rather than traditional LDP auditing methods that relies on binary\noutput predictions, KV-Auditor estimates this lower bound by analyzing\nunbounded output distributions, supporting continuous data. Specifically, we\nclassify state-of-the-art LDP key-value mechanisms into interactive and\nnon-interactive types. For non-interactive mechanisms, we propose horizontal\nKV-Auditor for small domains with sufficient samples and vertical KV-Auditor\nfor large domains with limited samples. For interactive mechanisms, we design a\nsegmentation strategy to capture incremental privacy leakage across iterations.\nFinally, we perform extensive experiments to validate the effectiveness of our\napproach, offering insights for optimizing LDP-based key-value estimators."
                },
                "authors": [
                    {
                        "name": "Jingnan Xu"
                    },
                    {
                        "name": "Leixia Wang"
                    },
                    {
                        "name": "Xiaofeng Meng"
                    }
                ],
                "author_detail": {
                    "name": "Xiaofeng Meng"
                },
                "author": "Xiaofeng Meng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11495v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11495v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11291v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11291v1",
                "updated": "2025-08-15T07:55:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    7,
                    55,
                    5,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T07:55:05Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    7,
                    55,
                    5,
                    4,
                    227,
                    0
                ],
                "title": "Dynamic Quality-Latency Aware Routing for LLM Inference in Wireless\n  Edge-Device Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Quality-Latency Aware Routing for LLM Inference in Wireless\n  Edge-Device Networks"
                },
                "summary": "The integration of wireless communications and Large Language Models (LLMs)\nis poised to unlock ubiquitous intelligent services, yet deploying them in\nwireless edge-device collaborative environments presents a critical trade-off\nbetween inference quality and end-to-end latency. A fundamental mismatch exists\nbetween task complexity and resource allocation: offloading simple queries\ninvites prohibitive latency, while on-device models lack the capacity for\ndemanding computations. To address this challenge, we propose a dynamic,\nquality-latency aware routing framework that orchestrates inference between a\nlightweight model on the mobile device and a powerful model on the edge server.\nOur framework employs two distinct cost models: for single-turn queries, it\nfuses a BERT-predicted semantic score with communication and computation\noverheads; for multi-turn dialogues, it further quantifies context-aware costs\narising from model switching and KV-cache management. While maintaining full\ninference quality, extensive experiments demonstrate that our framework cuts\naverage response latency by 5-15% and reduces large model invocations by 10-20%\nagainst competitive baselines on MMLU, GSM8K, and MT-Bench-101 benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of wireless communications and Large Language Models (LLMs)\nis poised to unlock ubiquitous intelligent services, yet deploying them in\nwireless edge-device collaborative environments presents a critical trade-off\nbetween inference quality and end-to-end latency. A fundamental mismatch exists\nbetween task complexity and resource allocation: offloading simple queries\ninvites prohibitive latency, while on-device models lack the capacity for\ndemanding computations. To address this challenge, we propose a dynamic,\nquality-latency aware routing framework that orchestrates inference between a\nlightweight model on the mobile device and a powerful model on the edge server.\nOur framework employs two distinct cost models: for single-turn queries, it\nfuses a BERT-predicted semantic score with communication and computation\noverheads; for multi-turn dialogues, it further quantifies context-aware costs\narising from model switching and KV-cache management. While maintaining full\ninference quality, extensive experiments demonstrate that our framework cuts\naverage response latency by 5-15% and reduces large model invocations by 10-20%\nagainst competitive baselines on MMLU, GSM8K, and MT-Bench-101 benchmarks."
                },
                "authors": [
                    {
                        "name": "Rui Bao"
                    },
                    {
                        "name": "Nan Xue"
                    },
                    {
                        "name": "Yaping Sun"
                    },
                    {
                        "name": "Zhiyong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyong Chen"
                },
                "author": "Zhiyong Chen",
                "arxiv_comment": "accepted by IEEE/CIC ICCC workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11291v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11291v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11260v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11260v1",
                "updated": "2025-08-15T06:53:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    6,
                    53,
                    28,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T06:53:28Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    6,
                    53,
                    28,
                    4,
                    227,
                    0
                ],
                "title": "UNVEILING: What Makes Linguistics Olympiad Puzzles Tricky for LLMs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UNVEILING: What Makes Linguistics Olympiad Puzzles Tricky for LLMs?"
                },
                "summary": "Large language models (LLMs) have demonstrated potential in reasoning tasks,\nbut their performance on linguistics puzzles remains consistently poor. These\npuzzles, often derived from Linguistics Olympiad (LO) contests, provide a\nminimal contamination environment to assess LLMs' linguistic reasoning\nabilities across low-resource languages. This work analyses LLMs' performance\non 629 problems across 41 low-resource languages by labelling each with\nlinguistically informed features to unveil weaknesses. Our analyses show that\nLLMs struggle with puzzles involving higher morphological complexity and\nperform better on puzzles involving linguistic features that are also found in\nEnglish. We also show that splitting words into morphemes as a pre-processing\nstep improves solvability, indicating a need for more informed and\nlanguage-specific tokenisers. These findings thus offer insights into some\nchallenges in linguistic reasoning and modelling of low-resource languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated potential in reasoning tasks,\nbut their performance on linguistics puzzles remains consistently poor. These\npuzzles, often derived from Linguistics Olympiad (LO) contests, provide a\nminimal contamination environment to assess LLMs' linguistic reasoning\nabilities across low-resource languages. This work analyses LLMs' performance\non 629 problems across 41 low-resource languages by labelling each with\nlinguistically informed features to unveil weaknesses. Our analyses show that\nLLMs struggle with puzzles involving higher morphological complexity and\nperform better on puzzles involving linguistic features that are also found in\nEnglish. We also show that splitting words into morphemes as a pre-processing\nstep improves solvability, indicating a need for more informed and\nlanguage-specific tokenisers. These findings thus offer insights into some\nchallenges in linguistic reasoning and modelling of low-resource languages."
                },
                "authors": [
                    {
                        "name": "Mukund Choudhary"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Gaurja Aeron"
                    },
                    {
                        "name": "Antara Raaghavi Bhattacharya"
                    },
                    {
                        "name": "Dang Khoa Dang Dinh"
                    },
                    {
                        "name": "Ikhlasul Akmal Hanif"
                    },
                    {
                        "name": "Daria Kotova"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    },
                    {
                        "name": "Monojit Choudhury"
                    }
                ],
                "author_detail": {
                    "name": "Monojit Choudhury"
                },
                "author": "Monojit Choudhury",
                "arxiv_comment": "Accepted to COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11260v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11260v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10069v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10069v2",
                "updated": "2025-08-15T04:27:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    4,
                    27,
                    30,
                    4,
                    227,
                    0
                ],
                "published": "2025-07-14T08:53:48Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    53,
                    48,
                    0,
                    195,
                    0
                ],
                "title": "ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal\n  Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal\n  Parallelism"
                },
                "summary": "Multimodal large language models (MLLMs) extend LLMs to handle images,\nvideos, and audio by incorporating feature extractors and projection modules.\nHowever, these additional components -- combined with complex inference\npipelines and heterogeneous workloads -- introduce significant inference\noverhead. Therefore, efficiently serving MLLMs remains a major challenge.\nCurrent tightly coupled serving architectures struggle to distinguish between\nmixed request types or adapt parallelism strategies to different inference\nstages, leading to increased time-to-first-token (TTFT) latency and poor\nresource utilization. To address this, we introduce Elastic Multimodal\nParallelism (EMP), a new serving paradigm that elastically adapts to resource\nheterogeneity across request types and inference stages. Building upon EMP, we\ndevelop ElasticMM, an MLLM serving system that (1) separates requests into\nindependent modality groups with dynamic resource allocation via a\nmodality-aware load balancer; (2) decouples inference stages and enables\nparallelism adjustment and adaptive scaling via elastic partition scheduling;\nand (3) improves inference efficiency through unified multimodal prefix caching\nand non-blocking encoding. Experiments on diverse real-world datasets show that\nElasticMM outperforms state-of-the-art (SOTA) serving systems, reducing TTFT by\nup to 4.2x and achieving 3.2-4.5x higher throughput while meeting service-level\nobjectives (SLOs).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) extend LLMs to handle images,\nvideos, and audio by incorporating feature extractors and projection modules.\nHowever, these additional components -- combined with complex inference\npipelines and heterogeneous workloads -- introduce significant inference\noverhead. Therefore, efficiently serving MLLMs remains a major challenge.\nCurrent tightly coupled serving architectures struggle to distinguish between\nmixed request types or adapt parallelism strategies to different inference\nstages, leading to increased time-to-first-token (TTFT) latency and poor\nresource utilization. To address this, we introduce Elastic Multimodal\nParallelism (EMP), a new serving paradigm that elastically adapts to resource\nheterogeneity across request types and inference stages. Building upon EMP, we\ndevelop ElasticMM, an MLLM serving system that (1) separates requests into\nindependent modality groups with dynamic resource allocation via a\nmodality-aware load balancer; (2) decouples inference stages and enables\nparallelism adjustment and adaptive scaling via elastic partition scheduling;\nand (3) improves inference efficiency through unified multimodal prefix caching\nand non-blocking encoding. Experiments on diverse real-world datasets show that\nElasticMM outperforms state-of-the-art (SOTA) serving systems, reducing TTFT by\nup to 4.2x and achieving 3.2-4.5x higher throughput while meeting service-level\nobjectives (SLOs)."
                },
                "authors": [
                    {
                        "name": "Zedong Liu"
                    },
                    {
                        "name": "Shenggan Cheng"
                    },
                    {
                        "name": "Guangming Tan"
                    },
                    {
                        "name": "Yang You"
                    },
                    {
                        "name": "Dingwen Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dingwen Tao"
                },
                "author": "Dingwen Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10069v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10069v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10875v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10875v1",
                "updated": "2025-08-14T17:47:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    17,
                    47,
                    22,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T17:47:22Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    17,
                    47,
                    22,
                    3,
                    226,
                    0
                ],
                "title": "A Survey on Diffusion Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Diffusion Language Models"
                },
                "summary": "Diffusion Language Models (DLMs) are rapidly emerging as a powerful and\npromising alternative to the dominant autoregressive (AR) paradigm. By\ngenerating tokens in parallel through an iterative denoising process, DLMs\npossess inherent advantages in reducing inference latency and capturing\nbidirectional context, thereby enabling fine-grained control over the\ngeneration process. While achieving a several-fold speed-up, recent\nadvancements have allowed DLMs to show performance comparable to their\nautoregressive counterparts, making them a compelling choice for various\nnatural language processing tasks. In this survey, we provide a holistic\noverview of the current DLM landscape. We trace its evolution and relationship\nwith other paradigms, such as autoregressive and masked language models, and\ncover both foundational principles and state-of-the-art models. Our work offers\nan up-to-date, comprehensive taxonomy and an in-depth analysis of current\ntechniques, from pre-training strategies to advanced post-training methods.\nAnother contribution of this survey is a thorough review of DLM inference\nstrategies and optimizations, including improvements in decoding parallelism,\ncaching mechanisms, and generation quality. We also highlight the latest\napproaches to multimodal extensions of DLMs and delineate their applications\nacross various practical scenarios. Furthermore, our discussion addresses the\nlimitations and challenges of DLMs, including efficiency, long-sequence\nhandling, and infrastructure requirements, while outlining future research\ndirections to sustain progress in this rapidly evolving field. Project GitHub\nis available at https://github.com/VILA-Lab/Awesome-DLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Language Models (DLMs) are rapidly emerging as a powerful and\npromising alternative to the dominant autoregressive (AR) paradigm. By\ngenerating tokens in parallel through an iterative denoising process, DLMs\npossess inherent advantages in reducing inference latency and capturing\nbidirectional context, thereby enabling fine-grained control over the\ngeneration process. While achieving a several-fold speed-up, recent\nadvancements have allowed DLMs to show performance comparable to their\nautoregressive counterparts, making them a compelling choice for various\nnatural language processing tasks. In this survey, we provide a holistic\noverview of the current DLM landscape. We trace its evolution and relationship\nwith other paradigms, such as autoregressive and masked language models, and\ncover both foundational principles and state-of-the-art models. Our work offers\nan up-to-date, comprehensive taxonomy and an in-depth analysis of current\ntechniques, from pre-training strategies to advanced post-training methods.\nAnother contribution of this survey is a thorough review of DLM inference\nstrategies and optimizations, including improvements in decoding parallelism,\ncaching mechanisms, and generation quality. We also highlight the latest\napproaches to multimodal extensions of DLMs and delineate their applications\nacross various practical scenarios. Furthermore, our discussion addresses the\nlimitations and challenges of DLMs, including efficiency, long-sequence\nhandling, and infrastructure requirements, while outlining future research\ndirections to sustain progress in this rapidly evolving field. Project GitHub\nis available at https://github.com/VILA-Lab/Awesome-DLMs."
                },
                "authors": [
                    {
                        "name": "Tianyi Li"
                    },
                    {
                        "name": "Mingda Chen"
                    },
                    {
                        "name": "Bowei Guo"
                    },
                    {
                        "name": "Zhiqiang Shen"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Shen"
                },
                "author": "Zhiqiang Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10875v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10875v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13109v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13109v2",
                "updated": "2025-08-14T16:12:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    16,
                    12,
                    44,
                    3,
                    226,
                    0
                ],
                "published": "2025-05-19T13:36:45Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    13,
                    36,
                    45,
                    0,
                    139,
                    0
                ],
                "title": "FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference"
                },
                "summary": "Large language models (LLMs) have been widely deployed with rapidly expanding\ncontext windows to support increasingly demanding applications. However, long\ncontexts pose significant deployment challenges, primarily due to the KV cache\nwhose size grows proportionally with context length. While KV cache compression\nmethods are proposed to address this issue, KV dropping methods incur\nconsiderable accuracy loss, and KV retrieval methods suffer from significant\nefficiency bottlenecks. We propose FreeKV, an algorithm-system co-optimization\nframework to enhance KV retrieval efficiency while preserving accuracy. On the\nalgorithm side, FreeKV introduces speculative retrieval to shift the KV\nselection and recall processes out of the critical path, combined with\nfine-grained correction to ensure accuracy. On the system side, FreeKV employs\nhybrid KV layouts across CPU and GPU memory to eliminate fragmented data\ntransfers, and leverages double-buffered streamed recall to further improve\nefficiency. Experiments demonstrate that FreeKV achieves near-lossless accuracy\nacross various scenarios and models, delivering up to 13$\\times$ speedup\ncompared to SOTA KV retrieval methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been widely deployed with rapidly expanding\ncontext windows to support increasingly demanding applications. However, long\ncontexts pose significant deployment challenges, primarily due to the KV cache\nwhose size grows proportionally with context length. While KV cache compression\nmethods are proposed to address this issue, KV dropping methods incur\nconsiderable accuracy loss, and KV retrieval methods suffer from significant\nefficiency bottlenecks. We propose FreeKV, an algorithm-system co-optimization\nframework to enhance KV retrieval efficiency while preserving accuracy. On the\nalgorithm side, FreeKV introduces speculative retrieval to shift the KV\nselection and recall processes out of the critical path, combined with\nfine-grained correction to ensure accuracy. On the system side, FreeKV employs\nhybrid KV layouts across CPU and GPU memory to eliminate fragmented data\ntransfers, and leverages double-buffered streamed recall to further improve\nefficiency. Experiments demonstrate that FreeKV achieves near-lossless accuracy\nacross various scenarios and models, delivering up to 13$\\times$ speedup\ncompared to SOTA KV retrieval methods."
                },
                "authors": [
                    {
                        "name": "Guangda Liu"
                    },
                    {
                        "name": "Chengwei Li"
                    },
                    {
                        "name": "Zhenyu Ning"
                    },
                    {
                        "name": "Minyi Guo"
                    },
                    {
                        "name": "Jieru Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jieru Zhao"
                },
                "author": "Jieru Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13109v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13109v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18773v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18773v2",
                "updated": "2025-08-14T15:37:43Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    15,
                    37,
                    43,
                    3,
                    226,
                    0
                ],
                "published": "2025-03-24T15:22:41Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    15,
                    22,
                    41,
                    0,
                    83,
                    0
                ],
                "title": "BitDecoding: Unlocking Tensor Cores for Long-Context LLMs with Low-Bit\n  KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BitDecoding: Unlocking Tensor Cores for Long-Context LLMs with Low-Bit\n  KV Cache"
                },
                "summary": "The rise of long-context Large Language Models (LLMs) amplifies memory and\nbandwidth demands during autoregressive decoding, as the Key-Value (KV) cache\ngrows with each generated token. Low-bit KV-cache quantization (e.g., 4-bit or\n2-bit) can reduce memory footprint while preserving accuracy, but existing\nsystems suffer from slow decoding due to their exclusive reliance on CUDA\ncores, neglecting Tensor Cores (the primary source of compute on modern GPUs).\nWe present BitDecoding, a new long-context LLM inference system with a low-bit\nKV cache. BitDecoding enables efficient low-bit KV-cache decoding by\ncooperatively leveraging CUDA cores and Tensor Cores. It introduces methods for\nautomatically inducing optimized layouts to exploit Tensor Cores, along with\nwarp-level parallelization strategies for dequantization. For unified system\nsupport, BitDecoding includes a query transformation module supporting diverse\nattention variants, a quantization kernel that supports both tensor-wise and\nchannel-wise scaling used in various quantization algorithms with high\nperformance, and a dequantization kernel with a software-defined pipeline to\ncoordinate CUDA and Tensor Cores execution for mixed-precision operations.\nEvaluated on RTX 4090, A100, and H100, BitDecoding accelerates decoding by up\nto 7.5x, 4.8x, and 8.9x, respectively, over FP16 FlashDecoding-v2, and\nsurpasses the state-of-the-art low-bit system QServe by up to 4.3x. On\nLLaMA-3.1-8B with a 128K context, BitDecoding reduces single-batch decoding\nlatency by 3x, showing substantial improvements for long-context generation.\nThe code is available at https://github.com/DD-DuDa/BitDecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of long-context Large Language Models (LLMs) amplifies memory and\nbandwidth demands during autoregressive decoding, as the Key-Value (KV) cache\ngrows with each generated token. Low-bit KV-cache quantization (e.g., 4-bit or\n2-bit) can reduce memory footprint while preserving accuracy, but existing\nsystems suffer from slow decoding due to their exclusive reliance on CUDA\ncores, neglecting Tensor Cores (the primary source of compute on modern GPUs).\nWe present BitDecoding, a new long-context LLM inference system with a low-bit\nKV cache. BitDecoding enables efficient low-bit KV-cache decoding by\ncooperatively leveraging CUDA cores and Tensor Cores. It introduces methods for\nautomatically inducing optimized layouts to exploit Tensor Cores, along with\nwarp-level parallelization strategies for dequantization. For unified system\nsupport, BitDecoding includes a query transformation module supporting diverse\nattention variants, a quantization kernel that supports both tensor-wise and\nchannel-wise scaling used in various quantization algorithms with high\nperformance, and a dequantization kernel with a software-defined pipeline to\ncoordinate CUDA and Tensor Cores execution for mixed-precision operations.\nEvaluated on RTX 4090, A100, and H100, BitDecoding accelerates decoding by up\nto 7.5x, 4.8x, and 8.9x, respectively, over FP16 FlashDecoding-v2, and\nsurpasses the state-of-the-art low-bit system QServe by up to 4.3x. On\nLLaMA-3.1-8B with a 128K context, BitDecoding reduces single-batch decoding\nlatency by 3x, showing substantial improvements for long-context generation.\nThe code is available at https://github.com/DD-DuDa/BitDecoding."
                },
                "authors": [
                    {
                        "name": "Dayou Du"
                    },
                    {
                        "name": "Shijie Cao"
                    },
                    {
                        "name": "Jianyi Cheng"
                    },
                    {
                        "name": "Luo Mai"
                    },
                    {
                        "name": "Ting Cao"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18773v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18773v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10963v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10963v1",
                "updated": "2025-08-14T14:11:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    14,
                    11,
                    48,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T14:11:48Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    14,
                    11,
                    48,
                    3,
                    226,
                    0
                ],
                "title": "EVCtrl: Efficient Control Adapter for Visual Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EVCtrl: Efficient Control Adapter for Visual Generation"
                },
                "summary": "Visual generation includes both image and video generation, training\nprobabilistic models to create coherent, diverse, and semantically faithful\ncontent from scratch. While early research focused on unconditional sampling,\npractitioners now demand controllable generation that allows precise\nspecification of layout, pose, motion, or style. While ControlNet grants\nprecise spatial-temporal control, its auxiliary branch markedly increases\nlatency and introduces redundant computation in both uncontrolled regions and\ndenoising steps, especially for video. To address this problem, we introduce\nEVCtrl, a lightweight, plug-and-play control adapter that slashes overhead\nwithout retraining the model. Specifically, we propose a spatio-temporal dual\ncaching strategy for sparse control information. For spatial redundancy, we\nfirst profile how each layer of DiT-ControlNet responds to fine-grained\ncontrol, then partition the network into global and local functional zones. A\nlocality-aware cache focuses computation on the local zones that truly need the\ncontrol signal, skipping the bulk of redundant computation in global regions.\nFor temporal redundancy, we selectively omit unnecessary denoising steps to\nimprove efficiency. Extensive experiments on CogVideo-Controlnet,\nWan2.1-Controlnet, and Flux demonstrate that our method is effective in image\nand video control generation without the need for training. For example, it\nachieves 2.16 and 2.05 times speedups on CogVideo-Controlnet and\nWan2.1-Controlnet, respectively, with almost no degradation in generation\nquality.Codes are available in the supplementary materials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual generation includes both image and video generation, training\nprobabilistic models to create coherent, diverse, and semantically faithful\ncontent from scratch. While early research focused on unconditional sampling,\npractitioners now demand controllable generation that allows precise\nspecification of layout, pose, motion, or style. While ControlNet grants\nprecise spatial-temporal control, its auxiliary branch markedly increases\nlatency and introduces redundant computation in both uncontrolled regions and\ndenoising steps, especially for video. To address this problem, we introduce\nEVCtrl, a lightweight, plug-and-play control adapter that slashes overhead\nwithout retraining the model. Specifically, we propose a spatio-temporal dual\ncaching strategy for sparse control information. For spatial redundancy, we\nfirst profile how each layer of DiT-ControlNet responds to fine-grained\ncontrol, then partition the network into global and local functional zones. A\nlocality-aware cache focuses computation on the local zones that truly need the\ncontrol signal, skipping the bulk of redundant computation in global regions.\nFor temporal redundancy, we selectively omit unnecessary denoising steps to\nimprove efficiency. Extensive experiments on CogVideo-Controlnet,\nWan2.1-Controlnet, and Flux demonstrate that our method is effective in image\nand video control generation without the need for training. For example, it\nachieves 2.16 and 2.05 times speedups on CogVideo-Controlnet and\nWan2.1-Controlnet, respectively, with almost no degradation in generation\nquality.Codes are available in the supplementary materials."
                },
                "authors": [
                    {
                        "name": "Zixiang Yang"
                    },
                    {
                        "name": "Yue Ma"
                    },
                    {
                        "name": "Yinhan Zhang"
                    },
                    {
                        "name": "Shanhui Mo"
                    },
                    {
                        "name": "Dongrui Liu"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10963v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10963v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10613v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10613v1",
                "updated": "2025-08-14T13:10:43Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    13,
                    10,
                    43,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T13:10:43Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    13,
                    10,
                    43,
                    3,
                    226,
                    0
                ],
                "title": "Routing and Wavelength Assignment with Minimal Attack Radius for QKD\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Routing and Wavelength Assignment with Minimal Attack Radius for QKD\n  Networks"
                },
                "summary": "Quantum Key Distribution (QKD) can distribute keys with guaranteed security\nbut remains susceptible to key exchange interruption due to physical-layer\nthreats, such as high-power jamming attacks. To address this challenge, we\nfirst introduce a novel metric, namely Maximum Number of Affected Requests\n(maxNAR), to quantify the worst-case impact of a single physical-layer attack,\nand then we investigate a new problem of Routing and Wavelength Assignment with\nMinimal Attack Radius (RWA-MAR). We formulate the problem using an Integer\nLinear Programming (ILP) model and propose a scalable heuristic to efficiently\nminimize maxNAR. Our approach incorporates key caching through Quantum Key\nPools (QKPs) to enhance resilience and optimize resource utilization. Moreover,\nwe model the impact of different QKD network architectures, employing Optical\nBypass (OB) for optical switching of quantum channels and Trusted Relay (TR)\nfor secure key forwarding. Moreover, a tunable parameter is designed in the\nheuristic to guide the preference for OB or TR, offering enhanced adaptability\nand dynamic control in diverse network scenarios. Simulation results confirm\nthat our method significantly outperforms the baseline in terms of security and\nscalability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum Key Distribution (QKD) can distribute keys with guaranteed security\nbut remains susceptible to key exchange interruption due to physical-layer\nthreats, such as high-power jamming attacks. To address this challenge, we\nfirst introduce a novel metric, namely Maximum Number of Affected Requests\n(maxNAR), to quantify the worst-case impact of a single physical-layer attack,\nand then we investigate a new problem of Routing and Wavelength Assignment with\nMinimal Attack Radius (RWA-MAR). We formulate the problem using an Integer\nLinear Programming (ILP) model and propose a scalable heuristic to efficiently\nminimize maxNAR. Our approach incorporates key caching through Quantum Key\nPools (QKPs) to enhance resilience and optimize resource utilization. Moreover,\nwe model the impact of different QKD network architectures, employing Optical\nBypass (OB) for optical switching of quantum channels and Trusted Relay (TR)\nfor secure key forwarding. Moreover, a tunable parameter is designed in the\nheuristic to guide the preference for OB or TR, offering enhanced adaptability\nand dynamic control in diverse network scenarios. Simulation results confirm\nthat our method significantly outperforms the baseline in terms of security and\nscalability."
                },
                "authors": [
                    {
                        "name": "Mengyao Li"
                    },
                    {
                        "name": "Qiaolun Zhang"
                    },
                    {
                        "name": "Zongshuai Yang"
                    },
                    {
                        "name": "Stefano Bregni"
                    },
                    {
                        "name": "Alberto Gatto"
                    },
                    {
                        "name": "Raouf Boutaba"
                    },
                    {
                        "name": "Massimo Tornatore"
                    }
                ],
                "author_detail": {
                    "name": "Massimo Tornatore"
                },
                "author": "Massimo Tornatore",
                "arxiv_comment": "6 pages, this paper has been successfully accepted by GLOBECOM2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10613v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10613v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08601v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08601v3",
                "updated": "2025-08-14T10:26:51Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    10,
                    26,
                    51,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-12T03:34:21Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    3,
                    34,
                    21,
                    1,
                    224,
                    0
                ],
                "title": "Yan: Foundational Interactive Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yan: Foundational Interactive Video Generation"
                },
                "summary": "We present Yan, a foundational framework for interactive video generation,\ncovering the entire pipeline from simulation and generation to editing.\nSpecifically, Yan comprises three core modules. AAA-level Simulation: We design\na highly-compressed, low-latency 3D-VAE coupled with a KV-cache-based\nshift-window denoising inference process, achieving real-time 1080P/60FPS\ninteractive simulation. Multi-Modal Generation: We introduce a hierarchical\nautoregressive caption method that injects game-specific knowledge into\nopen-domain multi-modal video diffusion models (VDMs), then transforming the\nVDM into a frame-wise, action-controllable, real-time infinite interactive\nvideo generator. Notably, when the textual and visual prompts are sourced from\ndifferent domains, the model demonstrates strong generalization, allowing it to\nblend and compose the style and mechanics across domains flexibly according to\nuser prompts. Multi-Granularity Editing: We propose a hybrid model that\nexplicitly disentangles interactive mechanics simulation from visual rendering,\nenabling multi-granularity video content editing during interaction through\ntext. Collectively, Yan offers an integration of these modules, pushing\ninteractive video generation beyond isolated capabilities toward a\ncomprehensive AI-driven interactive creation paradigm, paving the way for the\nnext generation of creative tools, media, and entertainment. The project page\nis: https://greatx3.github.io/Yan/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Yan, a foundational framework for interactive video generation,\ncovering the entire pipeline from simulation and generation to editing.\nSpecifically, Yan comprises three core modules. AAA-level Simulation: We design\na highly-compressed, low-latency 3D-VAE coupled with a KV-cache-based\nshift-window denoising inference process, achieving real-time 1080P/60FPS\ninteractive simulation. Multi-Modal Generation: We introduce a hierarchical\nautoregressive caption method that injects game-specific knowledge into\nopen-domain multi-modal video diffusion models (VDMs), then transforming the\nVDM into a frame-wise, action-controllable, real-time infinite interactive\nvideo generator. Notably, when the textual and visual prompts are sourced from\ndifferent domains, the model demonstrates strong generalization, allowing it to\nblend and compose the style and mechanics across domains flexibly according to\nuser prompts. Multi-Granularity Editing: We propose a hybrid model that\nexplicitly disentangles interactive mechanics simulation from visual rendering,\nenabling multi-granularity video content editing during interaction through\ntext. Collectively, Yan offers an integration of these modules, pushing\ninteractive video generation beyond isolated capabilities toward a\ncomprehensive AI-driven interactive creation paradigm, paving the way for the\nnext generation of creative tools, media, and entertainment. The project page\nis: https://greatx3.github.io/Yan/."
                },
                "authors": [
                    {
                        "name": "Deheng Ye"
                    },
                    {
                        "name": "Fangyun Zhou"
                    },
                    {
                        "name": "Jiacheng Lv"
                    },
                    {
                        "name": "Jianqi Ma"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Junyan Lv"
                    },
                    {
                        "name": "Junyou Li"
                    },
                    {
                        "name": "Minwen Deng"
                    },
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Qiang Fu"
                    },
                    {
                        "name": "Wei Yang"
                    },
                    {
                        "name": "Wenkai Lv"
                    },
                    {
                        "name": "Yangbin Yu"
                    },
                    {
                        "name": "Yewen Wang"
                    },
                    {
                        "name": "Yonghang Guan"
                    },
                    {
                        "name": "Zhihao Hu"
                    },
                    {
                        "name": "Zhongbin Fang"
                    },
                    {
                        "name": "Zhongqian Sun"
                    }
                ],
                "author_detail": {
                    "name": "Zhongqian Sun"
                },
                "author": "Zhongqian Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08601v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08601v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08895v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08895v2",
                "updated": "2025-08-14T09:04:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    9,
                    4,
                    56,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-12T12:35:55Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    12,
                    35,
                    55,
                    1,
                    224,
                    0
                ],
                "title": "ASPD: Unlocking Adaptive Serial-Parallel Decoding by Exploring Intrinsic\n  Parallelism in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASPD: Unlocking Adaptive Serial-Parallel Decoding by Exploring Intrinsic\n  Parallelism in LLMs"
                },
                "summary": "The increasing scale and complexity of large language models (LLMs) pose\nsignificant inference latency challenges, primarily due to their autoregressive\ndecoding paradigm characterized by the sequential nature of next-token\nprediction. By re-examining the outputs of autoregressive models, we observed\nthat some segments exhibit parallelizable structures, which we term intrinsic\nparallelism. Decoding each parallelizable branch simultaneously (i.e. parallel\ndecoding) can significantly improve the overall inference speed of LLMs. In\nthis paper, we propose an Adaptive Serial-Parallel Decoding (ASPD), which\naddresses two core challenges: automated construction of parallelizable data\nand efficient parallel decoding mechanism. More specifically, we introduce a\nnon-invasive pipeline that automatically extracts and validates parallelizable\nstructures from the responses of autoregressive models. To empower efficient\nadaptive serial-parallel decoding, we implement a Hybrid Decoding Engine which\nenables seamless transitions between serial and parallel decoding modes while\nmaintaining a reusable KV cache, maximizing computational efficiency. Extensive\nevaluations across General Tasks, Retrieval-Augmented Generation, Mathematical\nReasoning, demonstrate that ASPD achieves unprecedented performance in both\neffectiveness and efficiency. Notably, on Vicuna Bench, our method achieves up\nto 3.19x speedup (1.85x on average) while maintaining response quality within\n1% difference compared to autoregressive models, realizing significant\nacceleration without compromising generation quality. Our framework sets a\ngroundbreaking benchmark for efficient LLM parallel inference, paving the way\nfor its deployment in latency-sensitive applications such as AI-powered\ncustomer service bots and answer retrieval engines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing scale and complexity of large language models (LLMs) pose\nsignificant inference latency challenges, primarily due to their autoregressive\ndecoding paradigm characterized by the sequential nature of next-token\nprediction. By re-examining the outputs of autoregressive models, we observed\nthat some segments exhibit parallelizable structures, which we term intrinsic\nparallelism. Decoding each parallelizable branch simultaneously (i.e. parallel\ndecoding) can significantly improve the overall inference speed of LLMs. In\nthis paper, we propose an Adaptive Serial-Parallel Decoding (ASPD), which\naddresses two core challenges: automated construction of parallelizable data\nand efficient parallel decoding mechanism. More specifically, we introduce a\nnon-invasive pipeline that automatically extracts and validates parallelizable\nstructures from the responses of autoregressive models. To empower efficient\nadaptive serial-parallel decoding, we implement a Hybrid Decoding Engine which\nenables seamless transitions between serial and parallel decoding modes while\nmaintaining a reusable KV cache, maximizing computational efficiency. Extensive\nevaluations across General Tasks, Retrieval-Augmented Generation, Mathematical\nReasoning, demonstrate that ASPD achieves unprecedented performance in both\neffectiveness and efficiency. Notably, on Vicuna Bench, our method achieves up\nto 3.19x speedup (1.85x on average) while maintaining response quality within\n1% difference compared to autoregressive models, realizing significant\nacceleration without compromising generation quality. Our framework sets a\ngroundbreaking benchmark for efficient LLM parallel inference, paving the way\nfor its deployment in latency-sensitive applications such as AI-powered\ncustomer service bots and answer retrieval engines."
                },
                "authors": [
                    {
                        "name": "Keyu Chen"
                    },
                    {
                        "name": "Zhifeng Shen"
                    },
                    {
                        "name": "Daohai Yu"
                    },
                    {
                        "name": "Haoqian Wu"
                    },
                    {
                        "name": "Wei Wen"
                    },
                    {
                        "name": "Jianfeng He"
                    },
                    {
                        "name": "Ruizhi Qiao"
                    },
                    {
                        "name": "Xing Sun"
                    }
                ],
                "author_detail": {
                    "name": "Xing Sun"
                },
                "author": "Xing Sun",
                "arxiv_comment": "20 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08895v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08895v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10431v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10431v1",
                "updated": "2025-08-14T08:04:15Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    8,
                    4,
                    15,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T08:04:15Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    8,
                    4,
                    15,
                    3,
                    226,
                    0
                ],
                "title": "Yet Another Mirage of Breaking MIRAGE: Debunking Occupancy-based\n  Side-Channel Attacks on Fully Associative Randomized Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yet Another Mirage of Breaking MIRAGE: Debunking Occupancy-based\n  Side-Channel Attacks on Fully Associative Randomized Caches"
                },
                "summary": "Recent work presented at USENIX Security 2025 claims that occupancy-based\nattacks can recover AES keys from the MIRAGE randomized cache. In this paper,\nwe examine these claims and find that they arise from fundamental modeling\nflaws. Most critically, the authors' simulation of MIRAGE uses a constant seed\nto initialize the random number generator used for global evictions in MIRAGE,\ncausing every AES encryption they trace to evict the same deterministic\nsequence of cache lines. This artificially creates a highly repeatable timing\npattern that is not representative of a realistic implementation of MIRAGE,\nwhere eviction sequences vary randomly between encryptions. When we instead\nrandomize the eviction seed for each run, reflecting realistic operation, the\ncorrelation between AES T-table accesses and attacker runtimes disappears, and\nthe attack fails. These findings show that the reported leakage is an artifact\nof incorrect modeling, and not an actual vulnerability in MIRAGE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work presented at USENIX Security 2025 claims that occupancy-based\nattacks can recover AES keys from the MIRAGE randomized cache. In this paper,\nwe examine these claims and find that they arise from fundamental modeling\nflaws. Most critically, the authors' simulation of MIRAGE uses a constant seed\nto initialize the random number generator used for global evictions in MIRAGE,\ncausing every AES encryption they trace to evict the same deterministic\nsequence of cache lines. This artificially creates a highly repeatable timing\npattern that is not representative of a realistic implementation of MIRAGE,\nwhere eviction sequences vary randomly between encryptions. When we instead\nrandomize the eviction seed for each run, reflecting realistic operation, the\ncorrelation between AES T-table accesses and attacker runtimes disappears, and\nthe attack fails. These findings show that the reported leakage is an artifact\nof incorrect modeling, and not an actual vulnerability in MIRAGE."
                },
                "authors": [
                    {
                        "name": "Chris Cao"
                    },
                    {
                        "name": "Gururaj Saileshwar"
                    }
                ],
                "author_detail": {
                    "name": "Gururaj Saileshwar"
                },
                "author": "Gururaj Saileshwar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10431v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10431v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10424v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10424v1",
                "updated": "2025-08-14T07:54:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    7,
                    54,
                    44,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T07:54:44Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    7,
                    54,
                    44,
                    3,
                    226,
                    0
                ],
                "title": "NanoControl: A Lightweight Framework for Precise and Efficient Control\n  in Diffusion Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NanoControl: A Lightweight Framework for Precise and Efficient Control\n  in Diffusion Transformer"
                },
                "summary": "Diffusion Transformers (DiTs) have demonstrated exceptional capabilities in\ntext-to-image synthesis. However, in the domain of controllable text-to-image\ngeneration using DiTs, most existing methods still rely on the ControlNet\nparadigm originally designed for UNet-based diffusion models. This paradigm\nintroduces significant parameter overhead and increased computational costs. To\naddress these challenges, we propose the Nano Control Diffusion Transformer\n(NanoControl), which employs Flux as the backbone network. Our model achieves\nstate-of-the-art controllable text-to-image generation performance while\nincurring only a 0.024\\% increase in parameter count and a 0.029\\% increase in\nGFLOPs, thus enabling highly efficient controllable generation. Specifically,\nrather than duplicating the DiT backbone for control, we design a LoRA-style\n(low-rank adaptation) control module that directly learns control signals from\nraw conditioning inputs. Furthermore, we introduce a KV-Context Augmentation\nmechanism that integrates condition-specific key-value information into the\nbackbone in a simple yet highly effective manner, facilitating deep fusion of\nconditional features. Extensive benchmark experiments demonstrate that\nNanoControl significantly reduces computational overhead compared to\nconventional control approaches, while maintaining superior generation quality\nand achieving improved controllability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have demonstrated exceptional capabilities in\ntext-to-image synthesis. However, in the domain of controllable text-to-image\ngeneration using DiTs, most existing methods still rely on the ControlNet\nparadigm originally designed for UNet-based diffusion models. This paradigm\nintroduces significant parameter overhead and increased computational costs. To\naddress these challenges, we propose the Nano Control Diffusion Transformer\n(NanoControl), which employs Flux as the backbone network. Our model achieves\nstate-of-the-art controllable text-to-image generation performance while\nincurring only a 0.024\\% increase in parameter count and a 0.029\\% increase in\nGFLOPs, thus enabling highly efficient controllable generation. Specifically,\nrather than duplicating the DiT backbone for control, we design a LoRA-style\n(low-rank adaptation) control module that directly learns control signals from\nraw conditioning inputs. Furthermore, we introduce a KV-Context Augmentation\nmechanism that integrates condition-specific key-value information into the\nbackbone in a simple yet highly effective manner, facilitating deep fusion of\nconditional features. Extensive benchmark experiments demonstrate that\nNanoControl significantly reduces computational overhead compared to\nconventional control approaches, while maintaining superior generation quality\nand achieving improved controllability."
                },
                "authors": [
                    {
                        "name": "Shanyuan Liu"
                    },
                    {
                        "name": "Jian Zhu"
                    },
                    {
                        "name": "Junda Lu"
                    },
                    {
                        "name": "Yue Gong"
                    },
                    {
                        "name": "Liuzhuozheng Li"
                    },
                    {
                        "name": "Bo Cheng"
                    },
                    {
                        "name": "Yuhang Ma"
                    },
                    {
                        "name": "Liebucha Wu"
                    },
                    {
                        "name": "Xiaoyu Wu"
                    },
                    {
                        "name": "Dawei Leng"
                    },
                    {
                        "name": "Yuhui Yin"
                    }
                ],
                "author_detail": {
                    "name": "Yuhui Yin"
                },
                "author": "Yuhui Yin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10424v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10424v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10395v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10395v1",
                "updated": "2025-08-14T06:52:38Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    6,
                    52,
                    38,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T06:52:38Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    6,
                    52,
                    38,
                    3,
                    226,
                    0
                ],
                "title": "XQuant: Breaking the Memory Wall for LLM Inference with KV Cache\n  Rematerialization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XQuant: Breaking the Memory Wall for LLM Inference with KV Cache\n  Rematerialization"
                },
                "summary": "Although LLM inference has emerged as a critical workload for many downstream\napplications, efficiently inferring LLMs is challenging due to the substantial\nmemory footprint and bandwidth requirements. In parallel, compute capabilities\nhave steadily outpaced both memory capacity and bandwidth over the last few\ndecades, a trend that remains evident in modern GPU hardware and exacerbates\nthe challenge of LLM inference. As such, new algorithms are emerging that trade\nincreased computation for reduced memory operations. To that end, we present\nXQuant, which takes advantage of this trend, enabling an order-of-magnitude\nreduction in memory consumption through low-bit quantization with substantial\naccuracy benefits relative to state-of-the-art KV cache quantization methods.\nWe accomplish this by quantizing and caching the layer input activations X,\ninstead of using standard KV caching, and then rematerializing the Keys and\nValues on-the-fly during inference. This results in an immediate 2$\\times$\nmemory savings compared to KV caching. By applying XQuant, we achieve up to\n$\\sim 7.7\\times$ memory savings with $<0.1$ perplexity degradation compared to\nthe FP16 baseline. Furthermore, our approach leverages the fact that X values\nare similar across layers. Building on this observation, we introduce\nXQuant-CL, which exploits the cross-layer similarity in the X embeddings for\nextreme compression. Across different models, XQuant-CL attains up to\n10$\\times$ memory savings relative to the FP16 baseline with only 0.01\nperplexity degradation, and 12.5$\\times$ memory savings with only $0.1$\nperplexity degradation. XQuant exploits the rapidly increasing compute\ncapabilities of hardware platforms to eliminate the memory bottleneck, while\nsurpassing state-of-the-art KV cache quantization methods and achieving\nnear-FP16 accuracy across a wide range of models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although LLM inference has emerged as a critical workload for many downstream\napplications, efficiently inferring LLMs is challenging due to the substantial\nmemory footprint and bandwidth requirements. In parallel, compute capabilities\nhave steadily outpaced both memory capacity and bandwidth over the last few\ndecades, a trend that remains evident in modern GPU hardware and exacerbates\nthe challenge of LLM inference. As such, new algorithms are emerging that trade\nincreased computation for reduced memory operations. To that end, we present\nXQuant, which takes advantage of this trend, enabling an order-of-magnitude\nreduction in memory consumption through low-bit quantization with substantial\naccuracy benefits relative to state-of-the-art KV cache quantization methods.\nWe accomplish this by quantizing and caching the layer input activations X,\ninstead of using standard KV caching, and then rematerializing the Keys and\nValues on-the-fly during inference. This results in an immediate 2$\\times$\nmemory savings compared to KV caching. By applying XQuant, we achieve up to\n$\\sim 7.7\\times$ memory savings with $<0.1$ perplexity degradation compared to\nthe FP16 baseline. Furthermore, our approach leverages the fact that X values\nare similar across layers. Building on this observation, we introduce\nXQuant-CL, which exploits the cross-layer similarity in the X embeddings for\nextreme compression. Across different models, XQuant-CL attains up to\n10$\\times$ memory savings relative to the FP16 baseline with only 0.01\nperplexity degradation, and 12.5$\\times$ memory savings with only $0.1$\nperplexity degradation. XQuant exploits the rapidly increasing compute\ncapabilities of hardware platforms to eliminate the memory bottleneck, while\nsurpassing state-of-the-art KV cache quantization methods and achieving\nnear-FP16 accuracy across a wide range of models."
                },
                "authors": [
                    {
                        "name": "Aditya Tomar"
                    },
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Minjae Lee"
                    },
                    {
                        "name": "Haocheng Xi"
                    },
                    {
                        "name": "Rishabh Tiwari"
                    },
                    {
                        "name": "Wonjun Kang"
                    },
                    {
                        "name": "Luca Manolache"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "arxiv_comment": "24 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10395v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10395v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14051v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14051v3",
                "updated": "2025-08-13T17:55:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    17,
                    55,
                    58,
                    2,
                    225,
                    0
                ],
                "published": "2025-02-19T19:12:46Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    19,
                    12,
                    46,
                    2,
                    50,
                    0
                ],
                "title": "RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache\n  Compression"
                },
                "summary": "Transformer-based Large Language Models rely critically on the KV cache to\nefficiently handle extended contexts during the decode phase. Yet, the size of\nthe KV cache grows proportionally with the input length, burdening both memory\nbandwidth and capacity as decoding progresses. To address this challenge, we\npresent RocketKV, a training-free KV cache compression strategy containing two\nconsecutive stages. In the first stage, it performs coarse-grain permanent KV\ncache eviction on the input sequence tokens. In the second stage, it adopts a\nhybrid sparse attention method to conduct fine-grain top-k sparse attention,\napproximating the attention scores by leveraging both head and sequence\ndimensionality reductions. We show that RocketKV provides a compression ratio\nof up to 400$\\times$, end-to-end speedup of up to 3.7$\\times$ as well as peak\nmemory reduction of up to 32.6% in the decode phase on an NVIDIA A100 GPU\ncompared to the full KV cache baseline, while achieving negligible accuracy\nloss on a variety of long-context tasks. We also propose a variant of RocketKV\nfor multi-turn scenarios, which consistently outperforms other existing methods\nand achieves accuracy nearly on par with an oracle top-k attention scheme. The\nsource code is available here: https://github.com/NVlabs/RocketKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based Large Language Models rely critically on the KV cache to\nefficiently handle extended contexts during the decode phase. Yet, the size of\nthe KV cache grows proportionally with the input length, burdening both memory\nbandwidth and capacity as decoding progresses. To address this challenge, we\npresent RocketKV, a training-free KV cache compression strategy containing two\nconsecutive stages. In the first stage, it performs coarse-grain permanent KV\ncache eviction on the input sequence tokens. In the second stage, it adopts a\nhybrid sparse attention method to conduct fine-grain top-k sparse attention,\napproximating the attention scores by leveraging both head and sequence\ndimensionality reductions. We show that RocketKV provides a compression ratio\nof up to 400$\\times$, end-to-end speedup of up to 3.7$\\times$ as well as peak\nmemory reduction of up to 32.6% in the decode phase on an NVIDIA A100 GPU\ncompared to the full KV cache baseline, while achieving negligible accuracy\nloss on a variety of long-context tasks. We also propose a variant of RocketKV\nfor multi-turn scenarios, which consistently outperforms other existing methods\nand achieves accuracy nearly on par with an oracle top-k attention scheme. The\nsource code is available here: https://github.com/NVlabs/RocketKV."
                },
                "authors": [
                    {
                        "name": "Payman Behnam"
                    },
                    {
                        "name": "Yaosheng Fu"
                    },
                    {
                        "name": "Ritchie Zhao"
                    },
                    {
                        "name": "Po-An Tsai"
                    },
                    {
                        "name": "Zhiding Yu"
                    },
                    {
                        "name": "Alexey Tumanov"
                    }
                ],
                "author_detail": {
                    "name": "Alexey Tumanov"
                },
                "author": "Alexey Tumanov",
                "arxiv_comment": "ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14051v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14051v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09822v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09822v1",
                "updated": "2025-08-13T13:54:51Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    13,
                    54,
                    51,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T13:54:51Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    13,
                    54,
                    51,
                    2,
                    225,
                    0
                ],
                "title": "Physical Autoregressive Model for Robotic Manipulation without Action\n  Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physical Autoregressive Model for Robotic Manipulation without Action\n  Pretraining"
                },
                "summary": "The scarcity of manipulation data has motivated the use of pretrained large\nmodels from other modalities in robotics. In this work, we build upon\nautoregressive video generation models to propose a Physical Autoregressive\nModel (PAR), where physical tokens combine frames and actions to represent the\njoint evolution of the robot and its environment. PAR leverages the world\nknowledge embedded in video pretraining to understand physical dynamics without\nrequiring action pretraining, enabling accurate video prediction and consistent\naction trajectories. It also adopts a DiT-based de-tokenizer to model frames\nand actions as continuous tokens, mitigating quantization errors and\nfacilitating mutual enhancement. Furthermore, we incorporate a causal mask with\ninverse kinematics, parallel training, and the KV-cache mechanism to further\nimprove performance and efficiency. Experiments on the ManiSkill benchmark show\nthat PAR achieves a 100\\% success rate on the PushCube task, matches the\nperformance of action-pretrained baselines on other tasks, and accurately\npredicts future videos with tightly aligned action trajectories. These findings\nunderscore a promising direction for robotic manipulation by transferring world\nknowledge from autoregressive video pretraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The scarcity of manipulation data has motivated the use of pretrained large\nmodels from other modalities in robotics. In this work, we build upon\nautoregressive video generation models to propose a Physical Autoregressive\nModel (PAR), where physical tokens combine frames and actions to represent the\njoint evolution of the robot and its environment. PAR leverages the world\nknowledge embedded in video pretraining to understand physical dynamics without\nrequiring action pretraining, enabling accurate video prediction and consistent\naction trajectories. It also adopts a DiT-based de-tokenizer to model frames\nand actions as continuous tokens, mitigating quantization errors and\nfacilitating mutual enhancement. Furthermore, we incorporate a causal mask with\ninverse kinematics, parallel training, and the KV-cache mechanism to further\nimprove performance and efficiency. Experiments on the ManiSkill benchmark show\nthat PAR achieves a 100\\% success rate on the PushCube task, matches the\nperformance of action-pretrained baselines on other tasks, and accurately\npredicts future videos with tightly aligned action trajectories. These findings\nunderscore a promising direction for robotic manipulation by transferring world\nknowledge from autoregressive video pretraining."
                },
                "authors": [
                    {
                        "name": "Zijian Song"
                    },
                    {
                        "name": "Sihan Qin"
                    },
                    {
                        "name": "Tianshui Chen"
                    },
                    {
                        "name": "Liang Lin"
                    },
                    {
                        "name": "Guangrun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Guangrun Wang"
                },
                "author": "Guangrun Wang",
                "arxiv_comment": "16 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09822v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09822v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09570v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09570v1",
                "updated": "2025-08-13T07:40:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    7,
                    40,
                    25,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T07:40:25Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    7,
                    40,
                    25,
                    2,
                    225,
                    0
                ],
                "title": "Re-thinking Memory-Bound Limitations in CGRAs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Re-thinking Memory-Bound Limitations in CGRAs"
                },
                "summary": "Coarse-Grained Reconfigurable Arrays (CGRAs) are specialized accelerators\ncommonly employed to boost performance in workloads with iterative structures.\nExisting research typically focuses on compiler or architecture optimizations\naimed at improving CGRA performance, energy efficiency, flexibility, and area\nutilization, under the idealistic assumption that kernels can access all data\nfrom Scratchpad Memory (SPM). However, certain complex workloads-particularly\nin fields like graph analytics, irregular database operations, and specialized\nforms of high-performance computing (e.g., unstructured mesh\nsimulations)-exhibit irregular memory access patterns that hinder CGRA\nutilization, sometimes dropping below 1.5%, making the CGRA memory-bound. To\naddress this challenge, we conduct a thorough analysis of the underlying causes\nof performance degradation, then propose a redesigned memory subsystem and\nrefine the memory model. With both microarchitectural and theoretical\noptimization, our solution can effectively manage irregular memory accesses\nthrough CGRA-specific runahead execution mechanism and cache reconfiguration\ntechniques. Our results demonstrate that we can achieve performance comparable\nto the original SPM-only system while requiring only 1.27% of the storage size.\nThe runahead execution mechanism achieves an average 3.04x speedup (up to\n6.91x), with cache reconfiguration technique providing an additional 6.02%\nimprovement, significantly enhancing CGRA performance for irregular memory\naccess patterns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coarse-Grained Reconfigurable Arrays (CGRAs) are specialized accelerators\ncommonly employed to boost performance in workloads with iterative structures.\nExisting research typically focuses on compiler or architecture optimizations\naimed at improving CGRA performance, energy efficiency, flexibility, and area\nutilization, under the idealistic assumption that kernels can access all data\nfrom Scratchpad Memory (SPM). However, certain complex workloads-particularly\nin fields like graph analytics, irregular database operations, and specialized\nforms of high-performance computing (e.g., unstructured mesh\nsimulations)-exhibit irregular memory access patterns that hinder CGRA\nutilization, sometimes dropping below 1.5%, making the CGRA memory-bound. To\naddress this challenge, we conduct a thorough analysis of the underlying causes\nof performance degradation, then propose a redesigned memory subsystem and\nrefine the memory model. With both microarchitectural and theoretical\noptimization, our solution can effectively manage irregular memory accesses\nthrough CGRA-specific runahead execution mechanism and cache reconfiguration\ntechniques. Our results demonstrate that we can achieve performance comparable\nto the original SPM-only system while requiring only 1.27% of the storage size.\nThe runahead execution mechanism achieves an average 3.04x speedup (up to\n6.91x), with cache reconfiguration technique providing an additional 6.02%\nimprovement, significantly enhancing CGRA performance for irregular memory\naccess patterns."
                },
                "authors": [
                    {
                        "name": "Xiangfeng Liu"
                    },
                    {
                        "name": "Zhe Jiang"
                    },
                    {
                        "name": "Anzhen Zhu"
                    },
                    {
                        "name": "Xiaomeng Han"
                    },
                    {
                        "name": "Mingsong Lyu"
                    },
                    {
                        "name": "Qingxu Deng"
                    },
                    {
                        "name": "Nan Guan"
                    }
                ],
                "author_detail": {
                    "name": "Nan Guan"
                },
                "author": "Nan Guan",
                "arxiv_doi": "10.1145/3760386",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3760386",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.09570v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09570v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "25 pages, 18 figures, CODES+ISSS 2025",
                "arxiv_journal_ref": "ACM Transactions on Embedded Computing Systems 2025",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.3.0; B.6.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00279v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00279v4",
                "updated": "2025-08-13T06:13:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    6,
                    13,
                    36,
                    2,
                    225,
                    0
                ],
                "published": "2024-12-31T05:24:30Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    5,
                    24,
                    30,
                    1,
                    366,
                    0
                ],
                "title": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement"
                },
                "summary": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace."
                },
                "authors": [
                    {
                        "name": "Junjie Li"
                    }
                ],
                "author_detail": {
                    "name": "Junjie Li"
                },
                "author": "Junjie Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00279v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00279v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.09500v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.09500v2",
                "updated": "2025-08-13T04:24:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    4,
                    24,
                    56,
                    2,
                    225,
                    0
                ],
                "published": "2025-07-13T05:37:33Z",
                "published_parsed": [
                    2025,
                    7,
                    13,
                    5,
                    37,
                    33,
                    6,
                    194,
                    0
                ],
                "title": "Advancing Reliable Test-Time Adaptation of Vision-Language Models under\n  Visual Variations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Reliable Test-Time Adaptation of Vision-Language Models under\n  Visual Variations"
                },
                "summary": "Vision-language models (VLMs) exhibit remarkable zero-shot capabilities but\nstruggle with distribution shifts in downstream tasks when labeled data is\nunavailable, which has motivated the development of Test-Time Adaptation (TTA)\nto improve VLMs' performance during inference without annotations. Among\nvarious TTA approaches, cache-based methods show promise by preserving\nhistorical knowledge from low-entropy samples in a dynamic cache and fostering\nefficient adaptation. However, these methods face two critical reliability\nchallenges: (1) entropy often becomes unreliable under distribution shifts,\ncausing error accumulation in the cache and degradation in adaptation\nperformance; (2) the final predictions may be unreliable due to inflexible\ndecision boundaries that fail to accommodate large downstream shifts. To\naddress these challenges, we propose a Reliable Test-time Adaptation (ReTA)\nmethod that integrates two complementary strategies to enhance reliability from\ntwo perspectives. First, to mitigate the unreliability of entropy as a sample\nselection criterion for cache construction, we introduce Consistency-aware\nEntropy Reweighting (CER), which incorporates consistency constraints to weight\nentropy during cache updating. While conventional approaches rely solely on low\nentropy for cache prioritization and risk introducing noise, our method\nleverages predictive consistency to maintain a high-quality cache and\nfacilitate more robust adaptation. Second, we present Diversity-driven\nDistribution Calibration (DDC), which models class-wise text embeddings as\nmultivariate Gaussian distributions, enabling adaptive decision boundaries for\nmore accurate predictions across visually diverse content. Extensive\nexperiments demonstrate that ReTA consistently outperforms state-of-the-art\nmethods, particularly under real-world distribution shifts. Code:\nhttps://github.com/Evelyn1ywliang/ReTA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) exhibit remarkable zero-shot capabilities but\nstruggle with distribution shifts in downstream tasks when labeled data is\nunavailable, which has motivated the development of Test-Time Adaptation (TTA)\nto improve VLMs' performance during inference without annotations. Among\nvarious TTA approaches, cache-based methods show promise by preserving\nhistorical knowledge from low-entropy samples in a dynamic cache and fostering\nefficient adaptation. However, these methods face two critical reliability\nchallenges: (1) entropy often becomes unreliable under distribution shifts,\ncausing error accumulation in the cache and degradation in adaptation\nperformance; (2) the final predictions may be unreliable due to inflexible\ndecision boundaries that fail to accommodate large downstream shifts. To\naddress these challenges, we propose a Reliable Test-time Adaptation (ReTA)\nmethod that integrates two complementary strategies to enhance reliability from\ntwo perspectives. First, to mitigate the unreliability of entropy as a sample\nselection criterion for cache construction, we introduce Consistency-aware\nEntropy Reweighting (CER), which incorporates consistency constraints to weight\nentropy during cache updating. While conventional approaches rely solely on low\nentropy for cache prioritization and risk introducing noise, our method\nleverages predictive consistency to maintain a high-quality cache and\nfacilitate more robust adaptation. Second, we present Diversity-driven\nDistribution Calibration (DDC), which models class-wise text embeddings as\nmultivariate Gaussian distributions, enabling adaptive decision boundaries for\nmore accurate predictions across visually diverse content. Extensive\nexperiments demonstrate that ReTA consistently outperforms state-of-the-art\nmethods, particularly under real-world distribution shifts. Code:\nhttps://github.com/Evelyn1ywliang/ReTA."
                },
                "authors": [
                    {
                        "name": "Yiwen Liang"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Yizhe Xiong"
                    },
                    {
                        "name": "Zihan Zhou"
                    },
                    {
                        "name": "Mengyao Lyu"
                    },
                    {
                        "name": "Zijia Lin"
                    },
                    {
                        "name": "Shuaicheng Niu"
                    },
                    {
                        "name": "Sicheng Zhao"
                    },
                    {
                        "name": "Jungong Han"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "arxiv_comment": "Accepted at the 33rd ACM International Conference on Multimedia(ACM\n  MM 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.09500v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.09500v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20002v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20002v4",
                "updated": "2025-08-13T04:03:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    4,
                    3,
                    10,
                    2,
                    225,
                    0
                ],
                "published": "2024-09-30T06:55:00Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    6,
                    55,
                    0,
                    0,
                    274,
                    0
                ],
                "title": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems"
                },
                "summary": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats."
                },
                "authors": [
                    {
                        "name": "Linke Song"
                    },
                    {
                        "name": "Zixuan Pang"
                    },
                    {
                        "name": "Wenhao Wang"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "XiaoFeng Wang"
                    },
                    {
                        "name": "Hongbo Chen"
                    },
                    {
                        "name": "Wei Song"
                    },
                    {
                        "name": "Yier Jin"
                    },
                    {
                        "name": "Dan Meng"
                    },
                    {
                        "name": "Rui Hou"
                    }
                ],
                "author_detail": {
                    "name": "Rui Hou"
                },
                "author": "Rui Hou",
                "arxiv_comment": "This work was submitted for review on Sept. 5, 2024, and the initial\n  version was uploaded to Arxiv on Sept. 30, 2024. The latest version reflects\n  the up-to-date experimental results",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20002v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20002v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09442v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09442v1",
                "updated": "2025-08-13T02:48:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    2,
                    48,
                    25,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T02:48:25Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    2,
                    48,
                    25,
                    2,
                    225,
                    0
                ],
                "title": "Shadow in the Cache: Unveiling and Mitigating Privacy Risks of KV-cache\n  in LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shadow in the Cache: Unveiling and Mitigating Privacy Risks of KV-cache\n  in LLM Inference"
                },
                "summary": "The Key-Value (KV) cache, which stores intermediate attention computations\n(Key and Value pairs) to avoid redundant calculations, is a fundamental\nmechanism for accelerating Large Language Model (LLM) inference. However, this\nefficiency optimization introduces significant yet underexplored privacy risks.\nThis paper provides the first comprehensive analysis of these vulnerabilities,\ndemonstrating that an attacker can reconstruct sensitive user inputs directly\nfrom the KV-cache. We design and implement three distinct attack vectors: a\ndirect Inversion Attack, a more broadly applicable and potent Collision Attack,\nand a semantic-based Injection Attack. These methods demonstrate the\npracticality and severity of KV-cache privacy leakage issues. To mitigate this,\nwe propose KV-Cloak, a novel, lightweight, and efficient defense mechanism.\nKV-Cloak uses a reversible matrix-based obfuscation scheme, combined with\noperator fusion, to secure the KV-cache. Our extensive experiments show that\nKV-Cloak effectively thwarts all proposed attacks, reducing reconstruction\nquality to random noise. Crucially, it achieves this robust security with\nvirtually no degradation in model accuracy and minimal performance overhead,\noffering a practical solution for trustworthy LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Key-Value (KV) cache, which stores intermediate attention computations\n(Key and Value pairs) to avoid redundant calculations, is a fundamental\nmechanism for accelerating Large Language Model (LLM) inference. However, this\nefficiency optimization introduces significant yet underexplored privacy risks.\nThis paper provides the first comprehensive analysis of these vulnerabilities,\ndemonstrating that an attacker can reconstruct sensitive user inputs directly\nfrom the KV-cache. We design and implement three distinct attack vectors: a\ndirect Inversion Attack, a more broadly applicable and potent Collision Attack,\nand a semantic-based Injection Attack. These methods demonstrate the\npracticality and severity of KV-cache privacy leakage issues. To mitigate this,\nwe propose KV-Cloak, a novel, lightweight, and efficient defense mechanism.\nKV-Cloak uses a reversible matrix-based obfuscation scheme, combined with\noperator fusion, to secure the KV-cache. Our extensive experiments show that\nKV-Cloak effectively thwarts all proposed attacks, reducing reconstruction\nquality to random noise. Crucially, it achieves this robust security with\nvirtually no degradation in model accuracy and minimal performance overhead,\noffering a practical solution for trustworthy LLM deployment."
                },
                "authors": [
                    {
                        "name": "Zhifan Luo"
                    },
                    {
                        "name": "Shuo Shao"
                    },
                    {
                        "name": "Su Zhang"
                    },
                    {
                        "name": "Lijing Zhou"
                    },
                    {
                        "name": "Yuke Hu"
                    },
                    {
                        "name": "Chenxu Zhao"
                    },
                    {
                        "name": "Zhihao Liu"
                    },
                    {
                        "name": "Zhan Qin"
                    }
                ],
                "author_detail": {
                    "name": "Zhan Qin"
                },
                "author": "Zhan Qin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09442v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09442v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09419v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09419v1",
                "updated": "2025-08-13T01:39:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    1,
                    39,
                    9,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T01:39:09Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    1,
                    39,
                    9,
                    2,
                    225,
                    0
                ],
                "title": "Design and Simulation of 6T SRAM Array",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design and Simulation of 6T SRAM Array"
                },
                "summary": "Conventional 6T SRAM is used in microprocessors in the cache memory design.\nThe basic 6T SRAM cell and a 6 bit memory array layout are designed in LEdit.\nThe design and analysis of key SRAM components, sense amplifiers, decoders,\nwrite drivers and precharge circuits are also provided. The pulse voltage\nwaveforms generated for read and write operations as well as Q and Qbar nodes\nare simulated in LTSpice. Parasitic capacitances are extracted and their impact\non the waveforms analyzed. Static noise margin, propagation delays, and power\ndissipation are calculated. Comparison of SRAM read and write operational\nperformance using CMOS transistors is made with edge-triggered D flip flops. If\ncertain size area and ratio constraints are satisfied, the 6T cell with CMOS\ntransistors will possess stability, speed, and power efficiency. Both\ntheoretical and simulated results are given.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional 6T SRAM is used in microprocessors in the cache memory design.\nThe basic 6T SRAM cell and a 6 bit memory array layout are designed in LEdit.\nThe design and analysis of key SRAM components, sense amplifiers, decoders,\nwrite drivers and precharge circuits are also provided. The pulse voltage\nwaveforms generated for read and write operations as well as Q and Qbar nodes\nare simulated in LTSpice. Parasitic capacitances are extracted and their impact\non the waveforms analyzed. Static noise margin, propagation delays, and power\ndissipation are calculated. Comparison of SRAM read and write operational\nperformance using CMOS transistors is made with edge-triggered D flip flops. If\ncertain size area and ratio constraints are satisfied, the 6T cell with CMOS\ntransistors will possess stability, speed, and power efficiency. Both\ntheoretical and simulated results are given."
                },
                "authors": [
                    {
                        "name": "Justin London"
                    }
                ],
                "author_detail": {
                    "name": "Justin London"
                },
                "author": "Justin London",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09419v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09419v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08744v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08744v2",
                "updated": "2025-08-13T01:39:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    1,
                    39,
                    3,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-12T08:39:32Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    8,
                    39,
                    32,
                    1,
                    224,
                    0
                ],
                "title": "Scalable Graph Indexing using GPUs for Approximate Nearest Neighbor\n  Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable Graph Indexing using GPUs for Approximate Nearest Neighbor\n  Search"
                },
                "summary": "Approximate nearest neighbor search (ANNS) in high-dimensional vector spaces\nhas a wide range of real-world applications. Numerous methods have been\nproposed to handle ANNS efficiently, while graph-based indexes have gained\nprominence due to their high accuracy and efficiency. However, the indexing\noverhead of graph-based indexes remains substantial. With exponential growth in\ndata volume and increasing demands for dynamic index adjustments, this overhead\ncontinues to escalate, posing a critical challenge. In this paper, we introduce\nTagore, a fast library accelerated by GPUs for graph indexing, which has\npowerful capabilities of constructing refinement-based graph indexes such as\nNSG and Vamana. We first introduce GNN-Descent, a GPU-specific algorithm for\nefficient k-Nearest Neighbor (k-NN) graph initialization. GNN-Descent speeds up\nthe similarity comparison by a two-phase descent procedure and enables highly\nparallelized neighbor updates. Next, aiming to support various k-NN graph\npruning strategies, we formulate a universal computing procedure termed CFS and\ndevise two generalized GPU kernels for parallel processing complex dependencies\nin neighbor relationships. For large-scale datasets exceeding GPU memory\ncapacity, we propose an asynchronous GPU-CPU-disk indexing framework with a\ncluster-aware caching mechanism to minimize the I/O pressure on the disk.\nExtensive experiments on 7 real-world datasets exhibit that Tagore achieves\n1.32x-112.79x speedup while maintaining the index quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximate nearest neighbor search (ANNS) in high-dimensional vector spaces\nhas a wide range of real-world applications. Numerous methods have been\nproposed to handle ANNS efficiently, while graph-based indexes have gained\nprominence due to their high accuracy and efficiency. However, the indexing\noverhead of graph-based indexes remains substantial. With exponential growth in\ndata volume and increasing demands for dynamic index adjustments, this overhead\ncontinues to escalate, posing a critical challenge. In this paper, we introduce\nTagore, a fast library accelerated by GPUs for graph indexing, which has\npowerful capabilities of constructing refinement-based graph indexes such as\nNSG and Vamana. We first introduce GNN-Descent, a GPU-specific algorithm for\nefficient k-Nearest Neighbor (k-NN) graph initialization. GNN-Descent speeds up\nthe similarity comparison by a two-phase descent procedure and enables highly\nparallelized neighbor updates. Next, aiming to support various k-NN graph\npruning strategies, we formulate a universal computing procedure termed CFS and\ndevise two generalized GPU kernels for parallel processing complex dependencies\nin neighbor relationships. For large-scale datasets exceeding GPU memory\ncapacity, we propose an asynchronous GPU-CPU-disk indexing framework with a\ncluster-aware caching mechanism to minimize the I/O pressure on the disk.\nExtensive experiments on 7 real-world datasets exhibit that Tagore achieves\n1.32x-112.79x speedup while maintaining the index quality."
                },
                "authors": [
                    {
                        "name": "Zhonggen Li"
                    },
                    {
                        "name": "Xiangyu Ke"
                    },
                    {
                        "name": "Yifan Zhu"
                    },
                    {
                        "name": "Bocheng Yu"
                    },
                    {
                        "name": "Baihua Zheng"
                    },
                    {
                        "name": "Yunjun Gao"
                    }
                ],
                "author_detail": {
                    "name": "Yunjun Gao"
                },
                "author": "Yunjun Gao",
                "arxiv_comment": "Accepted at SIGMOD 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08744v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08744v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09262v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09262v1",
                "updated": "2025-08-12T18:05:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    18,
                    5,
                    33,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T18:05:33Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    18,
                    5,
                    33,
                    1,
                    224,
                    0
                ],
                "title": "Harnessing Input-Adaptive Inference for Efficient VLN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing Input-Adaptive Inference for Efficient VLN"
                },
                "summary": "An emerging paradigm in vision-and-language navigation (VLN) is the use of\nhistory-aware multi-modal transformer models. Given a language instruction,\nthese models process observation and navigation history to predict the most\nappropriate action for an agent. While they have significantly improved\nperformance, the scale of these models can be a bottleneck in practical\nsettings with limited computational resources. In this work, we propose a novel\ninput-adaptive navigation method to enhance VLN model efficiency. We first show\nthat existing input-adaptive mechanisms fail to reduce computations without\nsubstantial performance degradation. To address this, we introduce three\nadaptive algorithms, each deployed at a different level: (1) To improve spatial\nefficiency, we selectively process panoramic views at each observation of an\nagent. (2) To improve intra-model efficiency, we propose importance-based\nadaptive thresholding for the early-exit methods. (3) To improve temporal\nefficiency, we implement a caching mechanism that prevents reprocessing of\nviews previously seen by the agent. In evaluations on seven VLN benchmarks, we\ndemonstrate over a 2$\\times$ reduction in computation across three\noff-the-shelf agents in both standard and continuous environments. Our code is\npublicly available at\nhttps://github.com/secure-ai-systems-group/adaptive-vision-and-language-navigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An emerging paradigm in vision-and-language navigation (VLN) is the use of\nhistory-aware multi-modal transformer models. Given a language instruction,\nthese models process observation and navigation history to predict the most\nappropriate action for an agent. While they have significantly improved\nperformance, the scale of these models can be a bottleneck in practical\nsettings with limited computational resources. In this work, we propose a novel\ninput-adaptive navigation method to enhance VLN model efficiency. We first show\nthat existing input-adaptive mechanisms fail to reduce computations without\nsubstantial performance degradation. To address this, we introduce three\nadaptive algorithms, each deployed at a different level: (1) To improve spatial\nefficiency, we selectively process panoramic views at each observation of an\nagent. (2) To improve intra-model efficiency, we propose importance-based\nadaptive thresholding for the early-exit methods. (3) To improve temporal\nefficiency, we implement a caching mechanism that prevents reprocessing of\nviews previously seen by the agent. In evaluations on seven VLN benchmarks, we\ndemonstrate over a 2$\\times$ reduction in computation across three\noff-the-shelf agents in both standard and continuous environments. Our code is\npublicly available at\nhttps://github.com/secure-ai-systems-group/adaptive-vision-and-language-navigation."
                },
                "authors": [
                    {
                        "name": "Dongwoo Kang"
                    },
                    {
                        "name": "Akhil Perincherry"
                    },
                    {
                        "name": "Zachary Coalson"
                    },
                    {
                        "name": "Aiden Gabriel"
                    },
                    {
                        "name": "Stefan Lee"
                    },
                    {
                        "name": "Sanghyun Hong"
                    }
                ],
                "author_detail": {
                    "name": "Sanghyun Hong"
                },
                "author": "Sanghyun Hong",
                "arxiv_comment": "Accepted to ICCV 2025 [Poster]",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09262v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09262v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09072v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09072v1",
                "updated": "2025-08-12T16:47:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    16,
                    47,
                    48,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T16:47:48Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    16,
                    47,
                    48,
                    1,
                    224,
                    0
                ],
                "title": "READER: Retrieval-Assisted Drafter for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "READER: Retrieval-Assisted Drafter for Efficient LLM Inference"
                },
                "summary": "Large Language Models (LLMs) generate tokens autoregressively, with each\ntoken depending on the preceding context. This sequential nature makes the\ninference process inherently difficult to accelerate, posing a significant\nchallenge for efficient deployment. In recent years, various methods have been\nproposed to address this issue, with the most effective approaches often\ninvolving the training of additional draft models. In this paper, we introduce\nREADER (Retrieval-Assisted Drafter for Efficient LLM Inference), a novel\nlossless speculative decoding method that enhances model-based approaches by\nleveraging self-repetitions in the text. Our algorithm expands the speculative\ndecoding tree using tokens obtained through statistical search. This work\nfocuses on large batch sizes (>= 8), an underexplored yet important area for\nindustrial applications. We also analyze the key-value (KV) cache size during\nspeculative decoding and propose an optimization to improve performance for\nlarge batches. As a result, READER outperforms existing speculative decoding\nmethods. Notably, READER requires no additional training and can reuse\npre-trained speculator models, increasing the speedup by over 40\\%. Our method\ndemonstrates particularly strong performance on search-based tasks, such as\nretrieval-augmented generation, where we achieve more than 10x speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) generate tokens autoregressively, with each\ntoken depending on the preceding context. This sequential nature makes the\ninference process inherently difficult to accelerate, posing a significant\nchallenge for efficient deployment. In recent years, various methods have been\nproposed to address this issue, with the most effective approaches often\ninvolving the training of additional draft models. In this paper, we introduce\nREADER (Retrieval-Assisted Drafter for Efficient LLM Inference), a novel\nlossless speculative decoding method that enhances model-based approaches by\nleveraging self-repetitions in the text. Our algorithm expands the speculative\ndecoding tree using tokens obtained through statistical search. This work\nfocuses on large batch sizes (>= 8), an underexplored yet important area for\nindustrial applications. We also analyze the key-value (KV) cache size during\nspeculative decoding and propose an optimization to improve performance for\nlarge batches. As a result, READER outperforms existing speculative decoding\nmethods. Notably, READER requires no additional training and can reuse\npre-trained speculator models, increasing the speedup by over 40\\%. Our method\ndemonstrates particularly strong performance on search-based tasks, such as\nretrieval-augmented generation, where we achieve more than 10x speedup."
                },
                "authors": [
                    {
                        "name": "Maxim Divilkovskiy"
                    },
                    {
                        "name": "Vitaly Malygin"
                    },
                    {
                        "name": "Sergey Zlobin"
                    },
                    {
                        "name": "Sultan Isali"
                    },
                    {
                        "name": "Vasily Kalugin"
                    },
                    {
                        "name": "Stanislav Ilyushin"
                    },
                    {
                        "name": "Nuriza Aitassova"
                    },
                    {
                        "name": "Yi Fei"
                    },
                    {
                        "name": "Zeng Weidi"
                    }
                ],
                "author_detail": {
                    "name": "Zeng Weidi"
                },
                "author": "Zeng Weidi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09072v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09072v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09001v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09001v1",
                "updated": "2025-08-12T15:11:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    15,
                    11,
                    47,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T15:11:47Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    15,
                    11,
                    47,
                    1,
                    224,
                    0
                ],
                "title": "Retrospective Sparse Attention for Efficient Long-Context Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrospective Sparse Attention for Efficient Long-Context Generation"
                },
                "summary": "Large Language Models (LLMs) are increasingly deployed in long-context tasks\nsuch as reasoning, code generation, and multi-turn dialogue. However, inference\nover extended contexts is bottlenecked by the Key-Value (KV) cache, whose\nmemory footprint grows linearly with sequence length and dominates latency at\neach decoding step. While recent KV cache compression methods identify and load\nimportant tokens, they focus predominantly on input contexts and fail to\naddress the cumulative attention errors that arise during long decoding. In\nthis paper, we introduce RetroAttention, a novel KV cache update technique that\nretrospectively revises past attention outputs using newly arrived KV entries\nfrom subsequent decoding steps. By maintaining a lightweight output cache,\nRetroAttention enables past queries to efficiently access more relevant\ncontext, while incurring minimal latency overhead. This breaks the\nfixed-attention-output paradigm and allows continual correction of prior\napproximations. Extensive experiments on long-generation benchmarks show that\nRetroAttention consistently outperforms state-of-the-art (SOTA) KV compression\nmethods, increasing effective KV exposure by up to 1.6$\\times$ and accuracy by\nup to 21.9\\%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed in long-context tasks\nsuch as reasoning, code generation, and multi-turn dialogue. However, inference\nover extended contexts is bottlenecked by the Key-Value (KV) cache, whose\nmemory footprint grows linearly with sequence length and dominates latency at\neach decoding step. While recent KV cache compression methods identify and load\nimportant tokens, they focus predominantly on input contexts and fail to\naddress the cumulative attention errors that arise during long decoding. In\nthis paper, we introduce RetroAttention, a novel KV cache update technique that\nretrospectively revises past attention outputs using newly arrived KV entries\nfrom subsequent decoding steps. By maintaining a lightweight output cache,\nRetroAttention enables past queries to efficiently access more relevant\ncontext, while incurring minimal latency overhead. This breaks the\nfixed-attention-output paradigm and allows continual correction of prior\napproximations. Extensive experiments on long-generation benchmarks show that\nRetroAttention consistently outperforms state-of-the-art (SOTA) KV compression\nmethods, increasing effective KV exposure by up to 1.6$\\times$ and accuracy by\nup to 21.9\\%."
                },
                "authors": [
                    {
                        "name": "Seonghwan Choi"
                    },
                    {
                        "name": "Beomseok Kang"
                    },
                    {
                        "name": "Dongwon Jo"
                    },
                    {
                        "name": "Jae-Joon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jae-Joon Kim"
                },
                "author": "Jae-Joon Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09001v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09001v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08978v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08978v1",
                "updated": "2025-08-12T14:40:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    14,
                    40,
                    36,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T14:40:36Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    14,
                    40,
                    36,
                    1,
                    224,
                    0
                ],
                "title": "TaoCache: Structure-Maintained Video Generation Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TaoCache: Structure-Maintained Video Generation Acceleration"
                },
                "summary": "Existing cache-based acceleration methods for video diffusion models\nprimarily skip early or mid denoising steps, which often leads to structural\ndiscrepancies relative to full-timestep generation and can hinder instruction\nfollowing and character consistency. We present TaoCache, a training-free,\nplug-and-play caching strategy that, instead of residual-based caching, adopts\na fixed-point perspective to predict the model's noise output and is\nspecifically effective in late denoising stages. By calibrating cosine\nsimilarities and norm ratios of consecutive noise deltas, TaoCache preserves\nhigh-resolution structure while enabling aggressive skipping. The approach is\northogonal to complementary accelerations such as Pyramid Attention Broadcast\n(PAB) and TeaCache, and it integrates seamlessly into DiT-based frameworks.\nAcross Latte-1, OpenSora-Plan v110, and Wan2.1, TaoCache attains substantially\nhigher visual quality (LPIPS, SSIM, PSNR) than prior caching methods under the\nsame speedups.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing cache-based acceleration methods for video diffusion models\nprimarily skip early or mid denoising steps, which often leads to structural\ndiscrepancies relative to full-timestep generation and can hinder instruction\nfollowing and character consistency. We present TaoCache, a training-free,\nplug-and-play caching strategy that, instead of residual-based caching, adopts\na fixed-point perspective to predict the model's noise output and is\nspecifically effective in late denoising stages. By calibrating cosine\nsimilarities and norm ratios of consecutive noise deltas, TaoCache preserves\nhigh-resolution structure while enabling aggressive skipping. The approach is\northogonal to complementary accelerations such as Pyramid Attention Broadcast\n(PAB) and TeaCache, and it integrates seamlessly into DiT-based frameworks.\nAcross Latte-1, OpenSora-Plan v110, and Wan2.1, TaoCache attains substantially\nhigher visual quality (LPIPS, SSIM, PSNR) than prior caching methods under the\nsame speedups."
                },
                "authors": [
                    {
                        "name": "Zhentao Fan"
                    },
                    {
                        "name": "Zongzuo Wang"
                    },
                    {
                        "name": "Weiwei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Weiwei Zhang"
                },
                "author": "Weiwei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08978v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08978v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.11488v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.11488v2",
                "updated": "2025-08-12T10:43:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    10,
                    43,
                    55,
                    1,
                    224,
                    0
                ],
                "published": "2023-11-30T16:02:04Z",
                "published_parsed": [
                    2023,
                    11,
                    30,
                    16,
                    2,
                    4,
                    3,
                    334,
                    0
                ],
                "title": "Keep Your Friends Close: Leveraging Affinity Groups to Accelerate AI\n  Inference Workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keep Your Friends Close: Leveraging Affinity Groups to Accelerate AI\n  Inference Workflows"
                },
                "summary": "AI inference workflows are typically structured as a pipeline or graph of AI\nprograms triggered by events. As events occur, the AIs perform inference or\nclassification tasks under time pressure to respond or take some action.\nStandard techniques that reduce latency in other streaming settings (such as\ncaching and optimization-driven scheduling) are of limited value because AI\ndata access patterns (models, databases) change depending on the triggering\nevent: a significant departure from traditional streaming. In this work, we\npropose a novel affinity grouping mechanism that makes it easier for developers\nto express application-specific data access correlations, enabling coordinated\nmanagement of data objects in server clusters hosting streaming inference\ntasks. Our proposals are thus complementary to other approaches such as caching\nand scheduling. Experiments confirm the limitations of standard techniques,\nwhile showing that the proposed mechanism is able to maintain significantly\nlower latency as workload and scale-out increase, and yet requires only minor\ncode changes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI inference workflows are typically structured as a pipeline or graph of AI\nprograms triggered by events. As events occur, the AIs perform inference or\nclassification tasks under time pressure to respond or take some action.\nStandard techniques that reduce latency in other streaming settings (such as\ncaching and optimization-driven scheduling) are of limited value because AI\ndata access patterns (models, databases) change depending on the triggering\nevent: a significant departure from traditional streaming. In this work, we\npropose a novel affinity grouping mechanism that makes it easier for developers\nto express application-specific data access correlations, enabling coordinated\nmanagement of data objects in server clusters hosting streaming inference\ntasks. Our proposals are thus complementary to other approaches such as caching\nand scheduling. Experiments confirm the limitations of standard techniques,\nwhile showing that the proposed mechanism is able to maintain significantly\nlower latency as workload and scale-out increase, and yet requires only minor\ncode changes."
                },
                "authors": [
                    {
                        "name": "Thiago Garrett"
                    },
                    {
                        "name": "Weijia Song"
                    },
                    {
                        "name": "Roman Vitenberg"
                    },
                    {
                        "name": "Ken Birman"
                    }
                ],
                "author_detail": {
                    "name": "Ken Birman"
                },
                "author": "Ken Birman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.11488v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.11488v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07772v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07772v3",
                "updated": "2025-08-12T05:51:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    5,
                    51,
                    37,
                    1,
                    224,
                    0
                ],
                "published": "2024-12-10T18:59:50Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    59,
                    50,
                    1,
                    345,
                    0
                ],
                "title": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models"
                },
                "summary": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to an autoregressive\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nachieves a total score of 84.27 on the VBench-Long benchmark, surpassing all\nprevious video generation models. It enables fast streaming generation of\nhigh-quality videos at 9.4 FPS on a single GPU thanks to KV caching. Our\napproach also enables streaming video-to-video translation, image-to-video, and\ndynamic prompting in a zero-shot manner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to an autoregressive\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nachieves a total score of 84.27 on the VBench-Long benchmark, surpassing all\nprevious video generation models. It enables fast streaming generation of\nhigh-quality videos at 9.4 FPS on a single GPU thanks to KV caching. Our\napproach also enables streaming video-to-video translation, image-to-video, and\ndynamic prompting in a zero-shot manner."
                },
                "authors": [
                    {
                        "name": "Tianwei Yin"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Richard Zhang"
                    },
                    {
                        "name": "William T. Freeman"
                    },
                    {
                        "name": "Fredo Durand"
                    },
                    {
                        "name": "Eli Shechtman"
                    },
                    {
                        "name": "Xun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xun Huang"
                },
                "author": "Xun Huang",
                "arxiv_comment": "Project Page: https://causvid.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07772v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07772v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08600v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08600v1",
                "updated": "2025-08-12T03:33:15Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    3,
                    33,
                    15,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T03:33:15Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    3,
                    33,
                    15,
                    1,
                    224,
                    0
                ],
                "title": "Rigorous quantum calculations for atom-molecule chemical reactions in\n  electric fields: from single to multiple partial wave regimes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rigorous quantum calculations for atom-molecule chemical reactions in\n  electric fields: from single to multiple partial wave regimes"
                },
                "summary": "We present an efficient method for rigorous quantum calculations of cross\nsections for atom-molecule reactive scattering in the presence of a dc electric\nfield. The wavefunction of the reaction complex is expanded in an overcomplete\nset of arrangement-dependent Fock-Delves hyperspherical basis functions and the\ninteractions of the reactants and products with electric fields are accounted\nfor in the total angular momentum representation. A significant computational\nchallenge affecting our previously developed approach [Phys. Rev. Lett.\n$\\mathbf{115}$, 023201 (2015)] is addressed by an efficient asymptotic frame\ntransformation between the hyperspherical and Jacobi coordinates in the\npresence of an external field. Using accurate {\\it ab initio} potential energy\nsurfaces, we calculate total and state-resolved cross sections for the chemical\nreactions LiF$(v=1,j=0)$ + H $\\to$ Li + HF($v'=0,j'$) and F + HD$(v=0,j=0)$\n$\\to$ HF + D, DF + H as functions of collision energy and electric field\nstrength. The field dependence of the cross sections for the LiF + H chemical\nreaction exhibits resonance structure mediated by tunneling-driven interactions\nbetween reactants and products. No significant field effects are found for the\nF + HD $\\to$ HF + D, DF + H chemical reaction at 1 Kelvin, even for\nstate-resolved transitions and with field magnitudes reaching 200 kV/cm. Our\ncalculations illustrate the essential role of basis set convergence for the\nproper interpretation of external field effects on chemical reaction dynamics.\nWhile reduced-basis calculations for the F + HD reaction indicate significant\neffects of electric fields on product state distributions, these effects vanish\nwhen the number of total angular momentum basis states is increased.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an efficient method for rigorous quantum calculations of cross\nsections for atom-molecule reactive scattering in the presence of a dc electric\nfield. The wavefunction of the reaction complex is expanded in an overcomplete\nset of arrangement-dependent Fock-Delves hyperspherical basis functions and the\ninteractions of the reactants and products with electric fields are accounted\nfor in the total angular momentum representation. A significant computational\nchallenge affecting our previously developed approach [Phys. Rev. Lett.\n$\\mathbf{115}$, 023201 (2015)] is addressed by an efficient asymptotic frame\ntransformation between the hyperspherical and Jacobi coordinates in the\npresence of an external field. Using accurate {\\it ab initio} potential energy\nsurfaces, we calculate total and state-resolved cross sections for the chemical\nreactions LiF$(v=1,j=0)$ + H $\\to$ Li + HF($v'=0,j'$) and F + HD$(v=0,j=0)$\n$\\to$ HF + D, DF + H as functions of collision energy and electric field\nstrength. The field dependence of the cross sections for the LiF + H chemical\nreaction exhibits resonance structure mediated by tunneling-driven interactions\nbetween reactants and products. No significant field effects are found for the\nF + HD $\\to$ HF + D, DF + H chemical reaction at 1 Kelvin, even for\nstate-resolved transitions and with field magnitudes reaching 200 kV/cm. Our\ncalculations illustrate the essential role of basis set convergence for the\nproper interpretation of external field effects on chemical reaction dynamics.\nWhile reduced-basis calculations for the F + HD reaction indicate significant\neffects of electric fields on product state distributions, these effects vanish\nwhen the number of total angular momentum basis states is increased."
                },
                "authors": [
                    {
                        "name": "Timur V. Tscherbul"
                    },
                    {
                        "name": "Roman V. Krems"
                    }
                ],
                "author_detail": {
                    "name": "Roman V. Krems"
                },
                "author": "Roman V. Krems",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08600v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08600v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.chem-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.atom-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07675v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07675v2",
                "updated": "2025-08-12T02:51:12Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    2,
                    51,
                    12,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-11T06:53:27Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    6,
                    53,
                    27,
                    0,
                    223,
                    0
                ],
                "title": "Semantic Caching for Low-Cost LLM Serving: From Offline Learning to\n  Online Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Caching for Low-Cost LLM Serving: From Offline Learning to\n  Online Adaptation"
                },
                "summary": "Large Language Models (LLMs) are revolutionizing how users interact with\ninformation systems, yet their high inference cost poses serious scalability\nand sustainability challenges. Caching inference responses, allowing them to be\nretrieved without another forward pass through the LLM, has emerged as one\npossible solution. Traditional exact-match caching, however, overlooks the\nsemantic similarity between queries, leading to unnecessary recomputation.\nSemantic caching addresses this by retrieving responses based on semantic\nsimilarity, but introduces a fundamentally different cache eviction problem:\none must account for mismatch costs between incoming queries and cached\nresponses. Moreover, key system parameters, such as query arrival probabilities\nand serving costs, are often unknown and must be learned over time. Existing\nsemantic caching methods are largely ad-hoc, lacking theoretical foundations\nand unable to adapt to real-world uncertainty. In this paper, we present a\nprincipled, learning-based framework for semantic cache eviction under unknown\nquery and cost distributions. We formulate both offline optimization and online\nlearning variants of the problem, and develop provably efficient algorithms\nwith state-of-the-art guarantees. We also evaluate our framework on a synthetic\ndataset, showing that our proposed algorithms perform matching or superior\nperformance compared with baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are revolutionizing how users interact with\ninformation systems, yet their high inference cost poses serious scalability\nand sustainability challenges. Caching inference responses, allowing them to be\nretrieved without another forward pass through the LLM, has emerged as one\npossible solution. Traditional exact-match caching, however, overlooks the\nsemantic similarity between queries, leading to unnecessary recomputation.\nSemantic caching addresses this by retrieving responses based on semantic\nsimilarity, but introduces a fundamentally different cache eviction problem:\none must account for mismatch costs between incoming queries and cached\nresponses. Moreover, key system parameters, such as query arrival probabilities\nand serving costs, are often unknown and must be learned over time. Existing\nsemantic caching methods are largely ad-hoc, lacking theoretical foundations\nand unable to adapt to real-world uncertainty. In this paper, we present a\nprincipled, learning-based framework for semantic cache eviction under unknown\nquery and cost distributions. We formulate both offline optimization and online\nlearning variants of the problem, and develop provably efficient algorithms\nwith state-of-the-art guarantees. We also evaluate our framework on a synthetic\ndataset, showing that our proposed algorithms perform matching or superior\nperformance compared with baselines."
                },
                "authors": [
                    {
                        "name": "Xutong Liu"
                    },
                    {
                        "name": "Baran Atalar"
                    },
                    {
                        "name": "Xiangxiang Dai"
                    },
                    {
                        "name": "Jinhang Zuo"
                    },
                    {
                        "name": "Siwei Wang"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Carlee Joe-Wong"
                    }
                ],
                "author_detail": {
                    "name": "Carlee Joe-Wong"
                },
                "author": "Carlee Joe-Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07675v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07675v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08134v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08134v2",
                "updated": "2025-08-12T02:27:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    2,
                    27,
                    5,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-11T16:10:00Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    16,
                    10,
                    0,
                    0,
                    223,
                    0
                ],
                "title": "Follow-Your-Shape: Shape-Aware Image Editing via Trajectory-Guided\n  Region Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Follow-Your-Shape: Shape-Aware Image Editing via Trajectory-Guided\n  Region Control"
                },
                "summary": "While recent flow-based image editing models demonstrate general-purpose\ncapabilities across diverse tasks, they often struggle to specialize in\nchallenging scenarios -- particularly those involving large-scale shape\ntransformations. When performing such structural edits, these methods either\nfail to achieve the intended shape change or inadvertently alter non-target\nregions, resulting in degraded background quality. We propose\nFollow-Your-Shape, a training-free and mask-free framework that supports\nprecise and controllable editing of object shapes while strictly preserving\nnon-target content. Motivated by the divergence between inversion and editing\ntrajectories, we compute a Trajectory Divergence Map (TDM) by comparing\ntoken-wise velocity differences between the inversion and denoising paths. The\nTDM enables precise localization of editable regions and guides a Scheduled KV\nInjection mechanism that ensures stable and faithful editing. To facilitate a\nrigorous evaluation, we introduce ReShapeBench, a new benchmark comprising 120\nnew images and enriched prompt pairs specifically curated for shape-aware\nediting. Experiments demonstrate that our method achieves superior editability\nand visual fidelity, particularly in tasks requiring large-scale shape\nreplacement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While recent flow-based image editing models demonstrate general-purpose\ncapabilities across diverse tasks, they often struggle to specialize in\nchallenging scenarios -- particularly those involving large-scale shape\ntransformations. When performing such structural edits, these methods either\nfail to achieve the intended shape change or inadvertently alter non-target\nregions, resulting in degraded background quality. We propose\nFollow-Your-Shape, a training-free and mask-free framework that supports\nprecise and controllable editing of object shapes while strictly preserving\nnon-target content. Motivated by the divergence between inversion and editing\ntrajectories, we compute a Trajectory Divergence Map (TDM) by comparing\ntoken-wise velocity differences between the inversion and denoising paths. The\nTDM enables precise localization of editable regions and guides a Scheduled KV\nInjection mechanism that ensures stable and faithful editing. To facilitate a\nrigorous evaluation, we introduce ReShapeBench, a new benchmark comprising 120\nnew images and enriched prompt pairs specifically curated for shape-aware\nediting. Experiments demonstrate that our method achieves superior editability\nand visual fidelity, particularly in tasks requiring large-scale shape\nreplacement."
                },
                "authors": [
                    {
                        "name": "Zeqian Long"
                    },
                    {
                        "name": "Mingzhe Zheng"
                    },
                    {
                        "name": "Kunyu Feng"
                    },
                    {
                        "name": "Xinhua Zhang"
                    },
                    {
                        "name": "Hongyu Liu"
                    },
                    {
                        "name": "Harry Yang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    },
                    {
                        "name": "Qifeng Chen"
                    },
                    {
                        "name": "Yue Ma"
                    }
                ],
                "author_detail": {
                    "name": "Yue Ma"
                },
                "author": "Yue Ma",
                "arxiv_comment": "Project webpage is available at https://follow-your-shape.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08134v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08134v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08531v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08531v1",
                "updated": "2025-08-12T00:06:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    12,
                    0,
                    6,
                    34,
                    1,
                    224,
                    0
                ],
                "published": "2025-08-12T00:06:34Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    0,
                    6,
                    34,
                    1,
                    224,
                    0
                ],
                "title": "Profiling Large Language Model Inference on Apple Silicon: A\n  Quantization Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Profiling Large Language Model Inference on Apple Silicon: A\n  Quantization Perspective"
                },
                "summary": "A systematic understanding of Apple Silicon is lacking in the current\nlandscape of hardware efficiency; research focus is largely centered on\naccelerating GPUs for large-scale training or inference on CUDA devices. This\npaper investigates Apple Silicon's unique memory architecture that offers a\nunified memory integrating CPU and GPU memory and its implications for\non-device LLM inference.\n  We decipher myths about whether Apple Silicon is efficient for on-device\ninference compared to competitors such as NVIDIA GPUs by directly conducting\nlatency and throughput comparison benchmarks. We explain the performance gap\nbetween them through profiling low level hardware metrics - ALU utilization,\nmemory bandwidth, buffer usage, cache residency etc. at runtime. We draw\nseveral insights regarding performance bottlenecks such as dequantization\noverhead, compute throughput and memory bandwidth. We debunk existing false\nclaims regarding large language model inference such as compressing models to\nlower bit precision is a defacto promise for faster inference across all\nhardware platforms. We find that the large unified memory enables Apple Silicon\nto be both cost effective and efficient against NVIDIA GPUs for ultra large\nlanguage models.\n  Our large scale evaluation on 5 hardware testbeds incorporating three Apple\nM-series devices: M2 Ultra, M2 Max and M4 Pro and two NVIDIA GPUs: NVIDIA RTX\nA6000, a multi GPU setup with 2xNVIDIA RTX A6000, 5 model scales ranging from\n8B to 405B parameters and 14 quantization schemes gives an understanding of how\nApple Silicon fits within the paradigm of on-device LLM inference. Our analysis\nreveals multiple resource interdependencies and unexpected findings, while also\nquantifying established insights. To the best of our knowledge, this study\nmakes the first attempt to present a thorough characterization and analysis of\nApple Silicon for on-device inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A systematic understanding of Apple Silicon is lacking in the current\nlandscape of hardware efficiency; research focus is largely centered on\naccelerating GPUs for large-scale training or inference on CUDA devices. This\npaper investigates Apple Silicon's unique memory architecture that offers a\nunified memory integrating CPU and GPU memory and its implications for\non-device LLM inference.\n  We decipher myths about whether Apple Silicon is efficient for on-device\ninference compared to competitors such as NVIDIA GPUs by directly conducting\nlatency and throughput comparison benchmarks. We explain the performance gap\nbetween them through profiling low level hardware metrics - ALU utilization,\nmemory bandwidth, buffer usage, cache residency etc. at runtime. We draw\nseveral insights regarding performance bottlenecks such as dequantization\noverhead, compute throughput and memory bandwidth. We debunk existing false\nclaims regarding large language model inference such as compressing models to\nlower bit precision is a defacto promise for faster inference across all\nhardware platforms. We find that the large unified memory enables Apple Silicon\nto be both cost effective and efficient against NVIDIA GPUs for ultra large\nlanguage models.\n  Our large scale evaluation on 5 hardware testbeds incorporating three Apple\nM-series devices: M2 Ultra, M2 Max and M4 Pro and two NVIDIA GPUs: NVIDIA RTX\nA6000, a multi GPU setup with 2xNVIDIA RTX A6000, 5 model scales ranging from\n8B to 405B parameters and 14 quantization schemes gives an understanding of how\nApple Silicon fits within the paradigm of on-device LLM inference. Our analysis\nreveals multiple resource interdependencies and unexpected findings, while also\nquantifying established insights. To the best of our knowledge, this study\nmakes the first attempt to present a thorough characterization and analysis of\nApple Silicon for on-device inference."
                },
                "authors": [
                    {
                        "name": "Afsara Benazir"
                    },
                    {
                        "name": "Felix Xiaozhu Lin"
                    }
                ],
                "author_detail": {
                    "name": "Felix Xiaozhu Lin"
                },
                "author": "Felix Xiaozhu Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08531v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08531v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08457v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08457v1",
                "updated": "2025-08-11T20:30:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    20,
                    30,
                    31,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T20:30:31Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    20,
                    30,
                    31,
                    0,
                    223,
                    0
                ],
                "title": "Architecting Long-Context LLM Acceleration with Packing-Prefetch\n  Scheduler and Ultra-Large Capacity On-Chip Memories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Architecting Long-Context LLM Acceleration with Packing-Prefetch\n  Scheduler and Ultra-Large Capacity On-Chip Memories"
                },
                "summary": "Long-context Large Language Model (LLM) inference faces increasing compute\nbottlenecks as attention calculations scale with context length, primarily due\nto the growing KV-cache transfer overhead that saturates High Bandwidth Memory\n(HBM). While prefetching techniques mitigate cache misses by fetching KV data\nin advance, their spatial and temporal benefits present new opportunities to\nexploit. This work proposes a packing-prefetch scheduling architecture with\nmonolithic 3D (M3D) back-end-of-line (BEOL) compatible embedded memories with\nultra-large on-chip capacity to accelerate long-context LLM inference. Our\noptimizations demonstrate 8.06x decode speedup and 1.83x overall latency\nreduction on Llama3.1-8B using TPUv6e-like hardware with additional 512MB BEOL\nmemories over the serial execution. Evaluations of multi-request workloads on\nTPU-like architectures show 1.7x-2.4x throughput improvement and 1.5x-2.4x HBM\nbandwidth reduction compared to packing-only methods on Llama3.1-8B and\nLlama3.1-70B models. With the co-design of packing, prefetching, and BEOL\nmemories, our approach alleviates HBM constraints and enables efficient\nlong-context LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context Large Language Model (LLM) inference faces increasing compute\nbottlenecks as attention calculations scale with context length, primarily due\nto the growing KV-cache transfer overhead that saturates High Bandwidth Memory\n(HBM). While prefetching techniques mitigate cache misses by fetching KV data\nin advance, their spatial and temporal benefits present new opportunities to\nexploit. This work proposes a packing-prefetch scheduling architecture with\nmonolithic 3D (M3D) back-end-of-line (BEOL) compatible embedded memories with\nultra-large on-chip capacity to accelerate long-context LLM inference. Our\noptimizations demonstrate 8.06x decode speedup and 1.83x overall latency\nreduction on Llama3.1-8B using TPUv6e-like hardware with additional 512MB BEOL\nmemories over the serial execution. Evaluations of multi-request workloads on\nTPU-like architectures show 1.7x-2.4x throughput improvement and 1.5x-2.4x HBM\nbandwidth reduction compared to packing-only methods on Llama3.1-8B and\nLlama3.1-70B models. With the co-design of packing, prefetching, and BEOL\nmemories, our approach alleviates HBM constraints and enables efficient\nlong-context LLM inference."
                },
                "authors": [
                    {
                        "name": "Ming-Yen Lee"
                    },
                    {
                        "name": "Faaiq Waqar"
                    },
                    {
                        "name": "Hanchen Yang"
                    },
                    {
                        "name": "Muhammed Ahosan Ul Karim"
                    },
                    {
                        "name": "Harsono Simka"
                    },
                    {
                        "name": "Shimeng Yu"
                    }
                ],
                "author_detail": {
                    "name": "Shimeng Yu"
                },
                "author": "Shimeng Yu",
                "arxiv_comment": "7 pages, 8 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08457v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08457v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.1.3; B.3.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08438v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08438v1",
                "updated": "2025-08-11T19:55:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    19,
                    55,
                    44,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T19:55:44Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    19,
                    55,
                    44,
                    0,
                    223,
                    0
                ],
                "title": "Selective KV-Cache Sharing to Mitigate Timing Side-Channels in LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Selective KV-Cache Sharing to Mitigate Timing Side-Channels in LLM\n  Inference"
                },
                "summary": "Global KV-cache sharing has emerged as a key optimization for accelerating\nlarge language model (LLM) inference. However, it exposes a new class of timing\nside-channel attacks, enabling adversaries to infer sensitive user inputs via\nshared cache entries. Existing defenses, such as per-user isolation, eliminate\nleakage but degrade performance by up to 38.9% in time-to-first-token (TTFT),\nmaking them impractical for high-throughput deployment. To address this gap, we\nintroduce SafeKV (Secure and Flexible KV Cache Sharing), a privacy-aware\nKV-cache management framework that selectively shares non-sensitive entries\nwhile confining sensitive content to private caches. SafeKV comprises three\ncomponents: (i) a hybrid, multi-tier detection pipeline that integrates\nrule-based pattern matching, a general-purpose privacy detector, and\ncontext-aware validation; (ii) a unified radix-tree index that manages public\nand private entries across heterogeneous memory tiers (HBM, DRAM, SSD); and\n(iii) entropy-based access monitoring to detect and mitigate residual\ninformation leakage. Our evaluation shows that SafeKV mitigates 94% - 97% of\ntiming-based side-channel attacks. Compared to per-user isolation method,\nSafeKV improves TTFT by up to 40.58% and throughput by up to 2.66X across\ndiverse LLMs and workloads. SafeKV reduces cache-induced TTFT overhead from\n50.41% to 11.74% on Qwen3-235B. By combining fine-grained privacy control with\nhigh cache reuse efficiency, SafeKV reclaims the performance advantages of\nglobal sharing while providing robust runtime privacy guarantees for LLM\ninference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Global KV-cache sharing has emerged as a key optimization for accelerating\nlarge language model (LLM) inference. However, it exposes a new class of timing\nside-channel attacks, enabling adversaries to infer sensitive user inputs via\nshared cache entries. Existing defenses, such as per-user isolation, eliminate\nleakage but degrade performance by up to 38.9% in time-to-first-token (TTFT),\nmaking them impractical for high-throughput deployment. To address this gap, we\nintroduce SafeKV (Secure and Flexible KV Cache Sharing), a privacy-aware\nKV-cache management framework that selectively shares non-sensitive entries\nwhile confining sensitive content to private caches. SafeKV comprises three\ncomponents: (i) a hybrid, multi-tier detection pipeline that integrates\nrule-based pattern matching, a general-purpose privacy detector, and\ncontext-aware validation; (ii) a unified radix-tree index that manages public\nand private entries across heterogeneous memory tiers (HBM, DRAM, SSD); and\n(iii) entropy-based access monitoring to detect and mitigate residual\ninformation leakage. Our evaluation shows that SafeKV mitigates 94% - 97% of\ntiming-based side-channel attacks. Compared to per-user isolation method,\nSafeKV improves TTFT by up to 40.58% and throughput by up to 2.66X across\ndiverse LLMs and workloads. SafeKV reduces cache-induced TTFT overhead from\n50.41% to 11.74% on Qwen3-235B. By combining fine-grained privacy control with\nhigh cache reuse efficiency, SafeKV reclaims the performance advantages of\nglobal sharing while providing robust runtime privacy guarantees for LLM\ninference."
                },
                "authors": [
                    {
                        "name": "Kexin Chu"
                    },
                    {
                        "name": "Zecheng Lin"
                    },
                    {
                        "name": "Dawei Xiang"
                    },
                    {
                        "name": "Zixu Shen"
                    },
                    {
                        "name": "Jianchang Su"
                    },
                    {
                        "name": "Cheng Chu"
                    },
                    {
                        "name": "Yiwei Yang"
                    },
                    {
                        "name": "Wenhui Zhang"
                    },
                    {
                        "name": "Wenfei Wu"
                    },
                    {
                        "name": "Wei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Zhang"
                },
                "author": "Wei Zhang",
                "arxiv_comment": "17 pages,17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08438v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08438v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08081v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08081v1",
                "updated": "2025-08-11T15:28:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    15,
                    28,
                    28,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T15:28:28Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    15,
                    28,
                    28,
                    0,
                    223,
                    0
                ],
                "title": "Numerical computation of linearized KV and the Deligne-Drinfeld and\n  Broadhurst-Kreimer conjectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Numerical computation of linearized KV and the Deligne-Drinfeld and\n  Broadhurst-Kreimer conjectures"
                },
                "summary": "We compute numerically the dimensions of the graded quotients of the\nlinearized Kashiwara-Vergne Lie algebra lkv in low weight, confirming a\nconjecture of Raphael-Schneps in those weights. The Lie algebra lkv appears in\na chain of inclusions of Lie algebras, including also the linearized double\nshuffle Lie algebra and the (depth associated graded of the)\nGrothendieck-Teichm\\\"uller Lie algebra. Hence our computations also allow us to\ncheck the validity of the Deligne-Drinfeld conjecture on the structure of the\nGrothendieck-Teichm\\\"uller group up to weight 29, and (a version of) the the\nBroadhurst-Kreimer conjecture on the number of multiple zeta values for a range\nof weight-depth pairs significantly exceeding the previous bounds. Our\ncomputations also verify a conjecture by Alekseev-Torossian on the\nKashiwara-Vergne Lie algebra up to weight 29.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We compute numerically the dimensions of the graded quotients of the\nlinearized Kashiwara-Vergne Lie algebra lkv in low weight, confirming a\nconjecture of Raphael-Schneps in those weights. The Lie algebra lkv appears in\na chain of inclusions of Lie algebras, including also the linearized double\nshuffle Lie algebra and the (depth associated graded of the)\nGrothendieck-Teichm\\\"uller Lie algebra. Hence our computations also allow us to\ncheck the validity of the Deligne-Drinfeld conjecture on the structure of the\nGrothendieck-Teichm\\\"uller group up to weight 29, and (a version of) the the\nBroadhurst-Kreimer conjecture on the number of multiple zeta values for a range\nof weight-depth pairs significantly exceeding the previous bounds. Our\ncomputations also verify a conjecture by Alekseev-Torossian on the\nKashiwara-Vergne Lie algebra up to weight 29."
                },
                "authors": [
                    {
                        "name": "Florian Naef"
                    },
                    {
                        "name": "Thomas Willwacher"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Willwacher"
                },
                "author": "Thomas Willwacher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08081v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08081v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.QA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.QA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06923v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06923v2",
                "updated": "2025-08-11T14:15:27Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    14,
                    15,
                    27,
                    0,
                    223,
                    0
                ],
                "published": "2025-03-10T05:09:42Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    5,
                    9,
                    42,
                    0,
                    69,
                    0
                ],
                "title": "From Reusing to Forecasting: Accelerating Diffusion Models with\n  TaylorSeers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Reusing to Forecasting: Accelerating Diffusion Models with\n  TaylorSeers"
                },
                "summary": "Diffusion Transformers (DiT) have revolutionized high-fidelity image and\nvideo synthesis, yet their computational demands remain prohibitive for\nreal-time applications. To solve this problem, feature caching has been\nproposed to accelerate diffusion models by caching the features in the previous\ntimesteps and then reusing them in the following timesteps. However, at\ntimesteps with significant intervals, the feature similarity in diffusion\nmodels decreases substantially, leading to a pronounced increase in errors\nintroduced by feature caching, significantly harming the generation quality. To\nsolve this problem, we propose TaylorSeer, which firstly shows that features of\ndiffusion models at future timesteps can be predicted based on their values at\nprevious timesteps. Based on the fact that features change slowly and\ncontinuously across timesteps, TaylorSeer employs a differential method to\napproximate the higher-order derivatives of features and predict features in\nfuture timesteps with Taylor series expansion. Extensive experiments\ndemonstrate its significant effectiveness in both image and video synthesis,\nespecially in high acceleration ratios. For instance, it achieves an almost\nlossless acceleration of 4.99$\\times$ on FLUX and 5.00$\\times$ on HunyuanVideo\nwithout additional training. On DiT, it achieves $3.41$ lower FID compared with\nprevious SOTA at $4.53$$\\times$ acceleration. %Our code is provided in the\nsupplementary materials and will be made publicly available on GitHub. Our\ncodes have been released in Github:https://github.com/Shenyi-Z/TaylorSeer",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) have revolutionized high-fidelity image and\nvideo synthesis, yet their computational demands remain prohibitive for\nreal-time applications. To solve this problem, feature caching has been\nproposed to accelerate diffusion models by caching the features in the previous\ntimesteps and then reusing them in the following timesteps. However, at\ntimesteps with significant intervals, the feature similarity in diffusion\nmodels decreases substantially, leading to a pronounced increase in errors\nintroduced by feature caching, significantly harming the generation quality. To\nsolve this problem, we propose TaylorSeer, which firstly shows that features of\ndiffusion models at future timesteps can be predicted based on their values at\nprevious timesteps. Based on the fact that features change slowly and\ncontinuously across timesteps, TaylorSeer employs a differential method to\napproximate the higher-order derivatives of features and predict features in\nfuture timesteps with Taylor series expansion. Extensive experiments\ndemonstrate its significant effectiveness in both image and video synthesis,\nespecially in high acceleration ratios. For instance, it achieves an almost\nlossless acceleration of 4.99$\\times$ on FLUX and 5.00$\\times$ on HunyuanVideo\nwithout additional training. On DiT, it achieves $3.41$ lower FID compared with\nprevious SOTA at $4.53$$\\times$ acceleration. %Our code is provided in the\nsupplementary materials and will be made publicly available on GitHub. Our\ncodes have been released in Github:https://github.com/Shenyi-Z/TaylorSeer"
                },
                "authors": [
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Yuanhuiyi Lyu"
                    },
                    {
                        "name": "Junjie Chen"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "15 pages, 14 figures; Accepted by ICCV2025; Mainly focus on feature\n  caching for diffusion transformers acceleration",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06923v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06923v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08343v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08343v1",
                "updated": "2025-08-11T10:47:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    10,
                    47,
                    35,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T10:47:35Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    10,
                    47,
                    35,
                    0,
                    223,
                    0
                ],
                "title": "Maximizing GPU Efficiency via Optimal Adapter Caching: An Analytical\n  Approach for Multi-Tenant LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Maximizing GPU Efficiency via Optimal Adapter Caching: An Analytical\n  Approach for Multi-Tenant LLM Serving"
                },
                "summary": "Serving LLM adapters has gained significant attention as an effective\napproach to adapt general-purpose language models to diverse, task-specific use\ncases. However, serving a wide range of adapters introduces several and\nsubstantial overheads, leading to performance degradation and challenges in\noptimal placement. To address these challenges, we present an analytical,\nAI-driven pipeline that accurately determines the optimal allocation of\nadapters in single-node setups. This allocation maximizes performance,\neffectively using GPU resources, while preventing request starvation.\nCrucially, the proposed allocation is given based on current workload patterns.\nThese insights in single-node setups can be leveraged in multi-replica\ndeployments for overall placement, load balancing and server configuration,\nultimately enhancing overall performance and improving resource efficiency. Our\napproach builds on an in-depth analysis of LLM adapter serving, accounting for\noverheads and performance variability, and includes the development of the\nfirst Digital Twin capable of replicating online LLM-adapter serving systems\nwith matching key performance metrics. The experimental results demonstrate\nthat the Digital Twin achieves a SMAPE difference of no more than 5.5% in\nthroughput compared to real results, and the proposed pipeline accurately\npredicts the optimal placement with minimal latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving LLM adapters has gained significant attention as an effective\napproach to adapt general-purpose language models to diverse, task-specific use\ncases. However, serving a wide range of adapters introduces several and\nsubstantial overheads, leading to performance degradation and challenges in\noptimal placement. To address these challenges, we present an analytical,\nAI-driven pipeline that accurately determines the optimal allocation of\nadapters in single-node setups. This allocation maximizes performance,\neffectively using GPU resources, while preventing request starvation.\nCrucially, the proposed allocation is given based on current workload patterns.\nThese insights in single-node setups can be leveraged in multi-replica\ndeployments for overall placement, load balancing and server configuration,\nultimately enhancing overall performance and improving resource efficiency. Our\napproach builds on an in-depth analysis of LLM adapter serving, accounting for\noverheads and performance variability, and includes the development of the\nfirst Digital Twin capable of replicating online LLM-adapter serving systems\nwith matching key performance metrics. The experimental results demonstrate\nthat the Digital Twin achieves a SMAPE difference of no more than 5.5% in\nthroughput compared to real results, and the proposed pipeline accurately\npredicts the optimal placement with minimal latency."
                },
                "authors": [
                    {
                        "name": "Ferran Agullo"
                    },
                    {
                        "name": "Joan Oliveras"
                    },
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Alberto Gutierrez-Torre"
                    },
                    {
                        "name": "Olivier Tardieu"
                    },
                    {
                        "name": "Alaa Youssef"
                    },
                    {
                        "name": "Jordi Torres"
                    },
                    {
                        "name": "Josep Ll. Berral"
                    }
                ],
                "author_detail": {
                    "name": "Josep Ll. Berral"
                },
                "author": "Josep Ll. Berral",
                "arxiv_comment": "Under review for a computer science conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08343v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08343v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07811v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07811v1",
                "updated": "2025-08-11T09:54:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    9,
                    54,
                    45,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T09:54:45Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    9,
                    54,
                    45,
                    0,
                    223,
                    0
                ],
                "title": "DiTVR: Zero-Shot Diffusion Transformer for Video Restoration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiTVR: Zero-Shot Diffusion Transformer for Video Restoration"
                },
                "summary": "Video restoration aims to reconstruct high quality video sequences from low\nquality inputs, addressing tasks such as super resolution, denoising, and\ndeblurring. Traditional regression based methods often produce unrealistic\ndetails and require extensive paired datasets, while recent generative\ndiffusion models face challenges in ensuring temporal consistency. We introduce\nDiTVR, a zero shot video restoration framework that couples a diffusion\ntransformer with trajectory aware attention and a wavelet guided, flow\nconsistent sampler. Unlike prior 3D convolutional or frame wise diffusion\napproaches, our attention mechanism aligns tokens along optical flow\ntrajectories, with particular emphasis on vital layers that exhibit the highest\nsensitivity to temporal dynamics. A spatiotemporal neighbour cache dynamically\nselects relevant tokens based on motion correspondences across frames. The flow\nguided sampler injects data consistency only into low-frequency bands,\npreserving high frequency priors while accelerating convergence. DiTVR\nestablishes a new zero shot state of the art on video restoration benchmarks,\ndemonstrating superior temporal consistency and detail preservation while\nremaining robust to flow noise and occlusions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video restoration aims to reconstruct high quality video sequences from low\nquality inputs, addressing tasks such as super resolution, denoising, and\ndeblurring. Traditional regression based methods often produce unrealistic\ndetails and require extensive paired datasets, while recent generative\ndiffusion models face challenges in ensuring temporal consistency. We introduce\nDiTVR, a zero shot video restoration framework that couples a diffusion\ntransformer with trajectory aware attention and a wavelet guided, flow\nconsistent sampler. Unlike prior 3D convolutional or frame wise diffusion\napproaches, our attention mechanism aligns tokens along optical flow\ntrajectories, with particular emphasis on vital layers that exhibit the highest\nsensitivity to temporal dynamics. A spatiotemporal neighbour cache dynamically\nselects relevant tokens based on motion correspondences across frames. The flow\nguided sampler injects data consistency only into low-frequency bands,\npreserving high frequency priors while accelerating convergence. DiTVR\nestablishes a new zero shot state of the art on video restoration benchmarks,\ndemonstrating superior temporal consistency and detail preservation while\nremaining robust to flow noise and occlusions."
                },
                "authors": [
                    {
                        "name": "Sicheng Gao"
                    },
                    {
                        "name": "Nancy Mehta"
                    },
                    {
                        "name": "Zongwei Wu"
                    },
                    {
                        "name": "Radu Timofte"
                    }
                ],
                "author_detail": {
                    "name": "Radu Timofte"
                },
                "author": "Radu Timofte",
                "arxiv_comment": "7 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07811v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07811v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20790v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20790v2",
                "updated": "2025-08-11T08:10:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    8,
                    10,
                    21,
                    0,
                    223,
                    0
                ],
                "published": "2024-10-28T07:13:25Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    7,
                    13,
                    25,
                    0,
                    302,
                    0
                ],
                "title": "SparseTem: Boosting the Efficiency of CNN-Based Video Encoders by\n  Exploiting Temporal Continuity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseTem: Boosting the Efficiency of CNN-Based Video Encoders by\n  Exploiting Temporal Continuity"
                },
                "summary": "Deep learning models have become pivotal in the field of video processing and\nis increasingly critical in practical applications such as autonomous driving\nand object detection. Although Vision Transformers (ViTs) have demonstrated\ntheir power, Convolutional Neural Networks (CNNs) remain a highly efficient and\nhigh-performance choice for feature extraction and encoding. However, the\nintensive computational demands of convolution operations hinder its broader\nadoption as a video encoder. Given the inherent temporal continuity in video\nframes, changes between consecutive frames are minimal, allowing for the\nskipping of redundant computations. This technique, which we term as Diff\nComputation, presents two primary challenges. First, Diff Computation requires\nto cache intermediate feature maps to ensure the correctness of non-linear\ncomputations, leading to significant memory consumption. Second, the imbalance\nof sparsity among layers, introduced by Diff Computation, incurs accuracy\ndegradation. To address these issues, we propose a memory-efficient scheduling\nmethod to eliminate memory overhead and an online adjustment mechanism to\nminimize accuracy degradation. We integrate these techniques into our\nframework, SparseTem, to seamlessly support various CNN-based video encoders.\nSparseTem achieves speedup of 1.79x for EfficientDet and 4.72x for CRNN, with\nminimal accuracy drop and no additional memory overhead. Extensive experimental\nresults demonstrate that SparseTem sets a new state-of-the-art by effectively\nutilizing temporal continuity to accelerate CNN-based video encoders.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning models have become pivotal in the field of video processing and\nis increasingly critical in practical applications such as autonomous driving\nand object detection. Although Vision Transformers (ViTs) have demonstrated\ntheir power, Convolutional Neural Networks (CNNs) remain a highly efficient and\nhigh-performance choice for feature extraction and encoding. However, the\nintensive computational demands of convolution operations hinder its broader\nadoption as a video encoder. Given the inherent temporal continuity in video\nframes, changes between consecutive frames are minimal, allowing for the\nskipping of redundant computations. This technique, which we term as Diff\nComputation, presents two primary challenges. First, Diff Computation requires\nto cache intermediate feature maps to ensure the correctness of non-linear\ncomputations, leading to significant memory consumption. Second, the imbalance\nof sparsity among layers, introduced by Diff Computation, incurs accuracy\ndegradation. To address these issues, we propose a memory-efficient scheduling\nmethod to eliminate memory overhead and an online adjustment mechanism to\nminimize accuracy degradation. We integrate these techniques into our\nframework, SparseTem, to seamlessly support various CNN-based video encoders.\nSparseTem achieves speedup of 1.79x for EfficientDet and 4.72x for CRNN, with\nminimal accuracy drop and no additional memory overhead. Extensive experimental\nresults demonstrate that SparseTem sets a new state-of-the-art by effectively\nutilizing temporal continuity to accelerate CNN-based video encoders."
                },
                "authors": [
                    {
                        "name": "Kunyun Wang"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Jieru Zhao"
                    },
                    {
                        "name": "Wenchao Ding"
                    },
                    {
                        "name": "Quan Chen"
                    },
                    {
                        "name": "Jingwen Leng"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "arxiv_comment": "9 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20790v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20790v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07570v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07570v1",
                "updated": "2025-08-11T03:03:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    11,
                    3,
                    3,
                    34,
                    0,
                    223,
                    0
                ],
                "published": "2025-08-11T03:03:34Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    3,
                    3,
                    34,
                    0,
                    223,
                    0
                ],
                "title": "Adaptive Cache Enhancement for Test-Time Adaptation of Vision-Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Cache Enhancement for Test-Time Adaptation of Vision-Language\n  Models"
                },
                "summary": "Vision-language models (VLMs) exhibit remarkable zero-shot generalization but\nsuffer performance degradation under distribution shifts in downstream tasks,\nparticularly in the absence of labeled data. Test-Time Adaptation (TTA)\naddresses this challenge by enabling online optimization of VLMs during\ninference, eliminating the need for annotated data. Cache-based TTA methods\nexploit historical knowledge by maintaining a dynamic memory cache of\nlow-entropy or high-confidence samples, promoting efficient adaptation to\nout-of-distribution data. Nevertheless, these methods face two critical\nchallenges: (1) unreliable confidence metrics under significant distribution\nshifts, resulting in error accumulation within the cache and degraded\nadaptation performance; and (2) rigid decision boundaries that fail to\naccommodate substantial distributional variations, leading to suboptimal\npredictions. To overcome these limitations, we introduce the Adaptive Cache\nEnhancement (ACE) framework, which constructs a robust cache by selectively\nstoring high-confidence or low-entropy image embeddings per class, guided by\ndynamic, class-specific thresholds initialized from zero-shot statistics and\niteratively refined using an exponential moving average and\nexploration-augmented updates. This approach enables adaptive, class-wise\ndecision boundaries, ensuring robust and accurate predictions across diverse\nvisual distributions. Extensive experiments on 15 diverse benchmark datasets\ndemonstrate that ACE achieves state-of-the-art performance, delivering superior\nrobustness and generalization compared to existing TTA methods in challenging\nout-of-distribution scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) exhibit remarkable zero-shot generalization but\nsuffer performance degradation under distribution shifts in downstream tasks,\nparticularly in the absence of labeled data. Test-Time Adaptation (TTA)\naddresses this challenge by enabling online optimization of VLMs during\ninference, eliminating the need for annotated data. Cache-based TTA methods\nexploit historical knowledge by maintaining a dynamic memory cache of\nlow-entropy or high-confidence samples, promoting efficient adaptation to\nout-of-distribution data. Nevertheless, these methods face two critical\nchallenges: (1) unreliable confidence metrics under significant distribution\nshifts, resulting in error accumulation within the cache and degraded\nadaptation performance; and (2) rigid decision boundaries that fail to\naccommodate substantial distributional variations, leading to suboptimal\npredictions. To overcome these limitations, we introduce the Adaptive Cache\nEnhancement (ACE) framework, which constructs a robust cache by selectively\nstoring high-confidence or low-entropy image embeddings per class, guided by\ndynamic, class-specific thresholds initialized from zero-shot statistics and\niteratively refined using an exponential moving average and\nexploration-augmented updates. This approach enables adaptive, class-wise\ndecision boundaries, ensuring robust and accurate predictions across diverse\nvisual distributions. Extensive experiments on 15 diverse benchmark datasets\ndemonstrate that ACE achieves state-of-the-art performance, delivering superior\nrobustness and generalization compared to existing TTA methods in challenging\nout-of-distribution scenarios."
                },
                "authors": [
                    {
                        "name": "Khanh-Binh Nguyen"
                    },
                    {
                        "name": "Phuoc-Nguyen Bui"
                    },
                    {
                        "name": "Hyunseung Choo"
                    },
                    {
                        "name": "Duc Thanh Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Duc Thanh Nguyen"
                },
                "author": "Duc Thanh Nguyen",
                "arxiv_comment": "12 pages, Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07570v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07570v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09208v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09208v1",
                "updated": "2025-08-10T14:05:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    10,
                    14,
                    5,
                    36,
                    6,
                    222,
                    0
                ],
                "published": "2025-08-10T14:05:36Z",
                "published_parsed": [
                    2025,
                    8,
                    10,
                    14,
                    5,
                    36,
                    6,
                    222,
                    0
                ],
                "title": "CoMoE: Collaborative Optimization of Expert Aggregation and Offloading\n  for MoE-based LLMs at Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoMoE: Collaborative Optimization of Expert Aggregation and Offloading\n  for MoE-based LLMs at Edge"
                },
                "summary": "The proliferation of large language models (LLMs) has driven the adoption of\nMixture-of-Experts (MoE) architectures as a promising solution to scale model\ncapacity while controlling computational costs. However, deploying MoE models\nin resource-constrained mobile edge computing environments presents significant\nchallenges due to their large memory footprint and dynamic expert activation\npatterns. To address these challenges, we propose a novel dynamic\nresource-aware collaborative optimization framework that jointly optimizes\nexpert aggregation granularity and offloading strategies based on real-time\ndevice resource states, network conditions, and input characteristics in mobile\nedge environments, denoted as CoMoE. In CoMoE, we first systematically analyze\nexisting expert aggregation techniques, including expert parameter\nmerging,knowledge distillation,and parameter sharing decomposition, identifying\ntheir limitations in dynamic mobile environments.We then investigate expert\noffloading strategies encompassing expert prediction and prefetching, expert\ncaching and scheduling, and multi-tier storage architectures, revealing the\ninterdependencies between routing decisions and offloading performance.The\nCoMoE incorporates adaptive scheduling mechanisms that respond to user mobility\nand varying network conditions, enabling efficient MoE deployment across\nheterogeneous edge devices. Extensive experiments on real mobile edge testbeds\ndemonstrate that CoMoE achieves approximately 70% reduction in memory usage\ncompared to baseline methods, 10.5% lower inference latency than existing\nexpert offloading techniques, while maintaining model performance stability.\nFor large-scale MoE models (e.g,7.4B-parameter Switch-Base-128), the CoMoE\nreduces memory requirements from 15.6GB to 4.7GB, enabling deployment on\nresource-constrained mobile edge devices that previously could only support\nmuch smaller models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of large language models (LLMs) has driven the adoption of\nMixture-of-Experts (MoE) architectures as a promising solution to scale model\ncapacity while controlling computational costs. However, deploying MoE models\nin resource-constrained mobile edge computing environments presents significant\nchallenges due to their large memory footprint and dynamic expert activation\npatterns. To address these challenges, we propose a novel dynamic\nresource-aware collaborative optimization framework that jointly optimizes\nexpert aggregation granularity and offloading strategies based on real-time\ndevice resource states, network conditions, and input characteristics in mobile\nedge environments, denoted as CoMoE. In CoMoE, we first systematically analyze\nexisting expert aggregation techniques, including expert parameter\nmerging,knowledge distillation,and parameter sharing decomposition, identifying\ntheir limitations in dynamic mobile environments.We then investigate expert\noffloading strategies encompassing expert prediction and prefetching, expert\ncaching and scheduling, and multi-tier storage architectures, revealing the\ninterdependencies between routing decisions and offloading performance.The\nCoMoE incorporates adaptive scheduling mechanisms that respond to user mobility\nand varying network conditions, enabling efficient MoE deployment across\nheterogeneous edge devices. Extensive experiments on real mobile edge testbeds\ndemonstrate that CoMoE achieves approximately 70% reduction in memory usage\ncompared to baseline methods, 10.5% lower inference latency than existing\nexpert offloading techniques, while maintaining model performance stability.\nFor large-scale MoE models (e.g,7.4B-parameter Switch-Base-128), the CoMoE\nreduces memory requirements from 15.6GB to 4.7GB, enabling deployment on\nresource-constrained mobile edge devices that previously could only support\nmuch smaller models."
                },
                "authors": [
                    {
                        "name": "Muqing Li"
                    },
                    {
                        "name": "Ning Li"
                    },
                    {
                        "name": "Xin Yuan"
                    },
                    {
                        "name": "Wenchao Xu"
                    },
                    {
                        "name": "Quan Chen"
                    },
                    {
                        "name": "Song Guo"
                    },
                    {
                        "name": "Haijun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Haijun Zhang"
                },
                "author": "Haijun Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09208v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09208v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.14769v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.14769v2",
                "updated": "2025-08-09T11:31:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    9,
                    11,
                    31,
                    44,
                    5,
                    221,
                    0
                ],
                "published": "2025-06-17T17:59:12Z",
                "published_parsed": [
                    2025,
                    6,
                    17,
                    17,
                    59,
                    12,
                    1,
                    168,
                    0
                ],
                "title": "CDP: Towards Robust Autoregressive Visuomotor Policy Learning via Causal\n  Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CDP: Towards Robust Autoregressive Visuomotor Policy Learning via Causal\n  Diffusion"
                },
                "summary": "Diffusion Policy (DP) enables robots to learn complex behaviors by imitating\nexpert demonstrations through action diffusion. However, in practical\napplications, hardware limitations often degrade data quality, while real-time\nconstraints restrict model inference to instantaneous state and scene\nobservations. These limitations seriously reduce the efficacy of learning from\nexpert demonstrations, resulting in failures in object localization, grasp\nplanning, and long-horizon task execution. To address these challenges, we\npropose Causal Diffusion Policy (CDP), a novel transformer-based diffusion\nmodel that enhances action prediction by conditioning on historical action\nsequences, thereby enabling more coherent and context-aware visuomotor policy\nlearning. To further mitigate the computational cost associated with\nautoregressive inference, a caching mechanism is also introduced to store\nattention key-value pairs from previous timesteps, substantially reducing\nredundant computations during execution. Extensive experiments in both\nsimulated and real-world environments, spanning diverse 2D and 3D manipulation\ntasks, demonstrate that CDP uniquely leverages historical action sequences to\nachieve significantly higher accuracy than existing methods. Moreover, even\nwhen faced with degraded input observation quality, CDP maintains remarkable\nprecision by reasoning through temporal continuity, which highlights its\npractical robustness for robotic control under realistic, imperfect conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Policy (DP) enables robots to learn complex behaviors by imitating\nexpert demonstrations through action diffusion. However, in practical\napplications, hardware limitations often degrade data quality, while real-time\nconstraints restrict model inference to instantaneous state and scene\nobservations. These limitations seriously reduce the efficacy of learning from\nexpert demonstrations, resulting in failures in object localization, grasp\nplanning, and long-horizon task execution. To address these challenges, we\npropose Causal Diffusion Policy (CDP), a novel transformer-based diffusion\nmodel that enhances action prediction by conditioning on historical action\nsequences, thereby enabling more coherent and context-aware visuomotor policy\nlearning. To further mitigate the computational cost associated with\nautoregressive inference, a caching mechanism is also introduced to store\nattention key-value pairs from previous timesteps, substantially reducing\nredundant computations during execution. Extensive experiments in both\nsimulated and real-world environments, spanning diverse 2D and 3D manipulation\ntasks, demonstrate that CDP uniquely leverages historical action sequences to\nachieve significantly higher accuracy than existing methods. Moreover, even\nwhen faced with degraded input observation quality, CDP maintains remarkable\nprecision by reasoning through temporal continuity, which highlights its\npractical robustness for robotic control under realistic, imperfect conditions."
                },
                "authors": [
                    {
                        "name": "Jiahua Ma"
                    },
                    {
                        "name": "Yiran Qin"
                    },
                    {
                        "name": "Yixiong Li"
                    },
                    {
                        "name": "Xuanqi Liao"
                    },
                    {
                        "name": "Yulan Guo"
                    },
                    {
                        "name": "Ruimao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ruimao Zhang"
                },
                "author": "Ruimao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.14769v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.14769v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06937v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06937v1",
                "updated": "2025-08-09T11:06:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    9,
                    11,
                    6,
                    58,
                    5,
                    221,
                    0
                ],
                "published": "2025-08-09T11:06:58Z",
                "published_parsed": [
                    2025,
                    8,
                    9,
                    11,
                    6,
                    58,
                    5,
                    221,
                    0
                ],
                "title": "CannyEdit: Selective Canny Control and Dual-Prompt Guidance for\n  Training-Free Image Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CannyEdit: Selective Canny Control and Dual-Prompt Guidance for\n  Training-Free Image Editing"
                },
                "summary": "Recent advances in text-to-image (T2I) models have enabled training-free\nregional image editing by leveraging the generative priors of foundation\nmodels. However, existing methods struggle to balance text adherence in edited\nregions, context fidelity in unedited areas, and seamless integration of edits.\nWe introduce CannyEdit, a novel training-free framework that addresses these\nchallenges through two key innovations: (1) Selective Canny Control, which\nmasks the structural guidance of Canny ControlNet in user-specified editable\nregions while strictly preserving details of the source images in unedited\nareas via inversion-phase ControlNet information retention. This enables\nprecise, text-driven edits without compromising contextual integrity. (2)\nDual-Prompt Guidance, which combines local prompts for object-specific edits\nwith a global target prompt to maintain coherent scene interactions. On\nreal-world image editing tasks (addition, replacement, removal), CannyEdit\noutperforms prior methods like KV-Edit, achieving a 2.93 to 10.49 percent\nimprovement in the balance of text adherence and context fidelity. In terms of\nediting seamlessness, user studies reveal only 49.2 percent of general users\nand 42.0 percent of AIGC experts identified CannyEdit's results as AI-edited\nwhen paired with real images without edits, versus 76.08 to 89.09 percent for\ncompetitor methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in text-to-image (T2I) models have enabled training-free\nregional image editing by leveraging the generative priors of foundation\nmodels. However, existing methods struggle to balance text adherence in edited\nregions, context fidelity in unedited areas, and seamless integration of edits.\nWe introduce CannyEdit, a novel training-free framework that addresses these\nchallenges through two key innovations: (1) Selective Canny Control, which\nmasks the structural guidance of Canny ControlNet in user-specified editable\nregions while strictly preserving details of the source images in unedited\nareas via inversion-phase ControlNet information retention. This enables\nprecise, text-driven edits without compromising contextual integrity. (2)\nDual-Prompt Guidance, which combines local prompts for object-specific edits\nwith a global target prompt to maintain coherent scene interactions. On\nreal-world image editing tasks (addition, replacement, removal), CannyEdit\noutperforms prior methods like KV-Edit, achieving a 2.93 to 10.49 percent\nimprovement in the balance of text adherence and context fidelity. In terms of\nediting seamlessness, user studies reveal only 49.2 percent of general users\nand 42.0 percent of AIGC experts identified CannyEdit's results as AI-edited\nwhen paired with real images without edits, versus 76.08 to 89.09 percent for\ncompetitor methods."
                },
                "authors": [
                    {
                        "name": "Weiyan Xie"
                    },
                    {
                        "name": "Han Gao"
                    },
                    {
                        "name": "Didan Deng"
                    },
                    {
                        "name": "Kaican Li"
                    },
                    {
                        "name": "April Hua Liu"
                    },
                    {
                        "name": "Yongxiang Huang"
                    },
                    {
                        "name": "Nevin L. Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Nevin L. Zhang"
                },
                "author": "Nevin L. Zhang",
                "arxiv_comment": "Project Page: vaynexie.github.io/CannyEdit/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06937v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06937v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.03974v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.03974v2",
                "updated": "2025-08-09T02:33:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    9,
                    2,
                    33,
                    21,
                    5,
                    221,
                    0
                ],
                "published": "2025-08-05T23:47:34Z",
                "published_parsed": [
                    2025,
                    8,
                    5,
                    23,
                    47,
                    34,
                    1,
                    217,
                    0
                ],
                "title": "Managing Data for Scalable and Interactive Event Sequence Visualization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Managing Data for Scalable and Interactive Event Sequence Visualization"
                },
                "summary": "Parallel event sequences, such as those collected in program execution traces\nand automated manufacturing pipelines, are typically visualized as interactive\nparallel timelines. As the dataset size grows, these charts frequently\nexperience lag during common interactions such as zooming, panning, and\nfiltering. Summarization approaches can improve interaction performance, but at\nthe cost of accuracy in representation. To address this challenge, we introduce\nESeMan (Event Sequence Manager), an event sequence management system designed\nto support interactive rendering of timeline visualizations with tunable\naccuracy. ESeMan employs hierarchical data structures and intelligent caching\nto provide visualizations with only the data necessary to generate accurate\nsummarizations with significantly reduced data fetch time. We evaluate ESeMan's\nquery times against summed area tables, M4 aggregation, and statistical\nsub-sampling on a variety of program execution traces. Our results demonstrate\nESeMan provides better performance, achieving sub-100ms fetch times while\nmaintaining visualization accuracy at the pixel level. We further present our\nbenchmarking harness, enabling future performance evaluations for event\nsequence visualization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel event sequences, such as those collected in program execution traces\nand automated manufacturing pipelines, are typically visualized as interactive\nparallel timelines. As the dataset size grows, these charts frequently\nexperience lag during common interactions such as zooming, panning, and\nfiltering. Summarization approaches can improve interaction performance, but at\nthe cost of accuracy in representation. To address this challenge, we introduce\nESeMan (Event Sequence Manager), an event sequence management system designed\nto support interactive rendering of timeline visualizations with tunable\naccuracy. ESeMan employs hierarchical data structures and intelligent caching\nto provide visualizations with only the data necessary to generate accurate\nsummarizations with significantly reduced data fetch time. We evaluate ESeMan's\nquery times against summed area tables, M4 aggregation, and statistical\nsub-sampling on a variety of program execution traces. Our results demonstrate\nESeMan provides better performance, achieving sub-100ms fetch times while\nmaintaining visualization accuracy at the pixel level. We further present our\nbenchmarking harness, enabling future performance evaluations for event\nsequence visualization."
                },
                "authors": [
                    {
                        "name": "Sayef Azad Sakin"
                    },
                    {
                        "name": "Katherine E. Isaacs"
                    }
                ],
                "author_detail": {
                    "name": "Katherine E. Isaacs"
                },
                "author": "Katherine E. Isaacs",
                "arxiv_comment": "The 15th IEEE Workshop on Large Data Analysis and Visualization",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.03974v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.03974v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01216v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01216v2",
                "updated": "2025-08-09T00:12:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    9,
                    0,
                    12,
                    1,
                    5,
                    221,
                    0
                ],
                "published": "2025-07-01T22:27:21Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    22,
                    27,
                    21,
                    1,
                    182,
                    0
                ],
                "title": "PAE MobiLLM: Privacy-Aware and Efficient LLM Fine-Tuning on the Mobile\n  Device via Additive Side-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PAE MobiLLM: Privacy-Aware and Efficient LLM Fine-Tuning on the Mobile\n  Device via Additive Side-Tuning"
                },
                "summary": "There is a huge gap between numerous intriguing applications fostered by\non-device large language model (LLM) fine-tuning (FT) from fresh mobile data\nand the limited resources of a mobile device. While existing server-assisted\nmethods (e.g., split learning or side-tuning) may enable LLM FT on the local\nmobile device, they suffer from heavy communication burdens of activation\ntransmissions, and may disclose data and labels to the server. To address those\nissues, we develop PAE MobiLLM, a a privacy-aware and efficient LLM FT method\nwhich can be deployed on the mobile device via server-assisted additive\nside-tuning. To further accelerate FT convergence and improve computing\nefficiency, PAE MobiLLM integrates activation caching on the server side, which\nallows the server to reuse historical activations and saves the mobile device\nfrom repeatedly computing forward passes for the recurring data samples.\nBesides, to reduce communication cost, PAE MobiLLM develops an activation\nshortcut that transmits only the token involved in the loss calculation instead\nof full activation matrices to guide the side network tuning. Last but not\nleast, PAE MobiLLM introduces the additive adapter side-network design which\nmakes the server train the adapter modules based on device-defined prediction\ndifferences rather than raw ground-truth labels. In this way, the server can\nonly assist device-defined side-network computing, and learn nothing about data\nand labels. Extensive experimental results demonstrate PAE MobiLLM's\nsuperiority.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is a huge gap between numerous intriguing applications fostered by\non-device large language model (LLM) fine-tuning (FT) from fresh mobile data\nand the limited resources of a mobile device. While existing server-assisted\nmethods (e.g., split learning or side-tuning) may enable LLM FT on the local\nmobile device, they suffer from heavy communication burdens of activation\ntransmissions, and may disclose data and labels to the server. To address those\nissues, we develop PAE MobiLLM, a a privacy-aware and efficient LLM FT method\nwhich can be deployed on the mobile device via server-assisted additive\nside-tuning. To further accelerate FT convergence and improve computing\nefficiency, PAE MobiLLM integrates activation caching on the server side, which\nallows the server to reuse historical activations and saves the mobile device\nfrom repeatedly computing forward passes for the recurring data samples.\nBesides, to reduce communication cost, PAE MobiLLM develops an activation\nshortcut that transmits only the token involved in the loss calculation instead\nof full activation matrices to guide the side network tuning. Last but not\nleast, PAE MobiLLM introduces the additive adapter side-network design which\nmakes the server train the adapter modules based on device-defined prediction\ndifferences rather than raw ground-truth labels. In this way, the server can\nonly assist device-defined side-network computing, and learn nothing about data\nand labels. Extensive experimental results demonstrate PAE MobiLLM's\nsuperiority."
                },
                "authors": [
                    {
                        "name": "Xingke Yang"
                    },
                    {
                        "name": "Liang Li"
                    },
                    {
                        "name": "Zhiyi Wan"
                    },
                    {
                        "name": "Sicong Li"
                    },
                    {
                        "name": "Xiaoqi Qi"
                    },
                    {
                        "name": "Jiang Liu"
                    },
                    {
                        "name": "Tomoaki Ohtsuki"
                    },
                    {
                        "name": "Xin Fu"
                    },
                    {
                        "name": "Miao Pan"
                    }
                ],
                "author_detail": {
                    "name": "Miao Pan"
                },
                "author": "Miao Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01216v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01216v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11132v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11132v3",
                "updated": "2025-08-08T18:16:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    18,
                    16,
                    33,
                    4,
                    220,
                    0
                ],
                "published": "2025-03-14T06:49:37Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    49,
                    37,
                    4,
                    73,
                    0
                ],
                "title": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and\n  Extreme KV Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and\n  Extreme KV Compression"
                },
                "summary": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid MLA variant through lightweight post-training\nadaptation, bypassing the need for extensive pre-training. We demonstrate that\nleveraging the dark knowledge of a well-trained model can enhance training\naccuracy and enable extreme KV cache compression in MLA without compromising\nmodel performance. The experimental results show that our proposed method can\neffectively compress the KV cache while preserving the performance on the\nbenchmarks; specifically, for Llama3.2-1B-Instruct baseline, a 6.4x compression\nachieves the same average score by using only 3.6B training tokens and 70 GPU\nhours on AMD MI300, whereas a 10.6x compression have less than 0.1% average\nscore drop with 7B training tokens and 140 GPU hours. The code for this work is\navailable at https://github.com/AMD-AIG-AIMA/AMD-Hybrid-Models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid MLA variant through lightweight post-training\nadaptation, bypassing the need for extensive pre-training. We demonstrate that\nleveraging the dark knowledge of a well-trained model can enhance training\naccuracy and enable extreme KV cache compression in MLA without compromising\nmodel performance. The experimental results show that our proposed method can\neffectively compress the KV cache while preserving the performance on the\nbenchmarks; specifically, for Llama3.2-1B-Instruct baseline, a 6.4x compression\nachieves the same average score by using only 3.6B training tokens and 70 GPU\nhours on AMD MI300, whereas a 10.6x compression have less than 0.1% average\nscore drop with 7B training tokens and 140 GPU hours. The code for this work is\navailable at https://github.com/AMD-AIG-AIMA/AMD-Hybrid-Models."
                },
                "authors": [
                    {
                        "name": "Guihong Li"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    },
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Vikram Appia"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11132v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11132v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06447v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06447v1",
                "updated": "2025-08-08T16:42:38Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    16,
                    42,
                    38,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T16:42:38Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    16,
                    42,
                    38,
                    4,
                    220,
                    0
                ],
                "title": "SlimInfer: Accelerating Long-Context LLM Inference via Dynamic Token\n  Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SlimInfer: Accelerating Long-Context LLM Inference via Dynamic Token\n  Pruning"
                },
                "summary": "Long-context inference for Large Language Models (LLMs) is heavily limited by\nhigh computational demands. While several existing methods optimize attention\ncomputation, they still process the full set of hidden states at each layer,\nlimiting overall efficiency. In this work, we propose SlimInfer, an innovative\nframework that aims to accelerate inference by directly pruning less critical\nprompt tokens during the forward pass. Our key insight is an information\ndiffusion phenomenon: As information from critical tokens propagates through\nlayers, it becomes distributed across the entire sequence. This diffusion\nprocess suggests that LLMs can maintain their semantic integrity when excessive\ntokens, even including these critical ones, are pruned in hidden states.\nMotivated by this, SlimInfer introduces a dynamic fine-grained pruning\nmechanism that accurately removes redundant tokens of hidden state at\nintermediate layers. This layer-wise pruning naturally enables an asynchronous\nKV cache manager that prefetches required token blocks without complex\npredictors, reducing both memory usage and I/O costs. Extensive experiments\nshow that SlimInfer can achieve up to $\\mathbf{2.53\\times}$ time-to-first-token\n(TTFT) speedup and $\\mathbf{1.88\\times}$ end-to-end latency reduction for\nLLaMA3.1-8B-Instruct on a single RTX 4090, without sacrificing performance on\nLongBench. Our code will be released upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context inference for Large Language Models (LLMs) is heavily limited by\nhigh computational demands. While several existing methods optimize attention\ncomputation, they still process the full set of hidden states at each layer,\nlimiting overall efficiency. In this work, we propose SlimInfer, an innovative\nframework that aims to accelerate inference by directly pruning less critical\nprompt tokens during the forward pass. Our key insight is an information\ndiffusion phenomenon: As information from critical tokens propagates through\nlayers, it becomes distributed across the entire sequence. This diffusion\nprocess suggests that LLMs can maintain their semantic integrity when excessive\ntokens, even including these critical ones, are pruned in hidden states.\nMotivated by this, SlimInfer introduces a dynamic fine-grained pruning\nmechanism that accurately removes redundant tokens of hidden state at\nintermediate layers. This layer-wise pruning naturally enables an asynchronous\nKV cache manager that prefetches required token blocks without complex\npredictors, reducing both memory usage and I/O costs. Extensive experiments\nshow that SlimInfer can achieve up to $\\mathbf{2.53\\times}$ time-to-first-token\n(TTFT) speedup and $\\mathbf{1.88\\times}$ end-to-end latency reduction for\nLLaMA3.1-8B-Instruct on a single RTX 4090, without sacrificing performance on\nLongBench. Our code will be released upon acceptance."
                },
                "authors": [
                    {
                        "name": "Lingkun Long"
                    },
                    {
                        "name": "Rubing Yang"
                    },
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Desheng Hui"
                    },
                    {
                        "name": "Ao Zhou"
                    },
                    {
                        "name": "Jianlei Yang"
                    }
                ],
                "author_detail": {
                    "name": "Jianlei Yang"
                },
                "author": "Jianlei Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06447v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06447v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07467v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07467v3",
                "updated": "2025-08-08T14:25:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    14,
                    25,
                    24,
                    4,
                    220,
                    0
                ],
                "published": "2024-06-11T17:13:18Z",
                "published_parsed": [
                    2024,
                    6,
                    11,
                    17,
                    13,
                    18,
                    1,
                    163,
                    0
                ],
                "title": "LLM meets ML: Data-efficient Anomaly Detection on Unstable Logs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM meets ML: Data-efficient Anomaly Detection on Unstable Logs"
                },
                "summary": "Most log-based anomaly detectors assume logs are stable, though logs are\noften unstable due to software or environmental changes. Anomaly detection on\nunstable logs (ULAD) is therefore a more realistic, yet under-investigated\nchallenge. Current approaches predominantly employ machine learning (ML)\nmodels, which often require extensive labeled data for training. To mitigate\ndata insufficiency, we propose FlexLog, a novel hybrid approach for ULAD that\ncombines ML models -- decision tree, k-nearest neighbors, and a feedforward\nneural network -- with a Large Language Model (Mistral) through ensemble\nlearning. FlexLog also incorporates a cache and retrieval-augmented generation\n(RAG) to further enhance efficiency and effectiveness. To evaluate FlexLog, we\nconfigured four datasets for \\task, namely ADFA-U, LOGEVOL-U, SynHDFS-U, and\nSYNEVOL-U. FlexLog outperforms all baselines by at least 1.2 percentage points\n(pp) in F1 score while using much less labeled data (62.87 pp reduction). When\ntrained on the same amount of data as the baselines, FlexLog achieves up to a\n13 pp increase in F1 score on ADFA-U across varying training dataset sizes.\nAdditionally, FlexLog maintains inference time under one second per log\nsequence, making it suitable for most applications, except latency-sensitive\nsystems. Further analysis reveals the positive impact of FlexLog's key\ncomponents: cache, RAG and ensemble learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most log-based anomaly detectors assume logs are stable, though logs are\noften unstable due to software or environmental changes. Anomaly detection on\nunstable logs (ULAD) is therefore a more realistic, yet under-investigated\nchallenge. Current approaches predominantly employ machine learning (ML)\nmodels, which often require extensive labeled data for training. To mitigate\ndata insufficiency, we propose FlexLog, a novel hybrid approach for ULAD that\ncombines ML models -- decision tree, k-nearest neighbors, and a feedforward\nneural network -- with a Large Language Model (Mistral) through ensemble\nlearning. FlexLog also incorporates a cache and retrieval-augmented generation\n(RAG) to further enhance efficiency and effectiveness. To evaluate FlexLog, we\nconfigured four datasets for \\task, namely ADFA-U, LOGEVOL-U, SynHDFS-U, and\nSYNEVOL-U. FlexLog outperforms all baselines by at least 1.2 percentage points\n(pp) in F1 score while using much less labeled data (62.87 pp reduction). When\ntrained on the same amount of data as the baselines, FlexLog achieves up to a\n13 pp increase in F1 score on ADFA-U across varying training dataset sizes.\nAdditionally, FlexLog maintains inference time under one second per log\nsequence, making it suitable for most applications, except latency-sensitive\nsystems. Further analysis reveals the positive impact of FlexLog's key\ncomponents: cache, RAG and ensemble learning."
                },
                "authors": [
                    {
                        "name": "Fatemeh Hadadi"
                    },
                    {
                        "name": "Qinghua Xu"
                    },
                    {
                        "name": "Domenico Bianculli"
                    },
                    {
                        "name": "Lionel Briand"
                    }
                ],
                "author_detail": {
                    "name": "Lionel Briand"
                },
                "author": "Lionel Briand",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07467v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07467v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06297v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06297v1",
                "updated": "2025-08-08T13:19:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    13,
                    19,
                    30,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T13:19:30Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    13,
                    19,
                    30,
                    4,
                    220,
                    0
                ],
                "title": "KV Cache Compression for Inference Efficiency in LLMs: A Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Cache Compression for Inference Efficiency in LLMs: A Review"
                },
                "summary": "Withtherapid advancement of large language models (LLMs), the context length\nfor inference has been continuously increasing, leading to an exponential\ngrowth in the demand for Key-Value (KV) caching. This has resulted in a\nsignificant memory bottleneck, limiting the inference efficiency and\nscalability of the models. Therefore, optimizing the KV cache during inference\nis crucial for enhancing performance and efficiency. This review systematically\nexamines current KV cache optimization techniques, including compression\nstrategies such as selective token strategies, quantization, and attention\ncompression. We evaluate the effectiveness, trade-offs, and application\nscenarios of these methods, providing a comprehensive analysis of their impact\non memory usage and inference speed. We focus on identifying the limitations\nand challenges of existing methods, such as compatibility issues with different\nmodels and tasks. Additionally, this review highlights future research\ndirections, including hybrid optimization techniques, adaptive dynamic\nstrategies, and software-hardware co-design. These approaches aim to improve\ninference efficiency and promote the practical application of large language\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Withtherapid advancement of large language models (LLMs), the context length\nfor inference has been continuously increasing, leading to an exponential\ngrowth in the demand for Key-Value (KV) caching. This has resulted in a\nsignificant memory bottleneck, limiting the inference efficiency and\nscalability of the models. Therefore, optimizing the KV cache during inference\nis crucial for enhancing performance and efficiency. This review systematically\nexamines current KV cache optimization techniques, including compression\nstrategies such as selective token strategies, quantization, and attention\ncompression. We evaluate the effectiveness, trade-offs, and application\nscenarios of these methods, providing a comprehensive analysis of their impact\non memory usage and inference speed. We focus on identifying the limitations\nand challenges of existing methods, such as compatibility issues with different\nmodels and tasks. Additionally, this review highlights future research\ndirections, including hybrid optimization techniques, adaptive dynamic\nstrategies, and software-hardware co-design. These approaches aim to improve\ninference efficiency and promote the practical application of large language\nmodels."
                },
                "authors": [
                    {
                        "name": "Yanyu Liu"
                    },
                    {
                        "name": "Jingying Fu"
                    },
                    {
                        "name": "Sixiang Liu"
                    },
                    {
                        "name": "Yitian Zou"
                    },
                    {
                        "name": "You Fu"
                    },
                    {
                        "name": "Jiehan Zhou"
                    },
                    {
                        "name": "Shouhua Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shouhua Zhang"
                },
                "arxiv_affiliation": "University of Oulu",
                "author": "Shouhua Zhang",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06297v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06297v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06160v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06160v1",
                "updated": "2025-08-08T09:29:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    9,
                    29,
                    37,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T09:29:37Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    9,
                    29,
                    37,
                    4,
                    220,
                    0
                ],
                "title": "Fewer Denoising Steps or Cheaper Per-Step Inference: Towards\n  Compute-Optimal Diffusion Model Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fewer Denoising Steps or Cheaper Per-Step Inference: Towards\n  Compute-Optimal Diffusion Model Deployment"
                },
                "summary": "Diffusion models have shown remarkable success across generative tasks, yet\ntheir high computational demands challenge deployment on resource-limited\nplatforms. This paper investigates a critical question for compute-optimal\ndiffusion model deployment: Under a post-training setting without fine-tuning,\nis it more effective to reduce the number of denoising steps or to use a\ncheaper per-step inference? Intuitively, reducing the number of denoising steps\nincreases the variability of the distributions across steps, making the model\nmore sensitive to compression. In contrast, keeping more denoising steps makes\nthe differences smaller, preserving redundancy, and making post-training\ncompression more feasible. To systematically examine this, we propose PostDiff,\na training-free framework for accelerating pre-trained diffusion models by\nreducing redundancy at both the input level and module level in a post-training\nmanner. At the input level, we propose a mixed-resolution denoising scheme\nbased on the insight that reducing generation resolution in early denoising\nsteps can enhance low-frequency components and improve final generation\nfidelity. At the module level, we employ a hybrid module caching strategy to\nreuse computations across denoising steps. Extensive experiments and ablation\nstudies demonstrate that (1) PostDiff can significantly improve the\nfidelity-efficiency trade-off of state-of-the-art diffusion models, and (2) to\nboost efficiency while maintaining decent generation fidelity, reducing\nper-step inference cost is often more effective than reducing the number of\ndenoising steps. Our code is available at\nhttps://github.com/GATECH-EIC/PostDiff.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have shown remarkable success across generative tasks, yet\ntheir high computational demands challenge deployment on resource-limited\nplatforms. This paper investigates a critical question for compute-optimal\ndiffusion model deployment: Under a post-training setting without fine-tuning,\nis it more effective to reduce the number of denoising steps or to use a\ncheaper per-step inference? Intuitively, reducing the number of denoising steps\nincreases the variability of the distributions across steps, making the model\nmore sensitive to compression. In contrast, keeping more denoising steps makes\nthe differences smaller, preserving redundancy, and making post-training\ncompression more feasible. To systematically examine this, we propose PostDiff,\na training-free framework for accelerating pre-trained diffusion models by\nreducing redundancy at both the input level and module level in a post-training\nmanner. At the input level, we propose a mixed-resolution denoising scheme\nbased on the insight that reducing generation resolution in early denoising\nsteps can enhance low-frequency components and improve final generation\nfidelity. At the module level, we employ a hybrid module caching strategy to\nreuse computations across denoising steps. Extensive experiments and ablation\nstudies demonstrate that (1) PostDiff can significantly improve the\nfidelity-efficiency trade-off of state-of-the-art diffusion models, and (2) to\nboost efficiency while maintaining decent generation fidelity, reducing\nper-step inference cost is often more effective than reducing the number of\ndenoising steps. Our code is available at\nhttps://github.com/GATECH-EIC/PostDiff."
                },
                "authors": [
                    {
                        "name": "Zhenbang Du"
                    },
                    {
                        "name": "Yonggan Fu"
                    },
                    {
                        "name": "Lifu Wang"
                    },
                    {
                        "name": "Jiayi Qian"
                    },
                    {
                        "name": "Xiao Luo"
                    },
                    {
                        "name": "Yingyan"
                    },
                    {
                        "name": "Lin"
                    }
                ],
                "author_detail": {
                    "name": "Lin"
                },
                "arxiv_affiliation": "Celine",
                "author": "Lin",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06160v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06160v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06133v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06133v1",
                "updated": "2025-08-08T08:54:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    8,
                    54,
                    21,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T08:54:21Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    8,
                    54,
                    21,
                    4,
                    220,
                    0
                ],
                "title": "LLM Serving Optimization with Variable Prefill and Decode Lengths",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Serving Optimization with Variable Prefill and Decode Lengths"
                },
                "summary": "We study the problem of serving LLM (Large Language Model) requests where\neach request has heterogeneous prefill and decode lengths. In LLM serving, the\nprefill length corresponds to the input prompt length, which determines the\ninitial memory usage in the KV cache. The decode length refers to the number of\noutput tokens generated sequentially, with each additional token increasing the\nKV cache memory usage by one unit. Given a set of n requests, our goal is to\nschedule and process them to minimize the total completion time. We show that\nthis problem is NP-hard due to the interplay of batching, placement\nconstraints, precedence relationships, and linearly increasing memory usage. We\nthen analyze commonly used scheduling strategies in practice, such as\nFirst-Come-First-Serve (FCFS) and Shortest-First (SF), and prove that their\ncompetitive ratios scale up sublinearly with the memory limit-a significant\ndrawback in real-world settings where memory demand is large. To address this,\nwe propose a novel algorithm based on a new selection metric that efficiently\nforms batches over time. We prove that this algorithm achieves a constant\ncompetitive ratio. Finally, we develop and evaluate a few algorithm variants\ninspired by this approach, including dynamic programming variants, local search\nmethods, and an LP-based scheduler, demonstrating through comprehensive\nsimulations that they outperform standard baselines while maintaining\ncomputational efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the problem of serving LLM (Large Language Model) requests where\neach request has heterogeneous prefill and decode lengths. In LLM serving, the\nprefill length corresponds to the input prompt length, which determines the\ninitial memory usage in the KV cache. The decode length refers to the number of\noutput tokens generated sequentially, with each additional token increasing the\nKV cache memory usage by one unit. Given a set of n requests, our goal is to\nschedule and process them to minimize the total completion time. We show that\nthis problem is NP-hard due to the interplay of batching, placement\nconstraints, precedence relationships, and linearly increasing memory usage. We\nthen analyze commonly used scheduling strategies in practice, such as\nFirst-Come-First-Serve (FCFS) and Shortest-First (SF), and prove that their\ncompetitive ratios scale up sublinearly with the memory limit-a significant\ndrawback in real-world settings where memory demand is large. To address this,\nwe propose a novel algorithm based on a new selection metric that efficiently\nforms batches over time. We prove that this algorithm achieves a constant\ncompetitive ratio. Finally, we develop and evaluate a few algorithm variants\ninspired by this approach, including dynamic programming variants, local search\nmethods, and an LP-based scheduler, demonstrating through comprehensive\nsimulations that they outperform standard baselines while maintaining\ncomputational efficiency."
                },
                "authors": [
                    {
                        "name": "Meixuan Wang"
                    },
                    {
                        "name": "Yinyu Ye"
                    },
                    {
                        "name": "Zijie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zijie Zhou"
                },
                "author": "Zijie Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06133v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06133v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06064v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06064v1",
                "updated": "2025-08-08T06:53:50Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    6,
                    53,
                    50,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T06:53:50Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    6,
                    53,
                    50,
                    4,
                    220,
                    0
                ],
                "title": "A Generic Complete Anytime Beam Search for Optimal Decision Tree",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Generic Complete Anytime Beam Search for Optimal Decision Tree"
                },
                "summary": "Finding an optimal decision tree that minimizes classification error is known\nto be NP-hard. While exact algorithms based on MILP, CP, SAT, or dynamic\nprogramming guarantee optimality, they often suffer from poor anytime behavior\n-- meaning they struggle to find high-quality decision trees quickly when the\nsearch is stopped before completion -- due to unbalanced search space\nexploration. To address this, several anytime extensions of exact methods have\nbeen proposed, such as LDS-DL8.5, Top-k-DL8.5, and Blossom, but they have not\nbeen systematically compared, making it difficult to assess their relative\neffectiveness. In this paper, we propose CA-DL8.5, a generic, complete, and\nanytime beam search algorithm that extends the DL8.5 framework and unifies some\nexisting anytime strategies. In particular, CA-DL8.5 generalizes previous\napproaches LDS-DL8.5 and Top-k-DL8.5, by allowing the integration of various\nheuristics and relaxation mechanisms through a modular design. The algorithm\nreuses DL8.5's efficient branch-and-bound pruning and trie-based caching,\ncombined with a restart-based beam search that gradually relaxes pruning\ncriteria to improve solution quality over time. Our contributions are twofold:\n(1) We introduce this new generic framework for exact and anytime decision tree\nlearning, enabling the incorporation of diverse heuristics and search\nstrategies; (2) We conduct a rigorous empirical comparison of several\ninstantiations of CA-DL8.5 -- based on Purity, Gain, Discrepancy, and Top-k\nheuristics -- using an anytime evaluation metric called the primal gap\nintegral. Experimental results on standard classification benchmarks show that\nCA-DL8.5 using LDS (limited discrepancy) consistently provides the best anytime\nperformance, outperforming both other CA-DL8.5 variants and the Blossom\nalgorithm while maintaining completeness and optimality guarantees.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finding an optimal decision tree that minimizes classification error is known\nto be NP-hard. While exact algorithms based on MILP, CP, SAT, or dynamic\nprogramming guarantee optimality, they often suffer from poor anytime behavior\n-- meaning they struggle to find high-quality decision trees quickly when the\nsearch is stopped before completion -- due to unbalanced search space\nexploration. To address this, several anytime extensions of exact methods have\nbeen proposed, such as LDS-DL8.5, Top-k-DL8.5, and Blossom, but they have not\nbeen systematically compared, making it difficult to assess their relative\neffectiveness. In this paper, we propose CA-DL8.5, a generic, complete, and\nanytime beam search algorithm that extends the DL8.5 framework and unifies some\nexisting anytime strategies. In particular, CA-DL8.5 generalizes previous\napproaches LDS-DL8.5 and Top-k-DL8.5, by allowing the integration of various\nheuristics and relaxation mechanisms through a modular design. The algorithm\nreuses DL8.5's efficient branch-and-bound pruning and trie-based caching,\ncombined with a restart-based beam search that gradually relaxes pruning\ncriteria to improve solution quality over time. Our contributions are twofold:\n(1) We introduce this new generic framework for exact and anytime decision tree\nlearning, enabling the incorporation of diverse heuristics and search\nstrategies; (2) We conduct a rigorous empirical comparison of several\ninstantiations of CA-DL8.5 -- based on Purity, Gain, Discrepancy, and Top-k\nheuristics -- using an anytime evaluation metric called the primal gap\nintegral. Experimental results on standard classification benchmarks show that\nCA-DL8.5 using LDS (limited discrepancy) consistently provides the best anytime\nperformance, outperforming both other CA-DL8.5 variants and the Blossom\nalgorithm while maintaining completeness and optimality guarantees."
                },
                "authors": [
                    {
                        "name": "Harold Silvère Kiossou"
                    },
                    {
                        "name": "Siegfried Nijssen"
                    },
                    {
                        "name": "Pierre Schaus"
                    }
                ],
                "author_detail": {
                    "name": "Pierre Schaus"
                },
                "author": "Pierre Schaus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06064v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06064v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2104.13123v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2104.13123v3",
                "updated": "2025-08-08T06:38:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    6,
                    38,
                    1,
                    4,
                    220,
                    0
                ],
                "published": "2021-04-27T11:55:54Z",
                "published_parsed": [
                    2021,
                    4,
                    27,
                    11,
                    55,
                    54,
                    1,
                    117,
                    0
                ],
                "title": "Affine Springer fibers and depth zero L-packets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Affine Springer fibers and depth zero L-packets"
                },
                "summary": "Let $G$ be a connected reductive group over a field $F=\\mathbb{F}_q((t))$\nsplitting over $\\overline{\\mathbb{F}}_q((t))$. Following [KV,DR], a tamely\nunramified Langlands parameter $\\lambda:W_F\\to{}^L\nG(\\overline{\\mathbb{Q}}_{\\ell})$ in general position gives rise to a finite set\n$\\Pi_{\\lambda}$ of irreducible admissible representations of $G(F)$, called the\n$L$-packet.\n  The main goal of this work is to provide a geometric description of\ncharacters $\\chi_{\\pi}$ of $\\pi\\in\\Pi_{\\lambda}$ and of their endoscopic linear\ncombinations $\\chi_{\\lambda}^{\\kappa}$ in terms of homology of affine Springer\nfibers, thus establishing an analog of Lusztig conjectures in this case.\nFurthermore, each $\\chi_{\\lambda}^{\\kappa}$ can be described as the trace of\nFrobenius function of a conjugation equivariant perverse sheaf on the loop\ngroup by the sheaf-function correspondence.\n  As another application, we prove that the sum\n$\\chi_{\\lambda}^{st}:=\\sum_{\\pi\\in\\Pi_{\\lambda}}\\chi_{\\pi}$ is stable and show\nthat the $\\chi_{\\lambda}^{st}$'s are compatible with inner twistings. More\ngenerally, we prove that each $\\chi_{\\lambda}^{\\kappa}$ is\n$\\mathcal{E}_{\\lambda,\\kappa}$-stable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Let $G$ be a connected reductive group over a field $F=\\mathbb{F}_q((t))$\nsplitting over $\\overline{\\mathbb{F}}_q((t))$. Following [KV,DR], a tamely\nunramified Langlands parameter $\\lambda:W_F\\to{}^L\nG(\\overline{\\mathbb{Q}}_{\\ell})$ in general position gives rise to a finite set\n$\\Pi_{\\lambda}$ of irreducible admissible representations of $G(F)$, called the\n$L$-packet.\n  The main goal of this work is to provide a geometric description of\ncharacters $\\chi_{\\pi}$ of $\\pi\\in\\Pi_{\\lambda}$ and of their endoscopic linear\ncombinations $\\chi_{\\lambda}^{\\kappa}$ in terms of homology of affine Springer\nfibers, thus establishing an analog of Lusztig conjectures in this case.\nFurthermore, each $\\chi_{\\lambda}^{\\kappa}$ can be described as the trace of\nFrobenius function of a conjugation equivariant perverse sheaf on the loop\ngroup by the sheaf-function correspondence.\n  As another application, we prove that the sum\n$\\chi_{\\lambda}^{st}:=\\sum_{\\pi\\in\\Pi_{\\lambda}}\\chi_{\\pi}$ is stable and show\nthat the $\\chi_{\\lambda}^{st}$'s are compatible with inner twistings. More\ngenerally, we prove that each $\\chi_{\\lambda}^{\\kappa}$ is\n$\\mathcal{E}_{\\lambda,\\kappa}$-stable."
                },
                "authors": [
                    {
                        "name": "Roman Bezrukavnikov"
                    },
                    {
                        "name": "Yakov Varshavsky"
                    }
                ],
                "author_detail": {
                    "name": "Yakov Varshavsky"
                },
                "author": "Yakov Varshavsky",
                "arxiv_comment": "v.3, 96 pages, minor changes in abstract and introduction",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2104.13123v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2104.13123v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.RT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.RT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.AG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "22E50, 22E57",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09192v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09192v1",
                "updated": "2025-08-08T04:51:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    8,
                    4,
                    51,
                    37,
                    4,
                    220,
                    0
                ],
                "published": "2025-08-08T04:51:37Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    4,
                    51,
                    37,
                    4,
                    220,
                    0
                ],
                "title": "Diffusion LLMs Can Do Faster-Than-AR Inference via Discrete Diffusion\n  Forcing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion LLMs Can Do Faster-Than-AR Inference via Discrete Diffusion\n  Forcing"
                },
                "summary": "Diffusion Large Language Models (dLLMs) have emerged as a promising\nalternative to autoregressive (AR) LLMs for text generation, with the potential\nto decode multiple tokens in a single iteration. However, none of the existing\nopen-source dLLMs have achieved superior inference speed over AR LLMs of\nsimilar size. This paper breaks this barrier based on a simple and effective\nstrategy named discrete diffusion forcing (D2F). D2F equips dLLMs with two key\ncapabilities: (1) block-wise autoregressive generation to enable KV cache\nutilization; (2) prediction of following tokens without requiring completion of\nprior blocks for inter-block parallel decoding. In this way, the vanilla dLLMs\nare refurbished into an AR-diffusion hybrid paradigm for efficient inference.\nD2F can be implemented with an asymmetric distillation process based on\npre-trained dLLMs. We further propose a pipelined parallel decoding algorithm,\nwhich enables a trade-off between efficiency and efficacy. Empirically, D2F\ndLLMs achieve more than $\\mathbf{2.5\\times}$ inference speed than LLaMA3 and\nQwen2.5 on GSM8K. Compared to vanilla dLLMs like LLaDA and Dream, the\nacceleration can be more than $\\mathbf{50\\times}$ while maintaining comparable\noutput quality. The code is available at\nhttps://github.com/zhijie-group/Discrete-Diffusion-Forcing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Large Language Models (dLLMs) have emerged as a promising\nalternative to autoregressive (AR) LLMs for text generation, with the potential\nto decode multiple tokens in a single iteration. However, none of the existing\nopen-source dLLMs have achieved superior inference speed over AR LLMs of\nsimilar size. This paper breaks this barrier based on a simple and effective\nstrategy named discrete diffusion forcing (D2F). D2F equips dLLMs with two key\ncapabilities: (1) block-wise autoregressive generation to enable KV cache\nutilization; (2) prediction of following tokens without requiring completion of\nprior blocks for inter-block parallel decoding. In this way, the vanilla dLLMs\nare refurbished into an AR-diffusion hybrid paradigm for efficient inference.\nD2F can be implemented with an asymmetric distillation process based on\npre-trained dLLMs. We further propose a pipelined parallel decoding algorithm,\nwhich enables a trade-off between efficiency and efficacy. Empirically, D2F\ndLLMs achieve more than $\\mathbf{2.5\\times}$ inference speed than LLaMA3 and\nQwen2.5 on GSM8K. Compared to vanilla dLLMs like LLaDA and Dream, the\nacceleration can be more than $\\mathbf{50\\times}$ while maintaining comparable\noutput quality. The code is available at\nhttps://github.com/zhijie-group/Discrete-Diffusion-Forcing."
                },
                "authors": [
                    {
                        "name": "Xu Wang"
                    },
                    {
                        "name": "Chenkai Xu"
                    },
                    {
                        "name": "Yijie Jin"
                    },
                    {
                        "name": "Jiachun Jin"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Zhijie Deng"
                    }
                ],
                "author_detail": {
                    "name": "Zhijie Deng"
                },
                "author": "Zhijie Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09192v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09192v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05904v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05904v1",
                "updated": "2025-08-07T23:53:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    23,
                    53,
                    31,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T23:53:31Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    23,
                    53,
                    31,
                    3,
                    219,
                    0
                ],
                "title": "Snowpark: Performant, Secure, User-Friendly Data Engineering and AI/ML\n  Next To Your Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Snowpark: Performant, Secure, User-Friendly Data Engineering and AI/ML\n  Next To Your Data"
                },
                "summary": "Snowflake revolutionized data analytics with an elastic architecture that\ndecouples compute and storage, enabling scalable solutions supporting data\narchitectures like data lake, data warehouse, data lakehouse, and data mesh.\nBuilding on this foundation, Snowflake has advanced its AI Data Cloud vision by\nintroducing Snowpark, a managed turnkey solution that supports data engineering\nand AI and ML workloads using Python and other programming languages.\n  This paper outlines Snowpark's design objectives towards high performance,\nstrong security and governance, and ease of use. We detail the architecture of\nSnowpark, highlighting its elastic scalability and seamless integration with\nSnowflake core compute infrastructure. This includes leveraging Snowflake\ncontrol plane for distributed computing and employing a secure sandbox for\nisolating Snowflake SQL workloads from Snowpark executions. Additionally, we\npresent core innovations in Snowpark that drive further performance\nenhancements, such as query initialization latency reduction through Python\npackage caching, improved workload scheduling for customized workloads, and\ndata skew management via efficient row redistribution. Finally, we showcase\nreal-world case studies that illustrate Snowpark's efficiency and effectiveness\nfor large-scale data engineering and AI and ML tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Snowflake revolutionized data analytics with an elastic architecture that\ndecouples compute and storage, enabling scalable solutions supporting data\narchitectures like data lake, data warehouse, data lakehouse, and data mesh.\nBuilding on this foundation, Snowflake has advanced its AI Data Cloud vision by\nintroducing Snowpark, a managed turnkey solution that supports data engineering\nand AI and ML workloads using Python and other programming languages.\n  This paper outlines Snowpark's design objectives towards high performance,\nstrong security and governance, and ease of use. We detail the architecture of\nSnowpark, highlighting its elastic scalability and seamless integration with\nSnowflake core compute infrastructure. This includes leveraging Snowflake\ncontrol plane for distributed computing and employing a secure sandbox for\nisolating Snowflake SQL workloads from Snowpark executions. Additionally, we\npresent core innovations in Snowpark that drive further performance\nenhancements, such as query initialization latency reduction through Python\npackage caching, improved workload scheduling for customized workloads, and\ndata skew management via efficient row redistribution. Finally, we showcase\nreal-world case studies that illustrate Snowpark's efficiency and effectiveness\nfor large-scale data engineering and AI and ML tasks."
                },
                "authors": [
                    {
                        "name": "Brandon Baker"
                    },
                    {
                        "name": "Elliott Brossard"
                    },
                    {
                        "name": "Chenwei Xie"
                    },
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Deen Liu"
                    },
                    {
                        "name": "Yijun Xie"
                    },
                    {
                        "name": "Arthur Zwiegincew"
                    },
                    {
                        "name": "Nitya Kumar Sharma"
                    },
                    {
                        "name": "Gaurav Jain"
                    },
                    {
                        "name": "Eugene Retunsky"
                    },
                    {
                        "name": "Mike Halcrow"
                    },
                    {
                        "name": "Derek Denny-Brown"
                    },
                    {
                        "name": "Istvan Cseri"
                    },
                    {
                        "name": "Tyler Akidau"
                    },
                    {
                        "name": "Yuxiong He"
                    }
                ],
                "author_detail": {
                    "name": "Yuxiong He"
                },
                "author": "Yuxiong He",
                "arxiv_comment": "12 pages, 6 figures, accepted in ICDCS 2025",
                "arxiv_journal_ref": "Proc. 45th IEEE International Conference on Distributed Computing\n  Systems (ICDCS), Glasgow, UK, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05904v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05904v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05898v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05898v1",
                "updated": "2025-08-07T23:11:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    23,
                    11,
                    33,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T23:11:33Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    23,
                    11,
                    33,
                    3,
                    219,
                    0
                ],
                "title": "ETTA: Efficient Test-Time Adaptation for Vision-Language Models through\n  Dynamic Embedding Updates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ETTA: Efficient Test-Time Adaptation for Vision-Language Models through\n  Dynamic Embedding Updates"
                },
                "summary": "Pretrained vision-language models (VLMs) like CLIP show strong zero-shot\nperformance but struggle with generalization under distribution shifts.\nTest-Time Adaptation (TTA) addresses this by adapting VLMs to unlabeled test\ndata in new domains. While some TTA methods rely on prompt-tuning,\ntraining-free cache-based approaches are preferred for efficiency. However,\ncurrent cache-based TTA models store only a limited set of high-confidence\nsamples, restricting the decision boundary to these samples and ignoring the\ninfluence of other incoming test data. To address this, we propose Efficient\nTest-Time Adaptation (ETTA), introducing a Recursive Updating module that\nintegrates all incoming test samples, progressively refining the decision\nboundary. This strategy mimics an unbounded cache, dynamically updating\ncontextual embeddings for improved accuracy with minimal memory and\ncomputational overhead. ETTA also includes an Adaptive Ensemble module to\nreduce prompt dependency in image-to-text scores by dynamically selecting\noptimal prompts for each class. Furthermore, ETTA adaptively combines scores\nfrom both modules based on confidence levels, leveraging their complementary\nstrengths. Extensive experiments on two benchmarks confirm that ETTA surpasses\nthe state-of-the-art TTA models in computational complexity and accuracy,\nsetting a new standard for effective, efficient test-time adaptation. The code\nhas been released at https://github.com/hamidreza-dastmalchi/ETTA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pretrained vision-language models (VLMs) like CLIP show strong zero-shot\nperformance but struggle with generalization under distribution shifts.\nTest-Time Adaptation (TTA) addresses this by adapting VLMs to unlabeled test\ndata in new domains. While some TTA methods rely on prompt-tuning,\ntraining-free cache-based approaches are preferred for efficiency. However,\ncurrent cache-based TTA models store only a limited set of high-confidence\nsamples, restricting the decision boundary to these samples and ignoring the\ninfluence of other incoming test data. To address this, we propose Efficient\nTest-Time Adaptation (ETTA), introducing a Recursive Updating module that\nintegrates all incoming test samples, progressively refining the decision\nboundary. This strategy mimics an unbounded cache, dynamically updating\ncontextual embeddings for improved accuracy with minimal memory and\ncomputational overhead. ETTA also includes an Adaptive Ensemble module to\nreduce prompt dependency in image-to-text scores by dynamically selecting\noptimal prompts for each class. Furthermore, ETTA adaptively combines scores\nfrom both modules based on confidence levels, leveraging their complementary\nstrengths. Extensive experiments on two benchmarks confirm that ETTA surpasses\nthe state-of-the-art TTA models in computational complexity and accuracy,\nsetting a new standard for effective, efficient test-time adaptation. The code\nhas been released at https://github.com/hamidreza-dastmalchi/ETTA."
                },
                "authors": [
                    {
                        "name": "Hamidreza Dastmalchi"
                    },
                    {
                        "name": "Aijun An"
                    },
                    {
                        "name": "Ali cheraghian"
                    }
                ],
                "author_detail": {
                    "name": "Ali cheraghian"
                },
                "author": "Ali cheraghian",
                "arxiv_comment": "BMVC2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05898v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05898v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10024v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10024v1",
                "updated": "2025-08-07T21:18:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    21,
                    18,
                    52,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T21:18:52Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    21,
                    18,
                    52,
                    3,
                    219,
                    0
                ],
                "title": "RTTC: Reward-Guided Collaborative Test-Time Compute",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RTTC: Reward-Guided Collaborative Test-Time Compute"
                },
                "summary": "Test-Time Compute (TTC) has emerged as a powerful paradigm for enhancing the\nperformance of Large Language Models (LLMs) at inference, leveraging strategies\nsuch as Test-Time Training (TTT) and Retrieval-Augmented Generation (RAG).\nHowever, the optimal adaptation strategy varies across queries, and\nindiscriminate application of TTC strategy incurs substantial computational\noverhead. In this work, we introduce Reward-Guided Test-Time Compute (RTTC), a\nnovel framework that adaptively selects the most effective TTC strategy for\neach query via a pretrained reward model, maximizing downstream accuracy across\ndiverse domains and tasks. RTTC operates in a distributed server-client\narchitecture, retrieving relevant samples from a remote knowledge base and\napplying RAG or lightweight fine-tuning on client devices only when necessary.\nTo further mitigate redundant computation, we propose Query-State Caching,\nwhich enables the efficient reuse of historical query states at both retrieval\nand adaptation levels. Extensive experiments across multiple LLMs and\nbenchmarks demonstrate that RTTC consistently achieves superior accuracy\ncompared to vanilla RAG or TTT, validating the necessity of adaptive,\nreward-guided TTC selection and the potential of RTTC for scalable,\nhigh-performance language model adaptation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-Time Compute (TTC) has emerged as a powerful paradigm for enhancing the\nperformance of Large Language Models (LLMs) at inference, leveraging strategies\nsuch as Test-Time Training (TTT) and Retrieval-Augmented Generation (RAG).\nHowever, the optimal adaptation strategy varies across queries, and\nindiscriminate application of TTC strategy incurs substantial computational\noverhead. In this work, we introduce Reward-Guided Test-Time Compute (RTTC), a\nnovel framework that adaptively selects the most effective TTC strategy for\neach query via a pretrained reward model, maximizing downstream accuracy across\ndiverse domains and tasks. RTTC operates in a distributed server-client\narchitecture, retrieving relevant samples from a remote knowledge base and\napplying RAG or lightweight fine-tuning on client devices only when necessary.\nTo further mitigate redundant computation, we propose Query-State Caching,\nwhich enables the efficient reuse of historical query states at both retrieval\nand adaptation levels. Extensive experiments across multiple LLMs and\nbenchmarks demonstrate that RTTC consistently achieves superior accuracy\ncompared to vanilla RAG or TTT, validating the necessity of adaptive,\nreward-guided TTC selection and the potential of RTTC for scalable,\nhigh-performance language model adaptation."
                },
                "authors": [
                    {
                        "name": "J. Pablo Muñoz"
                    },
                    {
                        "name": "Jinjie Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Jinjie Yuan"
                },
                "author": "Jinjie Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10024v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10024v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05211v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05211v1",
                "updated": "2025-08-07T09:47:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    9,
                    47,
                    21,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T09:47:21Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    9,
                    47,
                    21,
                    3,
                    219,
                    0
                ],
                "title": "VFlowOpt: A Token Pruning Framework for LMMs with Visual Information\n  Flow-Guided Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VFlowOpt: A Token Pruning Framework for LMMs with Visual Information\n  Flow-Guided Optimization"
                },
                "summary": "Large Multimodal Models (LMMs) excel in visual-language tasks by leveraging\nnumerous visual tokens for fine-grained visual information, but this token\nredundancy results in significant computational costs. Previous research aimed\nat reducing visual tokens during inference typically leverages importance maps\nderived from attention scores among vision-only tokens or vision-language\ntokens to prune tokens across one or multiple pruning stages. Despite this\nprogress, pruning frameworks and strategies remain simplistic and\ninsufficiently explored, often resulting in substantial performance\ndegradation. In this paper, we propose VFlowOpt, a token pruning framework that\nintroduces an importance map derivation process and a progressive pruning\nmodule with a recycling mechanism. The hyperparameters of its pruning strategy\nare further optimized by a visual information flow-guided method. Specifically,\nwe compute an importance map for image tokens based on their attention-derived\ncontext relevance and patch-level information entropy. We then decide which\ntokens to retain or prune and aggregate the pruned ones as recycled tokens to\navoid potential information loss. Finally, we apply a visual information\nflow-guided method that regards the last token in the LMM as the most\nrepresentative signal of text-visual interactions. This method minimizes the\ndiscrepancy between token representations in LMMs with and without pruning,\nthereby enabling superior pruning strategies tailored to different LMMs.\nExperiments demonstrate that VFlowOpt can prune 90% of visual tokens while\nmaintaining comparable performance, leading to an 89% reduction in KV-Cache\nmemory and 3.8 times faster inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Multimodal Models (LMMs) excel in visual-language tasks by leveraging\nnumerous visual tokens for fine-grained visual information, but this token\nredundancy results in significant computational costs. Previous research aimed\nat reducing visual tokens during inference typically leverages importance maps\nderived from attention scores among vision-only tokens or vision-language\ntokens to prune tokens across one or multiple pruning stages. Despite this\nprogress, pruning frameworks and strategies remain simplistic and\ninsufficiently explored, often resulting in substantial performance\ndegradation. In this paper, we propose VFlowOpt, a token pruning framework that\nintroduces an importance map derivation process and a progressive pruning\nmodule with a recycling mechanism. The hyperparameters of its pruning strategy\nare further optimized by a visual information flow-guided method. Specifically,\nwe compute an importance map for image tokens based on their attention-derived\ncontext relevance and patch-level information entropy. We then decide which\ntokens to retain or prune and aggregate the pruned ones as recycled tokens to\navoid potential information loss. Finally, we apply a visual information\nflow-guided method that regards the last token in the LMM as the most\nrepresentative signal of text-visual interactions. This method minimizes the\ndiscrepancy between token representations in LMMs with and without pruning,\nthereby enabling superior pruning strategies tailored to different LMMs.\nExperiments demonstrate that VFlowOpt can prune 90% of visual tokens while\nmaintaining comparable performance, leading to an 89% reduction in KV-Cache\nmemory and 3.8 times faster inference."
                },
                "authors": [
                    {
                        "name": "Sihan Yang"
                    },
                    {
                        "name": "Runsen Xu"
                    },
                    {
                        "name": "Chenhang Cui"
                    },
                    {
                        "name": "Tai Wang"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    }
                ],
                "author_detail": {
                    "name": "Jiangmiao Pang"
                },
                "author": "Jiangmiao Pang",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05211v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05211v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05091v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05091v1",
                "updated": "2025-08-07T07:19:02Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    7,
                    19,
                    2,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T07:19:02Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    7,
                    19,
                    2,
                    3,
                    219,
                    0
                ],
                "title": "PoseGen: In-Context LoRA Finetuning for Pose-Controllable Long Human\n  Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PoseGen: In-Context LoRA Finetuning for Pose-Controllable Long Human\n  Video Generation"
                },
                "summary": "Generating long, temporally coherent videos with precise control over subject\nidentity and motion is a formidable challenge for current diffusion models,\nwhich often suffer from identity drift and are limited to short clips. We\nintroduce PoseGen, a novel framework that generates arbitrarily long videos of\na specific subject from a single reference image and a driving pose sequence.\nOur core innovation is an in-context LoRA finetuning strategy that injects\nsubject appearance at the token level for identity preservation, while\nsimultaneously conditioning on pose information at the channel level for\nfine-grained motion control. To overcome duration limits, PoseGen pioneers an\ninterleaved segment generation method that seamlessly stitches video clips\ntogether, using a shared KV cache mechanism and a specialized transition\nprocess to ensure background consistency and temporal smoothness. Trained on a\nremarkably small 33-hour video dataset, extensive experiments show that PoseGen\nsignificantly outperforms state-of-the-art methods in identity fidelity, pose\naccuracy, and its unique ability to produce coherent, artifact-free videos of\nunlimited duration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating long, temporally coherent videos with precise control over subject\nidentity and motion is a formidable challenge for current diffusion models,\nwhich often suffer from identity drift and are limited to short clips. We\nintroduce PoseGen, a novel framework that generates arbitrarily long videos of\na specific subject from a single reference image and a driving pose sequence.\nOur core innovation is an in-context LoRA finetuning strategy that injects\nsubject appearance at the token level for identity preservation, while\nsimultaneously conditioning on pose information at the channel level for\nfine-grained motion control. To overcome duration limits, PoseGen pioneers an\ninterleaved segment generation method that seamlessly stitches video clips\ntogether, using a shared KV cache mechanism and a specialized transition\nprocess to ensure background consistency and temporal smoothness. Trained on a\nremarkably small 33-hour video dataset, extensive experiments show that PoseGen\nsignificantly outperforms state-of-the-art methods in identity fidelity, pose\naccuracy, and its unique ability to produce coherent, artifact-free videos of\nunlimited duration."
                },
                "authors": [
                    {
                        "name": "Jingxuan He"
                    },
                    {
                        "name": "Busheng Su"
                    },
                    {
                        "name": "Finn Wong"
                    }
                ],
                "author_detail": {
                    "name": "Finn Wong"
                },
                "author": "Finn Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05091v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05091v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05012v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05012v1",
                "updated": "2025-08-07T03:49:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    3,
                    49,
                    56,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T03:49:56Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    3,
                    49,
                    56,
                    3,
                    219,
                    0
                ],
                "title": "Making Prompts First-Class Citizens for Adaptive LLM Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Making Prompts First-Class Citizens for Adaptive LLM Pipelines"
                },
                "summary": "Modern LLM pipelines increasingly resemble data-centric systems: they\nretrieve external context, compose intermediate outputs, validate results, and\nadapt based on runtime feedback. Yet, the central element guiding this process\n-- the prompt -- remains a brittle, opaque string, disconnected from the\nsurrounding dataflow. This disconnect limits reuse, optimization, and runtime\ncontrol.\n  In this paper, we describe our vision and an initial design for SPEAR, a\nlanguage and runtime that fills this prompt management gap by making prompts\nstructured, adaptive, and first-class components of the execution model. SPEAR\nenables (1) runtime prompt refinement -- modifying prompts dynamically in\nresponse to execution-time signals such as confidence, latency, or missing\ncontext; and (2) structured prompt management -- organizing prompt fragments\ninto versioned views with support for introspection and logging.\n  SPEAR defines a prompt algebra that governs how prompts are constructed and\nadapted within a pipeline. It supports multiple refinement modes (manual,\nassisted, and automatic), giving developers a balance between control and\nautomation. By treating prompt logic as structured data, SPEAR enables\noptimizations such as operator fusion, prefix caching, and view reuse.\nPreliminary experiments quantify the behavior of different refinement modes\ncompared to static prompts and agentic retries, as well as the impact of\nprompt-level optimizations such as operator fusion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern LLM pipelines increasingly resemble data-centric systems: they\nretrieve external context, compose intermediate outputs, validate results, and\nadapt based on runtime feedback. Yet, the central element guiding this process\n-- the prompt -- remains a brittle, opaque string, disconnected from the\nsurrounding dataflow. This disconnect limits reuse, optimization, and runtime\ncontrol.\n  In this paper, we describe our vision and an initial design for SPEAR, a\nlanguage and runtime that fills this prompt management gap by making prompts\nstructured, adaptive, and first-class components of the execution model. SPEAR\nenables (1) runtime prompt refinement -- modifying prompts dynamically in\nresponse to execution-time signals such as confidence, latency, or missing\ncontext; and (2) structured prompt management -- organizing prompt fragments\ninto versioned views with support for introspection and logging.\n  SPEAR defines a prompt algebra that governs how prompts are constructed and\nadapted within a pipeline. It supports multiple refinement modes (manual,\nassisted, and automatic), giving developers a balance between control and\nautomation. By treating prompt logic as structured data, SPEAR enables\noptimizations such as operator fusion, prefix caching, and view reuse.\nPreliminary experiments quantify the behavior of different refinement modes\ncompared to static prompts and agentic retries, as well as the impact of\nprompt-level optimizations such as operator fusion."
                },
                "authors": [
                    {
                        "name": "Ugur Cetintemel"
                    },
                    {
                        "name": "Shu Chen"
                    },
                    {
                        "name": "Alexander W. Lee"
                    },
                    {
                        "name": "Deepti Raghavan"
                    }
                ],
                "author_detail": {
                    "name": "Deepti Raghavan"
                },
                "author": "Deepti Raghavan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05012v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05012v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04449v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04449v2",
                "updated": "2025-08-06T16:57:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    16,
                    57,
                    39,
                    2,
                    218,
                    0
                ],
                "published": "2024-12-05T18:58:03Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    58,
                    3,
                    3,
                    340,
                    0
                ],
                "title": "p-MoD: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "p-MoD: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay"
                },
                "summary": "Despite the remarkable performance of multimodal large language models\n(MLLMs) across diverse tasks, the substantial training and inference costs\nimpede their advancement. In this paper, we propose p-MoD, an efficient MLLM\narchitecture that significantly reduces training and inference costs while\nmaintaining model performance. The majority of computation in MLLMs stems from\nthe overwhelming volume of vision tokens processed by the transformer-based\nLLM. Accordingly, we leverage the Mixture-of-Depths (MoD) mechanism, where each\nLLM layer selects essential vision tokens to process while skipping redundant\nones. However, integrating MoD into MLLMs is non-trivial. To address the\nchallenges of training and inference stability as well as limited training\ndata, we adapt the MoD module with two novel designs: tanh-gated weight\nnormalization (TanhNorm) and symmetric token reweighting (STRing). Moreover, we\nobserve that vision tokens exhibit higher redundancy in deeper layers and thus\ndesign a progressive ratio decay (PRD) strategy, which gradually reduces the\ntoken retention ratio layer by layer, employing a shifted cosine schedule. This\ncrucial design fully unleashes the potential of MoD, significantly boosting the\nefficiency and performance of our models. Extensive experiments on two baseline\nmodels across 15 benchmarks show that our model matches or even surpasses the\nperformance of corresponding baselines, while requiring only 55.6% TFLOPs and\n53.7% KV cache storage during inference, and 77.7% GPU hours during training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the remarkable performance of multimodal large language models\n(MLLMs) across diverse tasks, the substantial training and inference costs\nimpede their advancement. In this paper, we propose p-MoD, an efficient MLLM\narchitecture that significantly reduces training and inference costs while\nmaintaining model performance. The majority of computation in MLLMs stems from\nthe overwhelming volume of vision tokens processed by the transformer-based\nLLM. Accordingly, we leverage the Mixture-of-Depths (MoD) mechanism, where each\nLLM layer selects essential vision tokens to process while skipping redundant\nones. However, integrating MoD into MLLMs is non-trivial. To address the\nchallenges of training and inference stability as well as limited training\ndata, we adapt the MoD module with two novel designs: tanh-gated weight\nnormalization (TanhNorm) and symmetric token reweighting (STRing). Moreover, we\nobserve that vision tokens exhibit higher redundancy in deeper layers and thus\ndesign a progressive ratio decay (PRD) strategy, which gradually reduces the\ntoken retention ratio layer by layer, employing a shifted cosine schedule. This\ncrucial design fully unleashes the potential of MoD, significantly boosting the\nefficiency and performance of our models. Extensive experiments on two baseline\nmodels across 15 benchmarks show that our model matches or even surpasses the\nperformance of corresponding baselines, while requiring only 55.6% TFLOPs and\n53.7% KV cache storage during inference, and 77.7% GPU hours during training."
                },
                "authors": [
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Desen Meng"
                    },
                    {
                        "name": "Zhengming Zhang"
                    },
                    {
                        "name": "Zhenpeng Huang"
                    },
                    {
                        "name": "Tao Wu"
                    },
                    {
                        "name": "Limin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Limin Wang"
                },
                "author": "Limin Wang",
                "arxiv_comment": "Accepted by ICCV 2025; Code released at\n  https://github.com/MCG-NJU/p-MoD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04449v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04449v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04581v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04581v1",
                "updated": "2025-08-06T16:06:43Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    16,
                    6,
                    43,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T16:06:43Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    16,
                    6,
                    43,
                    2,
                    218,
                    0
                ],
                "title": "Share Your Attention: Transformer Weight Sharing via Matrix-based\n  Dictionary Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Share Your Attention: Transformer Weight Sharing via Matrix-based\n  Dictionary Learning"
                },
                "summary": "Large language models (LLMs) have revolutionized AI applications, yet their\nhigh computational and memory demands hinder their widespread deployment.\nExisting compression techniques focus on intra-block optimizations (e.g.\nlow-rank approximation, attention head pruning), while the repetitive layered\nstructure of transformers implies significant inter-block redundancy - a\ndimension largely unexplored beyond key-value (KV) caching. Inspired by\ndictionary learning in CNNs, we propose a framework for structured weight\nsharing across transformer layers. Our approach decomposes attention projection\nmatrices into shared dictionary atoms, reducing the attention module's\nparameters by 66.7% while achieving on-par performance. Unlike complex methods\nrequiring distillation or architectural changes, MASA (Matrix Atom Sharing in\nAttention) operates as a drop-in replacement - trained with standard optimizers\n- and represents each layer's weights as linear combinations of shared matrix\natoms. Experiments across scales (100M-700M parameters) show that MASA achieves\nbetter benchmark accuracy and perplexity than grouped-query attention (GQA),\nlow-rank baselines and recently proposed Repeat-all-over/Sequential sharing at\ncomparable parameter budgets. Ablation studies confirm robustness to the\ndictionary size and the efficacy of shared representations in capturing\ncross-layer statistical regularities. Extending to Vision Transformers (ViT),\nMASA matches performance metrics on image classification and detection tasks\nwith 66.7% fewer attention parameters. By combining dictionary learning\nstrategies with transformer efficiency, MASA offers a scalable blueprint for\nparameter-efficient models without sacrificing performance. Finally, we\ninvestigate the possibility of employing MASA on pretrained LLMs to reduce\ntheir number of parameters without experiencing any significant drop in their\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have revolutionized AI applications, yet their\nhigh computational and memory demands hinder their widespread deployment.\nExisting compression techniques focus on intra-block optimizations (e.g.\nlow-rank approximation, attention head pruning), while the repetitive layered\nstructure of transformers implies significant inter-block redundancy - a\ndimension largely unexplored beyond key-value (KV) caching. Inspired by\ndictionary learning in CNNs, we propose a framework for structured weight\nsharing across transformer layers. Our approach decomposes attention projection\nmatrices into shared dictionary atoms, reducing the attention module's\nparameters by 66.7% while achieving on-par performance. Unlike complex methods\nrequiring distillation or architectural changes, MASA (Matrix Atom Sharing in\nAttention) operates as a drop-in replacement - trained with standard optimizers\n- and represents each layer's weights as linear combinations of shared matrix\natoms. Experiments across scales (100M-700M parameters) show that MASA achieves\nbetter benchmark accuracy and perplexity than grouped-query attention (GQA),\nlow-rank baselines and recently proposed Repeat-all-over/Sequential sharing at\ncomparable parameter budgets. Ablation studies confirm robustness to the\ndictionary size and the efficacy of shared representations in capturing\ncross-layer statistical regularities. Extending to Vision Transformers (ViT),\nMASA matches performance metrics on image classification and detection tasks\nwith 66.7% fewer attention parameters. By combining dictionary learning\nstrategies with transformer efficiency, MASA offers a scalable blueprint for\nparameter-efficient models without sacrificing performance. Finally, we\ninvestigate the possibility of employing MASA on pretrained LLMs to reduce\ntheir number of parameters without experiencing any significant drop in their\nperformance."
                },
                "authors": [
                    {
                        "name": "Magauiya Zhussip"
                    },
                    {
                        "name": "Dmitriy Shopkhoev"
                    },
                    {
                        "name": "Ammar Ali"
                    },
                    {
                        "name": "Stamatios Lefkimmiatis"
                    }
                ],
                "author_detail": {
                    "name": "Stamatios Lefkimmiatis"
                },
                "author": "Stamatios Lefkimmiatis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04581v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04581v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.01516v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.01516v3",
                "updated": "2025-08-06T15:38:06Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    15,
                    38,
                    6,
                    2,
                    218,
                    0
                ],
                "published": "2023-05-02T15:27:16Z",
                "published_parsed": [
                    2023,
                    5,
                    2,
                    15,
                    27,
                    16,
                    1,
                    122,
                    0
                ],
                "title": "From FASTER to F2: Evolving Concurrent Key-Value Store Designs for Large\n  Skewed Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From FASTER to F2: Evolving Concurrent Key-Value Store Designs for Large\n  Skewed Workloads"
                },
                "summary": "Modern large-scale services such as search engines, messaging platforms, and\nserverless functions, rely on key-value (KV) stores to maintain high\nperformance at scale. When such services are deployed in constrained memory\nenvironments, they present challenging requirements: point operations requiring\nhigh throughput, working sets much larger than main memory, and natural skew in\nkey access patterns. Traditional KV stores, based on LSM- and B-Trees, have\nbeen widely used to handle such use cases, but they often suffer from\nsuboptimal use of modern hardware resources. The FASTER project, developed as a\nhigh-performance open-source KV storage library, has demonstrated remarkable\nsuccess in both in-memory and hybrid storage environments. However, when tasked\nwith serving large skewed workloads, it faced challenges, including high\nindexing and compactions overheads, and inefficient management of\nnon-overlapping read-hot and write-hot working sets.\n  In this paper, we introduce F2 (for FASTER v2), an evolution of FASTER\ndesigned to meet the requirements of large skewed workloads common in industry\napplications. F2 adopts a two-tier record-oriented design to handle\nlarger-than-memory skewed workloads, along with new concurrent latch-free\nmechanisms and components to maximize performance on modern hardware. To\nrealize this design, F2 tackles key challenges and introduces several\ninnovations, including new latch-free algorithms for multi-threaded log\ncompaction, a two-level hash index to reduce indexing overhead for cold\nrecords, and a read-cache for serving read-hot records. Our evaluation shows\nthat F2 achieves 2-11.9x better throughput compared to existing KV stores,\neffectively serving the target workload. F2 is open-source and available as\npart of the FASTER project.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large-scale services such as search engines, messaging platforms, and\nserverless functions, rely on key-value (KV) stores to maintain high\nperformance at scale. When such services are deployed in constrained memory\nenvironments, they present challenging requirements: point operations requiring\nhigh throughput, working sets much larger than main memory, and natural skew in\nkey access patterns. Traditional KV stores, based on LSM- and B-Trees, have\nbeen widely used to handle such use cases, but they often suffer from\nsuboptimal use of modern hardware resources. The FASTER project, developed as a\nhigh-performance open-source KV storage library, has demonstrated remarkable\nsuccess in both in-memory and hybrid storage environments. However, when tasked\nwith serving large skewed workloads, it faced challenges, including high\nindexing and compactions overheads, and inefficient management of\nnon-overlapping read-hot and write-hot working sets.\n  In this paper, we introduce F2 (for FASTER v2), an evolution of FASTER\ndesigned to meet the requirements of large skewed workloads common in industry\napplications. F2 adopts a two-tier record-oriented design to handle\nlarger-than-memory skewed workloads, along with new concurrent latch-free\nmechanisms and components to maximize performance on modern hardware. To\nrealize this design, F2 tackles key challenges and introduces several\ninnovations, including new latch-free algorithms for multi-threaded log\ncompaction, a two-level hash index to reduce indexing overhead for cold\nrecords, and a read-cache for serving read-hot records. Our evaluation shows\nthat F2 achieves 2-11.9x better throughput compared to existing KV stores,\neffectively serving the target workload. F2 is open-source and available as\npart of the FASTER project."
                },
                "authors": [
                    {
                        "name": "Konstantinos Kanellis"
                    },
                    {
                        "name": "Badrish Chandramouli"
                    },
                    {
                        "name": "Ted Hart"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "arxiv_comment": "Proceedings of the VLDB Endowment 18 (VLDB'25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.01516v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.01516v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04462v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04462v1",
                "updated": "2025-08-06T14:02:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    14,
                    2,
                    10,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T14:02:10Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    14,
                    2,
                    10,
                    2,
                    218,
                    0
                ],
                "title": "CARD: Cache-Assisted Parallel Speculative Decoding for Efficient Large\n  Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CARD: Cache-Assisted Parallel Speculative Decoding for Efficient Large\n  Language Model Inference"
                },
                "summary": "Speculative decoding (SD), where an extra draft model first provides multiple\ndraft tokens and the original target model then verifies these tokens in\nparallel, has shown great power for LLM inference acceleration. However,\nexisting SD methods must adhere to the 'draft-then-verify' paradigm, which\nforces drafting and verification processes to execute sequentially during SD,\nresulting in inefficient inference performance and limiting the size of the\ndraft model. Furthermore, once a single token in the candidate sequence is\nrejected during the drafting process, all subsequent candidate tokens must be\ndiscarded, leading to inefficient drafting. To address these challenges, we\npropose a cache-based parallel speculative decoding framework employing a\n'query-and-correct' paradigm. Specifically, CARD decouples drafting and\nverification: the draft model generates candidate tokens to populate a shared\ncache, while the target model concurrently rectifies the draft model's\ngeneration direction. This effectively enables the target model to perform\ninference at speed approaching that of the draft model. Our approach achieves\nup to 4.83 speedup over vanilla decoding without requiring fine-tuning of\neither the draft or target models. Our code is available at\nhttps://github.com/hunzhizi/CARD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding (SD), where an extra draft model first provides multiple\ndraft tokens and the original target model then verifies these tokens in\nparallel, has shown great power for LLM inference acceleration. However,\nexisting SD methods must adhere to the 'draft-then-verify' paradigm, which\nforces drafting and verification processes to execute sequentially during SD,\nresulting in inefficient inference performance and limiting the size of the\ndraft model. Furthermore, once a single token in the candidate sequence is\nrejected during the drafting process, all subsequent candidate tokens must be\ndiscarded, leading to inefficient drafting. To address these challenges, we\npropose a cache-based parallel speculative decoding framework employing a\n'query-and-correct' paradigm. Specifically, CARD decouples drafting and\nverification: the draft model generates candidate tokens to populate a shared\ncache, while the target model concurrently rectifies the draft model's\ngeneration direction. This effectively enables the target model to perform\ninference at speed approaching that of the draft model. Our approach achieves\nup to 4.83 speedup over vanilla decoding without requiring fine-tuning of\neither the draft or target models. Our code is available at\nhttps://github.com/hunzhizi/CARD."
                },
                "authors": [
                    {
                        "name": "Enyu Zhou"
                    },
                    {
                        "name": "Kai Sheng"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Xin He"
                    }
                ],
                "author_detail": {
                    "name": "Xin He"
                },
                "author": "Xin He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04462v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04462v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24007v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24007v3",
                "updated": "2025-08-06T11:46:11Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    11,
                    46,
                    11,
                    2,
                    218,
                    0
                ],
                "published": "2025-03-31T12:32:23Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    32,
                    23,
                    0,
                    90,
                    0
                ],
                "title": "CITRAS: Covariate-Informed Transformer for Time Series Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CITRAS: Covariate-Informed Transformer for Time Series Forecasting"
                },
                "summary": "In practical time series forecasting, covariates provide rich contextual\ninformation that can potentially enhance the forecast of target variables.\nAlthough some covariates extend into the future forecasting horizon (e.g.,\ncalendar events, discount schedules), most multivariate models fail to leverage\nthis pivotal insight due to the length discrepancy with target variables.\nAdditionally, capturing the dependency between target variables and covariates\nis non-trivial, as models must precisely reflect the local impact of covariates\nwhile also capturing global cross-variate dependencies. To overcome these\nchallenges, we propose CITRAS, a decoder-only Transformer that flexibly\nleverages multiple targets, past covariates, and future covariates. While\npreserving strong autoregressive capabilities, CITRAS introduces two novel\nmechanisms in patch-wise cross-variate attention: Key-Value (KV) Shift and\nAttention Score Smoothing. KV Shift seamlessly incorporates future covariates\ninto the forecasting of target variables based on their concurrent\ndependencies. Additionally, Attention Score Smoothing refines locally accurate\npatch-wise cross-variate dependencies into global variate-level dependencies by\nsmoothing the past series of attention scores. Experimentally, CITRAS\noutperforms state-of-the-art models on thirteen real-world benchmarks from both\ncovariate-informed and multivariate settings, demonstrating its versatile\nability to leverage cross-variate and cross-time dependencies for improved\nforecasting accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In practical time series forecasting, covariates provide rich contextual\ninformation that can potentially enhance the forecast of target variables.\nAlthough some covariates extend into the future forecasting horizon (e.g.,\ncalendar events, discount schedules), most multivariate models fail to leverage\nthis pivotal insight due to the length discrepancy with target variables.\nAdditionally, capturing the dependency between target variables and covariates\nis non-trivial, as models must precisely reflect the local impact of covariates\nwhile also capturing global cross-variate dependencies. To overcome these\nchallenges, we propose CITRAS, a decoder-only Transformer that flexibly\nleverages multiple targets, past covariates, and future covariates. While\npreserving strong autoregressive capabilities, CITRAS introduces two novel\nmechanisms in patch-wise cross-variate attention: Key-Value (KV) Shift and\nAttention Score Smoothing. KV Shift seamlessly incorporates future covariates\ninto the forecasting of target variables based on their concurrent\ndependencies. Additionally, Attention Score Smoothing refines locally accurate\npatch-wise cross-variate dependencies into global variate-level dependencies by\nsmoothing the past series of attention scores. Experimentally, CITRAS\noutperforms state-of-the-art models on thirteen real-world benchmarks from both\ncovariate-informed and multivariate settings, demonstrating its versatile\nability to leverage cross-variate and cross-time dependencies for improved\nforecasting accuracy."
                },
                "authors": [
                    {
                        "name": "Yosuke Yamaguchi"
                    },
                    {
                        "name": "Issei Suemitsu"
                    },
                    {
                        "name": "Wenpeng Wei"
                    }
                ],
                "author_detail": {
                    "name": "Wenpeng Wei"
                },
                "author": "Wenpeng Wei",
                "arxiv_comment": "Submission under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24007v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24007v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04257v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04257v1",
                "updated": "2025-08-06T09:40:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    9,
                    40,
                    9,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T09:40:09Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    9,
                    40,
                    9,
                    2,
                    218,
                    0
                ],
                "title": "KVSink: Understanding and Enhancing the Preservation of Attention Sinks\n  in KV Cache Quantization for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVSink: Understanding and Enhancing the Preservation of Attention Sinks\n  in KV Cache Quantization for LLMs"
                },
                "summary": "Key-Value (KV) cache quantization has become a widely adopted optimization\ntechnique for efficient large language models (LLMs) inference by reducing KV\ncache memory usage and mitigating memory-bound constraints. Recent studies have\nemphasized the importance of preserving the original precision of KVs for the\nfirst few tokens to ensure the protection of attention sinks. While this\napproach has proven effective in mitigating performance degradation, its\nunderlying principles remain insufficiently understood. Moreover, it fails to\naddress the recent discovery that attention sinks can emerge beyond the initial\ntoken positions. In this work, we elucidate the underlying mechanisms of\nattention sinks during inference by examining their role in the cross-layer\nevolution of extreme activation outliers. Additionally, we provide a\ncomprehensive analysis of the interplay between attention sinks and KV cache\nquantization. Based on our enhanced understanding, we introduce\n\\textit{\\textbf{KVSink}}, a plug-and-play method that effectively predicts sink\ntokens with negligible overhead, enabling more thorough preservation. Extensive\nexperiments demonstrate that KVSink outperforms the existing Preserve-First-N\n(PFN) strategy, offering more effective preservation of attention sinks during\nKV cache quantization. Moreover, when applied to the well-established KVQuant\nmethod, KVSink further improves perplexity (PPL) and reduces reliance on 16-bit\nnumerical outliers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) cache quantization has become a widely adopted optimization\ntechnique for efficient large language models (LLMs) inference by reducing KV\ncache memory usage and mitigating memory-bound constraints. Recent studies have\nemphasized the importance of preserving the original precision of KVs for the\nfirst few tokens to ensure the protection of attention sinks. While this\napproach has proven effective in mitigating performance degradation, its\nunderlying principles remain insufficiently understood. Moreover, it fails to\naddress the recent discovery that attention sinks can emerge beyond the initial\ntoken positions. In this work, we elucidate the underlying mechanisms of\nattention sinks during inference by examining their role in the cross-layer\nevolution of extreme activation outliers. Additionally, we provide a\ncomprehensive analysis of the interplay between attention sinks and KV cache\nquantization. Based on our enhanced understanding, we introduce\n\\textit{\\textbf{KVSink}}, a plug-and-play method that effectively predicts sink\ntokens with negligible overhead, enabling more thorough preservation. Extensive\nexperiments demonstrate that KVSink outperforms the existing Preserve-First-N\n(PFN) strategy, offering more effective preservation of attention sinks during\nKV cache quantization. Moreover, when applied to the well-established KVQuant\nmethod, KVSink further improves perplexity (PPL) and reduces reliance on 16-bit\nnumerical outliers."
                },
                "authors": [
                    {
                        "name": "Zunhai Su"
                    },
                    {
                        "name": "Kehong Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Kehong Yuan"
                },
                "author": "Kehong Yuan",
                "arxiv_comment": "Published as a conference paper at COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04257v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04257v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00370v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00370v2",
                "updated": "2025-08-06T08:32:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    8,
                    32,
                    53,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-01T07:03:16Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    7,
                    3,
                    16,
                    4,
                    213,
                    0
                ],
                "title": "EdgeInfinite-Instruct: Bridging SFT-Based Optimization and NPU-Level\n  Efficiency for Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EdgeInfinite-Instruct: Bridging SFT-Based Optimization and NPU-Level\n  Efficiency for Edge Devices"
                },
                "summary": "Deploying Transformer-based large language models (LLMs) on\nresource-constrained edge devices for long-sequence tasks remains challenging\ndue to the quadratic time complexity of self-attention and growing Key-Value\n(KV) cache demands. While existing KV cache optimizations improve memory\nefficiency, they often fail to reduce time to first token (TTFT) and may\ndegrade performance through token pruning. Alternative sequence modeling\narchitectures address some of these limitations, but typically require full\nretraining and lack infrastructure support. EdgeInfinite offers an efficient\nsolution by fine-tuning only a small subset of parameters, maintaining quality\nwhile reducing both computational and memory costs, including improved TTFT.\nHowever, its instruction-following ability is limited, and it lacks\nmobile-specific optimizations. To address these issues, we propose\nEdgeInfinite-Instruct, which introduces a Segmented Supervised Fine-Tuning\n(S-SFT) strategy tailored to long-sequence tasks such as summarization and\nquestion answering. We further optimized EdgeInfinite-Instruct for efficient\ndeployment on edge NPUs by employing fine-grained post-training quantization\n(PTQ) to reduce computational demands while maintaining accuracy, and by\nimplementing a fixed-shape computation graph that balances memory usage and\non-device efficiency through scenario-specific customization of input token and\ncache sizes. Experiments on long-context benchmarks and real-world mobile tasks\nshow that our approach improves domain-specific performance while maintaining\nefficiency on NPU-accelerated edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying Transformer-based large language models (LLMs) on\nresource-constrained edge devices for long-sequence tasks remains challenging\ndue to the quadratic time complexity of self-attention and growing Key-Value\n(KV) cache demands. While existing KV cache optimizations improve memory\nefficiency, they often fail to reduce time to first token (TTFT) and may\ndegrade performance through token pruning. Alternative sequence modeling\narchitectures address some of these limitations, but typically require full\nretraining and lack infrastructure support. EdgeInfinite offers an efficient\nsolution by fine-tuning only a small subset of parameters, maintaining quality\nwhile reducing both computational and memory costs, including improved TTFT.\nHowever, its instruction-following ability is limited, and it lacks\nmobile-specific optimizations. To address these issues, we propose\nEdgeInfinite-Instruct, which introduces a Segmented Supervised Fine-Tuning\n(S-SFT) strategy tailored to long-sequence tasks such as summarization and\nquestion answering. We further optimized EdgeInfinite-Instruct for efficient\ndeployment on edge NPUs by employing fine-grained post-training quantization\n(PTQ) to reduce computational demands while maintaining accuracy, and by\nimplementing a fixed-shape computation graph that balances memory usage and\non-device efficiency through scenario-specific customization of input token and\ncache sizes. Experiments on long-context benchmarks and real-world mobile tasks\nshow that our approach improves domain-specific performance while maintaining\nefficiency on NPU-accelerated edge devices."
                },
                "authors": [
                    {
                        "name": "Jiyu Chen"
                    },
                    {
                        "name": "Poh Seng Lim"
                    },
                    {
                        "name": "Shuang Peng"
                    },
                    {
                        "name": "Daxiong Luo"
                    },
                    {
                        "name": "JungHau Foo"
                    },
                    {
                        "name": "Yap Deep"
                    },
                    {
                        "name": "Timothy Lee Jun Jie"
                    },
                    {
                        "name": "Kelvin Teh Kae Wen"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Danyu Feng"
                    },
                    {
                        "name": "Hao-Yun Chen"
                    },
                    {
                        "name": "Peng-Wen Chen"
                    },
                    {
                        "name": "Fangyuan Li"
                    },
                    {
                        "name": "Xiaoxin Chen"
                    },
                    {
                        "name": "Wong Wai Mun"
                    }
                ],
                "author_detail": {
                    "name": "Wong Wai Mun"
                },
                "author": "Wong Wai Mun",
                "arxiv_comment": "The data and method in the paper need to be re-audited",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00370v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00370v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11661v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11661v1",
                "updated": "2025-08-06T02:53:14Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    2,
                    53,
                    14,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T02:53:14Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    2,
                    53,
                    14,
                    2,
                    218,
                    0
                ],
                "title": "Sparse Attention across Multiple-context KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Attention across Multiple-context KV Cache"
                },
                "summary": "Large language models face significant cost challenges in long-sequence\ninference. To address this, reusing historical Key-Value (KV) Cache for\nimproved inference efficiency has become a mainstream approach. Recent advances\nfurther enhance throughput by sparse attention mechanisms to select the most\nrelevant KV Cache, thereby reducing sequence length. However, such techniques\nare limited to single-context scenarios, where historical KV Cache is computed\nsequentially with causal-attention dependencies. In retrieval-augmented\ngeneration (RAG) scenarios, where retrieved documents as context are unknown\nbeforehand, each document's KV Cache is computed and stored independently\n(termed multiple-context KV Cache), lacking cross-attention between contexts.\nThis renders existing methods ineffective. Although prior work partially\nrecomputes multiple-context KV Cache to mitigate accuracy loss from missing\ncross-attention, it requires retaining all KV Cache throughout, failing to\nreduce memory overhead. This paper presents SamKV, the first exploration of\nattention sparsification for multiple-context KV Cache. Specifically, SamKV\ntakes into account the complementary information of other contexts when\nsparsifying one context, and then locally recomputes the sparsified\ninformation. Experiments demonstrate that our method compresses sequence length\nto 15% without accuracy degradation compared with full-recompuation baselines,\nsignificantly boosting throughput in multi-context RAG scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models face significant cost challenges in long-sequence\ninference. To address this, reusing historical Key-Value (KV) Cache for\nimproved inference efficiency has become a mainstream approach. Recent advances\nfurther enhance throughput by sparse attention mechanisms to select the most\nrelevant KV Cache, thereby reducing sequence length. However, such techniques\nare limited to single-context scenarios, where historical KV Cache is computed\nsequentially with causal-attention dependencies. In retrieval-augmented\ngeneration (RAG) scenarios, where retrieved documents as context are unknown\nbeforehand, each document's KV Cache is computed and stored independently\n(termed multiple-context KV Cache), lacking cross-attention between contexts.\nThis renders existing methods ineffective. Although prior work partially\nrecomputes multiple-context KV Cache to mitigate accuracy loss from missing\ncross-attention, it requires retaining all KV Cache throughout, failing to\nreduce memory overhead. This paper presents SamKV, the first exploration of\nattention sparsification for multiple-context KV Cache. Specifically, SamKV\ntakes into account the complementary information of other contexts when\nsparsifying one context, and then locally recomputes the sparsified\ninformation. Experiments demonstrate that our method compresses sequence length\nto 15% without accuracy degradation compared with full-recompuation baselines,\nsignificantly boosting throughput in multi-context RAG scenarios."
                },
                "authors": [
                    {
                        "name": "Ziyi Cao"
                    },
                    {
                        "name": "Qingyi Si"
                    },
                    {
                        "name": "Jingbin Zhang"
                    },
                    {
                        "name": "Bingquan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Bingquan Liu"
                },
                "author": "Bingquan Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11661v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11661v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.03837v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.03837v1",
                "updated": "2025-08-05T18:34:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    5,
                    18,
                    34,
                    48,
                    1,
                    217,
                    0
                ],
                "published": "2025-08-05T18:34:48Z",
                "published_parsed": [
                    2025,
                    8,
                    5,
                    18,
                    34,
                    48,
                    1,
                    217,
                    0
                ],
                "title": "Rhea: a Framework for Fast Design and Validation of RTL Cache-Coherent\n  Memory Subsystems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rhea: a Framework for Fast Design and Validation of RTL Cache-Coherent\n  Memory Subsystems"
                },
                "summary": "Designing and validating efficient cache-coherent memory subsystems is a\ncritical yet complex task in the development of modern multi-core\nsystem-on-chip architectures. Rhea is a unified framework that streamlines the\ndesign and system-level validation of RTL cache-coherent memory subsystems. On\nthe design side, Rhea generates synthesizable, highly configurable RTL\nsupporting various architectural parameters. On the validation side, Rhea\nintegrates Verilator's cycle-accurate RTL simulation with gem5's full-system\nsimulation, allowing realistic workloads and operating systems to run alongside\nthe actual RTL under test. We apply Rhea to design MSI-based RTL memory\nsubsystems with one and two levels of private caches and scaling up to sixteen\ncores. Their evaluation with 22 applications from state-of-the-art benchmark\nsuites shows intermediate performance relative to gem5 Ruby's MI and MOESI\nmodels. The hybrid gem5-Verilator co-simulation flow incurs a moderate\nsimulation overhead, up to 2.7 times compared to gem5 MI, but achieves higher\nfidelity by simulating real RTL hardware. This overhead decreases with scale,\ndown to 1.6 times in sixteen-core scenarios. These results demonstrate Rhea's\neffectiveness and scalability in enabling fast development of RTL\ncache-coherent memory subsystem designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing and validating efficient cache-coherent memory subsystems is a\ncritical yet complex task in the development of modern multi-core\nsystem-on-chip architectures. Rhea is a unified framework that streamlines the\ndesign and system-level validation of RTL cache-coherent memory subsystems. On\nthe design side, Rhea generates synthesizable, highly configurable RTL\nsupporting various architectural parameters. On the validation side, Rhea\nintegrates Verilator's cycle-accurate RTL simulation with gem5's full-system\nsimulation, allowing realistic workloads and operating systems to run alongside\nthe actual RTL under test. We apply Rhea to design MSI-based RTL memory\nsubsystems with one and two levels of private caches and scaling up to sixteen\ncores. Their evaluation with 22 applications from state-of-the-art benchmark\nsuites shows intermediate performance relative to gem5 Ruby's MI and MOESI\nmodels. The hybrid gem5-Verilator co-simulation flow incurs a moderate\nsimulation overhead, up to 2.7 times compared to gem5 MI, but achieves higher\nfidelity by simulating real RTL hardware. This overhead decreases with scale,\ndown to 1.6 times in sixteen-core scenarios. These results demonstrate Rhea's\neffectiveness and scalability in enabling fast development of RTL\ncache-coherent memory subsystem designs."
                },
                "authors": [
                    {
                        "name": "Davide Zoni"
                    },
                    {
                        "name": "Andrea Galimberti"
                    },
                    {
                        "name": "Adriano Guarisco"
                    }
                ],
                "author_detail": {
                    "name": "Adriano Guarisco"
                },
                "author": "Adriano Guarisco",
                "arxiv_comment": "9 pages, 13 figures, 1 table, accepted for presentation at 2025\n  International Conference on Computer-Aided Design (ICCAD), Munich, Germany,\n  October 26-30, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.03837v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.03837v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07120v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07120v2",
                "updated": "2025-08-05T16:17:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    5,
                    16,
                    17,
                    1,
                    1,
                    217,
                    0
                ],
                "published": "2025-03-10T09:49:18Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    9,
                    49,
                    18,
                    0,
                    69,
                    0
                ],
                "title": "FEB-Cache: Frequency-Guided Exposure Bias Reduction for Enhancing\n  Diffusion Transformer Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FEB-Cache: Frequency-Guided Exposure Bias Reduction for Enhancing\n  Diffusion Transformer Caching"
                },
                "summary": "Diffusion Transformer (DiT) has exhibited impressive generation capabilities\nbut faces great challenges due to its high computational complexity. To address\nthis issue, various methods, notably feature caching, have been introduced.\nHowever, these approaches focus on aligning non-cache diffusion without\nanalyzing why caching damage the generation processes. In this paper, we first\nconfirm that the cache greatly amplifies the exposure bias, resulting in a\ndecline in the generation quality. However, directly applying noise scaling is\nchallenging for this issue due to the non-smoothness of exposure bias. We found\nthat this phenomenon stems from the mismatch between its frequency response\ncharacteristics and the simple cache of Attention and MLP. Since these two\ncomponents exhibit unique preferences for frequency signals, which provides us\nwith a caching strategy to separate Attention and MLP to achieve an enhanced\nfit of exposure bias and reduce it. Based on this, we introduced FEB-Cache, a\njoint caching strategy that aligns with the non-exposed bias diffusion process\n(which gives us a higher performance cap) of caching Attention and MLP based on\nthe frequency-guided cache table. Our approach combines a comprehensive\nunderstanding of the caching mechanism and offers a new perspective on\nleveraging caching to accelerate the diffusion process. Empirical results\nindicate that FEB-Cache optimizes model performance while concurrently\nfacilitating acceleration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformer (DiT) has exhibited impressive generation capabilities\nbut faces great challenges due to its high computational complexity. To address\nthis issue, various methods, notably feature caching, have been introduced.\nHowever, these approaches focus on aligning non-cache diffusion without\nanalyzing why caching damage the generation processes. In this paper, we first\nconfirm that the cache greatly amplifies the exposure bias, resulting in a\ndecline in the generation quality. However, directly applying noise scaling is\nchallenging for this issue due to the non-smoothness of exposure bias. We found\nthat this phenomenon stems from the mismatch between its frequency response\ncharacteristics and the simple cache of Attention and MLP. Since these two\ncomponents exhibit unique preferences for frequency signals, which provides us\nwith a caching strategy to separate Attention and MLP to achieve an enhanced\nfit of exposure bias and reduce it. Based on this, we introduced FEB-Cache, a\njoint caching strategy that aligns with the non-exposed bias diffusion process\n(which gives us a higher performance cap) of caching Attention and MLP based on\nthe frequency-guided cache table. Our approach combines a comprehensive\nunderstanding of the caching mechanism and offers a new perspective on\nleveraging caching to accelerate the diffusion process. Empirical results\nindicate that FEB-Cache optimizes model performance while concurrently\nfacilitating acceleration."
                },
                "authors": [
                    {
                        "name": "Zhen Zou"
                    },
                    {
                        "name": "Feng Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Feng Zhao"
                },
                "author": "Feng Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07120v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07120v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.03321v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.03321v1",
                "updated": "2025-08-05T11:00:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    5,
                    11,
                    0,
                    41,
                    1,
                    217,
                    0
                ],
                "published": "2025-08-05T11:00:41Z",
                "published_parsed": [
                    2025,
                    8,
                    5,
                    11,
                    0,
                    41,
                    1,
                    217,
                    0
                ],
                "title": "Bidirectional TLS Handshake Caching for Constrained Industrial IoT\n  Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bidirectional TLS Handshake Caching for Constrained Industrial IoT\n  Scenarios"
                },
                "summary": "While TLS has become the de-facto standard for end-to-end security, its use\nto secure critical communication in evolving industrial IoT scenarios is\nseverely limited by prevalent resource constraints of devices and networks.\nMost notably, the TLS handshake to establish secure connections incurs\nsignificant bandwidth and processing overhead that often cannot be handled in\nconstrained environments. To alleviate this situation, we present BiTHaC which\nrealizes bidirectional TLS handshake caching by exploiting that significant\nparts of repeated TLS handshakes, especially certificates, are static. Thus,\nredundant information neither needs to be transmitted nor corresponding\ncomputations performed, saving valuable bandwidth and processing resources. By\nimplementing BiTHaC for wolfSSL, we show that we can reduce the bandwidth\nconsumption of TLS handshakes by up to 61.1% and the computational overhead by\nup to 8.5%, while incurring only well-manageable memory overhead and preserving\nthe strict security guarantees of TLS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While TLS has become the de-facto standard for end-to-end security, its use\nto secure critical communication in evolving industrial IoT scenarios is\nseverely limited by prevalent resource constraints of devices and networks.\nMost notably, the TLS handshake to establish secure connections incurs\nsignificant bandwidth and processing overhead that often cannot be handled in\nconstrained environments. To alleviate this situation, we present BiTHaC which\nrealizes bidirectional TLS handshake caching by exploiting that significant\nparts of repeated TLS handshakes, especially certificates, are static. Thus,\nredundant information neither needs to be transmitted nor corresponding\ncomputations performed, saving valuable bandwidth and processing resources. By\nimplementing BiTHaC for wolfSSL, we show that we can reduce the bandwidth\nconsumption of TLS handshakes by up to 61.1% and the computational overhead by\nup to 8.5%, while incurring only well-manageable memory overhead and preserving\nthe strict security guarantees of TLS."
                },
                "authors": [
                    {
                        "name": "Jörn Bodenhausen"
                    },
                    {
                        "name": "Simon Mangel"
                    },
                    {
                        "name": "Thomas Vogt"
                    },
                    {
                        "name": "Martin Henze"
                    }
                ],
                "author_detail": {
                    "name": "Martin Henze"
                },
                "author": "Martin Henze",
                "arxiv_comment": "Accepted for publication in Proceedings of the 2025 IEEE 50th\n  Conference on Local Computer Networks (LCN)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.03321v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.03321v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.03258v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.03258v1",
                "updated": "2025-08-05T09:35:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    5,
                    9,
                    35,
                    52,
                    1,
                    217,
                    0
                ],
                "published": "2025-08-05T09:35:52Z",
                "published_parsed": [
                    2025,
                    8,
                    5,
                    9,
                    35,
                    52,
                    1,
                    217,
                    0
                ],
                "title": "SmartLLMs Scheduler: A Framework for Cost-Effective LLMs Utilization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SmartLLMs Scheduler: A Framework for Cost-Effective LLMs Utilization"
                },
                "summary": "Large Language Models (LLMs) such as GPT-4 and Llama have shown remarkable\ncapabilities in a variety of software engineering tasks. Despite the\nadvancements, their practical deployment faces challenges, including high\nfinancial costs, long response time, and varying performance, especially when\nhandling a large number of queries (jobs). Existing optimization strategies for\ndeploying LLMs for diverse tasks focus on static scheduling, which requires\nextensive training data for performance prediction, increasing the\ncomputational costs and limiting the applicability and flexibility. In this\npaper, we propose the SmartLLMs Scheduler (SLS), a dynamic and cost-effective\nscheduling solution. The key idea is to learn LLMs' performance on diverse\ntasks and incorporate their real-time feedback to update strategies\nperiodically. Specifically, SLS incorporates three key components, including an\nAdaptive Cache Manager, a Performance-Cost Optimized Scheduler, and a Dynamic\nUpdate Manager. The Cache Manager stores the outputs of previously processed\nqueries and employs an adaptive strategy to reduce redundant computations and\nminimize response times. For queries not found in the cache, the Scheduler\ndynamically allocates them to the most suitable LLM based on the predicted\nperformance and cost from models that take both query-specific and LLM-specific\nfeatures as input. The Update Manager continuously refines the cache and\nscheduling strategies based on real-time feedback from the assigned queries to\nenhance decision-making and adapt to evolving task characteristics. To evaluate\nthe effectiveness of SLS, we conduct extensive experiments on two LLM-based\nsoftware engineering tasks, including log parsing and code generation. The\nresults show that SLS significantly outperforms the baseline methods, achieving\nan average performance improvement of 198.82% and an average processing time\nreduction of 63.28%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) such as GPT-4 and Llama have shown remarkable\ncapabilities in a variety of software engineering tasks. Despite the\nadvancements, their practical deployment faces challenges, including high\nfinancial costs, long response time, and varying performance, especially when\nhandling a large number of queries (jobs). Existing optimization strategies for\ndeploying LLMs for diverse tasks focus on static scheduling, which requires\nextensive training data for performance prediction, increasing the\ncomputational costs and limiting the applicability and flexibility. In this\npaper, we propose the SmartLLMs Scheduler (SLS), a dynamic and cost-effective\nscheduling solution. The key idea is to learn LLMs' performance on diverse\ntasks and incorporate their real-time feedback to update strategies\nperiodically. Specifically, SLS incorporates three key components, including an\nAdaptive Cache Manager, a Performance-Cost Optimized Scheduler, and a Dynamic\nUpdate Manager. The Cache Manager stores the outputs of previously processed\nqueries and employs an adaptive strategy to reduce redundant computations and\nminimize response times. For queries not found in the cache, the Scheduler\ndynamically allocates them to the most suitable LLM based on the predicted\nperformance and cost from models that take both query-specific and LLM-specific\nfeatures as input. The Update Manager continuously refines the cache and\nscheduling strategies based on real-time feedback from the assigned queries to\nenhance decision-making and adapt to evolving task characteristics. To evaluate\nthe effectiveness of SLS, we conduct extensive experiments on two LLM-based\nsoftware engineering tasks, including log parsing and code generation. The\nresults show that SLS significantly outperforms the baseline methods, achieving\nan average performance improvement of 198.82% and an average processing time\nreduction of 63.28%."
                },
                "authors": [
                    {
                        "name": "Yueyue Liu"
                    },
                    {
                        "name": "Hongyu Zhang"
                    },
                    {
                        "name": "Yuantian Miao"
                    }
                ],
                "author_detail": {
                    "name": "Yuantian Miao"
                },
                "author": "Yuantian Miao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.03258v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.03258v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02240v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02240v2",
                "updated": "2025-08-05T02:13:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    5,
                    2,
                    13,
                    39,
                    1,
                    217,
                    0
                ],
                "published": "2025-08-04T09:39:31Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    9,
                    39,
                    31,
                    0,
                    216,
                    0
                ],
                "title": "Forecasting When to Forecast: Accelerating Diffusion Models with\n  Confidence-Gated Taylor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasting When to Forecast: Accelerating Diffusion Models with\n  Confidence-Gated Taylor"
                },
                "summary": "Diffusion Transformers (DiTs) have demonstrated remarkable performance in\nvisual generation tasks. However, their low inference speed limits their\ndeployment in low-resource applications. Recent training-free approaches\nexploit the redundancy of features across timesteps by caching and reusing past\nrepresentations to accelerate inference. Building on this idea, TaylorSeer\ninstead uses cached features to predict future ones via Taylor expansion.\nHowever, its module-level prediction across all transformer blocks (e.g.,\nattention or feedforward modules) requires storing fine-grained intermediate\nfeatures, leading to notable memory and computation overhead. Moreover, it\nadopts a fixed caching schedule without considering the varying accuracy of\npredictions across timesteps, which can lead to degraded outputs when\nprediction fails. To address these limitations, we propose a novel approach to\nbetter leverage Taylor-based acceleration. First, we shift the Taylor\nprediction target from the module level to the last block level, significantly\nreducing the number of cached features. Furthermore, observing strong\nsequential dependencies among Transformer blocks, we propose to use the error\nbetween the Taylor-estimated and actual outputs of the first block as an\nindicator of prediction reliability. If the error is small, we trust the Taylor\nprediction for the last block; otherwise, we fall back to full computation,\nthereby enabling a dynamic caching mechanism. Empirical results show that our\nmethod achieves a better balance between speed and quality, achieving a 3.17x\nacceleration on FLUX, 2.36x on DiT, and 4.14x on Wan Video with negligible\nquality drop. The Project Page is\n\\href{https://cg-taylor-acce.github.io/CG-Taylor/}{here.}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have demonstrated remarkable performance in\nvisual generation tasks. However, their low inference speed limits their\ndeployment in low-resource applications. Recent training-free approaches\nexploit the redundancy of features across timesteps by caching and reusing past\nrepresentations to accelerate inference. Building on this idea, TaylorSeer\ninstead uses cached features to predict future ones via Taylor expansion.\nHowever, its module-level prediction across all transformer blocks (e.g.,\nattention or feedforward modules) requires storing fine-grained intermediate\nfeatures, leading to notable memory and computation overhead. Moreover, it\nadopts a fixed caching schedule without considering the varying accuracy of\npredictions across timesteps, which can lead to degraded outputs when\nprediction fails. To address these limitations, we propose a novel approach to\nbetter leverage Taylor-based acceleration. First, we shift the Taylor\nprediction target from the module level to the last block level, significantly\nreducing the number of cached features. Furthermore, observing strong\nsequential dependencies among Transformer blocks, we propose to use the error\nbetween the Taylor-estimated and actual outputs of the first block as an\nindicator of prediction reliability. If the error is small, we trust the Taylor\nprediction for the last block; otherwise, we fall back to full computation,\nthereby enabling a dynamic caching mechanism. Empirical results show that our\nmethod achieves a better balance between speed and quality, achieving a 3.17x\nacceleration on FLUX, 2.36x on DiT, and 4.14x on Wan Video with negligible\nquality drop. The Project Page is\n\\href{https://cg-taylor-acce.github.io/CG-Taylor/}{here.}"
                },
                "authors": [
                    {
                        "name": "Xiaoliu Guan"
                    },
                    {
                        "name": "Lielin Jiang"
                    },
                    {
                        "name": "Hanqi Chen"
                    },
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Jiaxing Yan"
                    },
                    {
                        "name": "Guanzhong Wang"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Zetao Zhang"
                    },
                    {
                        "name": "Yu Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wu"
                },
                "author": "Yu Wu",
                "arxiv_comment": "15 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02240v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02240v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14866v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14866v5",
                "updated": "2025-08-05T00:25:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    5,
                    0,
                    25,
                    53,
                    1,
                    217,
                    0
                ],
                "published": "2025-04-21T05:27:33Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    5,
                    27,
                    33,
                    0,
                    111,
                    0
                ],
                "title": "GainSight: A Unified Framework for Data Lifetime Profiling and\n  Heterogeneous Memory Composition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GainSight: A Unified Framework for Data Lifetime Profiling and\n  Heterogeneous Memory Composition"
                },
                "summary": "As AI workloads drive increasing memory requirements, domain-specific\naccelerators need higher-density on-chip memory beyond what current SRAM\nscaling trends can provide. Simultaneously, the vast amounts of short-lived\ndata in these workloads make SRAM overprovisioned in retention capability. To\naddress this mismatch, we propose a wholesale shift from uniform SRAM arrays to\nheterogeneous on-chip memory, incorporating denser short-term RAM (StRAM)\ndevices whose limited retention times align with transient data lifetimes. To\nfacilitate this shift, we introduce GainSight, the first comprehensive,\nopen-source framework that aligns dynamic, fine-grained workload lifetime\nprofiles with memory device characteristics to enable generation of optimal\nStRAM memory compositions. GainSight combines retargetable profiling backends\nwith an architecture-agnostic analytical frontend. The various backends capture\ncycle-accurate data lifetimes, while the frontend correlates workload patterns\nwith StRAM retention properties to generate optimal memory compositions and\nproject performance. GainSight elevates data lifetime to a first-class design\nconsideration for next-generation AI accelerators, enabling systematic\nexploitation of data transience for improved on-chip memory density and\nefficiency. Applying GainSight to MLPerf Inference and PolyBench workloads\nreveals that 64.3% of first-level GPU cache accesses and 79.01% of systolic\narray scratchpad accesses exhibit sub-microsecond lifetimes suitable for\nhigh-density StRAM, with optimal heterogeneous on-chip memory compositions\nachieving up to 3x active energy and 4x area reductions compared to uniform\nSRAM hierarchies. To facilitate adoption and further research, GainSight is\nopen-sourced at https://gainsight.stanford.edu/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI workloads drive increasing memory requirements, domain-specific\naccelerators need higher-density on-chip memory beyond what current SRAM\nscaling trends can provide. Simultaneously, the vast amounts of short-lived\ndata in these workloads make SRAM overprovisioned in retention capability. To\naddress this mismatch, we propose a wholesale shift from uniform SRAM arrays to\nheterogeneous on-chip memory, incorporating denser short-term RAM (StRAM)\ndevices whose limited retention times align with transient data lifetimes. To\nfacilitate this shift, we introduce GainSight, the first comprehensive,\nopen-source framework that aligns dynamic, fine-grained workload lifetime\nprofiles with memory device characteristics to enable generation of optimal\nStRAM memory compositions. GainSight combines retargetable profiling backends\nwith an architecture-agnostic analytical frontend. The various backends capture\ncycle-accurate data lifetimes, while the frontend correlates workload patterns\nwith StRAM retention properties to generate optimal memory compositions and\nproject performance. GainSight elevates data lifetime to a first-class design\nconsideration for next-generation AI accelerators, enabling systematic\nexploitation of data transience for improved on-chip memory density and\nefficiency. Applying GainSight to MLPerf Inference and PolyBench workloads\nreveals that 64.3% of first-level GPU cache accesses and 79.01% of systolic\narray scratchpad accesses exhibit sub-microsecond lifetimes suitable for\nhigh-density StRAM, with optimal heterogeneous on-chip memory compositions\nachieving up to 3x active energy and 4x area reductions compared to uniform\nSRAM hierarchies. To facilitate adoption and further research, GainSight is\nopen-sourced at https://gainsight.stanford.edu/."
                },
                "authors": [
                    {
                        "name": "Peijing Li"
                    },
                    {
                        "name": "Matthew Hung"
                    },
                    {
                        "name": "Yiming Tan"
                    },
                    {
                        "name": "Konstantin Hoßfeld"
                    },
                    {
                        "name": "Jake Cheng Jiajun"
                    },
                    {
                        "name": "Shuhan Liu"
                    },
                    {
                        "name": "Lixian Yan"
                    },
                    {
                        "name": "Xinxin Wang"
                    },
                    {
                        "name": "Philip Levis"
                    },
                    {
                        "name": "H. -S. Philip Wong"
                    },
                    {
                        "name": "Thierry Tambe"
                    }
                ],
                "author_detail": {
                    "name": "Thierry Tambe"
                },
                "author": "Thierry Tambe",
                "arxiv_comment": "14 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14866v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14866v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.7.1; B.3.1; C.3; I.6; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02558v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02558v1",
                "updated": "2025-08-04T16:14:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    16,
                    14,
                    3,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T16:14:03Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    16,
                    14,
                    3,
                    0,
                    216,
                    0
                ],
                "title": "Sparse-dLLM: Accelerating Diffusion LLMs with Dynamic Cache Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse-dLLM: Accelerating Diffusion LLMs with Dynamic Cache Eviction"
                },
                "summary": "Diffusion Large Language Models (dLLMs) enable breakthroughs in reasoning and\nparallel decoding but suffer from prohibitive quadratic computational\ncomplexity and memory overhead during inference. Current caching techniques\naccelerate decoding by storing full-layer states, yet impose substantial memory\nusage that limit long-context applications. Our analysis of attention patterns\nin dLLMs reveals persistent cross-layer sparsity, with pivotal tokens remaining\nsalient across decoding steps and low-relevance tokens staying unimportant,\nmotivating selective cache eviction. We propose Sparse-dLLM, the first\ntraining-free framework integrating dynamic cache eviction with sparse\nattention via delayed bidirectional sparse caching. By leveraging the stability\nof token saliency over steps, it retains critical tokens and dynamically evicts\nunimportant prefix/suffix entries using an attention-guided strategy. Extensive\nexperiments on LLaDA and Dream series demonstrate Sparse-dLLM achieves up to\n10$\\times$ higher throughput than vanilla dLLMs, with comparable performance\nand similar peak memory costs, outperforming previous methods in efficiency and\neffectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Large Language Models (dLLMs) enable breakthroughs in reasoning and\nparallel decoding but suffer from prohibitive quadratic computational\ncomplexity and memory overhead during inference. Current caching techniques\naccelerate decoding by storing full-layer states, yet impose substantial memory\nusage that limit long-context applications. Our analysis of attention patterns\nin dLLMs reveals persistent cross-layer sparsity, with pivotal tokens remaining\nsalient across decoding steps and low-relevance tokens staying unimportant,\nmotivating selective cache eviction. We propose Sparse-dLLM, the first\ntraining-free framework integrating dynamic cache eviction with sparse\nattention via delayed bidirectional sparse caching. By leveraging the stability\nof token saliency over steps, it retains critical tokens and dynamically evicts\nunimportant prefix/suffix entries using an attention-guided strategy. Extensive\nexperiments on LLaDA and Dream series demonstrate Sparse-dLLM achieves up to\n10$\\times$ higher throughput than vanilla dLLMs, with comparable performance\nand similar peak memory costs, outperforming previous methods in efficiency and\neffectiveness."
                },
                "authors": [
                    {
                        "name": "Yuerong Song"
                    },
                    {
                        "name": "Xiaoran Liu"
                    },
                    {
                        "name": "Ruixiao Li"
                    },
                    {
                        "name": "Zhigeng Liu"
                    },
                    {
                        "name": "Zengfeng Huang"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Ziwei He"
                    },
                    {
                        "name": "Xipeng Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xipeng Qiu"
                },
                "author": "Xipeng Qiu",
                "arxiv_comment": "11 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02558v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02558v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02401v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02401v1",
                "updated": "2025-08-04T13:26:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    13,
                    26,
                    16,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T13:26:16Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    13,
                    26,
                    16,
                    0,
                    216,
                    0
                ],
                "title": "CompressKV: Semantic Retrieval Heads Know What Tokens are Not Important\n  Before Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CompressKV: Semantic Retrieval Heads Know What Tokens are Not Important\n  Before Generation"
                },
                "summary": "Recent advances in large language models (LLMs) have significantly boosted\nlong-context processing. However, the increasing key-value (KV) cache size\nposes critical challenges to memory and execution efficiency. Most KV cache\ncompression methods rely on heuristic token eviction using all attention heads\nin Grouped Query Attention (GQA)-based LLMs. This method ignores the different\nfunctionalities of attention heads, leading to the eviction of critical tokens\nand thus degrades the performance of LLMs.\n  To address the issue above, instead of using all the attention heads in\nGQA-based LLMs to determine important tokens as in the previous work, we first\nidentify the attention heads in each layer that are not only capable of\nretrieving the initial and final tokens of a prompt, but also capable of\nretrieving important tokens within the text and attending to their surrounding\nsemantic context. Afterwards, we exploit such heads to determine the important\ntokens and retain their corresponding KV cache pairs. Furthermore, we analyze\nthe cache eviction error of each layer individually and introduce a\nlayer-adaptive KV cache allocation strategy. Experimental results demonstrate\nthe proposed CompressKV consistently outperforms state-of-the-art approaches\nunder various memory budgets on LongBench and Needle-in-a-Haystack benchmarks.\nOur code is publicly available at: https://github.com/TUDa-HWAI/CompressKV.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have significantly boosted\nlong-context processing. However, the increasing key-value (KV) cache size\nposes critical challenges to memory and execution efficiency. Most KV cache\ncompression methods rely on heuristic token eviction using all attention heads\nin Grouped Query Attention (GQA)-based LLMs. This method ignores the different\nfunctionalities of attention heads, leading to the eviction of critical tokens\nand thus degrades the performance of LLMs.\n  To address the issue above, instead of using all the attention heads in\nGQA-based LLMs to determine important tokens as in the previous work, we first\nidentify the attention heads in each layer that are not only capable of\nretrieving the initial and final tokens of a prompt, but also capable of\nretrieving important tokens within the text and attending to their surrounding\nsemantic context. Afterwards, we exploit such heads to determine the important\ntokens and retain their corresponding KV cache pairs. Furthermore, we analyze\nthe cache eviction error of each layer individually and introduce a\nlayer-adaptive KV cache allocation strategy. Experimental results demonstrate\nthe proposed CompressKV consistently outperforms state-of-the-art approaches\nunder various memory budgets on LongBench and Needle-in-a-Haystack benchmarks.\nOur code is publicly available at: https://github.com/TUDa-HWAI/CompressKV.git."
                },
                "authors": [
                    {
                        "name": "Xiaolin Lin"
                    },
                    {
                        "name": "Jingcun Wang"
                    },
                    {
                        "name": "Olga Kondrateva"
                    },
                    {
                        "name": "Yiyu Shi"
                    },
                    {
                        "name": "Bing Li"
                    },
                    {
                        "name": "Grace Li Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Grace Li Zhang"
                },
                "author": "Grace Li Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02401v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02401v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2506.15006v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15006v2",
                "updated": "2025-08-19T17:56:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    17,
                    56,
                    18,
                    1,
                    231,
                    0
                ],
                "published": "2025-06-17T22:29:37Z",
                "published_parsed": [
                    2025,
                    6,
                    17,
                    22,
                    29,
                    37,
                    1,
                    168,
                    0
                ],
                "title": "Scaling Intelligence: Designing Data Centers for Next-Gen Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Intelligence: Designing Data Centers for Next-Gen Language\n  Models"
                },
                "summary": "The explosive growth of Large Language Models (LLMs), such as GPT-4 with 1.8\ntrillion parameters, demands a fundamental rethinking of data center\narchitecture to ensure scalability, efficiency, and cost-effectiveness. Our\nwork provides a comprehensive co-design framework that jointly explores FLOPS,\nHBM bandwidth and capacity, multiple network topologies (two-tier vs. FullFlat\noptical), the size of the scale-out domain, and popular\nparallelism/optimization strategies used in LLMs. We introduce and evaluate\nFullFlat network architectures, which provide uniform high-bandwidth,\nlow-latency connectivity between all nodes, and demonstrate their\ntransformative impact on performance and scalability. Through detailed\nsensitivity analyses, we quantify the benefits of overlapping compute and\ncommunication, leveraging hardware-accelerated collectives, widening the\nscale-out domain, and increasing memory capacity. Our study spans both sparse\n(mixture of experts) and dense transformer-based LLMs, revealing how system\ndesign choices affect Model FLOPS Utilization (MFU = Model FLOPS per token *\nObserved tokens per second / Peak FLOPS of the hardware) and overall\nthroughput. For the co-design study, we utilized an analytical performance\nmodeling tool capable of predicting LLM runtime within 10% of real-world\nmeasurements. Our findings offer actionable insights and a practical roadmap\nfor designing AI data centers that can efficiently support trillion-parameter\nmodels, reduce optimization complexity, and sustain the rapid evolution of AI\ncapabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The explosive growth of Large Language Models (LLMs), such as GPT-4 with 1.8\ntrillion parameters, demands a fundamental rethinking of data center\narchitecture to ensure scalability, efficiency, and cost-effectiveness. Our\nwork provides a comprehensive co-design framework that jointly explores FLOPS,\nHBM bandwidth and capacity, multiple network topologies (two-tier vs. FullFlat\noptical), the size of the scale-out domain, and popular\nparallelism/optimization strategies used in LLMs. We introduce and evaluate\nFullFlat network architectures, which provide uniform high-bandwidth,\nlow-latency connectivity between all nodes, and demonstrate their\ntransformative impact on performance and scalability. Through detailed\nsensitivity analyses, we quantify the benefits of overlapping compute and\ncommunication, leveraging hardware-accelerated collectives, widening the\nscale-out domain, and increasing memory capacity. Our study spans both sparse\n(mixture of experts) and dense transformer-based LLMs, revealing how system\ndesign choices affect Model FLOPS Utilization (MFU = Model FLOPS per token *\nObserved tokens per second / Peak FLOPS of the hardware) and overall\nthroughput. For the co-design study, we utilized an analytical performance\nmodeling tool capable of predicting LLM runtime within 10% of real-world\nmeasurements. Our findings offer actionable insights and a practical roadmap\nfor designing AI data centers that can efficiently support trillion-parameter\nmodels, reduce optimization complexity, and sustain the rapid evolution of AI\ncapabilities."
                },
                "authors": [
                    {
                        "name": "Jesmin Jahan Tithi"
                    },
                    {
                        "name": "Hanjiang Wu"
                    },
                    {
                        "name": "Avishaii Abuhatzera"
                    },
                    {
                        "name": "Fabrizio Petrini"
                    }
                ],
                "author_detail": {
                    "name": "Fabrizio Petrini"
                },
                "author": "Fabrizio Petrini",
                "arxiv_comment": "14 pages, submitted to SC25 for review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15006v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15006v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16438v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16438v2",
                "updated": "2025-08-19T17:56:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    17,
                    56,
                    16,
                    1,
                    231,
                    0
                ],
                "published": "2025-04-23T05:57:20Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    5,
                    57,
                    20,
                    2,
                    113,
                    0
                ],
                "title": "POPri: Private Federated Learning using Preference-Optimized Synthetic\n  Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "POPri: Private Federated Learning using Preference-Optimized Synthetic\n  Data"
                },
                "summary": "In practical settings, differentially private Federated learning (DP-FL) is\nthe dominant method for training models from private, on-device client data.\nRecent work has suggested that DP-FL may be enhanced or outperformed by methods\nthat use DP synthetic data (Wu et al., 2024; Hou et al., 2024). The primary\nalgorithms for generating DP synthetic data for FL applications require careful\nprompt engineering based on public information and/or iterative private client\nfeedback. Our key insight is that the private client feedback collected by\nprior DP synthetic data methods (Hou et al., 2024; Xie et al., 2024) can be\nviewed as an RL (reinforcement learning) reward. Our algorithm, Policy\nOptimization for Private Data (POPri) harnesses client feedback using policy\noptimization algorithms such as Direct Preference Optimization (DPO) to\nfine-tune LLMs to generate high-quality DP synthetic data. To evaluate POPri,\nwe release LargeFedBench, a new federated text benchmark for uncontaminated LLM\nevaluations on federated client data. POPri substantially improves the utility\nof DP synthetic data relative to prior work on LargeFedBench datasets and an\nexisting benchmark from Xie et al. (2024). POPri closes the gap between\nnext-token prediction accuracy in the fully-private and non-private settings by\nup to 58%, compared to 28% for prior synthetic data methods, and 3% for\nstate-of-the-art DP federated learning methods. The code and data are available\nat https://github.com/meiyuw/POPri.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In practical settings, differentially private Federated learning (DP-FL) is\nthe dominant method for training models from private, on-device client data.\nRecent work has suggested that DP-FL may be enhanced or outperformed by methods\nthat use DP synthetic data (Wu et al., 2024; Hou et al., 2024). The primary\nalgorithms for generating DP synthetic data for FL applications require careful\nprompt engineering based on public information and/or iterative private client\nfeedback. Our key insight is that the private client feedback collected by\nprior DP synthetic data methods (Hou et al., 2024; Xie et al., 2024) can be\nviewed as an RL (reinforcement learning) reward. Our algorithm, Policy\nOptimization for Private Data (POPri) harnesses client feedback using policy\noptimization algorithms such as Direct Preference Optimization (DPO) to\nfine-tune LLMs to generate high-quality DP synthetic data. To evaluate POPri,\nwe release LargeFedBench, a new federated text benchmark for uncontaminated LLM\nevaluations on federated client data. POPri substantially improves the utility\nof DP synthetic data relative to prior work on LargeFedBench datasets and an\nexisting benchmark from Xie et al. (2024). POPri closes the gap between\nnext-token prediction accuracy in the fully-private and non-private settings by\nup to 58%, compared to 28% for prior synthetic data methods, and 3% for\nstate-of-the-art DP federated learning methods. The code and data are available\nat https://github.com/meiyuw/POPri."
                },
                "authors": [
                    {
                        "name": "Charlie Hou"
                    },
                    {
                        "name": "Mei-Yu Wang"
                    },
                    {
                        "name": "Yige Zhu"
                    },
                    {
                        "name": "Daniel Lazar"
                    },
                    {
                        "name": "Giulia Fanti"
                    }
                ],
                "author_detail": {
                    "name": "Giulia Fanti"
                },
                "author": "Giulia Fanti",
                "arxiv_comment": "ICML 2025 camera-ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16438v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16438v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14032v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14032v1",
                "updated": "2025-08-19T17:54:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    17,
                    54,
                    56,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T17:54:56Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    17,
                    54,
                    56,
                    1,
                    231,
                    0
                ],
                "title": "The Promise of Large Language Models in Digital Health: Evidence from\n  Sentiment Analysis in Online Health Communities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Promise of Large Language Models in Digital Health: Evidence from\n  Sentiment Analysis in Online Health Communities"
                },
                "summary": "Digital health analytics face critical challenges nowadays. The sophisticated\nanalysis of patient-generated health content, which contains complex emotional\nand medical contexts, requires scarce domain expertise, while traditional ML\napproaches are constrained by data shortage and privacy limitations in\nhealthcare settings. Online Health Communities (OHCs) exemplify these\nchallenges with mixed-sentiment posts, clinical terminology, and implicit\nemotional expressions that demand specialised knowledge for accurate Sentiment\nAnalysis (SA). To address these challenges, this study explores how Large\nLanguage Models (LLMs) can integrate expert knowledge through in-context\nlearning for SA, providing a scalable solution for sophisticated health data\nanalysis. Specifically, we develop a structured codebook that systematically\nencodes expert interpretation guidelines, enabling LLMs to apply\ndomain-specific knowledge through targeted prompting rather than extensive\ntraining. Six GPT models validated alongside DeepSeek and LLaMA 3.1 are\ncompared with pre-trained language models (BioBERT variants) and lexicon-based\nmethods, using 400 expert-annotated posts from two OHCs. LLMs achieve superior\nperformance while demonstrating expert-level agreement. This high agreement,\nwith no statistically significant difference from inter-expert agreement\nlevels, suggests knowledge integration beyond surface-level pattern\nrecognition. The consistent performance across diverse LLM models, supported by\nin-context learning, offers a promising solution for digital health analytics.\nThis approach addresses the critical challenge of expert knowledge shortage in\ndigital health research, enabling real-time, expert-quality analysis for\npatient monitoring, intervention assessment, and evidence-based health\nstrategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital health analytics face critical challenges nowadays. The sophisticated\nanalysis of patient-generated health content, which contains complex emotional\nand medical contexts, requires scarce domain expertise, while traditional ML\napproaches are constrained by data shortage and privacy limitations in\nhealthcare settings. Online Health Communities (OHCs) exemplify these\nchallenges with mixed-sentiment posts, clinical terminology, and implicit\nemotional expressions that demand specialised knowledge for accurate Sentiment\nAnalysis (SA). To address these challenges, this study explores how Large\nLanguage Models (LLMs) can integrate expert knowledge through in-context\nlearning for SA, providing a scalable solution for sophisticated health data\nanalysis. Specifically, we develop a structured codebook that systematically\nencodes expert interpretation guidelines, enabling LLMs to apply\ndomain-specific knowledge through targeted prompting rather than extensive\ntraining. Six GPT models validated alongside DeepSeek and LLaMA 3.1 are\ncompared with pre-trained language models (BioBERT variants) and lexicon-based\nmethods, using 400 expert-annotated posts from two OHCs. LLMs achieve superior\nperformance while demonstrating expert-level agreement. This high agreement,\nwith no statistically significant difference from inter-expert agreement\nlevels, suggests knowledge integration beyond surface-level pattern\nrecognition. The consistent performance across diverse LLM models, supported by\nin-context learning, offers a promising solution for digital health analytics.\nThis approach addresses the critical challenge of expert knowledge shortage in\ndigital health research, enabling real-time, expert-quality analysis for\npatient monitoring, intervention assessment, and evidence-based health\nstrategies."
                },
                "authors": [
                    {
                        "name": "Xiancheng Li"
                    },
                    {
                        "name": "Georgios D. Karampatakis"
                    },
                    {
                        "name": "Helen E. Wood"
                    },
                    {
                        "name": "Chris J. Griffiths"
                    },
                    {
                        "name": "Borislava Mihaylova"
                    },
                    {
                        "name": "Neil S. Coulson"
                    },
                    {
                        "name": "Alessio Pasinato"
                    },
                    {
                        "name": "Pietro Panzarasa"
                    },
                    {
                        "name": "Marco Viviani"
                    },
                    {
                        "name": "Anna De Simoni"
                    }
                ],
                "author_detail": {
                    "name": "Anna De Simoni"
                },
                "author": "Anna De Simoni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14032v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14032v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14031v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14031v1",
                "updated": "2025-08-19T17:53:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    17,
                    53,
                    35,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T17:53:35Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    17,
                    53,
                    35,
                    1,
                    231,
                    0
                ],
                "title": "Unintended Misalignment from Agentic Fine-Tuning: Risks and Mitigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unintended Misalignment from Agentic Fine-Tuning: Risks and Mitigation"
                },
                "summary": "Beyond simple text generation, Large Language Models (LLMs) have evolved into\nagentic systems capable of planning and interacting with external tools to\nsolve complex tasks. This evolution involves fine-tuning LLMs on agent-specific\ntasks to enhance their proficiency. However, safety concerns are frequently\noverlooked during this fine-tuning process. In this work, we show that aligned\nLLMs can become unintentionally misaligned, leading to a higher likelihood of\nexecuting harmful tasks and a reduced tendency to refuse them when fine-tuned\nto execute agentic tasks. To address these safety challenges, we propose Prefix\nINjection Guard (PING), a simple yet effective method that prepends\nautomatically generated natural language prefixes to agent responses, guiding\nthem to refuse harmful requests while preserving performance on benign tasks.\nSpecifically, we introduce an iterative approach that alternates between (1)\ngenerating candidate prefixes and (2) selecting those that optimize both task\nperformance and refusal behavior. Experimental results demonstrate that PING\nsignificantly enhances the safety of fine-tuned LLM agents without sacrificing\ntheir effectiveness. PING consistently outperforms existing prompting\napproaches across diverse benchmarks in both web navigation and code generation\ntasks. Our analysis of internal hidden states via linear probes reveals that\nprefix tokens are crucial for behavior modification, explaining the performance\ngains. WARNING: This paper contains contents that are unethical or offensive in\nnature.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond simple text generation, Large Language Models (LLMs) have evolved into\nagentic systems capable of planning and interacting with external tools to\nsolve complex tasks. This evolution involves fine-tuning LLMs on agent-specific\ntasks to enhance their proficiency. However, safety concerns are frequently\noverlooked during this fine-tuning process. In this work, we show that aligned\nLLMs can become unintentionally misaligned, leading to a higher likelihood of\nexecuting harmful tasks and a reduced tendency to refuse them when fine-tuned\nto execute agentic tasks. To address these safety challenges, we propose Prefix\nINjection Guard (PING), a simple yet effective method that prepends\nautomatically generated natural language prefixes to agent responses, guiding\nthem to refuse harmful requests while preserving performance on benign tasks.\nSpecifically, we introduce an iterative approach that alternates between (1)\ngenerating candidate prefixes and (2) selecting those that optimize both task\nperformance and refusal behavior. Experimental results demonstrate that PING\nsignificantly enhances the safety of fine-tuned LLM agents without sacrificing\ntheir effectiveness. PING consistently outperforms existing prompting\napproaches across diverse benchmarks in both web navigation and code generation\ntasks. Our analysis of internal hidden states via linear probes reveals that\nprefix tokens are crucial for behavior modification, explaining the performance\ngains. WARNING: This paper contains contents that are unethical or offensive in\nnature."
                },
                "authors": [
                    {
                        "name": "Dongyoon Hahm"
                    },
                    {
                        "name": "Taywon Min"
                    },
                    {
                        "name": "Woogyeol Jin"
                    },
                    {
                        "name": "Kimin Lee"
                    }
                ],
                "author_detail": {
                    "name": "Kimin Lee"
                },
                "author": "Kimin Lee",
                "arxiv_comment": "Source code: https://github.com/HahmDY/prefix_injection_guard",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14031v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14031v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14029v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14029v2",
                "updated": "2025-08-20T01:21:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    1,
                    21,
                    25,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-19T17:42:45Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    17,
                    42,
                    45,
                    1,
                    231,
                    0
                ],
                "title": "Beyond Pass@1: Self-Play with Variational Problem Synthesis Sustains\n  RLVR",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Pass@1: Self-Play with Variational Problem Synthesis Sustains\n  RLVR"
                },
                "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as\na key paradigm for post-training Large Language Models (LLMs), particularly for\ncomplex reasoning tasks. However, vanilla RLVR training has been shown to\nimprove Pass@1 performance at the expense of policy entropy, leading to reduced\ngeneration diversity and limiting the Pass@k performance, which typically\nrepresents the upper bound of LLM reasoning capability. In this paper, we\nsystematically analyze the policy's generation diversity from the perspective\nof training problems and find that augmenting and updating training problems\nhelps mitigate entropy collapse during training. Based on these observations,\nwe propose an online Self-play with Variational problem Synthesis (SvS)\nstrategy for RLVR training, which uses the policy's correct solutions to\nsynthesize variational problems while ensuring their reference answers remain\nidentical to the originals. This self-improving strategy effectively maintains\npolicy entropy during training and substantially improves Pass@k compared with\nstandard RLVR, sustaining prolonged improvements and achieving absolute gains\nof 18.3% and 22.8% in Pass@32 performance on the competition-level AIME24 and\nAIME25 benchmarks. Experiments on 12 reasoning benchmarks across varying model\nsizes from 3B to 32B consistently demonstrate the generalizability and\nrobustness of SvS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as\na key paradigm for post-training Large Language Models (LLMs), particularly for\ncomplex reasoning tasks. However, vanilla RLVR training has been shown to\nimprove Pass@1 performance at the expense of policy entropy, leading to reduced\ngeneration diversity and limiting the Pass@k performance, which typically\nrepresents the upper bound of LLM reasoning capability. In this paper, we\nsystematically analyze the policy's generation diversity from the perspective\nof training problems and find that augmenting and updating training problems\nhelps mitigate entropy collapse during training. Based on these observations,\nwe propose an online Self-play with Variational problem Synthesis (SvS)\nstrategy for RLVR training, which uses the policy's correct solutions to\nsynthesize variational problems while ensuring their reference answers remain\nidentical to the originals. This self-improving strategy effectively maintains\npolicy entropy during training and substantially improves Pass@k compared with\nstandard RLVR, sustaining prolonged improvements and achieving absolute gains\nof 18.3% and 22.8% in Pass@32 performance on the competition-level AIME24 and\nAIME25 benchmarks. Experiments on 12 reasoning benchmarks across varying model\nsizes from 3B to 32B consistently demonstrate the generalizability and\nrobustness of SvS."
                },
                "authors": [
                    {
                        "name": "Xiao Liang"
                    },
                    {
                        "name": "Zhongzhi Li"
                    },
                    {
                        "name": "Yeyun Gong"
                    },
                    {
                        "name": "Yelong Shen"
                    },
                    {
                        "name": "Ying Nian Wu"
                    },
                    {
                        "name": "Zhijiang Guo"
                    },
                    {
                        "name": "Weizhu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Weizhu Chen"
                },
                "author": "Weizhu Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14029v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14029v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14025v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14025v1",
                "updated": "2025-08-19T17:31:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    17,
                    31,
                    42,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T17:31:42Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    17,
                    31,
                    42,
                    1,
                    231,
                    0
                ],
                "title": "Ask Good Questions for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ask Good Questions for Large Language Models"
                },
                "summary": "Recent advances in large language models (LLMs) have significantly improved\nthe performance of dialog systems, yet current approaches often fail to provide\naccurate guidance of topic due to their inability to discern user confusion in\nrelated concepts. To address this, we introduce the Ask-Good-Question (AGQ)\nframework, which features an improved Concept-Enhanced Item Response Theory\n(CEIRT) model to better identify users' knowledge levels. Our contributions\ninclude applying the CEIRT model along with LLMs to directly generate guiding\nquestions based on the inspiring text, greatly improving information retrieval\nefficiency during the question & answer process. Through comparisons with other\nbaseline methods, our approach outperforms by significantly enhencing the\nusers' information retrieval experiences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have significantly improved\nthe performance of dialog systems, yet current approaches often fail to provide\naccurate guidance of topic due to their inability to discern user confusion in\nrelated concepts. To address this, we introduce the Ask-Good-Question (AGQ)\nframework, which features an improved Concept-Enhanced Item Response Theory\n(CEIRT) model to better identify users' knowledge levels. Our contributions\ninclude applying the CEIRT model along with LLMs to directly generate guiding\nquestions based on the inspiring text, greatly improving information retrieval\nefficiency during the question & answer process. Through comparisons with other\nbaseline methods, our approach outperforms by significantly enhencing the\nusers' information retrieval experiences."
                },
                "authors": [
                    {
                        "name": "Qi Wu"
                    },
                    {
                        "name": "Zhongqi Lu"
                    }
                ],
                "author_detail": {
                    "name": "Zhongqi Lu"
                },
                "author": "Zhongqi Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14025v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14025v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12152v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12152v2",
                "updated": "2025-08-19T17:29:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    17,
                    29,
                    28,
                    1,
                    231,
                    0
                ],
                "published": "2025-01-21T14:02:39Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    14,
                    2,
                    39,
                    1,
                    21,
                    0
                ],
                "title": "Contextualizing Recommendation Explanations with LLMs: A User Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contextualizing Recommendation Explanations with LLMs: A User Study"
                },
                "summary": "Large language models (LLMs) are increasingly prevalent in recommender\nsystems, where LLMs can be used to generate personalized recommendations. Here,\nwe examine how different LLM-generated explanations for movie recommendations\naffect users' perceptions of cognitive, affective, and utilitarian needs and\nconsumption intentions. In a pre-registered, between-subject online experiment\n(N=759) and follow-up interviews (N=30), we compare (a) LLM-generated generic\nexplanations, and (b) LLM-generated contextualized explanations. Our findings\nshow that contextualized explanations (i.e., explanations that incorporate\nusers' past behaviors) effectively meet users' cognitive needs while increasing\nusers' intentions to watch recommended movies. However, adding explanations\noffers limited benefits in meeting users' utilitarian and affective needs,\nraising concerns about the proper design and implications of LLM-generated\nexplanations. Qualitative insights from interviews reveal that referencing\nusers' past preferences enhances trust and understanding but can feel excessive\nif overused. Furthermore, users with more active and positive engagement with\nthe recommender system and movie-watching get substantial gains from\ncontextualized explanations. Overall, our research clarifies how LLM-generated\nrecommendations influence users' motivations and behaviors, providing valuable\ninsights for the future development of user-centric recommender systems, a key\nelement in social media platforms and online ecosystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly prevalent in recommender\nsystems, where LLMs can be used to generate personalized recommendations. Here,\nwe examine how different LLM-generated explanations for movie recommendations\naffect users' perceptions of cognitive, affective, and utilitarian needs and\nconsumption intentions. In a pre-registered, between-subject online experiment\n(N=759) and follow-up interviews (N=30), we compare (a) LLM-generated generic\nexplanations, and (b) LLM-generated contextualized explanations. Our findings\nshow that contextualized explanations (i.e., explanations that incorporate\nusers' past behaviors) effectively meet users' cognitive needs while increasing\nusers' intentions to watch recommended movies. However, adding explanations\noffers limited benefits in meeting users' utilitarian and affective needs,\nraising concerns about the proper design and implications of LLM-generated\nexplanations. Qualitative insights from interviews reveal that referencing\nusers' past preferences enhances trust and understanding but can feel excessive\nif overused. Furthermore, users with more active and positive engagement with\nthe recommender system and movie-watching get substantial gains from\ncontextualized explanations. Overall, our research clarifies how LLM-generated\nrecommendations influence users' motivations and behaviors, providing valuable\ninsights for the future development of user-centric recommender systems, a key\nelement in social media platforms and online ecosystems."
                },
                "authors": [
                    {
                        "name": "Yuanjun Feng"
                    },
                    {
                        "name": "Stefan Feuerriegel"
                    },
                    {
                        "name": "Yash Raj Shrestha"
                    }
                ],
                "author_detail": {
                    "name": "Yash Raj Shrestha"
                },
                "author": "Yash Raj Shrestha",
                "arxiv_comment": "Accepted to the International AAAI Conference on Web and Social Media\n  (ICWSM 2026)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12152v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12152v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14022v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14022v1",
                "updated": "2025-08-19T17:28:14Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    17,
                    28,
                    14,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T17:28:14Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    17,
                    28,
                    14,
                    1,
                    231,
                    0
                ],
                "title": "BLIPs: Bayesian Learned Interatomic Potentials",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BLIPs: Bayesian Learned Interatomic Potentials"
                },
                "summary": "Machine Learning Interatomic Potentials (MLIPs) are becoming a central tool\nin simulation-based chemistry. However, like most deep learning models, MLIPs\nstruggle to make accurate predictions on out-of-distribution data or when\ntrained in a data-scarce regime, both common scenarios in simulation-based\nchemistry. Moreover, MLIPs do not provide uncertainty estimates by\nconstruction, which are fundamental to guide active learning pipelines and to\nensure the accuracy of simulation results compared to quantum calculations. To\naddress this shortcoming, we propose BLIPs: Bayesian Learned Interatomic\nPotentials. BLIP is a scalable, architecture-agnostic variational Bayesian\nframework for training or fine-tuning MLIPs, built on an adaptive version of\nVariational Dropout. BLIP delivers well-calibrated uncertainty estimates and\nminimal computational overhead for energy and forces prediction at inference\ntime, while integrating seamlessly with (equivariant) message-passing\narchitectures. Empirical results on simulation-based computational chemistry\ntasks demonstrate improved predictive accuracy with respect to standard MLIPs,\nand trustworthy uncertainty estimates, especially in data-scarse or heavy\nout-of-distribution regimes. Moreover, fine-tuning pretrained MLIPs with BLIP\nyields consistent performance gains and calibrated uncertainties.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine Learning Interatomic Potentials (MLIPs) are becoming a central tool\nin simulation-based chemistry. However, like most deep learning models, MLIPs\nstruggle to make accurate predictions on out-of-distribution data or when\ntrained in a data-scarce regime, both common scenarios in simulation-based\nchemistry. Moreover, MLIPs do not provide uncertainty estimates by\nconstruction, which are fundamental to guide active learning pipelines and to\nensure the accuracy of simulation results compared to quantum calculations. To\naddress this shortcoming, we propose BLIPs: Bayesian Learned Interatomic\nPotentials. BLIP is a scalable, architecture-agnostic variational Bayesian\nframework for training or fine-tuning MLIPs, built on an adaptive version of\nVariational Dropout. BLIP delivers well-calibrated uncertainty estimates and\nminimal computational overhead for energy and forces prediction at inference\ntime, while integrating seamlessly with (equivariant) message-passing\narchitectures. Empirical results on simulation-based computational chemistry\ntasks demonstrate improved predictive accuracy with respect to standard MLIPs,\nand trustworthy uncertainty estimates, especially in data-scarse or heavy\nout-of-distribution regimes. Moreover, fine-tuning pretrained MLIPs with BLIP\nyields consistent performance gains and calibrated uncertainties."
                },
                "authors": [
                    {
                        "name": "Dario Coscia"
                    },
                    {
                        "name": "Pim de Haan"
                    },
                    {
                        "name": "Max Welling"
                    }
                ],
                "author_detail": {
                    "name": "Max Welling"
                },
                "author": "Max Welling",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14022v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14022v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14021v1",
                "updated": "2025-08-19T17:27:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    17,
                    27,
                    47,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T17:27:47Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    17,
                    27,
                    47,
                    1,
                    231,
                    0
                ],
                "title": "Data Compression with Noise Suppression for Inference under Noisy\n  Covariance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data Compression with Noise Suppression for Inference under Noisy\n  Covariance"
                },
                "summary": "In many fields including cosmology, statistical inference often relies on\nGaussian likelihoods whose covariance matrices are estimated from a finite\nnumber of simulations. This finite-sample estimation introduces noise into the\ncovariance, which propagates to parameter estimates, a phenomenon known as the\nDodelson-Schneider (DS) effect, leading to inflated uncertainties. While the\nMassively Optimized Parameter Estimation and Data compression (MOPED) algorithm\noffers lossless Fisher information-preserving compression, it does not mitigate\nthe DS effect when the compression matrix itself is derived from noisy\ncovariances. In this paper, we propose a modified compression scheme, powered\nMOPED ($p$-MOPED), which suppresses noise propagation by balancing information\nretention and covariance estimate noise reduction through a tunable power-law\ntransformation of the sample correlation matrix. We test $p$-MOPED against\nstandard and diagonal MOPED on toy models and on cosmological data from the\nSubaru Hyper Suprime-Cam Year 3 weak lensing survey. Our results demonstrate\nthat $p$-MOPED consistently outperforms other approaches, especially in regimes\nwith limited simulations, offering a robust compression strategy for\nhigh-dimensional data analyses under practical constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In many fields including cosmology, statistical inference often relies on\nGaussian likelihoods whose covariance matrices are estimated from a finite\nnumber of simulations. This finite-sample estimation introduces noise into the\ncovariance, which propagates to parameter estimates, a phenomenon known as the\nDodelson-Schneider (DS) effect, leading to inflated uncertainties. While the\nMassively Optimized Parameter Estimation and Data compression (MOPED) algorithm\noffers lossless Fisher information-preserving compression, it does not mitigate\nthe DS effect when the compression matrix itself is derived from noisy\ncovariances. In this paper, we propose a modified compression scheme, powered\nMOPED ($p$-MOPED), which suppresses noise propagation by balancing information\nretention and covariance estimate noise reduction through a tunable power-law\ntransformation of the sample correlation matrix. We test $p$-MOPED against\nstandard and diagonal MOPED on toy models and on cosmological data from the\nSubaru Hyper Suprime-Cam Year 3 weak lensing survey. Our results demonstrate\nthat $p$-MOPED consistently outperforms other approaches, especially in regimes\nwith limited simulations, offering a robust compression strategy for\nhigh-dimensional data analyses under practical constraints."
                },
                "authors": [
                    {
                        "name": "Sunao Sugiyama"
                    },
                    {
                        "name": "Minsu Park"
                    }
                ],
                "author_detail": {
                    "name": "Minsu Park"
                },
                "author": "Minsu Park",
                "arxiv_comment": "10 pages, 5 figures, comments welcomed!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19061v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19061v3",
                "updated": "2025-08-20T14:24:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    14,
                    24,
                    25,
                    2,
                    232,
                    0
                ],
                "published": "2025-04-27T00:39:12Z",
                "published_parsed": [
                    2025,
                    4,
                    27,
                    0,
                    39,
                    12,
                    6,
                    117,
                    0
                ],
                "title": "Hallucinations and Key Information Extraction in Medical Texts: A\n  Comprehensive Assessment of Open-Source Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucinations and Key Information Extraction in Medical Texts: A\n  Comprehensive Assessment of Open-Source Large Language Models"
                },
                "summary": "Clinical summarization is crucial in healthcare as it distills complex\nmedical data into digestible information, enhancing patient understanding and\ncare management. Large language models (LLMs) have shown significant potential\nin automating and improving the accuracy of such summarizations due to their\nadvanced natural language understanding capabilities. These models are\nparticularly applicable in the context of summarizing medical/clinical texts,\nwhere precise and concise information transfer is essential. In this paper, we\ninvestigate the effectiveness of open-source LLMs in extracting key events from\ndischarge reports, including admission reasons, major in-hospital events, and\ncritical follow-up actions. In addition, we also assess the prevalence of\nvarious types of hallucinations in the summaries produced by these models.\nDetecting hallucinations is vital as it directly influences the reliability of\nthe information, potentially affecting patient care and treatment outcomes. We\nconduct comprehensive simulations to rigorously evaluate the performance of\nthese models, further probing the accuracy and fidelity of the extracted\ncontent in clinical summarization. Our results reveal that while the LLMs\n(e.g., Qwen2.5 and DeepSeek-v2) perform quite well in capturing admission\nreasons and hospitalization events, they are generally less consistent when it\ncomes to identifying follow-up recommendations, highlighting broader challenges\nin leveraging LLMs for comprehensive summarization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clinical summarization is crucial in healthcare as it distills complex\nmedical data into digestible information, enhancing patient understanding and\ncare management. Large language models (LLMs) have shown significant potential\nin automating and improving the accuracy of such summarizations due to their\nadvanced natural language understanding capabilities. These models are\nparticularly applicable in the context of summarizing medical/clinical texts,\nwhere precise and concise information transfer is essential. In this paper, we\ninvestigate the effectiveness of open-source LLMs in extracting key events from\ndischarge reports, including admission reasons, major in-hospital events, and\ncritical follow-up actions. In addition, we also assess the prevalence of\nvarious types of hallucinations in the summaries produced by these models.\nDetecting hallucinations is vital as it directly influences the reliability of\nthe information, potentially affecting patient care and treatment outcomes. We\nconduct comprehensive simulations to rigorously evaluate the performance of\nthese models, further probing the accuracy and fidelity of the extracted\ncontent in clinical summarization. Our results reveal that while the LLMs\n(e.g., Qwen2.5 and DeepSeek-v2) perform quite well in capturing admission\nreasons and hospitalization events, they are generally less consistent when it\ncomes to identifying follow-up recommendations, highlighting broader challenges\nin leveraging LLMs for comprehensive summarization."
                },
                "authors": [
                    {
                        "name": "Anindya Bijoy Das"
                    },
                    {
                        "name": "Shibbir Ahmed"
                    },
                    {
                        "name": "Shahnewaz Karim Sakib"
                    }
                ],
                "author_detail": {
                    "name": "Shahnewaz Karim Sakib"
                },
                "author": "Shahnewaz Karim Sakib",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19061v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19061v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07961v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07961v2",
                "updated": "2025-08-19T17:06:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    17,
                    6,
                    41,
                    1,
                    231,
                    0
                ],
                "published": "2025-04-10T17:59:55Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    17,
                    59,
                    55,
                    3,
                    100,
                    0
                ],
                "title": "Geo4D: Leveraging Video Generators for Geometric 4D Scene Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geo4D: Leveraging Video Generators for Geometric 4D Scene Reconstruction"
                },
                "summary": "We introduce Geo4D, a method to repurpose video diffusion models for\nmonocular 3D reconstruction of dynamic scenes. By leveraging the strong dynamic\npriors captured by large-scale pre-trained video models, Geo4D can be trained\nusing only synthetic data while generalizing well to real data in a zero-shot\nmanner. Geo4D predicts several complementary geometric modalities, namely\npoint, disparity, and ray maps. We propose a new multi-modal alignment\nalgorithm to align and fuse these modalities, as well as a sliding window\napproach at inference time, thus enabling robust and accurate 4D reconstruction\nof long videos. Extensive experiments across multiple benchmarks show that\nGeo4D significantly surpasses state-of-the-art video depth estimation methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Geo4D, a method to repurpose video diffusion models for\nmonocular 3D reconstruction of dynamic scenes. By leveraging the strong dynamic\npriors captured by large-scale pre-trained video models, Geo4D can be trained\nusing only synthetic data while generalizing well to real data in a zero-shot\nmanner. Geo4D predicts several complementary geometric modalities, namely\npoint, disparity, and ray maps. We propose a new multi-modal alignment\nalgorithm to align and fuse these modalities, as well as a sliding window\napproach at inference time, thus enabling robust and accurate 4D reconstruction\nof long videos. Extensive experiments across multiple benchmarks show that\nGeo4D significantly surpasses state-of-the-art video depth estimation methods."
                },
                "authors": [
                    {
                        "name": "Zeren Jiang"
                    },
                    {
                        "name": "Chuanxia Zheng"
                    },
                    {
                        "name": "Iro Laina"
                    },
                    {
                        "name": "Diane Larlus"
                    },
                    {
                        "name": "Andrea Vedaldi"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Vedaldi"
                },
                "author": "Andrea Vedaldi",
                "arxiv_comment": "17 pages, 6 figures, ICCV 2025 Highlight, Project page:\n  https://geo4d.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07961v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07961v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.4.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10486v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10486v2",
                "updated": "2025-08-19T16:47:51Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    16,
                    47,
                    51,
                    1,
                    231,
                    0
                ],
                "published": "2025-04-14T17:59:58Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    17,
                    59,
                    58,
                    0,
                    104,
                    0
                ],
                "title": "DNF-Avatar: Distilling Neural Fields for Real-time Animatable Avatar\n  Relighting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DNF-Avatar: Distilling Neural Fields for Real-time Animatable Avatar\n  Relighting"
                },
                "summary": "Creating relightable and animatable human avatars from monocular videos is a\nrising research topic with a range of applications, e.g. virtual reality,\nsports, and video games. Previous works utilize neural fields together with\nphysically based rendering (PBR), to estimate geometry and disentangle\nappearance properties of human avatars. However, one drawback of these methods\nis the slow rendering speed due to the expensive Monte Carlo ray tracing. To\ntackle this problem, we proposed to distill the knowledge from implicit neural\nfields (teacher) to explicit 2D Gaussian splatting (student) representation to\ntake advantage of the fast rasterization property of Gaussian splatting. To\navoid ray-tracing, we employ the split-sum approximation for PBR appearance. We\nalso propose novel part-wise ambient occlusion probes for shadow computation.\nShadow prediction is achieved by querying these probes only once per pixel,\nwhich paves the way for real-time relighting of avatars. These techniques\ncombined give high-quality relighting results with realistic shadow effects.\nOur experiments demonstrate that the proposed student model achieves comparable\nor even better relighting results with our teacher model while being 370 times\nfaster at inference time, achieving a 67 FPS rendering speed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Creating relightable and animatable human avatars from monocular videos is a\nrising research topic with a range of applications, e.g. virtual reality,\nsports, and video games. Previous works utilize neural fields together with\nphysically based rendering (PBR), to estimate geometry and disentangle\nappearance properties of human avatars. However, one drawback of these methods\nis the slow rendering speed due to the expensive Monte Carlo ray tracing. To\ntackle this problem, we proposed to distill the knowledge from implicit neural\nfields (teacher) to explicit 2D Gaussian splatting (student) representation to\ntake advantage of the fast rasterization property of Gaussian splatting. To\navoid ray-tracing, we employ the split-sum approximation for PBR appearance. We\nalso propose novel part-wise ambient occlusion probes for shadow computation.\nShadow prediction is achieved by querying these probes only once per pixel,\nwhich paves the way for real-time relighting of avatars. These techniques\ncombined give high-quality relighting results with realistic shadow effects.\nOur experiments demonstrate that the proposed student model achieves comparable\nor even better relighting results with our teacher model while being 370 times\nfaster at inference time, achieving a 67 FPS rendering speed."
                },
                "authors": [
                    {
                        "name": "Zeren Jiang"
                    },
                    {
                        "name": "Shaofei Wang"
                    },
                    {
                        "name": "Siyu Tang"
                    }
                ],
                "author_detail": {
                    "name": "Siyu Tang"
                },
                "author": "Siyu Tang",
                "arxiv_comment": "17 pages, 9 figures, ICCV 2025 Findings Oral, Project pages:\n  https://jzr99.github.io/DNF-Avatar/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10486v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10486v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03092v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03092v2",
                "updated": "2025-08-19T16:37:32Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    16,
                    37,
                    32,
                    1,
                    231,
                    0
                ],
                "published": "2025-06-03T17:24:08Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    24,
                    8,
                    1,
                    154,
                    0
                ],
                "title": "Thin coronal jets and plasmoid-mediated reconnection: Insights from\n  Solar Orbiter observations and Bifrost simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thin coronal jets and plasmoid-mediated reconnection: Insights from\n  Solar Orbiter observations and Bifrost simulations"
                },
                "summary": "Coronal jets are ubiquitous, collimated million-degree ejections that\ncontribute to the energy and mass supply of the upper solar atmosphere and the\nsolar wind. Solar Orbiter provides an unprecedented opportunity to observe\nfine-scale jets from a unique vantage point close to the Sun. We aim to uncover\nthin jets originating from Coronal Bright Points (CBPs) and investigate\nobservable features of plasmoid-mediated reconnection. We analyze eleven\ndatasets from the High Resolution Imager 174 \\r{A} of the Extreme Ultraviolet\nImager (HRIEUV) onboard Solar Orbiter, focusing on narrow jets from CBPs and\nsignatures of magnetic reconnection within current sheets and outflow regions.\nTo support the observations, we compare with CBP simulations performed with the\nBifrost code. We have identified thin coronal jets originating from CBPs with\nwidths ranging from 253 km to 706 km: scales that could not be resolved with\nprevious EUV imaging instruments. Remarkably, these jets are 30-85% brighter\nthan their surroundings and can extend up to 22 Mm while maintaining their\nnarrow form. In one of the datasets, we directly identify plasmoid-mediated\nreconnection through the development within the current sheet of a small-scale\nplasmoid that reaches a size of 332 km and propagates at 40 km/s. In another\ndataset, we infer plasmoid signatures through the intermittent boomerang-like\npattern that appears in the outflow region. Both direct and indirect\nplasmoid-mediated reconnection signatures are supported by comparisons with the\nsynthetic HRIEUV emission from the simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coronal jets are ubiquitous, collimated million-degree ejections that\ncontribute to the energy and mass supply of the upper solar atmosphere and the\nsolar wind. Solar Orbiter provides an unprecedented opportunity to observe\nfine-scale jets from a unique vantage point close to the Sun. We aim to uncover\nthin jets originating from Coronal Bright Points (CBPs) and investigate\nobservable features of plasmoid-mediated reconnection. We analyze eleven\ndatasets from the High Resolution Imager 174 \\r{A} of the Extreme Ultraviolet\nImager (HRIEUV) onboard Solar Orbiter, focusing on narrow jets from CBPs and\nsignatures of magnetic reconnection within current sheets and outflow regions.\nTo support the observations, we compare with CBP simulations performed with the\nBifrost code. We have identified thin coronal jets originating from CBPs with\nwidths ranging from 253 km to 706 km: scales that could not be resolved with\nprevious EUV imaging instruments. Remarkably, these jets are 30-85% brighter\nthan their surroundings and can extend up to 22 Mm while maintaining their\nnarrow form. In one of the datasets, we directly identify plasmoid-mediated\nreconnection through the development within the current sheet of a small-scale\nplasmoid that reaches a size of 332 km and propagates at 40 km/s. In another\ndataset, we infer plasmoid signatures through the intermittent boomerang-like\npattern that appears in the outflow region. Both direct and indirect\nplasmoid-mediated reconnection signatures are supported by comparisons with the\nsynthetic HRIEUV emission from the simulations."
                },
                "authors": [
                    {
                        "name": "D. Nóbrega-Siverio"
                    },
                    {
                        "name": "R. Joshi"
                    },
                    {
                        "name": "E. Sola-Viladesau"
                    },
                    {
                        "name": "D. Berghmans"
                    },
                    {
                        "name": "D. Lim"
                    }
                ],
                "author_detail": {
                    "name": "D. Lim"
                },
                "author": "D. Lim",
                "arxiv_comment": "Accepted in A&A, 18 pages, 10 figures. All the movies are available\n  in https://zenodo.org/records/16903189",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03092v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03092v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13993v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13993v1",
                "updated": "2025-08-19T16:33:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    16,
                    33,
                    55,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T16:33:55Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    16,
                    33,
                    55,
                    1,
                    231,
                    0
                ],
                "title": "Chunks as Arms: Multi-Armed Bandit-Guided Sampling for Long-Context LLM\n  Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chunks as Arms: Multi-Armed Bandit-Guided Sampling for Long-Context LLM\n  Preference Optimization"
                },
                "summary": "Long-context modeling is critical for a wide range of real-world tasks,\nincluding long-context question answering, summarization, and complex reasoning\ntasks. Recent studies have explored fine-tuning Large Language Models (LLMs)\nwith synthetic data to enhance their long-context capabilities. However, the\neffectiveness of such approaches is often limited by the low diversity and\nfactual inconsistencies in the generated data. To address these challenges, we\npropose LongMab-PO, a novel framework that leverages a Multi-Armed Bandit (MAB)\nrollout strategy to identify the most informative chunks from the given long\ncontext for sampling high-quality and diverse responses and constructing\npreference data pairs for Direct Preference Optimization (DPO) training.\nSpecifically, we treat context chunks as arms of MAB, select chunks based on\ntheir expected reward scores to input into LLMs to generate responses, and\niteratively update these scores based on reward feedback. This exploration and\nexploitation process enables the model to focus on the most relevant context\nsegments, thereby generating and collecting high-quality and diverse responses.\nFinally, we collect these generated responses from the rollout process and\napply the DPO method to further optimize the LLM. Experimental results show\nthat LongMab-PO significantly improves the diversity and quality of preference\ndata pairs, achieving state-of-the-art performance on long-context reasoning\nbenchmarks. All code and data will be released on\nhttps://github.com/NEUIR/LongMab-PO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context modeling is critical for a wide range of real-world tasks,\nincluding long-context question answering, summarization, and complex reasoning\ntasks. Recent studies have explored fine-tuning Large Language Models (LLMs)\nwith synthetic data to enhance their long-context capabilities. However, the\neffectiveness of such approaches is often limited by the low diversity and\nfactual inconsistencies in the generated data. To address these challenges, we\npropose LongMab-PO, a novel framework that leverages a Multi-Armed Bandit (MAB)\nrollout strategy to identify the most informative chunks from the given long\ncontext for sampling high-quality and diverse responses and constructing\npreference data pairs for Direct Preference Optimization (DPO) training.\nSpecifically, we treat context chunks as arms of MAB, select chunks based on\ntheir expected reward scores to input into LLMs to generate responses, and\niteratively update these scores based on reward feedback. This exploration and\nexploitation process enables the model to focus on the most relevant context\nsegments, thereby generating and collecting high-quality and diverse responses.\nFinally, we collect these generated responses from the rollout process and\napply the DPO method to further optimize the LLM. Experimental results show\nthat LongMab-PO significantly improves the diversity and quality of preference\ndata pairs, achieving state-of-the-art performance on long-context reasoning\nbenchmarks. All code and data will be released on\nhttps://github.com/NEUIR/LongMab-PO."
                },
                "authors": [
                    {
                        "name": "Shaohua Duan"
                    },
                    {
                        "name": "Xinze Li"
                    },
                    {
                        "name": "Zhenghao Liu"
                    },
                    {
                        "name": "Xiaoyuan Yi"
                    },
                    {
                        "name": "Yukun Yan"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Yu Gu"
                    },
                    {
                        "name": "Ge Yu"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13993v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13993v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13443v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13443v2",
                "updated": "2025-08-19T16:13:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    16,
                    13,
                    16,
                    1,
                    231,
                    0
                ],
                "published": "2025-04-18T03:49:53Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    3,
                    49,
                    53,
                    4,
                    108,
                    0
                ],
                "title": "Trust, but verify",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trust, but verify"
                },
                "summary": "Decentralized AI agent networks, such as Gaia, allows individuals to run\ncustomized LLMs on their own computers and then provide services to the public.\nHowever, in order to maintain service quality, the network must verify that\nindividual nodes are running their designated LLMs. In this paper, we\ndemonstrate that in a cluster of mostly honest nodes, we can detect nodes that\nrun unauthorized or incorrect LLM through social consensus of its peers. We\nwill discuss the algorithm and experimental data from the Gaia network. We will\nalso discuss the intersubjective validation system, implemented as an\nEigenLayer AVS to introduce financial incentives and penalties to encourage\nhonest behavior from LLM nodes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralized AI agent networks, such as Gaia, allows individuals to run\ncustomized LLMs on their own computers and then provide services to the public.\nHowever, in order to maintain service quality, the network must verify that\nindividual nodes are running their designated LLMs. In this paper, we\ndemonstrate that in a cluster of mostly honest nodes, we can detect nodes that\nrun unauthorized or incorrect LLM through social consensus of its peers. We\nwill discuss the algorithm and experimental data from the Gaia network. We will\nalso discuss the intersubjective validation system, implemented as an\nEigenLayer AVS to introduce financial incentives and penalties to encourage\nhonest behavior from LLM nodes."
                },
                "authors": [
                    {
                        "name": "Michael J. Yuan"
                    },
                    {
                        "name": "Carlos Lospoy"
                    },
                    {
                        "name": "Sydney Lai"
                    },
                    {
                        "name": "James Snewin"
                    },
                    {
                        "name": "Ju Long"
                    }
                ],
                "author_detail": {
                    "name": "Ju Long"
                },
                "author": "Ju Long",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13443v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13443v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22884v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22884v2",
                "updated": "2025-08-19T16:13:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    16,
                    13,
                    9,
                    1,
                    231,
                    0
                ],
                "published": "2025-03-28T21:21:35Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    21,
                    21,
                    35,
                    4,
                    87,
                    0
                ],
                "title": "AutoComPose: Automatic Generation of Pose Transition Descriptions for\n  Composed Pose Retrieval Using Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoComPose: Automatic Generation of Pose Transition Descriptions for\n  Composed Pose Retrieval Using Multimodal LLMs"
                },
                "summary": "Composed pose retrieval (CPR) enables users to search for human poses by\nspecifying a reference pose and a transition description, but progress in this\nfield is hindered by the scarcity and inconsistency of annotated pose\ntransitions. Existing CPR datasets rely on costly human annotations or\nheuristic-based rule generation, both of which limit scalability and diversity.\nIn this work, we introduce AutoComPose, the first framework that leverages\nmultimodal large language models (MLLMs) to automatically generate rich and\nstructured pose transition descriptions. Our method enhances annotation quality\nby structuring transitions into fine-grained body part movements and\nintroducing mirrored/swapped variations, while a cyclic consistency constraint\nensures logical coherence between forward and reverse transitions. To advance\nCPR research, we construct and release two dedicated benchmarks, AIST-CPR and\nPoseFixCPR, supplementing prior datasets with enhanced attributes. Extensive\nexperiments demonstrate that training retrieval models with AutoComPose yields\nsuperior performance over human-annotated and heuristic-based methods,\nsignificantly reducing annotation costs while improving retrieval quality. Our\nwork pioneers the automatic annotation of pose transitions, establishing a\nscalable foundation for future CPR research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Composed pose retrieval (CPR) enables users to search for human poses by\nspecifying a reference pose and a transition description, but progress in this\nfield is hindered by the scarcity and inconsistency of annotated pose\ntransitions. Existing CPR datasets rely on costly human annotations or\nheuristic-based rule generation, both of which limit scalability and diversity.\nIn this work, we introduce AutoComPose, the first framework that leverages\nmultimodal large language models (MLLMs) to automatically generate rich and\nstructured pose transition descriptions. Our method enhances annotation quality\nby structuring transitions into fine-grained body part movements and\nintroducing mirrored/swapped variations, while a cyclic consistency constraint\nensures logical coherence between forward and reverse transitions. To advance\nCPR research, we construct and release two dedicated benchmarks, AIST-CPR and\nPoseFixCPR, supplementing prior datasets with enhanced attributes. Extensive\nexperiments demonstrate that training retrieval models with AutoComPose yields\nsuperior performance over human-annotated and heuristic-based methods,\nsignificantly reducing annotation costs while improving retrieval quality. Our\nwork pioneers the automatic annotation of pose transitions, establishing a\nscalable foundation for future CPR research."
                },
                "authors": [
                    {
                        "name": "Yi-Ting Shen"
                    },
                    {
                        "name": "Sungmin Eum"
                    },
                    {
                        "name": "Doheon Lee"
                    },
                    {
                        "name": "Rohit Shete"
                    },
                    {
                        "name": "Chiao-Yi Wang"
                    },
                    {
                        "name": "Heesung Kwon"
                    },
                    {
                        "name": "Shuvra S. Bhattacharyya"
                    }
                ],
                "author_detail": {
                    "name": "Shuvra S. Bhattacharyya"
                },
                "author": "Shuvra S. Bhattacharyya",
                "arxiv_comment": "ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22884v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22884v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13975v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13975v1",
                "updated": "2025-08-19T16:12:51Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    16,
                    12,
                    51,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T16:12:51Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    16,
                    12,
                    51,
                    1,
                    231,
                    0
                ],
                "title": "ChronoLLM: Customizing Language Models for Physics-Based Simulation Code\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChronoLLM: Customizing Language Models for Physics-Based Simulation Code\n  Generation"
                },
                "summary": "This contribution is concerned with the following issue: can pretrained large\nlanguage models (LLMs) be refined and customized to the point where they become\nvirtual assistants helping experts with the effective use of a simulation tool?\nIn this case study, the ``simulation tool'' considered is PyChrono, an open\nsource multi-physics dynamics engine for multibody systems. We present a\nframework for refining and customizing both open- and closed-source LLMs to\nharness the power of AI in generating scripts that perform PyChrono virtual\nexperiments. We refine and customize several classes of LLMs through a process\nthat leads to a quantifiable improvement in the quality of the generated\nPyChrono simulation scripts. These scripts can range from simple\nsingle-pendulum simulations to complex virtual experiments involving full\nvehicles on deformable terrain. While the generated scripts are rarely perfect,\nthey often serve as strong starting points for the user to modify and improve\non. Additionally, the LLM can answer specific API questions about the\nsimulator, or recommend modeling approaches. The framework discussed is general\nand can be applied to lower the entry barrier for simulation tools associated\nwith other application domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This contribution is concerned with the following issue: can pretrained large\nlanguage models (LLMs) be refined and customized to the point where they become\nvirtual assistants helping experts with the effective use of a simulation tool?\nIn this case study, the ``simulation tool'' considered is PyChrono, an open\nsource multi-physics dynamics engine for multibody systems. We present a\nframework for refining and customizing both open- and closed-source LLMs to\nharness the power of AI in generating scripts that perform PyChrono virtual\nexperiments. We refine and customize several classes of LLMs through a process\nthat leads to a quantifiable improvement in the quality of the generated\nPyChrono simulation scripts. These scripts can range from simple\nsingle-pendulum simulations to complex virtual experiments involving full\nvehicles on deformable terrain. While the generated scripts are rarely perfect,\nthey often serve as strong starting points for the user to modify and improve\non. Additionally, the LLM can answer specific API questions about the\nsimulator, or recommend modeling approaches. The framework discussed is general\nand can be applied to lower the entry barrier for simulation tools associated\nwith other application domains."
                },
                "authors": [
                    {
                        "name": "Jingquan Wang"
                    },
                    {
                        "name": "Andrew Negrut"
                    },
                    {
                        "name": "Harry Zhang"
                    },
                    {
                        "name": "Khailanii Slaton"
                    },
                    {
                        "name": "Shu Wang"
                    },
                    {
                        "name": "Radu Serban"
                    },
                    {
                        "name": "Jinlong Wu"
                    },
                    {
                        "name": "Dan Negrut"
                    }
                ],
                "author_detail": {
                    "name": "Dan Negrut"
                },
                "author": "Dan Negrut",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13975v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13975v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05985v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05985v2",
                "updated": "2025-08-19T16:01:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    16,
                    1,
                    10,
                    1,
                    231,
                    0
                ],
                "published": "2025-01-10T14:17:48Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    14,
                    17,
                    48,
                    4,
                    10,
                    0
                ],
                "title": "Exploring LLMs for Automated Generation and Adaptation of Questionnaires",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring LLMs for Automated Generation and Adaptation of Questionnaires"
                },
                "summary": "Effective questionnaire design improves the validity of the results, but\ncreating and adapting questionnaires across contexts is challenging due to\nresource constraints and limited expert access. Recently, the emergence of LLMs\nhas led researchers to explore their potential in survey research. In this\nwork, we focus on the suitability of LLMs in assisting the generation and\nadaptation of questionnaires. We introduce a novel pipeline that leverages LLMs\nto create new questionnaires, pretest with a target audience to determine\npotential issues and adapt existing standardized questionnaires for different\ncontexts. We evaluated our pipeline for creation and adaptation through two\nstudies on Prolific, involving 238 participants from the US and 118\nparticipants from South Africa. Our findings show that participants found\nLLM-generated text clearer, LLM-pretested text more specific, and LLM-adapted\nquestions slightly clearer and less biased than traditional ones. Our work\nopens new opportunities for LLM-driven questionnaire support in survey\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective questionnaire design improves the validity of the results, but\ncreating and adapting questionnaires across contexts is challenging due to\nresource constraints and limited expert access. Recently, the emergence of LLMs\nhas led researchers to explore their potential in survey research. In this\nwork, we focus on the suitability of LLMs in assisting the generation and\nadaptation of questionnaires. We introduce a novel pipeline that leverages LLMs\nto create new questionnaires, pretest with a target audience to determine\npotential issues and adapt existing standardized questionnaires for different\ncontexts. We evaluated our pipeline for creation and adaptation through two\nstudies on Prolific, involving 238 participants from the US and 118\nparticipants from South Africa. Our findings show that participants found\nLLM-generated text clearer, LLM-pretested text more specific, and LLM-adapted\nquestions slightly clearer and less biased than traditional ones. Our work\nopens new opportunities for LLM-driven questionnaire support in survey\nresearch."
                },
                "authors": [
                    {
                        "name": "Divya Mani Adhikari"
                    },
                    {
                        "name": "Alexander Hartland"
                    },
                    {
                        "name": "Ingmar Weber"
                    },
                    {
                        "name": "Vikram Kamath Cannanure"
                    }
                ],
                "author_detail": {
                    "name": "Vikram Kamath Cannanure"
                },
                "author": "Vikram Kamath Cannanure",
                "arxiv_doi": "10.1145/3719160.3736606",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3719160.3736606",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.05985v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05985v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in the Proceedings of the 7th ACM Conference on\n  Conversational User Interfaces (CUI '25)",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13962v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13962v1",
                "updated": "2025-08-19T15:54:51Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    15,
                    54,
                    51,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T15:54:51Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    15,
                    54,
                    51,
                    1,
                    231,
                    0
                ],
                "title": "Learning to Use AI for Learning: How Can We Effectively Teach and\n  Measure Prompting Literacy for K-12 Students?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Use AI for Learning: How Can We Effectively Teach and\n  Measure Prompting Literacy for K-12 Students?"
                },
                "summary": "As Artificial Intelligence (AI) becomes increasingly integrated into daily\nlife, there is a growing need to equip the next generation with the ability to\napply, interact with, evaluate, and collaborate with AI systems responsibly.\nPrior research highlights the urgent demand from K-12 educators to teach\nstudents the ethical and effective use of AI for learning. To address this\nneed, we designed an Large-Language Model (LLM)-based module to teach prompting\nliteracy. This includes scenario-based deliberate practice activities with\ndirect interaction with intelligent LLM agents, aiming to foster secondary\nschool students' responsible engagement with AI chatbots. We conducted two\niterations of classroom deployment in 11 authentic secondary education\nclassrooms, and evaluated 1) AI-based auto-grader's capability; 2) students'\nprompting performance and confidence changes towards using AI for learning; and\n3) the quality of learning and assessment materials. Results indicated that the\nAI-based auto-grader could grade student-written prompts with satisfactory\nquality. In addition, the instructional materials supported students in\nimproving their prompting skills through practice and led to positive shifts in\ntheir perceptions of using AI for learning. Furthermore, data from Study 1\ninformed assessment revisions in Study 2. Analyses of item difficulty and\ndiscrimination in Study 2 showed that True/False and open-ended questions could\nmeasure prompting literacy more effectively than multiple-choice questions for\nour target learners. These promising outcomes highlight the potential for\nbroader deployment and highlight the need for broader studies to assess\nlearning effectiveness and assessment design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Artificial Intelligence (AI) becomes increasingly integrated into daily\nlife, there is a growing need to equip the next generation with the ability to\napply, interact with, evaluate, and collaborate with AI systems responsibly.\nPrior research highlights the urgent demand from K-12 educators to teach\nstudents the ethical and effective use of AI for learning. To address this\nneed, we designed an Large-Language Model (LLM)-based module to teach prompting\nliteracy. This includes scenario-based deliberate practice activities with\ndirect interaction with intelligent LLM agents, aiming to foster secondary\nschool students' responsible engagement with AI chatbots. We conducted two\niterations of classroom deployment in 11 authentic secondary education\nclassrooms, and evaluated 1) AI-based auto-grader's capability; 2) students'\nprompting performance and confidence changes towards using AI for learning; and\n3) the quality of learning and assessment materials. Results indicated that the\nAI-based auto-grader could grade student-written prompts with satisfactory\nquality. In addition, the instructional materials supported students in\nimproving their prompting skills through practice and led to positive shifts in\ntheir perceptions of using AI for learning. Furthermore, data from Study 1\ninformed assessment revisions in Study 2. Analyses of item difficulty and\ndiscrimination in Study 2 showed that True/False and open-ended questions could\nmeasure prompting literacy more effectively than multiple-choice questions for\nour target learners. These promising outcomes highlight the potential for\nbroader deployment and highlight the need for broader studies to assess\nlearning effectiveness and assessment design."
                },
                "authors": [
                    {
                        "name": "Ruiwei Xiao"
                    },
                    {
                        "name": "Xinying Hou"
                    },
                    {
                        "name": "Ying-Jui Tseng"
                    },
                    {
                        "name": "Hsuan Nieu"
                    },
                    {
                        "name": "Guanze Liao"
                    },
                    {
                        "name": "John Stamper"
                    },
                    {
                        "name": "Kenneth R. Koedinger"
                    }
                ],
                "author_detail": {
                    "name": "Kenneth R. Koedinger"
                },
                "author": "Kenneth R. Koedinger",
                "arxiv_comment": "7 pages + 2 pages references; under review for an [anonymized\n  according to the conference policy] conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13962v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13962v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24688v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24688v3",
                "updated": "2025-08-19T15:45:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    15,
                    45,
                    20,
                    1,
                    231,
                    0
                ],
                "published": "2025-05-30T15:11:52Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    11,
                    52,
                    4,
                    150,
                    0
                ],
                "title": "Soft Reasoning: Navigating Solution Spaces in Large Language Models\n  through Controlled Embedding Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Soft Reasoning: Navigating Solution Spaces in Large Language Models\n  through Controlled Embedding Exploration"
                },
                "summary": "Large Language Models (LLMs) struggle with complex reasoning due to limited\ndiversity and inefficient search. We propose Soft Reasoning, an embedding-based\nsearch framework that optimises the embedding of the first token to guide\ngeneration. It combines (1) embedding perturbation for controlled exploration\nand (2) Bayesian optimisation to refine embeddings via a verifier-guided\nobjective, balancing exploration and exploitation. This approach improves\nreasoning accuracy and coherence while avoiding reliance on heuristic search.\nExperiments demonstrate superior correctness with minimal computation, making\nit a scalable, model-agnostic solution. The code is released at\nhttps://github.com/alickzhu/Soft-Reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) struggle with complex reasoning due to limited\ndiversity and inefficient search. We propose Soft Reasoning, an embedding-based\nsearch framework that optimises the embedding of the first token to guide\ngeneration. It combines (1) embedding perturbation for controlled exploration\nand (2) Bayesian optimisation to refine embeddings via a verifier-guided\nobjective, balancing exploration and exploitation. This approach improves\nreasoning accuracy and coherence while avoiding reliance on heuristic search.\nExperiments demonstrate superior correctness with minimal computation, making\nit a scalable, model-agnostic solution. The code is released at\nhttps://github.com/alickzhu/Soft-Reasoning."
                },
                "authors": [
                    {
                        "name": "Qinglin Zhu"
                    },
                    {
                        "name": "Runcong Zhao"
                    },
                    {
                        "name": "Hanqi Yan"
                    },
                    {
                        "name": "Yulan He"
                    },
                    {
                        "name": "Yudong Chen"
                    },
                    {
                        "name": "Lin Gui"
                    }
                ],
                "author_detail": {
                    "name": "Lin Gui"
                },
                "author": "Lin Gui",
                "arxiv_comment": "Accepted as a Spotlight at ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24688v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24688v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13953v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13953v1",
                "updated": "2025-08-19T15:44:27Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    15,
                    44,
                    27,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T15:44:27Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    15,
                    44,
                    27,
                    1,
                    231,
                    0
                ],
                "title": "ReviewGraph: A Knowledge Graph Embedding Based Framework for Review\n  Rating Prediction with Sentiment Features",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReviewGraph: A Knowledge Graph Embedding Based Framework for Review\n  Rating Prediction with Sentiment Features"
                },
                "summary": "In the hospitality industry, understanding the factors that drive customer\nreview ratings is critical for improving guest satisfaction and business\nperformance. This work proposes ReviewGraph for Review Rating Prediction (RRP),\na novel framework that transforms textual customer reviews into knowledge\ngraphs by extracting (subject, predicate, object) triples and associating\nsentiment scores. Using graph embeddings (Node2Vec) and sentiment features, the\nframework predicts review rating scores through machine learning classifiers.\nWe compare ReviewGraph performance with traditional NLP baselines (such as Bag\nof Words, TF-IDF, and Word2Vec) and large language models (LLMs), evaluating\nthem in the HotelRec dataset. In comparison to the state of the art literature,\nour proposed model performs similar to their best performing model but with\nlower computational cost (without ensemble).\n  While ReviewGraph achieves comparable predictive performance to LLMs and\noutperforms baselines on agreement-based metrics such as Cohen's Kappa, it\noffers additional advantages in interpretability, visual exploration, and\npotential integration into Retrieval-Augmented Generation (RAG) systems. This\nwork highlights the potential of graph-based representations for enhancing\nreview analytics and lays the groundwork for future research integrating\nadvanced graph neural networks and fine-tuned LLM-based extraction methods. We\nwill share ReviewGraph output and platform open-sourced on our GitHub page\nhttps://github.com/aaronlifenghan/ReviewGraph",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the hospitality industry, understanding the factors that drive customer\nreview ratings is critical for improving guest satisfaction and business\nperformance. This work proposes ReviewGraph for Review Rating Prediction (RRP),\na novel framework that transforms textual customer reviews into knowledge\ngraphs by extracting (subject, predicate, object) triples and associating\nsentiment scores. Using graph embeddings (Node2Vec) and sentiment features, the\nframework predicts review rating scores through machine learning classifiers.\nWe compare ReviewGraph performance with traditional NLP baselines (such as Bag\nof Words, TF-IDF, and Word2Vec) and large language models (LLMs), evaluating\nthem in the HotelRec dataset. In comparison to the state of the art literature,\nour proposed model performs similar to their best performing model but with\nlower computational cost (without ensemble).\n  While ReviewGraph achieves comparable predictive performance to LLMs and\noutperforms baselines on agreement-based metrics such as Cohen's Kappa, it\noffers additional advantages in interpretability, visual exploration, and\npotential integration into Retrieval-Augmented Generation (RAG) systems. This\nwork highlights the potential of graph-based representations for enhancing\nreview analytics and lays the groundwork for future research integrating\nadvanced graph neural networks and fine-tuned LLM-based extraction methods. We\nwill share ReviewGraph output and platform open-sourced on our GitHub page\nhttps://github.com/aaronlifenghan/ReviewGraph"
                },
                "authors": [
                    {
                        "name": "A. J. W. de Vink"
                    },
                    {
                        "name": "Natalia Amat-Lefort"
                    },
                    {
                        "name": "Lifeng Han"
                    }
                ],
                "author_detail": {
                    "name": "Lifeng Han"
                },
                "author": "Lifeng Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13953v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13953v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13948v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13948v1",
                "updated": "2025-08-19T15:37:29Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    15,
                    37,
                    29,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T15:37:29Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    15,
                    37,
                    29,
                    1,
                    231,
                    0
                ],
                "title": "Prompt Orchestration Markup Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt Orchestration Markup Language"
                },
                "summary": "Large Language Models (LLMs) require sophisticated prompting, yet current\npractices face challenges in structure, data integration, format sensitivity,\nand tooling. Existing methods lack comprehensive solutions for organizing\ncomplex prompts involving diverse data types (documents, tables, images) or\nmanaging presentation variations systematically. To address these gaps, we\nintroduce POML (Prompt Orchestration Markup Language). POML employs\ncomponent-based markup for logical structure (roles, tasks, examples),\nspecialized tags for seamless data integration, and a CSS-like styling system\nto decouple content from presentation, reducing formatting sensitivity. It\nincludes templating for dynamic prompts and a comprehensive developer toolkit\n(IDE support, SDKs) to improve version control and collaboration. We validate\nPOML through two case studies demonstrating its impact on complex application\nintegration (PomLink) and accuracy performance (TableQA), as well as a user\nstudy assessing its effectiveness in real-world development scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) require sophisticated prompting, yet current\npractices face challenges in structure, data integration, format sensitivity,\nand tooling. Existing methods lack comprehensive solutions for organizing\ncomplex prompts involving diverse data types (documents, tables, images) or\nmanaging presentation variations systematically. To address these gaps, we\nintroduce POML (Prompt Orchestration Markup Language). POML employs\ncomponent-based markup for logical structure (roles, tasks, examples),\nspecialized tags for seamless data integration, and a CSS-like styling system\nto decouple content from presentation, reducing formatting sensitivity. It\nincludes templating for dynamic prompts and a comprehensive developer toolkit\n(IDE support, SDKs) to improve version control and collaboration. We validate\nPOML through two case studies demonstrating its impact on complex application\nintegration (PomLink) and accuracy performance (TableQA), as well as a user\nstudy assessing its effectiveness in real-world development scenarios."
                },
                "authors": [
                    {
                        "name": "Yuge Zhang"
                    },
                    {
                        "name": "Nan Chen"
                    },
                    {
                        "name": "Jiahang Xu"
                    },
                    {
                        "name": "Yuqing Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yuqing Yang"
                },
                "author": "Yuqing Yang",
                "arxiv_comment": "All findings in this paper are derived from a POML snapshot as of\n  February 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13948v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13948v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13943v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13943v1",
                "updated": "2025-08-19T15:31:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    15,
                    31,
                    37,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T15:31:37Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    15,
                    31,
                    37,
                    1,
                    231,
                    0
                ],
                "title": "LLM-Powered Virtual Patient Agents for Interactive Clinical Skills\n  Training with Automated Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Powered Virtual Patient Agents for Interactive Clinical Skills\n  Training with Automated Feedback"
                },
                "summary": "Objective Structured Clinical Examinations (OSCEs) are essential for medical\ntraining, but they require significant resources, including professional actors\nand expert medical feedback. Although Large Language Models (LLMs) have\nintroduced text-based virtual patients for communication practice, these\nsimulations often lack the capability for richer, non-textual interactions.\nThis paper presents a novel framework that significantly enhances LLM-based\nsimulated patients by equipping them with action spaces, thereby enabling more\nrealistic and dynamic patient behaviors that extend beyond text. Furthermore,\nour system incorporates virtual tutors that provide students with instant,\npersonalized feedback on their performance at any time during these simulated\nencounters. We have conducted a rigorous evaluation of the framework's\nreal-time performance, including system latency and component accuracy.\nPreliminary evaluations with medical experts assessed the naturalness and\ncoherence of the simulated patients, as well as the usefulness and\nappropriateness of the virtual tutor's assessments. This innovative system\nprovides medical students with a low-cost, accessible platform for personalized\nOSCE preparation at home.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Objective Structured Clinical Examinations (OSCEs) are essential for medical\ntraining, but they require significant resources, including professional actors\nand expert medical feedback. Although Large Language Models (LLMs) have\nintroduced text-based virtual patients for communication practice, these\nsimulations often lack the capability for richer, non-textual interactions.\nThis paper presents a novel framework that significantly enhances LLM-based\nsimulated patients by equipping them with action spaces, thereby enabling more\nrealistic and dynamic patient behaviors that extend beyond text. Furthermore,\nour system incorporates virtual tutors that provide students with instant,\npersonalized feedback on their performance at any time during these simulated\nencounters. We have conducted a rigorous evaluation of the framework's\nreal-time performance, including system latency and component accuracy.\nPreliminary evaluations with medical experts assessed the naturalness and\ncoherence of the simulated patients, as well as the usefulness and\nappropriateness of the virtual tutor's assessments. This innovative system\nprovides medical students with a low-cost, accessible platform for personalized\nOSCE preparation at home."
                },
                "authors": [
                    {
                        "name": "Henrik Voigt"
                    },
                    {
                        "name": "Yurina Sugamiya"
                    },
                    {
                        "name": "Kai Lawonn"
                    },
                    {
                        "name": "Sina Zarrieß"
                    },
                    {
                        "name": "Atsuo Takanishi"
                    }
                ],
                "author_detail": {
                    "name": "Atsuo Takanishi"
                },
                "author": "Atsuo Takanishi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13943v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13943v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13942v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13942v1",
                "updated": "2025-08-19T15:31:23Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    15,
                    31,
                    23,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T15:31:23Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    15,
                    31,
                    23,
                    1,
                    231,
                    0
                ],
                "title": "The Collaboration Paradox: Why Generative AI Requires Both Strategic\n  Intelligence and Operational Stability in Supply Chain Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Collaboration Paradox: Why Generative AI Requires Both Strategic\n  Intelligence and Operational Stability in Supply Chain Management"
                },
                "summary": "The rise of autonomous, AI-driven agents in economic settings raises critical\nquestions about their emergent strategic behavior. This paper investigates\nthese dynamics in the cooperative context of a multi-echelon supply chain, a\nsystem famously prone to instabilities like the bullwhip effect. We conduct\ncomputational experiments with generative AI agents, powered by Large Language\nModels (LLMs), within a controlled supply chain simulation designed to isolate\ntheir behavioral tendencies. Our central finding is the \"collaboration\nparadox\": a novel, catastrophic failure mode where theoretically superior\ncollaborative AI agents, designed with Vendor-Managed Inventory (VMI)\nprinciples, perform even worse than non-AI baselines. We demonstrate that this\nparadox arises from an operational flaw where agents hoard inventory, starving\nthe system. We then show that resilience is only achieved through a synthesis\nof two distinct layers: high-level, AI-driven proactive policy-setting to\nestablish robust operational targets, and a low-level, collaborative execution\nprotocol with proactive downstream replenishment to maintain stability. Our\nfinal framework, which implements this synthesis, can autonomously generate,\nevaluate, and quantify a portfolio of viable strategic choices. The work\nprovides a crucial insight into the emergent behaviors of collaborative AI\nagents and offers a blueprint for designing stable, effective AI-driven systems\nfor business analytics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of autonomous, AI-driven agents in economic settings raises critical\nquestions about their emergent strategic behavior. This paper investigates\nthese dynamics in the cooperative context of a multi-echelon supply chain, a\nsystem famously prone to instabilities like the bullwhip effect. We conduct\ncomputational experiments with generative AI agents, powered by Large Language\nModels (LLMs), within a controlled supply chain simulation designed to isolate\ntheir behavioral tendencies. Our central finding is the \"collaboration\nparadox\": a novel, catastrophic failure mode where theoretically superior\ncollaborative AI agents, designed with Vendor-Managed Inventory (VMI)\nprinciples, perform even worse than non-AI baselines. We demonstrate that this\nparadox arises from an operational flaw where agents hoard inventory, starving\nthe system. We then show that resilience is only achieved through a synthesis\nof two distinct layers: high-level, AI-driven proactive policy-setting to\nestablish robust operational targets, and a low-level, collaborative execution\nprotocol with proactive downstream replenishment to maintain stability. Our\nfinal framework, which implements this synthesis, can autonomously generate,\nevaluate, and quantify a portfolio of viable strategic choices. The work\nprovides a crucial insight into the emergent behaviors of collaborative AI\nagents and offers a blueprint for designing stable, effective AI-driven systems\nfor business analytics."
                },
                "authors": [
                    {
                        "name": "Soumyadeep Dhar"
                    }
                ],
                "author_detail": {
                    "name": "Soumyadeep Dhar"
                },
                "author": "Soumyadeep Dhar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13942v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13942v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13941v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13941v1",
                "updated": "2025-08-19T15:30:57Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    15,
                    30,
                    57,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T15:30:57Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    15,
                    30,
                    57,
                    1,
                    231,
                    0
                ],
                "title": "A Catalog of M&M Eclipsing Binaries with TESS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Catalog of M&M Eclipsing Binaries with TESS"
                },
                "summary": "We present a catalog of 1292 low-mass (M&M) short-period eclipsing binaries\nobserved by the TESS mission. Eclipsing binaries are useful for many aspects of\nstellar astrophysics, including calibrating stellar models. Catalogs of\neclipsing binary properties provide context for population-level inferences. In\nthis work, we present our selection criteria for our catalog, along with steps\ntaken to characterize both the orbital and physical properties of our EBs. We\nfurther detail crucial steps in vetting our catalog to ensure that the stars in\nour catalog have a high likelihood of being true M&Ms. We compare distributions\nof orbital and physical properties to other relevant datasets. Our sample\nconsists primarily of binaries with short-period, circular orbits and often\nmanifest as high-mass ratio \"twin\" M&Ms. We find that M&Ms do not exhibit an\noverdensity of contact binaries, which is different from previous results for\nsolar-type binaries. Further, we find a tidal circularization period of\napproximately 2.7 days, which is significantly shorter compared to the\nliterature value of 7 days. Finally, we explore the prospects for additional\ncompanions to our M&Ms. Future avenues include spectroscopic follow-up of\nbright M&Ms to better-characterize low-mass stellar properties and a deeper\nassessment of the presence of tertiary companions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a catalog of 1292 low-mass (M&M) short-period eclipsing binaries\nobserved by the TESS mission. Eclipsing binaries are useful for many aspects of\nstellar astrophysics, including calibrating stellar models. Catalogs of\neclipsing binary properties provide context for population-level inferences. In\nthis work, we present our selection criteria for our catalog, along with steps\ntaken to characterize both the orbital and physical properties of our EBs. We\nfurther detail crucial steps in vetting our catalog to ensure that the stars in\nour catalog have a high likelihood of being true M&Ms. We compare distributions\nof orbital and physical properties to other relevant datasets. Our sample\nconsists primarily of binaries with short-period, circular orbits and often\nmanifest as high-mass ratio \"twin\" M&Ms. We find that M&Ms do not exhibit an\noverdensity of contact binaries, which is different from previous results for\nsolar-type binaries. Further, we find a tidal circularization period of\napproximately 2.7 days, which is significantly shorter compared to the\nliterature value of 7 days. Finally, we explore the prospects for additional\ncompanions to our M&Ms. Future avenues include spectroscopic follow-up of\nbright M&Ms to better-characterize low-mass stellar properties and a deeper\nassessment of the presence of tertiary companions."
                },
                "authors": [
                    {
                        "name": "Dominic Oddo"
                    },
                    {
                        "name": "Diana Dragomir"
                    },
                    {
                        "name": "Brian P. Powell"
                    },
                    {
                        "name": "Veselin B. Kostov"
                    },
                    {
                        "name": "Te Han"
                    }
                ],
                "author_detail": {
                    "name": "Te Han"
                },
                "author": "Te Han",
                "arxiv_comment": "29 pages, 14 figures. Submitted to APJ, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13941v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13941v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13940v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13940v1",
                "updated": "2025-08-19T15:29:57Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    15,
                    29,
                    57,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T15:29:57Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    15,
                    29,
                    57,
                    1,
                    231,
                    0
                ],
                "title": "Convergence Rates for Realizations of Gaussian Random Variables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Convergence Rates for Realizations of Gaussian Random Variables"
                },
                "summary": "This paper investigates the approximation of Gaussian random variables in\nBanach spaces, focusing on the high-probability bounds for the approximation of\nGaussian random variables using finitely many observations. We derive\nnon-asymptotic error bounds for the approximation of a Gaussian process $ X $\nby its conditional expectation, given finitely many linear functionals.\nSpecifically, we quantify the difference between the covariance of $ X $ and\nits finite-dimensional approximation, establishing a direct relationship\nbetween the quality of the covariance approximation and the convergence of the\nprocess in the Banach space norm. Our approach avoids the reliance on spectral\nmethods or eigenfunction expansions commonly used in Hilbert space settings,\nand instead uses finite, linear observations. This makes our result\nparticularly suitable for practical applications in nonparametric statistics,\nmachine learning, and Bayesian inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the approximation of Gaussian random variables in\nBanach spaces, focusing on the high-probability bounds for the approximation of\nGaussian random variables using finitely many observations. We derive\nnon-asymptotic error bounds for the approximation of a Gaussian process $ X $\nby its conditional expectation, given finitely many linear functionals.\nSpecifically, we quantify the difference between the covariance of $ X $ and\nits finite-dimensional approximation, establishing a direct relationship\nbetween the quality of the covariance approximation and the convergence of the\nprocess in the Banach space norm. Our approach avoids the reliance on spectral\nmethods or eigenfunction expansions commonly used in Hilbert space settings,\nand instead uses finite, linear observations. This makes our result\nparticularly suitable for practical applications in nonparametric statistics,\nmachine learning, and Bayesian inference."
                },
                "authors": [
                    {
                        "name": "Daniel Winkle"
                    },
                    {
                        "name": "Ingo Steinwart"
                    },
                    {
                        "name": "Bernard Haasdonk"
                    }
                ],
                "author_detail": {
                    "name": "Bernard Haasdonk"
                },
                "author": "Bernard Haasdonk",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13940v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13940v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "Primary 60G15, Secondary 62F15",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10369v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10369v3",
                "updated": "2025-08-19T15:25:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    15,
                    25,
                    33,
                    1,
                    231,
                    0
                ],
                "published": "2024-08-19T19:26:49Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    19,
                    26,
                    49,
                    0,
                    232,
                    0
                ],
                "title": "Boolean Matrix Logic Programming on the GPU",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boolean Matrix Logic Programming on the GPU"
                },
                "summary": "Traditional logic programming relies on symbolic computation on the CPU,\nwhich can limit performance for large-scale inference tasks. Recent advances in\nGPU hardware enable high-throughput matrix operations, motivating a shift\ntoward parallel logic inference. Boolean Matrix Logic Programming (BMLP)\nintroduces a novel approach to datalog query evaluation using Boolean matrix\nalgebra, well-suited to GPU acceleration. Building on this paradigm, we present\ntwo GPU-accelerated BMLP algorithms for bottom-up inference over linear dyadic\nrecursive datalog programs. We further extend the BMLP theoretical framework to\nsupport general linear recursion with binary predicates. Empirical evaluations\non reachability queries in large directed graphs and the Freebase 15K dataset\nshow that our methods achieve 1-4 orders of magnitude speed up over\nstate-of-the-art systems. These results demonstrate that Boolean matrix-based\nreasoning can significantly advance the scalability and efficiency of logic\nprogramming on modern hardware. Source code is available on\nhttps://github.com/lun-ai/BMLP.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional logic programming relies on symbolic computation on the CPU,\nwhich can limit performance for large-scale inference tasks. Recent advances in\nGPU hardware enable high-throughput matrix operations, motivating a shift\ntoward parallel logic inference. Boolean Matrix Logic Programming (BMLP)\nintroduces a novel approach to datalog query evaluation using Boolean matrix\nalgebra, well-suited to GPU acceleration. Building on this paradigm, we present\ntwo GPU-accelerated BMLP algorithms for bottom-up inference over linear dyadic\nrecursive datalog programs. We further extend the BMLP theoretical framework to\nsupport general linear recursion with binary predicates. Empirical evaluations\non reachability queries in large directed graphs and the Freebase 15K dataset\nshow that our methods achieve 1-4 orders of magnitude speed up over\nstate-of-the-art systems. These results demonstrate that Boolean matrix-based\nreasoning can significantly advance the scalability and efficiency of logic\nprogramming on modern hardware. Source code is available on\nhttps://github.com/lun-ai/BMLP.git."
                },
                "authors": [
                    {
                        "name": "Lun Ai"
                    }
                ],
                "author_detail": {
                    "name": "Lun Ai"
                },
                "author": "Lun Ai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10369v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10369v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13930v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13930v1",
                "updated": "2025-08-19T15:23:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    15,
                    23,
                    18,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T15:23:18Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    15,
                    23,
                    18,
                    1,
                    231,
                    0
                ],
                "title": "InPars+: Supercharging Synthetic Data Generation for Information\n  Retrieval Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InPars+: Supercharging Synthetic Data Generation for Information\n  Retrieval Systems"
                },
                "summary": "This work revisits and extends synthetic query generation pipelines for\nNeural Information Retrieval (NIR) by leveraging the InPars Toolkit, a\nreproducible, end-to-end framework for generating training data using large\nlanguage models (LLMs). We first assess the reproducibility of the original\nInPars, InPars-V2, and Promptagator pipelines on the SciFact benchmark and\nvalidate their effectiveness using open-source reranker and generator models.\nBuilding on this foundation, we introduce two key extensions to the pipeline:\n(1) fine-tuning a query generator LLM via Contrastive Preference Optimization\n(CPO) to improve the signal quality in generated queries, and (2) replacing\nstatic prompt templates with dynamic, Chain-of-Thought (CoT) optimized prompts\nusing the DSPy framework. Our results show that both extensions reduce the need\nfor aggressive filtering while improving retrieval performance. All code,\nmodels, and synthetic datasets are publicly released to support further\nresearch at: \\href{https://github.com/danilotpnta/IR2-project}{this https URL}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work revisits and extends synthetic query generation pipelines for\nNeural Information Retrieval (NIR) by leveraging the InPars Toolkit, a\nreproducible, end-to-end framework for generating training data using large\nlanguage models (LLMs). We first assess the reproducibility of the original\nInPars, InPars-V2, and Promptagator pipelines on the SciFact benchmark and\nvalidate their effectiveness using open-source reranker and generator models.\nBuilding on this foundation, we introduce two key extensions to the pipeline:\n(1) fine-tuning a query generator LLM via Contrastive Preference Optimization\n(CPO) to improve the signal quality in generated queries, and (2) replacing\nstatic prompt templates with dynamic, Chain-of-Thought (CoT) optimized prompts\nusing the DSPy framework. Our results show that both extensions reduce the need\nfor aggressive filtering while improving retrieval performance. All code,\nmodels, and synthetic datasets are publicly released to support further\nresearch at: \\href{https://github.com/danilotpnta/IR2-project}{this https URL}."
                },
                "authors": [
                    {
                        "name": "Matey Krastev"
                    },
                    {
                        "name": "Miklos Hamar"
                    },
                    {
                        "name": "Danilo Toapanta"
                    },
                    {
                        "name": "Jesse Brouwers"
                    },
                    {
                        "name": "Yibin Lei"
                    }
                ],
                "author_detail": {
                    "name": "Yibin Lei"
                },
                "author": "Yibin Lei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13930v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13930v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13920v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13920v1",
                "updated": "2025-08-19T15:17:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    15,
                    17,
                    31,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T15:17:31Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    15,
                    17,
                    31,
                    1,
                    231,
                    0
                ],
                "title": "LLMind 2.0: Distributed IoT Automation with Natural Language M2M\n  Communication and Lightweight LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMind 2.0: Distributed IoT Automation with Natural Language M2M\n  Communication and Lightweight LLM Agents"
                },
                "summary": "Recent advances in large language models (LLMs) have sparked interest in\ntheir application to IoT and automation systems, particularly for facilitating\ndevice management through natural language instructions. However, existing\ncentralized approaches face significant scalability challenges when managing\nand coordinating the collaboration between IoT devices of diverse capabilities\nin large-scale heterogeneous IoT systems. This paper introduces LLMind 2.0, a\ndistributed IoT automation framework that addresses the scalability challenges\nthrough lightweight LLM-empowered device agents via natural language-based\nmachine-to-machine (M2M) communication. Unlike previous LLM-controlled\nautomation systems that rely on a centralized coordinator to generate\ndevice-specific code to be executed on individual devices, LLMind 2.0\ndistributes intelligence across individual devices through lightweight LLMs\nembedded in IoT devices. The central coordinator translates human instructions\ninto simple subtasks described in natural human language, which are then\nprocessed by device-specific agents to generate device-specific code locally at\nthe associated devices. This approach transcends device heterogeneity barriers\nby using natural language as a unified communication medium, enabling seamless\ncollaboration between devices from different manufacturers. The system\nincorporates several key innovations: a Retrieval-Augmented Generation (RAG)\nmechanism for accurate subtask-to-API mapping, fine-tuned lightweight LLMs for\nreliable code generation, and a finite state machine-based task execution\nframework. Experimental validation in multi-robot warehouse scenarios and\nreal-world WiFi network deployments demonstrates significant improvements in\nscalability, reliability, and privacy protection compared to the centralized\napproach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have sparked interest in\ntheir application to IoT and automation systems, particularly for facilitating\ndevice management through natural language instructions. However, existing\ncentralized approaches face significant scalability challenges when managing\nand coordinating the collaboration between IoT devices of diverse capabilities\nin large-scale heterogeneous IoT systems. This paper introduces LLMind 2.0, a\ndistributed IoT automation framework that addresses the scalability challenges\nthrough lightweight LLM-empowered device agents via natural language-based\nmachine-to-machine (M2M) communication. Unlike previous LLM-controlled\nautomation systems that rely on a centralized coordinator to generate\ndevice-specific code to be executed on individual devices, LLMind 2.0\ndistributes intelligence across individual devices through lightweight LLMs\nembedded in IoT devices. The central coordinator translates human instructions\ninto simple subtasks described in natural human language, which are then\nprocessed by device-specific agents to generate device-specific code locally at\nthe associated devices. This approach transcends device heterogeneity barriers\nby using natural language as a unified communication medium, enabling seamless\ncollaboration between devices from different manufacturers. The system\nincorporates several key innovations: a Retrieval-Augmented Generation (RAG)\nmechanism for accurate subtask-to-API mapping, fine-tuned lightweight LLMs for\nreliable code generation, and a finite state machine-based task execution\nframework. Experimental validation in multi-robot warehouse scenarios and\nreal-world WiFi network deployments demonstrates significant improvements in\nscalability, reliability, and privacy protection compared to the centralized\napproach."
                },
                "authors": [
                    {
                        "name": "Yuyang Du"
                    },
                    {
                        "name": "Qun Yang"
                    },
                    {
                        "name": "Liujianfu Wang"
                    },
                    {
                        "name": "Jingqi Lin"
                    },
                    {
                        "name": "Hongwei Cui"
                    },
                    {
                        "name": "Soung Chang Liew"
                    }
                ],
                "author_detail": {
                    "name": "Soung Chang Liew"
                },
                "author": "Soung Chang Liew",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13920v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13920v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13915v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13915v1",
                "updated": "2025-08-19T15:14:49Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    15,
                    14,
                    49,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T15:14:49Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    15,
                    14,
                    49,
                    1,
                    231,
                    0
                ],
                "title": "Structured Agentic Workflows for Financial Time-Series Modeling with\n  LLMs and Reflective Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structured Agentic Workflows for Financial Time-Series Modeling with\n  LLMs and Reflective Feedback"
                },
                "summary": "Time-series data is central to decision-making in financial markets, yet\nbuilding high-performing, interpretable, and auditable models remains a major\nchallenge. While Automated Machine Learning (AutoML) frameworks streamline\nmodel development, they often lack adaptability and responsiveness to\ndomain-specific needs and evolving objectives. Concurrently, Large Language\nModels (LLMs) have enabled agentic systems capable of reasoning, memory\nmanagement, and dynamic code generation, offering a path toward more flexible\nworkflow automation. In this paper, we introduce \\textsf{TS-Agent}, a modular\nagentic framework designed to automate and enhance time-series modeling\nworkflows for financial applications. The agent formalizes the pipeline as a\nstructured, iterative decision process across three stages: model selection,\ncode refinement, and fine-tuning, guided by contextual reasoning and\nexperimental feedback. Central to our architecture is a planner agent equipped\nwith structured knowledge banks, curated libraries of models and refinement\nstrategies, which guide exploration, while improving interpretability and\nreducing error propagation. \\textsf{TS-Agent} supports adaptive learning,\nrobust debugging, and transparent auditing, key requirements for high-stakes\nenvironments such as financial services. Empirical evaluations on diverse\nfinancial forecasting and synthetic data generation tasks demonstrate that\n\\textsf{TS-Agent} consistently outperforms state-of-the-art AutoML and agentic\nbaselines, achieving superior accuracy, robustness, and decision traceability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-series data is central to decision-making in financial markets, yet\nbuilding high-performing, interpretable, and auditable models remains a major\nchallenge. While Automated Machine Learning (AutoML) frameworks streamline\nmodel development, they often lack adaptability and responsiveness to\ndomain-specific needs and evolving objectives. Concurrently, Large Language\nModels (LLMs) have enabled agentic systems capable of reasoning, memory\nmanagement, and dynamic code generation, offering a path toward more flexible\nworkflow automation. In this paper, we introduce \\textsf{TS-Agent}, a modular\nagentic framework designed to automate and enhance time-series modeling\nworkflows for financial applications. The agent formalizes the pipeline as a\nstructured, iterative decision process across three stages: model selection,\ncode refinement, and fine-tuning, guided by contextual reasoning and\nexperimental feedback. Central to our architecture is a planner agent equipped\nwith structured knowledge banks, curated libraries of models and refinement\nstrategies, which guide exploration, while improving interpretability and\nreducing error propagation. \\textsf{TS-Agent} supports adaptive learning,\nrobust debugging, and transparent auditing, key requirements for high-stakes\nenvironments such as financial services. Empirical evaluations on diverse\nfinancial forecasting and synthetic data generation tasks demonstrate that\n\\textsf{TS-Agent} consistently outperforms state-of-the-art AutoML and agentic\nbaselines, achieving superior accuracy, robustness, and decision traceability."
                },
                "authors": [
                    {
                        "name": "Yihao Ang"
                    },
                    {
                        "name": "Yifan Bao"
                    },
                    {
                        "name": "Lei Jiang"
                    },
                    {
                        "name": "Jiajie Tao"
                    },
                    {
                        "name": "Anthony K. H. Tung"
                    },
                    {
                        "name": "Lukasz Szpruch"
                    },
                    {
                        "name": "Hao Ni"
                    }
                ],
                "author_detail": {
                    "name": "Hao Ni"
                },
                "author": "Hao Ni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13915v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13915v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13908v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13908v1",
                "updated": "2025-08-19T15:07:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    15,
                    7,
                    21,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T15:07:21Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    15,
                    7,
                    21,
                    1,
                    231,
                    0
                ],
                "title": "Translating the Force Concept Inventory in the age of AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Translating the Force Concept Inventory in the age of AI"
                },
                "summary": "We present a study that translates the Force Concept Inventory (FCI) using\nOpenAI GPT-4o and assess the specific difficulties of translating a\nscientific-focused topic using Large Language Models (LLMs). The FCI is a\nphysics exam meant to evaluate outcomes of a student cohort before and after\ninstruction in Newtonian physics. We examine the problem-solving ability of the\nLLM in both the translated document and the translation back into English,\ndetailing the language-dependent issues that complicate the translation. While\nChatGPT performs remarkably well on answering the questions in both the\ntranslated language as well as the back-translation into English, problems\narise with language-specific nuances and formatting. Pitfalls include words or\nphrases that lack one-to-one matching terms in another language, especially\ndiscipline-specific scientific terms, or outright mistranslations. Depending on\nthe context, these translations can result in a critical change in the physical\nmeaning of the problem. Additionally, issues with question numbering and\nlettering are found in some languages. The issues around the translations of\nnumbering and lettering provide insight into the abilities of the LLM and\nsuggest that it is not simply relying upon FCI questions that may have been\npart of the LLM training data to provide answers. These findings underscore\nthat while LLMs can accelerate multilingual access to educational tools,\ncareful review is still needed to ensure fidelity and clarity in translated\nassessments. LLMs provide a new opportunity to expand educational tools and\nassessments. At the same time, there are unique challenges using LLMs to\nfacilitate translations that this case study examines in detail.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a study that translates the Force Concept Inventory (FCI) using\nOpenAI GPT-4o and assess the specific difficulties of translating a\nscientific-focused topic using Large Language Models (LLMs). The FCI is a\nphysics exam meant to evaluate outcomes of a student cohort before and after\ninstruction in Newtonian physics. We examine the problem-solving ability of the\nLLM in both the translated document and the translation back into English,\ndetailing the language-dependent issues that complicate the translation. While\nChatGPT performs remarkably well on answering the questions in both the\ntranslated language as well as the back-translation into English, problems\narise with language-specific nuances and formatting. Pitfalls include words or\nphrases that lack one-to-one matching terms in another language, especially\ndiscipline-specific scientific terms, or outright mistranslations. Depending on\nthe context, these translations can result in a critical change in the physical\nmeaning of the problem. Additionally, issues with question numbering and\nlettering are found in some languages. The issues around the translations of\nnumbering and lettering provide insight into the abilities of the LLM and\nsuggest that it is not simply relying upon FCI questions that may have been\npart of the LLM training data to provide answers. These findings underscore\nthat while LLMs can accelerate multilingual access to educational tools,\ncareful review is still needed to ensure fidelity and clarity in translated\nassessments. LLMs provide a new opportunity to expand educational tools and\nassessments. At the same time, there are unique challenges using LLMs to\nfacilitate translations that this case study examines in detail."
                },
                "authors": [
                    {
                        "name": "Marina Babayeva"
                    },
                    {
                        "name": "Justin Dunlap"
                    },
                    {
                        "name": "Marie Snětinová"
                    },
                    {
                        "name": "Ralf Widenhorn"
                    }
                ],
                "author_detail": {
                    "name": "Ralf Widenhorn"
                },
                "author": "Ralf Widenhorn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13908v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13908v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ed-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ed-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13905v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13905v1",
                "updated": "2025-08-19T15:06:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    15,
                    6,
                    4,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T15:06:04Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    15,
                    6,
                    4,
                    1,
                    231,
                    0
                ],
                "title": "Automated Energy-Aware Time-Series Model Deployment on Embedded FPGAs\n  for Resilient Combined Sewer Overflow Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Energy-Aware Time-Series Model Deployment on Embedded FPGAs\n  for Resilient Combined Sewer Overflow Management"
                },
                "summary": "Extreme weather events, intensified by climate change, increasingly challenge\naging combined sewer systems, raising the risk of untreated wastewater\noverflow. Accurate forecasting of sewer overflow basin filling levels can\nprovide actionable insights for early intervention, helping mitigating\nuncontrolled discharge. In recent years, AI-based forecasting methods have\noffered scalable alternatives to traditional physics-based models, but their\nreliance on cloud computing limits their reliability during communication\noutages. To address this, we propose an end-to-end forecasting framework that\nenables energy-efficient inference directly on edge devices. Our solution\nintegrates lightweight Transformer and Long Short-Term Memory (LSTM) models,\ncompressed via integer-only quantization for efficient on-device execution.\nMoreover, an automated hardware-aware deployment pipeline is used to search for\noptimal model configurations by jointly minimizing prediction error and energy\nconsumption on an AMD Spartan-7 XC7S15 FPGA. Evaluated on real-world sewer\ndata, the selected 8-bit Transformer model, trained on 24 hours of historical\nmeasurements, achieves high accuracy (MSE 0.0376) at an energy cost of 0.370 mJ\nper inference. In contrast, the optimal 8-bit LSTM model requires significantly\nless energy (0.009 mJ, over 40x lower) but yields 14.89% worse accuracy (MSE\n0.0432) and much longer training time. This trade-off highlights the need to\nalign model selection with deployment priorities, favoring LSTM for ultra-low\nenergy consumption or Transformer for higher predictive accuracy. In general,\nour work enables local, energy-efficient forecasting, contributing to more\nresilient combined sewer systems. All code can be found in the GitHub\nRepository (https://github.com/tianheng-ling/EdgeOverflowForecast).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extreme weather events, intensified by climate change, increasingly challenge\naging combined sewer systems, raising the risk of untreated wastewater\noverflow. Accurate forecasting of sewer overflow basin filling levels can\nprovide actionable insights for early intervention, helping mitigating\nuncontrolled discharge. In recent years, AI-based forecasting methods have\noffered scalable alternatives to traditional physics-based models, but their\nreliance on cloud computing limits their reliability during communication\noutages. To address this, we propose an end-to-end forecasting framework that\nenables energy-efficient inference directly on edge devices. Our solution\nintegrates lightweight Transformer and Long Short-Term Memory (LSTM) models,\ncompressed via integer-only quantization for efficient on-device execution.\nMoreover, an automated hardware-aware deployment pipeline is used to search for\noptimal model configurations by jointly minimizing prediction error and energy\nconsumption on an AMD Spartan-7 XC7S15 FPGA. Evaluated on real-world sewer\ndata, the selected 8-bit Transformer model, trained on 24 hours of historical\nmeasurements, achieves high accuracy (MSE 0.0376) at an energy cost of 0.370 mJ\nper inference. In contrast, the optimal 8-bit LSTM model requires significantly\nless energy (0.009 mJ, over 40x lower) but yields 14.89% worse accuracy (MSE\n0.0432) and much longer training time. This trade-off highlights the need to\nalign model selection with deployment priorities, favoring LSTM for ultra-low\nenergy consumption or Transformer for higher predictive accuracy. In general,\nour work enables local, energy-efficient forecasting, contributing to more\nresilient combined sewer systems. All code can be found in the GitHub\nRepository (https://github.com/tianheng-ling/EdgeOverflowForecast)."
                },
                "authors": [
                    {
                        "name": "Tianheng Ling"
                    },
                    {
                        "name": "Vipin Singh"
                    },
                    {
                        "name": "Chao Qian"
                    },
                    {
                        "name": "Felix Biessmann"
                    },
                    {
                        "name": "Gregor Schiele"
                    }
                ],
                "author_detail": {
                    "name": "Gregor Schiele"
                },
                "author": "Gregor Schiele",
                "arxiv_comment": "6 pages, 6 figures, 1 table, accepted by the 11th IEEE International\n  Smart Cities Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13905v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13905v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13904v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13904v1",
                "updated": "2025-08-19T15:05:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    15,
                    5,
                    55,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T15:05:55Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    15,
                    5,
                    55,
                    1,
                    231,
                    0
                ],
                "title": "Revisiting Diffusion Q-Learning: From Iterative Denoising to One-Step\n  Action Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting Diffusion Q-Learning: From Iterative Denoising to One-Step\n  Action Generation"
                },
                "summary": "The generative power of diffusion models (DMs) has recently enabled\nhigh-performing decision-making algorithms in offline reinforcement learning\n(RL), achieving state-of-the-art results across standard benchmarks. Among\nthem, Diffusion Q-Learning (DQL) stands out as a leading method for its\nconsistently strong performance. Nevertheless, DQL remains limited in practice\ndue to its reliance on multi-step denoising for action generation during both\ntraining and inference. Although one-step denoising is desirable, simply\napplying it to DQL leads to a drastic performance drop. In this work, we\nrevisit DQL and identify its core limitations. We then propose One-Step Flow\nQ-Learning (OFQL), a novel framework that enables efficient one-step action\ngeneration during both training and inference, without requiring auxiliary\nmodels, distillation, or multi-phase training. Specifically, OFQL reformulates\nDQL within the sample-efficient Flow Matching (FM) framework. While\nconventional FM induces curved generative trajectories that impede one-step\ngeneration, OFQL instead learns an average velocity field that facilitates\ndirect, accurate action generation. Collectively, OFQL eliminates the need for\nmulti-step sampling and recursive gradient updates in DQL, resulting in faster\nand more robust training and inference. Extensive experiments on the D4RL\nbenchmark demonstrate that OFQL outperforms DQL and other diffusion-based\nbaselines, while substantially reducing both training and inference time\ncompared to DQL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The generative power of diffusion models (DMs) has recently enabled\nhigh-performing decision-making algorithms in offline reinforcement learning\n(RL), achieving state-of-the-art results across standard benchmarks. Among\nthem, Diffusion Q-Learning (DQL) stands out as a leading method for its\nconsistently strong performance. Nevertheless, DQL remains limited in practice\ndue to its reliance on multi-step denoising for action generation during both\ntraining and inference. Although one-step denoising is desirable, simply\napplying it to DQL leads to a drastic performance drop. In this work, we\nrevisit DQL and identify its core limitations. We then propose One-Step Flow\nQ-Learning (OFQL), a novel framework that enables efficient one-step action\ngeneration during both training and inference, without requiring auxiliary\nmodels, distillation, or multi-phase training. Specifically, OFQL reformulates\nDQL within the sample-efficient Flow Matching (FM) framework. While\nconventional FM induces curved generative trajectories that impede one-step\ngeneration, OFQL instead learns an average velocity field that facilitates\ndirect, accurate action generation. Collectively, OFQL eliminates the need for\nmulti-step sampling and recursive gradient updates in DQL, resulting in faster\nand more robust training and inference. Extensive experiments on the D4RL\nbenchmark demonstrate that OFQL outperforms DQL and other diffusion-based\nbaselines, while substantially reducing both training and inference time\ncompared to DQL."
                },
                "authors": [
                    {
                        "name": "Thanh Nguyen"
                    },
                    {
                        "name": "Chang D. Yoo"
                    }
                ],
                "author_detail": {
                    "name": "Chang D. Yoo"
                },
                "author": "Chang D. Yoo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13904v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13904v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03470v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03470v3",
                "updated": "2025-08-19T15:00:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    15,
                    0,
                    4,
                    1,
                    231,
                    0
                ],
                "published": "2025-05-06T12:22:45Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    12,
                    22,
                    45,
                    1,
                    126,
                    0
                ],
                "title": "Blending 3D Geometry and Machine Learning for Multi-View Stereopsis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Blending 3D Geometry and Machine Learning for Multi-View Stereopsis"
                },
                "summary": "Traditional multi-view stereo (MVS) methods primarily depend on photometric\nand geometric consistency constraints. In contrast, modern learning-based\nalgorithms often rely on the plane sweep algorithm to infer 3D geometry,\napplying explicit geometric consistency (GC) checks only as a post-processing\nstep, with no impact on the learning process itself. In this work, we introduce\nGC MVSNet plus plus, a novel approach that actively enforces geometric\nconsistency of reference view depth maps across multiple source views (multi\nview) and at various scales (multi scale) during the learning phase (see Fig.\n1). This integrated GC check significantly accelerates the learning process by\ndirectly penalizing geometrically inconsistent pixels, effectively halving the\nnumber of training iterations compared to other MVS methods. Furthermore, we\nintroduce a densely connected cost regularization network with two distinct\nblock designs simple and feature dense optimized to harness dense feature\nconnections for enhanced regularization. Extensive experiments demonstrate that\nour approach achieves a new state of the art on the DTU and BlendedMVS datasets\nand secures second place on the Tanks and Temples benchmark. To our knowledge,\nGC MVSNet plus plus is the first method to enforce multi-view, multi-scale\nsupervised geometric consistency during learning. Our code is available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional multi-view stereo (MVS) methods primarily depend on photometric\nand geometric consistency constraints. In contrast, modern learning-based\nalgorithms often rely on the plane sweep algorithm to infer 3D geometry,\napplying explicit geometric consistency (GC) checks only as a post-processing\nstep, with no impact on the learning process itself. In this work, we introduce\nGC MVSNet plus plus, a novel approach that actively enforces geometric\nconsistency of reference view depth maps across multiple source views (multi\nview) and at various scales (multi scale) during the learning phase (see Fig.\n1). This integrated GC check significantly accelerates the learning process by\ndirectly penalizing geometrically inconsistent pixels, effectively halving the\nnumber of training iterations compared to other MVS methods. Furthermore, we\nintroduce a densely connected cost regularization network with two distinct\nblock designs simple and feature dense optimized to harness dense feature\nconnections for enhanced regularization. Extensive experiments demonstrate that\nour approach achieves a new state of the art on the DTU and BlendedMVS datasets\nand secures second place on the Tanks and Temples benchmark. To our knowledge,\nGC MVSNet plus plus is the first method to enforce multi-view, multi-scale\nsupervised geometric consistency during learning. Our code is available."
                },
                "authors": [
                    {
                        "name": "Vibhas Vats"
                    },
                    {
                        "name": "Md. Alimoor Reza"
                    },
                    {
                        "name": "David Crandall"
                    },
                    {
                        "name": "Soon-heung Jung"
                    }
                ],
                "author_detail": {
                    "name": "Soon-heung Jung"
                },
                "author": "Soon-heung Jung",
                "arxiv_comment": "A pre-print -- accepted at Neurocomputing. arXiv admin note:\n  substantial text overlap with arXiv:2310.19583",
                "arxiv_journal_ref": "Neurocomputing, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03470v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03470v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13890v1",
                "updated": "2025-08-19T14:54:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    14,
                    54,
                    20,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T14:54:20Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    14,
                    54,
                    20,
                    1,
                    231,
                    0
                ],
                "title": "Diffusion-Driven High-Dimensional Variable Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-Driven High-Dimensional Variable Selection"
                },
                "summary": "Variable selection for high-dimensional, highly correlated data has long been\na challenging problem, often yielding unstable and unreliable models. We\npropose a resample-aggregate framework that exploits diffusion models' ability\nto generate high-fidelity synthetic data. Specifically, we draw multiple\npseudo-data sets from a diffusion model fitted to the original data, apply any\noff-the-shelf selector (e.g., lasso or SCAD), and store the resulting inclusion\nindicators and coefficients. Aggregating across replicas produces a stable\nsubset of predictors with calibrated stability scores for variable selection.\nTheoretically, we show that the proposed method is selection consistent under\nmild assumptions. Because the generative model imports knowledge from large\npre-trained weights, the procedure naturally benefits from transfer learning,\nboosting power when the observed sample is small or noisy. We also extend the\nframework of aggregating synthetic data to other model selection problems,\nincluding graphical model selection, and statistical inference that supports\nvalid confidence intervals and hypothesis tests. Extensive simulations show\nconsistent gains over the lasso, stability selection, and knockoff baselines,\nespecially when predictors are strongly correlated, achieving higher\ntrue-positive rates and lower false-discovery proportions. By coupling\ndiffusion-based data augmentation with principled aggregation, our method\nadvances variable selection methodology and broadens the toolkit for\ninterpretable, statistically rigorous analysis in complex scientific\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variable selection for high-dimensional, highly correlated data has long been\na challenging problem, often yielding unstable and unreliable models. We\npropose a resample-aggregate framework that exploits diffusion models' ability\nto generate high-fidelity synthetic data. Specifically, we draw multiple\npseudo-data sets from a diffusion model fitted to the original data, apply any\noff-the-shelf selector (e.g., lasso or SCAD), and store the resulting inclusion\nindicators and coefficients. Aggregating across replicas produces a stable\nsubset of predictors with calibrated stability scores for variable selection.\nTheoretically, we show that the proposed method is selection consistent under\nmild assumptions. Because the generative model imports knowledge from large\npre-trained weights, the procedure naturally benefits from transfer learning,\nboosting power when the observed sample is small or noisy. We also extend the\nframework of aggregating synthetic data to other model selection problems,\nincluding graphical model selection, and statistical inference that supports\nvalid confidence intervals and hypothesis tests. Extensive simulations show\nconsistent gains over the lasso, stability selection, and knockoff baselines,\nespecially when predictors are strongly correlated, achieving higher\ntrue-positive rates and lower false-discovery proportions. By coupling\ndiffusion-based data augmentation with principled aggregation, our method\nadvances variable selection methodology and broadens the toolkit for\ninterpretable, statistically rigorous analysis in complex scientific\napplications."
                },
                "authors": [
                    {
                        "name": "Minjie Wang"
                    },
                    {
                        "name": "Xiaotong Shen"
                    },
                    {
                        "name": "Wei Pan"
                    }
                ],
                "author_detail": {
                    "name": "Wei Pan"
                },
                "author": "Wei Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13889v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13889v1",
                "updated": "2025-08-19T14:53:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    14,
                    53,
                    30,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T14:53:30Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    14,
                    53,
                    30,
                    1,
                    231,
                    0
                ],
                "title": "CARE: Contextual Adaptation of Recommenders for LLM-based Conversational\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CARE: Contextual Adaptation of Recommenders for LLM-based Conversational\n  Recommendation"
                },
                "summary": "We tackle the challenge of integrating large language models (LLMs) with\nexternal recommender systems to enhance domain expertise in conversational\nrecommendation (CRS). Current LLM-based CRS approaches primarily rely on zero-\nor few-shot methods for generating item recommendations based on user queries,\nbut this method faces two significant challenges: (1) without domain-specific\nadaptation, LLMs frequently recommend items not in the target item space,\nresulting in low recommendation accuracy; and (2) LLMs largely rely on dialogue\ncontext for content-based recommendations, neglecting the collaborative\nrelationships among entities or item sequences. To address these limitations,\nwe introduce the CARE (Contextual Adaptation of Recommenders) framework. CARE\ncustomizes LLMs for CRS tasks, and synergizes them with external recommendation\nsystems. CARE (a) integrates external recommender systems as domain experts,\nproducing recommendations through entity-level insights, and (b) enhances those\nrecommendations by leveraging contextual information for more accurate and\nunbiased final recommendations using LLMs. Our results demonstrate that\nincorporating external recommender systems with entity-level information\nsignificantly enhances recommendation accuracy of LLM-based CRS by an average\nof 54% and 25% for ReDial and INSPIRED datasets. The most effective strategy in\nthe CARE framework involves LLMs selecting and reranking candidate items that\nexternal recommenders provide based on contextual insights. Our analysis\nindicates that the CARE framework effectively addresses the identified\nchallenges and mitigates the popularity bias in the external recommender.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We tackle the challenge of integrating large language models (LLMs) with\nexternal recommender systems to enhance domain expertise in conversational\nrecommendation (CRS). Current LLM-based CRS approaches primarily rely on zero-\nor few-shot methods for generating item recommendations based on user queries,\nbut this method faces two significant challenges: (1) without domain-specific\nadaptation, LLMs frequently recommend items not in the target item space,\nresulting in low recommendation accuracy; and (2) LLMs largely rely on dialogue\ncontext for content-based recommendations, neglecting the collaborative\nrelationships among entities or item sequences. To address these limitations,\nwe introduce the CARE (Contextual Adaptation of Recommenders) framework. CARE\ncustomizes LLMs for CRS tasks, and synergizes them with external recommendation\nsystems. CARE (a) integrates external recommender systems as domain experts,\nproducing recommendations through entity-level insights, and (b) enhances those\nrecommendations by leveraging contextual information for more accurate and\nunbiased final recommendations using LLMs. Our results demonstrate that\nincorporating external recommender systems with entity-level information\nsignificantly enhances recommendation accuracy of LLM-based CRS by an average\nof 54% and 25% for ReDial and INSPIRED datasets. The most effective strategy in\nthe CARE framework involves LLMs selecting and reranking candidate items that\nexternal recommenders provide based on contextual insights. Our analysis\nindicates that the CARE framework effectively addresses the identified\nchallenges and mitigates the popularity bias in the external recommender."
                },
                "authors": [
                    {
                        "name": "Chuang Li"
                    },
                    {
                        "name": "Yang Deng"
                    },
                    {
                        "name": "Hengchang Hu"
                    },
                    {
                        "name": "See-Kiong Ng"
                    },
                    {
                        "name": "Min-Yen Kan"
                    },
                    {
                        "name": "Haizhou Li"
                    }
                ],
                "author_detail": {
                    "name": "Haizhou Li"
                },
                "author": "Haizhou Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13889v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13889v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13881v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13881v1",
                "updated": "2025-08-19T14:43:51Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    14,
                    43,
                    51,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T14:43:51Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    14,
                    43,
                    51,
                    1,
                    231,
                    0
                ],
                "title": "Driving Style Recognition Like an Expert Using Semantic Privileged\n  Information from Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Driving Style Recognition Like an Expert Using Semantic Privileged\n  Information from Large Language Models"
                },
                "summary": "Existing driving style recognition systems largely depend on low-level\nsensor-derived features for training, neglecting the rich semantic reasoning\ncapability inherent to human experts. This discrepancy results in a fundamental\nmisalignment between algorithmic classifications and expert judgments. To\nbridge this gap, we propose a novel framework that integrates Semantic\nPrivileged Information (SPI) derived from large language models (LLMs) to align\nrecognition outcomes with human-interpretable reasoning. First, we introduce\nDriBehavGPT, an interactive LLM-based module that generates natural-language\ndescriptions of driving behaviors. These descriptions are then encoded into\nmachine learning-compatible representations via text embedding and\ndimensionality reduction. Finally, we incorporate them as privileged\ninformation into Support Vector Machine Plus (SVM+) for training, enabling the\nmodel to approximate human-like interpretation patterns. Experiments across\ndiverse real-world driving scenarios demonstrate that our SPI-enhanced\nframework outperforms conventional methods, achieving F1-score improvements of\n7.6% (car-following) and 7.9% (lane-changing). Importantly, SPI is exclusively\nused during training, while inference relies solely on sensor data, ensuring\ncomputational efficiency without sacrificing performance. These results\nhighlight the pivotal role of semantic behavioral representations in improving\nrecognition accuracy while advancing interpretable, human-centric driving\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing driving style recognition systems largely depend on low-level\nsensor-derived features for training, neglecting the rich semantic reasoning\ncapability inherent to human experts. This discrepancy results in a fundamental\nmisalignment between algorithmic classifications and expert judgments. To\nbridge this gap, we propose a novel framework that integrates Semantic\nPrivileged Information (SPI) derived from large language models (LLMs) to align\nrecognition outcomes with human-interpretable reasoning. First, we introduce\nDriBehavGPT, an interactive LLM-based module that generates natural-language\ndescriptions of driving behaviors. These descriptions are then encoded into\nmachine learning-compatible representations via text embedding and\ndimensionality reduction. Finally, we incorporate them as privileged\ninformation into Support Vector Machine Plus (SVM+) for training, enabling the\nmodel to approximate human-like interpretation patterns. Experiments across\ndiverse real-world driving scenarios demonstrate that our SPI-enhanced\nframework outperforms conventional methods, achieving F1-score improvements of\n7.6% (car-following) and 7.9% (lane-changing). Importantly, SPI is exclusively\nused during training, while inference relies solely on sensor data, ensuring\ncomputational efficiency without sacrificing performance. These results\nhighlight the pivotal role of semantic behavioral representations in improving\nrecognition accuracy while advancing interpretable, human-centric driving\nsystems."
                },
                "authors": [
                    {
                        "name": "Zhaokun Chen"
                    },
                    {
                        "name": "Chaopeng Zhang"
                    },
                    {
                        "name": "Xiaohan Li"
                    },
                    {
                        "name": "Wenshuo Wang"
                    },
                    {
                        "name": "Gentiane Venture"
                    },
                    {
                        "name": "Junqiang Xi"
                    }
                ],
                "author_detail": {
                    "name": "Junqiang Xi"
                },
                "author": "Junqiang Xi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13881v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13881v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13876v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13876v1",
                "updated": "2025-08-19T14:42:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    14,
                    42,
                    18,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T14:42:18Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    14,
                    42,
                    18,
                    1,
                    231,
                    0
                ],
                "title": "Improved Generalized Planning with LLMs through Strategy Refinement and\n  Reflection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improved Generalized Planning with LLMs through Strategy Refinement and\n  Reflection"
                },
                "summary": "LLMs have recently been used to generate Python programs representing\ngeneralized plans in PDDL planning, i.e., plans that generalize across the\ntasks of a given PDDL domain. Previous work proposed a framework consisting of\nthree steps: the LLM first generates a summary and then a strategy for the\ndomain, both in natural language, and then implements that strategy as a Python\nprogram, that gets debugged on example planning tasks. In that work, only one\nstrategy is generated and passed directly to the program generation. If the\nstrategy is incorrect, its implementation will therefore result in an incorrect\ngeneralized plan. Here, we introduce an approach that generates the strategy in\nthe form of pseudocode and enables automatic debugging of the pseudocode, hence\nallowing us to identify and fix errors prior to the generation of the\ngeneralized plan itself. Additionally, we extend the Python debugging phase\nwith a reflection step prompting the LLM to pinpoint the reason for the\nobserved plan failure. Finally, we take inspiration from LLM code generation to\nproduce several program variants and pick the best one. Running experiments on\n17 benchmark domains, we show that these extensions substantially improve (and\nnever deteriorate) the quality of the generalized plans. In 12 of the domains,\nour best Python programs solve all tasks that can be generated with the\nrespective instance generator.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs have recently been used to generate Python programs representing\ngeneralized plans in PDDL planning, i.e., plans that generalize across the\ntasks of a given PDDL domain. Previous work proposed a framework consisting of\nthree steps: the LLM first generates a summary and then a strategy for the\ndomain, both in natural language, and then implements that strategy as a Python\nprogram, that gets debugged on example planning tasks. In that work, only one\nstrategy is generated and passed directly to the program generation. If the\nstrategy is incorrect, its implementation will therefore result in an incorrect\ngeneralized plan. Here, we introduce an approach that generates the strategy in\nthe form of pseudocode and enables automatic debugging of the pseudocode, hence\nallowing us to identify and fix errors prior to the generation of the\ngeneralized plan itself. Additionally, we extend the Python debugging phase\nwith a reflection step prompting the LLM to pinpoint the reason for the\nobserved plan failure. Finally, we take inspiration from LLM code generation to\nproduce several program variants and pick the best one. Running experiments on\n17 benchmark domains, we show that these extensions substantially improve (and\nnever deteriorate) the quality of the generalized plans. In 12 of the domains,\nour best Python programs solve all tasks that can be generated with the\nrespective instance generator."
                },
                "authors": [
                    {
                        "name": "Katharina Stein"
                    },
                    {
                        "name": "Nils Hodel"
                    },
                    {
                        "name": "Daniel Fišer"
                    },
                    {
                        "name": "Jörg Hoffmann"
                    },
                    {
                        "name": "Michael Katz"
                    },
                    {
                        "name": "Alexander Koller"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Koller"
                },
                "author": "Alexander Koller",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13876v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13876v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13875v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13875v1",
                "updated": "2025-08-19T14:41:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    14,
                    41,
                    22,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T14:41:22Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    14,
                    41,
                    22,
                    1,
                    231,
                    0
                ],
                "title": "A Novel Attention-Augmented Wavelet YOLO System for Real-time Brain\n  Vessel Segmentation on Transcranial Color-coded Doppler",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Novel Attention-Augmented Wavelet YOLO System for Real-time Brain\n  Vessel Segmentation on Transcranial Color-coded Doppler"
                },
                "summary": "The Circle of Willis (CoW), vital for ensuring consistent blood flow to the\nbrain, is closely linked to ischemic stroke. Accurate assessment of the CoW is\nimportant for identifying individuals at risk and guiding appropriate clinical\nmanagement. Among existing imaging methods, Transcranial Color-coded Doppler\n(TCCD) offers unique advantages due to its radiation-free nature,\naffordability, and accessibility. However, reliable TCCD assessments depend\nheavily on operator expertise for identifying anatomical landmarks and\nperforming accurate angle correction, which limits its widespread adoption. To\naddress this challenge, we propose an AI-powered, real-time CoW\nauto-segmentation system capable of efficiently capturing cerebral arteries. No\nprior studies have explored AI-driven cerebrovascular segmentation using TCCD.\nIn this work, we introduce a novel Attention-Augmented Wavelet YOLO (AAW-YOLO)\nnetwork tailored for TCCD data, designed to provide real-time guidance for\nbrain vessel segmentation in the CoW. We prospectively collected TCCD data\ncomprising 738 annotated frames and 3,419 labeled artery instances to establish\na high-quality dataset for model training and evaluation. The proposed AAW-YOLO\ndemonstrated strong performance in segmenting both ipsilateral and\ncontralateral CoW vessels, achieving an average Dice score of 0.901, IoU of\n0.823, precision of 0.882, recall of 0.926, and mAP of 0.953, with a per-frame\ninference speed of 14.199 ms. This system offers a practical solution to reduce\nreliance on operator experience in TCCD-based cerebrovascular screening, with\npotential applications in routine clinical workflows and resource-constrained\nsettings. Future research will explore bilateral modeling and larger-scale\nvalidation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Circle of Willis (CoW), vital for ensuring consistent blood flow to the\nbrain, is closely linked to ischemic stroke. Accurate assessment of the CoW is\nimportant for identifying individuals at risk and guiding appropriate clinical\nmanagement. Among existing imaging methods, Transcranial Color-coded Doppler\n(TCCD) offers unique advantages due to its radiation-free nature,\naffordability, and accessibility. However, reliable TCCD assessments depend\nheavily on operator expertise for identifying anatomical landmarks and\nperforming accurate angle correction, which limits its widespread adoption. To\naddress this challenge, we propose an AI-powered, real-time CoW\nauto-segmentation system capable of efficiently capturing cerebral arteries. No\nprior studies have explored AI-driven cerebrovascular segmentation using TCCD.\nIn this work, we introduce a novel Attention-Augmented Wavelet YOLO (AAW-YOLO)\nnetwork tailored for TCCD data, designed to provide real-time guidance for\nbrain vessel segmentation in the CoW. We prospectively collected TCCD data\ncomprising 738 annotated frames and 3,419 labeled artery instances to establish\na high-quality dataset for model training and evaluation. The proposed AAW-YOLO\ndemonstrated strong performance in segmenting both ipsilateral and\ncontralateral CoW vessels, achieving an average Dice score of 0.901, IoU of\n0.823, precision of 0.882, recall of 0.926, and mAP of 0.953, with a per-frame\ninference speed of 14.199 ms. This system offers a practical solution to reduce\nreliance on operator experience in TCCD-based cerebrovascular screening, with\npotential applications in routine clinical workflows and resource-constrained\nsettings. Future research will explore bilateral modeling and larger-scale\nvalidation."
                },
                "authors": [
                    {
                        "name": "Wenxuan Zhang"
                    },
                    {
                        "name": "Shuai Li"
                    },
                    {
                        "name": "Xinyi Wang"
                    },
                    {
                        "name": "Yu Sun"
                    },
                    {
                        "name": "Hongyu Kang"
                    },
                    {
                        "name": "Pui Yuk Chryste Wan"
                    },
                    {
                        "name": "Yong-Ping Zheng"
                    },
                    {
                        "name": "Sai-Kit Lam"
                    }
                ],
                "author_detail": {
                    "name": "Sai-Kit Lam"
                },
                "arxiv_affiliation": ", the Research Institute of Smart Ageing, The Hong Kong Polytechnic University, Hong Kong SAR, China",
                "author": "Sai-Kit Lam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13875v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13875v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15392v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15392v2",
                "updated": "2025-08-19T14:13:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    14,
                    13,
                    35,
                    1,
                    231,
                    0
                ],
                "published": "2024-08-27T20:32:27Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    20,
                    32,
                    27,
                    1,
                    240,
                    0
                ],
                "title": "The Traceplot Thickens: Developing All-Purpose Convergence Diagnostics\n  for any Markov Chain Monte Carlo Algorithm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Traceplot Thickens: Developing All-Purpose Convergence Diagnostics\n  for any Markov Chain Monte Carlo Algorithm"
                },
                "summary": "Markov Chain Monte Carlo (MCMC) algorithms are frequently used to perform\ninference under a Bayesian modeling framework. Convergence diagnostics, such as\ntraceplots, the Gelman-Rubin potential scale reduction factor, and effective\nsample size, are used to visualize and monitor how well the sampler has\nexplored the parameter space and the mixing of multiple chains. However, these\nclassic diagnostics can be undefined or ineffective when the sample space of\nthe algorithm varies in dimension or has a large number of discrete parameters.\nIn this article, we develop a novel approach to produce convergence diagnostics\nin these difficult scenarios by mapping the original sample space to the\nreal-line and then evaluating the convergence diagnostics on the mapped values.\nThe effectiveness of our method is demonstrated on a MCMC algorithm sampling\nfrom a Dirichlet process mixture model. The proposed diagnostics are also used\nto evaluate the performance of a Bayesian kernel machine regression model for\nestimating the health effect of multi-pollutant mixtures in the Study of\nEnvironment, Lifestyle, and Fibroids. Based on diagnostics for the latter\ndataset, we then explain how we modify the MCMC sampler to improve convergence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Markov Chain Monte Carlo (MCMC) algorithms are frequently used to perform\ninference under a Bayesian modeling framework. Convergence diagnostics, such as\ntraceplots, the Gelman-Rubin potential scale reduction factor, and effective\nsample size, are used to visualize and monitor how well the sampler has\nexplored the parameter space and the mixing of multiple chains. However, these\nclassic diagnostics can be undefined or ineffective when the sample space of\nthe algorithm varies in dimension or has a large number of discrete parameters.\nIn this article, we develop a novel approach to produce convergence diagnostics\nin these difficult scenarios by mapping the original sample space to the\nreal-line and then evaluating the convergence diagnostics on the mapped values.\nThe effectiveness of our method is demonstrated on a MCMC algorithm sampling\nfrom a Dirichlet process mixture model. The proposed diagnostics are also used\nto evaluate the performance of a Bayesian kernel machine regression model for\nestimating the health effect of multi-pollutant mixtures in the Study of\nEnvironment, Lifestyle, and Fibroids. Based on diagnostics for the latter\ndataset, we then explain how we modify the MCMC sampler to improve convergence."
                },
                "authors": [
                    {
                        "name": "Luke Duttweiler"
                    },
                    {
                        "name": "Jonathan Klus"
                    },
                    {
                        "name": "Brent A. Coull"
                    },
                    {
                        "name": "Ruth J. Geller"
                    },
                    {
                        "name": "Birgit Claus Henn"
                    },
                    {
                        "name": "Sally W. Thurston"
                    }
                ],
                "author_detail": {
                    "name": "Sally W. Thurston"
                },
                "author": "Sally W. Thurston",
                "arxiv_comment": "23 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15392v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15392v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62F15",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13845v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13845v1",
                "updated": "2025-08-19T14:09:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    14,
                    9,
                    24,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T14:09:24Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    14,
                    9,
                    24,
                    1,
                    231,
                    0
                ],
                "title": "Extraction of the self energy and Eliashberg function from angle\n  resolved photoemission spectroscopy using the \\textsc{xARPES} code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extraction of the self energy and Eliashberg function from angle\n  resolved photoemission spectroscopy using the \\textsc{xARPES} code"
                },
                "summary": "Angle-resolved photoemission spectroscopy is a powerful experimental\ntechnique for studying anisotropic many-body interactions through the electron\nspectral function. Existing attempts to decompose the spectral function into\nnon-interacting dispersions and electron-phonon, electron-electron, and\nelectron-impurity self-energies rely on linearization of the bands and manual\nassignment of self-energy magnitudes. Here, we show how self-energies can be\nextracted consistently for curved dispersions. We extend the maximum-entropy\nmethod to Eliashberg-function extraction with Bayesian inference, optimizing\nthe parameters describing the dispersions and the magnitudes of\nelectron-electron and electron-impurity interactions. We compare these novel\nmethodologies with state-of-the-art approaches on model data, then demonstrate\ntheir applicability with two high-quality experimental data sets. With the\nfirst set, we identify the phonon modes of a two-dimensional electron liquid on\nTiO$_2$-terminated SrTiO$_3$. With the second set, we obtain unprecedented\nagreement between two Eliashberg functions of Li-doped graphene extracted from\nseparate dispersions. We release these functionalities in the novel Python code\n\\textsc{xARPES}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Angle-resolved photoemission spectroscopy is a powerful experimental\ntechnique for studying anisotropic many-body interactions through the electron\nspectral function. Existing attempts to decompose the spectral function into\nnon-interacting dispersions and electron-phonon, electron-electron, and\nelectron-impurity self-energies rely on linearization of the bands and manual\nassignment of self-energy magnitudes. Here, we show how self-energies can be\nextracted consistently for curved dispersions. We extend the maximum-entropy\nmethod to Eliashberg-function extraction with Bayesian inference, optimizing\nthe parameters describing the dispersions and the magnitudes of\nelectron-electron and electron-impurity interactions. We compare these novel\nmethodologies with state-of-the-art approaches on model data, then demonstrate\ntheir applicability with two high-quality experimental data sets. With the\nfirst set, we identify the phonon modes of a two-dimensional electron liquid on\nTiO$_2$-terminated SrTiO$_3$. With the second set, we obtain unprecedented\nagreement between two Eliashberg functions of Li-doped graphene extracted from\nseparate dispersions. We release these functionalities in the novel Python code\n\\textsc{xARPES}."
                },
                "authors": [
                    {
                        "name": "Thomas P. van Waas"
                    },
                    {
                        "name": "Christophe Berthod"
                    },
                    {
                        "name": "Jan Berges"
                    },
                    {
                        "name": "Nicola Marzari"
                    },
                    {
                        "name": "J. Hugo Dil"
                    },
                    {
                        "name": "Samuel Poncé"
                    }
                ],
                "author_detail": {
                    "name": "Samuel Poncé"
                },
                "author": "Samuel Poncé",
                "arxiv_comment": "10 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13845v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13845v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07928v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07928v3",
                "updated": "2025-08-19T13:36:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    13,
                    36,
                    20,
                    1,
                    231,
                    0
                ],
                "published": "2025-03-11T00:17:07Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    0,
                    17,
                    7,
                    1,
                    70,
                    0
                ],
                "title": "The StudyChat Dataset: Student Dialogues With ChatGPT in an Artificial\n  Intelligence Course",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The StudyChat Dataset: Student Dialogues With ChatGPT in an Artificial\n  Intelligence Course"
                },
                "summary": "The widespread availability of large language models (LLMs), such as ChatGPT,\nhas significantly impacted education, raising both opportunities and\nchallenges. Students can frequently interact with LLM-powered, interactive\nlearning tools, but their usage patterns need to be monitored and understood.\nWe introduce StudyChat, a publicly available dataset capturing real-world\nstudent interactions with an LLM-powered tutoring chatbot in a semester-long,\nuniversity-level artificial intelligence (AI) course. We deploy a web\napplication that replicates ChatGPTs core functionalities, and use it to log\nstudent interactions with the LLM while working on programming assignments. We\ncollect 16,851 interactions, which we annotate using a dialogue act labeling\nschema inspired by observed interaction patterns and prior research. We analyze\nthese interactions, highlight usage trends, and analyze how specific student\nbehavior correlates with their course outcome. We find that students who prompt\nLLMs for conceptual understanding and coding help tend to perform better on\nassignments and exams. Moreover, students who use LLMs to write reports and\ncircumvent assignment learning objectives have lower outcomes on exams than\nothers. StudyChat serves as a shared resource to facilitate further research on\nthe evolving role of LLMs in education.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread availability of large language models (LLMs), such as ChatGPT,\nhas significantly impacted education, raising both opportunities and\nchallenges. Students can frequently interact with LLM-powered, interactive\nlearning tools, but their usage patterns need to be monitored and understood.\nWe introduce StudyChat, a publicly available dataset capturing real-world\nstudent interactions with an LLM-powered tutoring chatbot in a semester-long,\nuniversity-level artificial intelligence (AI) course. We deploy a web\napplication that replicates ChatGPTs core functionalities, and use it to log\nstudent interactions with the LLM while working on programming assignments. We\ncollect 16,851 interactions, which we annotate using a dialogue act labeling\nschema inspired by observed interaction patterns and prior research. We analyze\nthese interactions, highlight usage trends, and analyze how specific student\nbehavior correlates with their course outcome. We find that students who prompt\nLLMs for conceptual understanding and coding help tend to perform better on\nassignments and exams. Moreover, students who use LLMs to write reports and\ncircumvent assignment learning objectives have lower outcomes on exams than\nothers. StudyChat serves as a shared resource to facilitate further research on\nthe evolving role of LLMs in education."
                },
                "authors": [
                    {
                        "name": "Hunter McNichols"
                    },
                    {
                        "name": "Fareya Ikram"
                    },
                    {
                        "name": "Andrew Lan"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Lan"
                },
                "author": "Andrew Lan",
                "arxiv_comment": "Pre-print v0.3",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07928v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07928v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.06095v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.06095v2",
                "updated": "2025-08-19T13:26:23Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    13,
                    26,
                    23,
                    1,
                    231,
                    0
                ],
                "published": "2025-06-06T13:54:34Z",
                "published_parsed": [
                    2025,
                    6,
                    6,
                    13,
                    54,
                    34,
                    4,
                    157,
                    0
                ],
                "title": "Flexible Operator Fusion for Fast Sparse Transformer with Diverse\n  Masking on GPU",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flexible Operator Fusion for Fast Sparse Transformer with Diverse\n  Masking on GPU"
                },
                "summary": "Large language models are popular around the world due to their powerful\nunderstanding capabilities. As the core component of LLMs, accelerating\nTransformer through parallelization has gradually become a hot research topic.\nMask layers introduce sparsity into Transformer to reduce calculations.\nHowever, previous works rarely focus on the performance optimization of sparse\nTransformer. Moreover, rule-based mechanisms ignore the fusion opportunities of\nmixed-type operators and fail to adapt to various sequence lengths. To address\nthe above problems, we propose STOF, a framework that incorporates\noptimizations for Sparse Transformer via flexible masking and operator fusion\non GPU. We firstly unify the storage format and kernel implementation for the\nmulti-head attention. Then, we map fusion schemes to compilation templates and\ndetermine the optimal parameter setting through a two-stage search engine. The\nexperimental results show that compared to the state-of-the-art work, STOF\nachieves maximum speedups of 1.7x in MHA computation and 1.5x in end-to-end\ninference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models are popular around the world due to their powerful\nunderstanding capabilities. As the core component of LLMs, accelerating\nTransformer through parallelization has gradually become a hot research topic.\nMask layers introduce sparsity into Transformer to reduce calculations.\nHowever, previous works rarely focus on the performance optimization of sparse\nTransformer. Moreover, rule-based mechanisms ignore the fusion opportunities of\nmixed-type operators and fail to adapt to various sequence lengths. To address\nthe above problems, we propose STOF, a framework that incorporates\noptimizations for Sparse Transformer via flexible masking and operator fusion\non GPU. We firstly unify the storage format and kernel implementation for the\nmulti-head attention. Then, we map fusion schemes to compilation templates and\ndetermine the optimal parameter setting through a two-stage search engine. The\nexperimental results show that compared to the state-of-the-art work, STOF\nachieves maximum speedups of 1.7x in MHA computation and 1.5x in end-to-end\ninference."
                },
                "authors": [
                    {
                        "name": "Wenhao Dai"
                    },
                    {
                        "name": "Haodong Deng"
                    },
                    {
                        "name": "Mengfei Rong"
                    },
                    {
                        "name": "Xinyu Yang"
                    },
                    {
                        "name": "Hongyu Liu"
                    },
                    {
                        "name": "Fangxin Liu"
                    },
                    {
                        "name": "Hailong Yang"
                    },
                    {
                        "name": "Qianwen Cao"
                    },
                    {
                        "name": "Qingxiao Sun"
                    }
                ],
                "author_detail": {
                    "name": "Qingxiao Sun"
                },
                "author": "Qingxiao Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.06095v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.06095v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13816v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13816v1",
                "updated": "2025-08-19T13:22:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    13,
                    22,
                    41,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T13:22:41Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    13,
                    22,
                    41,
                    1,
                    231,
                    0
                ],
                "title": "The illusion of a perfect metric: Why evaluating AI's words is harder\n  than it looks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The illusion of a perfect metric: Why evaluating AI's words is harder\n  than it looks"
                },
                "summary": "Evaluating Natural Language Generation (NLG) is crucial for the practical\nadoption of AI, but has been a longstanding research challenge. While human\nevaluation is considered the de-facto standard, it is expensive and lacks\nscalability. Practical applications have driven the development of various\nautomatic evaluation metrics (AEM), designed to compare the model output with\nhuman-written references, generating a score which approximates human judgment.\nOver time, AEMs have evolved from simple lexical comparisons, to semantic\nsimilarity models and, more recently, to LLM-based evaluators. However, it\nseems that no single metric has emerged as a definitive solution, resulting in\nstudies using different ones without fully considering the implications. This\npaper aims to show this by conducting a thorough examination of the\nmethodologies of existing metrics, their documented strengths and limitations,\nvalidation methods, and correlations with human judgment. We identify several\nkey challenges: metrics often capture only specific aspects of text quality,\ntheir effectiveness varies by task and dataset, validation practices remain\nunstructured, and correlations with human judgment are inconsistent.\nImportantly, we find that these challenges persist in the most recent type of\nmetric, LLM-as-a-Judge, as well as in the evaluation of Retrieval Augmented\nGeneration (RAG), an increasingly relevant task in academia and industry. Our\nfindings challenge the quest for the 'perfect metric'. We propose selecting\nmetrics based on task-specific needs and leveraging complementary evaluations\nand advocate that new metrics should focus on enhanced validation\nmethodologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Natural Language Generation (NLG) is crucial for the practical\nadoption of AI, but has been a longstanding research challenge. While human\nevaluation is considered the de-facto standard, it is expensive and lacks\nscalability. Practical applications have driven the development of various\nautomatic evaluation metrics (AEM), designed to compare the model output with\nhuman-written references, generating a score which approximates human judgment.\nOver time, AEMs have evolved from simple lexical comparisons, to semantic\nsimilarity models and, more recently, to LLM-based evaluators. However, it\nseems that no single metric has emerged as a definitive solution, resulting in\nstudies using different ones without fully considering the implications. This\npaper aims to show this by conducting a thorough examination of the\nmethodologies of existing metrics, their documented strengths and limitations,\nvalidation methods, and correlations with human judgment. We identify several\nkey challenges: metrics often capture only specific aspects of text quality,\ntheir effectiveness varies by task and dataset, validation practices remain\nunstructured, and correlations with human judgment are inconsistent.\nImportantly, we find that these challenges persist in the most recent type of\nmetric, LLM-as-a-Judge, as well as in the evaluation of Retrieval Augmented\nGeneration (RAG), an increasingly relevant task in academia and industry. Our\nfindings challenge the quest for the 'perfect metric'. We propose selecting\nmetrics based on task-specific needs and leveraging complementary evaluations\nand advocate that new metrics should focus on enhanced validation\nmethodologies."
                },
                "authors": [
                    {
                        "name": "Maria Paz Oliva"
                    },
                    {
                        "name": "Adriana Correia"
                    },
                    {
                        "name": "Ivan Vankov"
                    },
                    {
                        "name": "Viktor Botev"
                    }
                ],
                "author_detail": {
                    "name": "Viktor Botev"
                },
                "author": "Viktor Botev",
                "arxiv_comment": "11 pages, 1 figure. Accepted to RANLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13816v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13816v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.12769v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.12769v3",
                "updated": "2025-08-20T08:11:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    8,
                    11,
                    10,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-18T09:43:07Z",
                "published_parsed": [
                    2025,
                    8,
                    18,
                    9,
                    43,
                    7,
                    0,
                    230,
                    0
                ],
                "title": "CRED-SQL: Enhancing Real-world Large Scale Database Text-to-SQL Parsing\n  through Cluster Retrieval and Execution Description",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CRED-SQL: Enhancing Real-world Large Scale Database Text-to-SQL Parsing\n  through Cluster Retrieval and Execution Description"
                },
                "summary": "Recent advances in large language models (LLMs) have significantly improved\nthe accuracy of Text-to-SQL systems. However, a critical challenge remains: the\nsemantic mismatch between natural language questions (NLQs) and their\ncorresponding SQL queries. This issue is exacerbated in large-scale databases,\nwhere semantically similar attributes hinder schema linking and semantic drift\nduring SQL generation, ultimately reducing model accuracy. To address these\nchallenges, we introduce CRED-SQL, a framework designed for large-scale\ndatabases that integrates Cluster Retrieval and Execution Description. CRED-SQL\nfirst performs cluster-based large-scale schema retrieval to pinpoint the\ntables and columns most relevant to a given NLQ, alleviating schema mismatch.\nIt then introduces an intermediate natural language representation-Execution\nDescription Language (EDL)-to bridge the gap between NLQs and SQL. This\nreformulation decomposes the task into two stages: Text-to-EDL and EDL-to-SQL,\nleveraging LLMs' strong general reasoning capabilities while reducing semantic\ndeviation. Extensive experiments on two large-scale, cross-domain\nbenchmarks-SpiderUnion and BirdUnion-demonstrate that CRED-SQL achieves new\nstate-of-the-art (SOTA) performance, validating its effectiveness and\nscalability. Our code is available at https://github.com/smduan/CRED-SQL.git",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have significantly improved\nthe accuracy of Text-to-SQL systems. However, a critical challenge remains: the\nsemantic mismatch between natural language questions (NLQs) and their\ncorresponding SQL queries. This issue is exacerbated in large-scale databases,\nwhere semantically similar attributes hinder schema linking and semantic drift\nduring SQL generation, ultimately reducing model accuracy. To address these\nchallenges, we introduce CRED-SQL, a framework designed for large-scale\ndatabases that integrates Cluster Retrieval and Execution Description. CRED-SQL\nfirst performs cluster-based large-scale schema retrieval to pinpoint the\ntables and columns most relevant to a given NLQ, alleviating schema mismatch.\nIt then introduces an intermediate natural language representation-Execution\nDescription Language (EDL)-to bridge the gap between NLQs and SQL. This\nreformulation decomposes the task into two stages: Text-to-EDL and EDL-to-SQL,\nleveraging LLMs' strong general reasoning capabilities while reducing semantic\ndeviation. Extensive experiments on two large-scale, cross-domain\nbenchmarks-SpiderUnion and BirdUnion-demonstrate that CRED-SQL achieves new\nstate-of-the-art (SOTA) performance, validating its effectiveness and\nscalability. Our code is available at https://github.com/smduan/CRED-SQL.git"
                },
                "authors": [
                    {
                        "name": "Shaoming Duan"
                    },
                    {
                        "name": "Zirui Wang"
                    },
                    {
                        "name": "Chuanyi Liu"
                    },
                    {
                        "name": "Zhibin Zhu"
                    },
                    {
                        "name": "Yuhao Zhang"
                    },
                    {
                        "name": "Peiyi Han"
                    },
                    {
                        "name": "Liang Yan"
                    },
                    {
                        "name": "Zewu Peng"
                    }
                ],
                "author_detail": {
                    "name": "Zewu Peng"
                },
                "author": "Zewu Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.12769v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.12769v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13805v1",
                "updated": "2025-08-19T13:12:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    13,
                    12,
                    1,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T13:12:01Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    13,
                    12,
                    1,
                    1,
                    231,
                    0
                ],
                "title": "Prompt-Based One-Shot Exact Length-Controlled Generation with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt-Based One-Shot Exact Length-Controlled Generation with LLMs"
                },
                "summary": "Controlling the length of text produced by large language models (LLMs)\nremains challenging: models frequently overshoot or undershoot explicit length\ninstructions because they cannot reliably keep an internal token count. We\npresent a prompt-based, one-shot strategy that compels an off-the-shelf LLM to\ngenerate exactly a desired number of tokens - words (English) or characters\n(Chinese) - without any fine-tuning or iterative sampling. The prompt appends\ncountdown markers and explicit counting rules so that the model \"writes while\ncounting.\" We evaluate on four settings: open-ended generation (1-1000 tokens),\nXSUM summarization, MT-Bench-LI instruction following, and the LIFEBENCH\nequal-length track. On MT-Bench-LI, strict length compliance with GPT-4.1 leaps\nfrom below 30% under naive prompts to above 95% with our countdown prompt,\nsurpassing the popular draft-then-revise baseline, while judged answer quality\nis preserved. These results show that precise length control can be achieved\nthrough prompt engineering alone, offering a lightweight alternative to\ntraining- or decoding-based methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Controlling the length of text produced by large language models (LLMs)\nremains challenging: models frequently overshoot or undershoot explicit length\ninstructions because they cannot reliably keep an internal token count. We\npresent a prompt-based, one-shot strategy that compels an off-the-shelf LLM to\ngenerate exactly a desired number of tokens - words (English) or characters\n(Chinese) - without any fine-tuning or iterative sampling. The prompt appends\ncountdown markers and explicit counting rules so that the model \"writes while\ncounting.\" We evaluate on four settings: open-ended generation (1-1000 tokens),\nXSUM summarization, MT-Bench-LI instruction following, and the LIFEBENCH\nequal-length track. On MT-Bench-LI, strict length compliance with GPT-4.1 leaps\nfrom below 30% under naive prompts to above 95% with our countdown prompt,\nsurpassing the popular draft-then-revise baseline, while judged answer quality\nis preserved. These results show that precise length control can be achieved\nthrough prompt engineering alone, offering a lightweight alternative to\ntraining- or decoding-based methods."
                },
                "authors": [
                    {
                        "name": "Juncheng Xie"
                    },
                    {
                        "name": "Hung-yi Lee"
                    }
                ],
                "author_detail": {
                    "name": "Hung-yi Lee"
                },
                "author": "Hung-yi Lee",
                "arxiv_comment": "18 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13804v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13804v1",
                "updated": "2025-08-19T13:05:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    13,
                    5,
                    48,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T13:05:48Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    13,
                    5,
                    48,
                    1,
                    231,
                    0
                ],
                "title": "Beyond Human Judgment: A Bayesian Evaluation of LLMs' Moral Values\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Human Judgment: A Bayesian Evaluation of LLMs' Moral Values\n  Understanding"
                },
                "summary": "How do large language models understand moral dimensions compared to humans?\n  This first large-scale Bayesian evaluation of market-leading language models\nprovides the answer. In contrast to prior work using deterministic ground truth\n(majority or inclusion rules), we model annotator disagreements to capture both\naleatoric uncertainty (inherent human disagreement) and epistemic uncertainty\n(model domain sensitivity). We evaluate top language models (Claude Sonnet 4,\nDeepSeek-V3, Llama 4 Maverick) across 250K+ annotations from ~700 annotators on\n100K+ texts spanning social media, news, and forums.\n  Our GPU-optimized Bayesian framework processed 1M+ model queries, revealing\nthat AI models typically rank among the top 25\\% of human annotators, achieving\nmuch better-than-average balanced accuracy. Importantly, we find that AI\nproduces far fewer false negatives than humans, highlighting their more\nsensitive moral detection capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How do large language models understand moral dimensions compared to humans?\n  This first large-scale Bayesian evaluation of market-leading language models\nprovides the answer. In contrast to prior work using deterministic ground truth\n(majority or inclusion rules), we model annotator disagreements to capture both\naleatoric uncertainty (inherent human disagreement) and epistemic uncertainty\n(model domain sensitivity). We evaluate top language models (Claude Sonnet 4,\nDeepSeek-V3, Llama 4 Maverick) across 250K+ annotations from ~700 annotators on\n100K+ texts spanning social media, news, and forums.\n  Our GPU-optimized Bayesian framework processed 1M+ model queries, revealing\nthat AI models typically rank among the top 25\\% of human annotators, achieving\nmuch better-than-average balanced accuracy. Importantly, we find that AI\nproduces far fewer false negatives than humans, highlighting their more\nsensitive moral detection capabilities."
                },
                "authors": [
                    {
                        "name": "Maciej Skorski"
                    },
                    {
                        "name": "Alina Landowska"
                    }
                ],
                "author_detail": {
                    "name": "Alina Landowska"
                },
                "author": "Alina Landowska",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13804v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13804v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50, 62F15, 62P25",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; K.4.1; J.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13798v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13798v1",
                "updated": "2025-08-19T12:57:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    12,
                    57,
                    45,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T12:57:45Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    12,
                    57,
                    45,
                    1,
                    231,
                    0
                ],
                "title": "TracSum: A New Benchmark for Aspect-Based Summarization with\n  Sentence-Level Traceability in Medical Domain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TracSum: A New Benchmark for Aspect-Based Summarization with\n  Sentence-Level Traceability in Medical Domain"
                },
                "summary": "While document summarization with LLMs has enhanced access to textual\ninformation, concerns about the factual accuracy of these summaries persist,\nespecially in the medical domain. Tracing evidence from which summaries are\nderived enables users to assess their accuracy, thereby alleviating this\nconcern. In this paper, we introduce TracSum, a novel benchmark for traceable,\naspect-based summarization, in which generated summaries are paired with\nsentence-level citations, enabling users to trace back to the original context.\nFirst, we annotate 500 medical abstracts for seven key medical aspects,\nyielding 3.5K summary-citation pairs. We then propose a fine-grained evaluation\nframework for this new task, designed to assess the completeness and\nconsistency of generated content using four metrics. Finally, we introduce a\nsummarization pipeline, Track-Then-Sum, which serves as a baseline method for\ncomparison. In experiments, we evaluate both this baseline and a set of LLMs on\nTracSum, and conduct a human evaluation to assess the evaluation results. The\nfindings demonstrate that TracSum can serve as an effective benchmark for\ntraceable, aspect-based summarization tasks. We also observe that explicitly\nperforming sentence-level tracking prior to summarization enhances generation\naccuracy, while incorporating the full context further improves completeness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While document summarization with LLMs has enhanced access to textual\ninformation, concerns about the factual accuracy of these summaries persist,\nespecially in the medical domain. Tracing evidence from which summaries are\nderived enables users to assess their accuracy, thereby alleviating this\nconcern. In this paper, we introduce TracSum, a novel benchmark for traceable,\naspect-based summarization, in which generated summaries are paired with\nsentence-level citations, enabling users to trace back to the original context.\nFirst, we annotate 500 medical abstracts for seven key medical aspects,\nyielding 3.5K summary-citation pairs. We then propose a fine-grained evaluation\nframework for this new task, designed to assess the completeness and\nconsistency of generated content using four metrics. Finally, we introduce a\nsummarization pipeline, Track-Then-Sum, which serves as a baseline method for\ncomparison. In experiments, we evaluate both this baseline and a set of LLMs on\nTracSum, and conduct a human evaluation to assess the evaluation results. The\nfindings demonstrate that TracSum can serve as an effective benchmark for\ntraceable, aspect-based summarization tasks. We also observe that explicitly\nperforming sentence-level tracking prior to summarization enhances generation\naccuracy, while incorporating the full context further improves completeness."
                },
                "authors": [
                    {
                        "name": "Bohao Chu"
                    },
                    {
                        "name": "Meijie Li"
                    },
                    {
                        "name": "Sameh Frihat"
                    },
                    {
                        "name": "Chengyu Gu"
                    },
                    {
                        "name": "Georg Lodde"
                    },
                    {
                        "name": "Elisabeth Livingstone"
                    },
                    {
                        "name": "Norbert Fuhr"
                    }
                ],
                "author_detail": {
                    "name": "Norbert Fuhr"
                },
                "author": "Norbert Fuhr",
                "arxiv_comment": "8 main pages, 12 appendix pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13798v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13798v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13792v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13792v1",
                "updated": "2025-08-19T12:52:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    12,
                    52,
                    16,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T12:52:16Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    12,
                    52,
                    16,
                    1,
                    231,
                    0
                ],
                "title": "VisionLaw: Inferring Interpretable Intrinsic Dynamics from Visual\n  Observations via Bilevel Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VisionLaw: Inferring Interpretable Intrinsic Dynamics from Visual\n  Observations via Bilevel Optimization"
                },
                "summary": "The intrinsic dynamics of an object governs its physical behavior in the real\nworld, playing a critical role in enabling physically plausible interactive\nsimulation with 3D assets. Existing methods have attempted to infer the\nintrinsic dynamics of objects from visual observations, but generally face two\nmajor challenges: one line of work relies on manually defined constitutive\npriors, making it difficult to generalize to complex scenarios; the other\nmodels intrinsic dynamics using neural networks, resulting in limited\ninterpretability and poor generalization. To address these challenges, we\npropose VisionLaw, a bilevel optimization framework that infers interpretable\nexpressions of intrinsic dynamics from visual observations. At the upper level,\nwe introduce an LLMs-driven decoupled constitutive evolution strategy, where\nLLMs are prompted as a knowledgeable physics expert to generate and revise\nconstitutive laws, with a built-in decoupling mechanism that substantially\nreduces the search complexity of LLMs. At the lower level, we introduce a\nvision-guided constitutive evaluation mechanism, which utilizes visual\nsimulation to evaluate the consistency between the generated constitutive law\nand the underlying intrinsic dynamics, thereby guiding the upper-level\nevolution. Experiments on both synthetic and real-world datasets demonstrate\nthat VisionLaw can effectively infer interpretable intrinsic dynamics from\nvisual observations. It significantly outperforms existing state-of-the-art\nmethods and exhibits strong generalization for interactive simulation in novel\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The intrinsic dynamics of an object governs its physical behavior in the real\nworld, playing a critical role in enabling physically plausible interactive\nsimulation with 3D assets. Existing methods have attempted to infer the\nintrinsic dynamics of objects from visual observations, but generally face two\nmajor challenges: one line of work relies on manually defined constitutive\npriors, making it difficult to generalize to complex scenarios; the other\nmodels intrinsic dynamics using neural networks, resulting in limited\ninterpretability and poor generalization. To address these challenges, we\npropose VisionLaw, a bilevel optimization framework that infers interpretable\nexpressions of intrinsic dynamics from visual observations. At the upper level,\nwe introduce an LLMs-driven decoupled constitutive evolution strategy, where\nLLMs are prompted as a knowledgeable physics expert to generate and revise\nconstitutive laws, with a built-in decoupling mechanism that substantially\nreduces the search complexity of LLMs. At the lower level, we introduce a\nvision-guided constitutive evaluation mechanism, which utilizes visual\nsimulation to evaluate the consistency between the generated constitutive law\nand the underlying intrinsic dynamics, thereby guiding the upper-level\nevolution. Experiments on both synthetic and real-world datasets demonstrate\nthat VisionLaw can effectively infer interpretable intrinsic dynamics from\nvisual observations. It significantly outperforms existing state-of-the-art\nmethods and exhibits strong generalization for interactive simulation in novel\nscenarios."
                },
                "authors": [
                    {
                        "name": "Jiajing Lin"
                    },
                    {
                        "name": "Shu Jiang"
                    },
                    {
                        "name": "Qingyuan Zeng"
                    },
                    {
                        "name": "Zhenzhong Wang"
                    },
                    {
                        "name": "Min Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Min Jiang"
                },
                "author": "Min Jiang",
                "arxiv_comment": "9 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13792v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13792v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13787v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13787v1",
                "updated": "2025-08-19T12:43:49Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    12,
                    43,
                    49,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T12:43:49Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    12,
                    43,
                    49,
                    1,
                    231,
                    0
                ],
                "title": "BetaWeb: Towards a Blockchain-enabled Trustworthy Agentic Web",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BetaWeb: Towards a Blockchain-enabled Trustworthy Agentic Web"
                },
                "summary": "The rapid development of large language models (LLMs) has significantly\npropelled the development of artificial intelligence (AI) agents, which are\nincreasingly evolving into diverse autonomous entities, advancing the LLM-based\nmulti-agent systems (LaMAS). However, current agentic ecosystems remain\nfragmented and closed. Establishing an interconnected and scalable paradigm for\nAgentic AI has become a critical prerequisite. Although Agentic Web proposes an\nopen architecture to break the ecosystem barriers, its implementation still\nfaces core challenges such as privacy protection, data management, and value\nmeasurement. Existing centralized or semi-centralized paradigms suffer from\ninherent limitations, making them inadequate for supporting large-scale,\nheterogeneous, and cross-domain autonomous interactions. To address these\nchallenges, this paper introduces the blockchain-enabled trustworthy Agentic\nWeb (BetaWeb). By leveraging the inherent strengths of blockchain, BetaWeb not\nonly offers a trustworthy and scalable infrastructure for LaMAS but also has\nthe potential to advance the Web paradigm from Web3 (centered on data\nownership) towards Web3.5, which emphasizes ownership of agent capabilities and\nthe monetization of intelligence. Beyond a systematic examination of the\nBetaWeb framework, this paper presents a five-stage evolutionary roadmap,\noutlining the path of LaMAS from passive execution to advanced collaboration\nand autonomous governance. We also conduct a comparative analysis of existing\nproducts and discuss key challenges of BetaWeb from multiple perspectives.\nUltimately, we argue that deep integration between blockchain and LaMAS can lay\nthe foundation for a resilient, trustworthy, and sustainably incentivized\ndigital ecosystem. A summary of the enabling technologies for each stage is\navailable at https://github.com/MatZaharia/BetaWeb.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of large language models (LLMs) has significantly\npropelled the development of artificial intelligence (AI) agents, which are\nincreasingly evolving into diverse autonomous entities, advancing the LLM-based\nmulti-agent systems (LaMAS). However, current agentic ecosystems remain\nfragmented and closed. Establishing an interconnected and scalable paradigm for\nAgentic AI has become a critical prerequisite. Although Agentic Web proposes an\nopen architecture to break the ecosystem barriers, its implementation still\nfaces core challenges such as privacy protection, data management, and value\nmeasurement. Existing centralized or semi-centralized paradigms suffer from\ninherent limitations, making them inadequate for supporting large-scale,\nheterogeneous, and cross-domain autonomous interactions. To address these\nchallenges, this paper introduces the blockchain-enabled trustworthy Agentic\nWeb (BetaWeb). By leveraging the inherent strengths of blockchain, BetaWeb not\nonly offers a trustworthy and scalable infrastructure for LaMAS but also has\nthe potential to advance the Web paradigm from Web3 (centered on data\nownership) towards Web3.5, which emphasizes ownership of agent capabilities and\nthe monetization of intelligence. Beyond a systematic examination of the\nBetaWeb framework, this paper presents a five-stage evolutionary roadmap,\noutlining the path of LaMAS from passive execution to advanced collaboration\nand autonomous governance. We also conduct a comparative analysis of existing\nproducts and discuss key challenges of BetaWeb from multiple perspectives.\nUltimately, we argue that deep integration between blockchain and LaMAS can lay\nthe foundation for a resilient, trustworthy, and sustainably incentivized\ndigital ecosystem. A summary of the enabling technologies for each stage is\navailable at https://github.com/MatZaharia/BetaWeb."
                },
                "authors": [
                    {
                        "name": "Zihan Guo"
                    },
                    {
                        "name": "Yuanjian Zhou"
                    },
                    {
                        "name": "Chenyi Wang"
                    },
                    {
                        "name": "Linlin You"
                    },
                    {
                        "name": "Minjie Bian"
                    },
                    {
                        "name": "Weinan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Weinan Zhang"
                },
                "author": "Weinan Zhang",
                "arxiv_comment": "A technical report with 21 pages, 3 figures, and 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13787v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13787v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16336v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16336v2",
                "updated": "2025-08-19T12:40:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    12,
                    40,
                    41,
                    1,
                    231,
                    0
                ],
                "published": "2025-02-22T19:54:14Z",
                "published_parsed": [
                    2025,
                    2,
                    22,
                    19,
                    54,
                    14,
                    5,
                    53,
                    0
                ],
                "title": "Rectifying Conformity Scores for Better Conditional Coverage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rectifying Conformity Scores for Better Conditional Coverage"
                },
                "summary": "We present a new method for generating confidence sets within the split\nconformal prediction framework. Our method performs a trainable transformation\nof any given conformity score to improve conditional coverage while ensuring\nexact marginal coverage. The transformation is based on an estimate of the\nconditional quantile of conformity scores. The resulting method is particularly\nbeneficial for constructing adaptive confidence sets in multi-output problems\nwhere standard conformal quantile regression approaches have limited\napplicability. We develop a theoretical bound that captures the influence of\nthe accuracy of the quantile estimate on the approximate conditional validity,\nunlike classical bounds for conformal prediction methods that only offer\nmarginal coverage. We experimentally show that our method is highly adaptive to\nthe local data structure and outperforms existing methods in terms of\nconditional coverage, improving the reliability of statistical inference in\nvarious applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a new method for generating confidence sets within the split\nconformal prediction framework. Our method performs a trainable transformation\nof any given conformity score to improve conditional coverage while ensuring\nexact marginal coverage. The transformation is based on an estimate of the\nconditional quantile of conformity scores. The resulting method is particularly\nbeneficial for constructing adaptive confidence sets in multi-output problems\nwhere standard conformal quantile regression approaches have limited\napplicability. We develop a theoretical bound that captures the influence of\nthe accuracy of the quantile estimate on the approximate conditional validity,\nunlike classical bounds for conformal prediction methods that only offer\nmarginal coverage. We experimentally show that our method is highly adaptive to\nthe local data structure and outperforms existing methods in terms of\nconditional coverage, improving the reliability of statistical inference in\nvarious applications."
                },
                "authors": [
                    {
                        "name": "Vincent Plassier"
                    },
                    {
                        "name": "Alexander Fishkov"
                    },
                    {
                        "name": "Victor Dheur"
                    },
                    {
                        "name": "Mohsen Guizani"
                    },
                    {
                        "name": "Souhaib Ben Taieb"
                    },
                    {
                        "name": "Maxim Panov"
                    },
                    {
                        "name": "Eric Moulines"
                    }
                ],
                "author_detail": {
                    "name": "Eric Moulines"
                },
                "author": "Eric Moulines",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16336v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16336v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19263v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19263v2",
                "updated": "2025-08-19T12:30:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    12,
                    30,
                    48,
                    1,
                    231,
                    0
                ],
                "published": "2025-07-25T13:38:44Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    13,
                    38,
                    44,
                    4,
                    206,
                    0
                ],
                "title": "Modeling Uncertainty: Constraint-Based Belief States in\n  Imperfect-Information Games",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling Uncertainty: Constraint-Based Belief States in\n  Imperfect-Information Games"
                },
                "summary": "In imperfect-information games, agents must make decisions based on partial\nknowledge of the game state. The Belief Stochastic Game model addresses this\nchallenge by delegating state estimation to the game model itself. This allows\nagents to operate on externally provided belief states, thereby reducing the\nneed for game-specific inference logic. This paper investigates two approaches\nto represent beliefs in games with hidden piece identities: a constraint-based\nmodel using Constraint Satisfaction Problems and a probabilistic extension\nusing Belief Propagation to estimate marginal probabilities. We evaluated the\nimpact of both representations using general-purpose agents across two\ndifferent games. Our findings indicate that constraint-based beliefs yield\nresults comparable to those of probabilistic inference, with minimal\ndifferences in agent performance. This suggests that constraint-based belief\nstates alone may suffice for effective decision-making in many settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In imperfect-information games, agents must make decisions based on partial\nknowledge of the game state. The Belief Stochastic Game model addresses this\nchallenge by delegating state estimation to the game model itself. This allows\nagents to operate on externally provided belief states, thereby reducing the\nneed for game-specific inference logic. This paper investigates two approaches\nto represent beliefs in games with hidden piece identities: a constraint-based\nmodel using Constraint Satisfaction Problems and a probabilistic extension\nusing Belief Propagation to estimate marginal probabilities. We evaluated the\nimpact of both representations using general-purpose agents across two\ndifferent games. Our findings indicate that constraint-based beliefs yield\nresults comparable to those of probabilistic inference, with minimal\ndifferences in agent performance. This suggests that constraint-based belief\nstates alone may suffice for effective decision-making in many settings."
                },
                "authors": [
                    {
                        "name": "Achille Morenville"
                    },
                    {
                        "name": "Éric Piette"
                    }
                ],
                "author_detail": {
                    "name": "Éric Piette"
                },
                "author": "Éric Piette",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19263v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19263v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.19289v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.19289v3",
                "updated": "2025-08-19T12:26:13Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    12,
                    26,
                    13,
                    1,
                    231,
                    0
                ],
                "published": "2024-06-27T16:04:40Z",
                "published_parsed": [
                    2024,
                    6,
                    27,
                    16,
                    4,
                    40,
                    3,
                    179,
                    0
                ],
                "title": "Joint Channel and Data Estimation for Multiuser Extremely Large-Scale\n  MIMO Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Channel and Data Estimation for Multiuser Extremely Large-Scale\n  MIMO Systems"
                },
                "summary": "This paper proposes a joint channel and data estimation (JCDE) algorithm for\nuplink multiuser extremely large-scale multiple-input-multiple-output (XL-MIMO)\nsystems. The initial channel estimation is formulated as a sparse\nreconstruction problem based on the angle and distance sparsity under the\nnear-field propagation condition. This problem is solved using non-orthogonal\npilots through an efficient low complexity two-stage compressed sensing\nalgorithm. Furthermore, the initial channel estimates are refined by employing\na JCDE framework driven by both non-orthogonal pilots and estimated data. The\nJCDE problem is solved by sequential expectation propagation (EP) algorithms,\nwhere the channel and data are alternately updated in an iterative manner. In\nthe channel estimation phase, integrating Bayesian inference with a model-based\ndeterministic approach provides precise estimations to effectively exploit the\nnear-field characteristics in the beam-domain. In the data estimation phase, a\nlinear minimum mean square error (LMMSE)-based filter is designed at each\nsub-array to address the correlation due to energy leakage in the beam-domain\narising from the near-field effects. Numerical simulations reveal that the\nproposed initial channel estimation and JCDE algorithm outperforms the\nstate-of-the-art approaches in terms of channel estimation, data detection, and\ncomputational complexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a joint channel and data estimation (JCDE) algorithm for\nuplink multiuser extremely large-scale multiple-input-multiple-output (XL-MIMO)\nsystems. The initial channel estimation is formulated as a sparse\nreconstruction problem based on the angle and distance sparsity under the\nnear-field propagation condition. This problem is solved using non-orthogonal\npilots through an efficient low complexity two-stage compressed sensing\nalgorithm. Furthermore, the initial channel estimates are refined by employing\na JCDE framework driven by both non-orthogonal pilots and estimated data. The\nJCDE problem is solved by sequential expectation propagation (EP) algorithms,\nwhere the channel and data are alternately updated in an iterative manner. In\nthe channel estimation phase, integrating Bayesian inference with a model-based\ndeterministic approach provides precise estimations to effectively exploit the\nnear-field characteristics in the beam-domain. In the data estimation phase, a\nlinear minimum mean square error (LMMSE)-based filter is designed at each\nsub-array to address the correlation due to energy leakage in the beam-domain\narising from the near-field effects. Numerical simulations reveal that the\nproposed initial channel estimation and JCDE algorithm outperforms the\nstate-of-the-art approaches in terms of channel estimation, data detection, and\ncomputational complexity."
                },
                "authors": [
                    {
                        "name": "Kabuto Arai"
                    },
                    {
                        "name": "Koji Ishibashi"
                    },
                    {
                        "name": "Hiroki Iimori"
                    },
                    {
                        "name": "Paulo Valente Klaine"
                    },
                    {
                        "name": "Szabolcs Malomsoky"
                    }
                ],
                "author_detail": {
                    "name": "Szabolcs Malomsoky"
                },
                "author": "Szabolcs Malomsoky",
                "arxiv_doi": "10.1109/TWC.2025.3597278.",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TWC.2025.3597278.",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.19289v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.19289v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Submitted to the IEEE Transactions on Wireless Communications",
                "arxiv_journal_ref": "IEEE Transactions on Wireless Communications 2025",
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13774v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13774v1",
                "updated": "2025-08-19T12:21:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    12,
                    21,
                    21,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T12:21:21Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    12,
                    21,
                    21,
                    1,
                    231,
                    0
                ],
                "title": "Agentic DraCor and the Art of Docstring Engineering: Evaluating\n  MCP-empowered LLM Usage of the DraCor API",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic DraCor and the Art of Docstring Engineering: Evaluating\n  MCP-empowered LLM Usage of the DraCor API"
                },
                "summary": "This paper reports on the implementation and evaluation of a Model Context\nProtocol (MCP) server for DraCor, enabling Large Language Models (LLM) to\nautonomously interact with the DraCor API. We conducted experiments focusing on\ntool selection and application by the LLM, employing a qualitative approach\nthat includes systematic observation of prompts to understand how LLMs behave\nwhen using MCP tools, evaluating \"Tool Correctness\", \"Tool-Calling Efficiency\",\nand \"Tool-Use Reliability\". Our findings highlight the importance of \"Docstring\nEngineering\", defined as reflexively crafting tool documentation to optimize\nLLM-tool interaction. Our experiments demonstrate both the promise of agentic\nAI for research in Computational Literary Studies and the essential\ninfrastructure development needs for reliable Digital Humanities\ninfrastructures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper reports on the implementation and evaluation of a Model Context\nProtocol (MCP) server for DraCor, enabling Large Language Models (LLM) to\nautonomously interact with the DraCor API. We conducted experiments focusing on\ntool selection and application by the LLM, employing a qualitative approach\nthat includes systematic observation of prompts to understand how LLMs behave\nwhen using MCP tools, evaluating \"Tool Correctness\", \"Tool-Calling Efficiency\",\nand \"Tool-Use Reliability\". Our findings highlight the importance of \"Docstring\nEngineering\", defined as reflexively crafting tool documentation to optimize\nLLM-tool interaction. Our experiments demonstrate both the promise of agentic\nAI for research in Computational Literary Studies and the essential\ninfrastructure development needs for reliable Digital Humanities\ninfrastructures."
                },
                "authors": [
                    {
                        "name": "Peer Trilcke"
                    },
                    {
                        "name": "Ingo Börner"
                    },
                    {
                        "name": "Henny Sluyter-Gäthje"
                    },
                    {
                        "name": "Daniil Skorinkin"
                    },
                    {
                        "name": "Frank Fischer"
                    },
                    {
                        "name": "Carsten Milling"
                    }
                ],
                "author_detail": {
                    "name": "Carsten Milling"
                },
                "author": "Carsten Milling",
                "arxiv_comment": "Preprint, submitted to the 2nd Workshop on Computational Drama\n  Analysis at DraCor Summit 2025, September 03, 2025, Berlin, Germany",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13774v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13774v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "J.5; I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08549v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08549v2",
                "updated": "2025-08-19T12:21:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    12,
                    21,
                    20,
                    1,
                    231,
                    0
                ],
                "published": "2025-03-11T15:36:38Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    15,
                    36,
                    38,
                    1,
                    70,
                    0
                ],
                "title": "GoAI: Enhancing AI Students' Learning Paths and Idea Generation via\n  Graph of AI Ideas",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GoAI: Enhancing AI Students' Learning Paths and Idea Generation via\n  Graph of AI Ideas"
                },
                "summary": "With the rapid advancement of artificial intelligence technology, AI students\nare confronted with a significant \"information-to-innovation\" gap: they must\nnavigate through the rapidly expanding body of literature, trace the\ndevelopment of a specific research field, and synthesize various techniques\ninto feasible innovative concepts. An additional critical step for students is\nto identify the necessary prerequisite knowledge and learning paths. Although\nmany approaches based on large language models (LLMs) can summarize the content\nof papers and trace the development of a field through citations, these methods\noften overlook the prerequisite knowledge involved in the papers and the rich\nsemantic information embedded in the citation relationships between papers.\nSuch information reveals how methods are interrelated, built upon, extended, or\nchallenged. To address these limitations, we propose GoAI, a tool for\nconstructing educational knowledge graphs from AI research papers that\nleverages these graphs to plan personalized learning paths and support creative\nideation. The nodes in the knowledge graph we have built include papers and the\nprerequisite knowledge, such as concepts, skills, and tools, that they involve;\nthe edges record the semantic information of citations. When a student queries\na specific paper, a beam search-based path search method can trace the current\ndevelopment trends of the field from the queried paper and plan a learning path\ntoward cutting-edge objectives. The integrated Idea Studio guides students to\nclarify problem statements, compare alternative designs, and provide formative\nfeedback on novelty, clarity, feasibility, and alignment with learning\nobjectives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid advancement of artificial intelligence technology, AI students\nare confronted with a significant \"information-to-innovation\" gap: they must\nnavigate through the rapidly expanding body of literature, trace the\ndevelopment of a specific research field, and synthesize various techniques\ninto feasible innovative concepts. An additional critical step for students is\nto identify the necessary prerequisite knowledge and learning paths. Although\nmany approaches based on large language models (LLMs) can summarize the content\nof papers and trace the development of a field through citations, these methods\noften overlook the prerequisite knowledge involved in the papers and the rich\nsemantic information embedded in the citation relationships between papers.\nSuch information reveals how methods are interrelated, built upon, extended, or\nchallenged. To address these limitations, we propose GoAI, a tool for\nconstructing educational knowledge graphs from AI research papers that\nleverages these graphs to plan personalized learning paths and support creative\nideation. The nodes in the knowledge graph we have built include papers and the\nprerequisite knowledge, such as concepts, skills, and tools, that they involve;\nthe edges record the semantic information of citations. When a student queries\na specific paper, a beam search-based path search method can trace the current\ndevelopment trends of the field from the queried paper and plan a learning path\ntoward cutting-edge objectives. The integrated Idea Studio guides students to\nclarify problem statements, compare alternative designs, and provide formative\nfeedback on novelty, clarity, feasibility, and alignment with learning\nobjectives."
                },
                "authors": [
                    {
                        "name": "Xian Gao"
                    },
                    {
                        "name": "Zongyun Zhang"
                    },
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Yuzhuo Fu"
                    }
                ],
                "author_detail": {
                    "name": "Yuzhuo Fu"
                },
                "author": "Yuzhuo Fu",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08549v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08549v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13769v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13769v1",
                "updated": "2025-08-19T12:13:54Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    12,
                    13,
                    54,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T12:13:54Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    12,
                    13,
                    54,
                    1,
                    231,
                    0
                ],
                "title": "Can Large Language Models (LLMs) Describe Pictures Like Children? A\n  Comparative Corpus Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models (LLMs) Describe Pictures Like Children? A\n  Comparative Corpus Study"
                },
                "summary": "The role of large language models (LLMs) in education is increasing, yet\nlittle attention has been paid to whether LLM-generated text resembles child\nlanguage. This study evaluates how LLMs replicate child-like language by\ncomparing LLM-generated texts to a collection of German children's descriptions\nof picture stories. We generated two LLM-based corpora using the same picture\nstories and two prompt types: zero-shot and few-shot prompts specifying a\ngeneral age from the children corpus. We conducted a comparative analysis\nacross psycholinguistic text properties, including word frequency, lexical\nrichness, sentence and word length, part-of-speech tags, and semantic\nsimilarity with word embeddings. The results show that LLM-generated texts are\nlonger but less lexically rich, rely more on high-frequency words, and\nunder-represent nouns. Semantic vector space analysis revealed low similarity,\nhighlighting differences between the two corpora on the level of corpus\nsemantics. Few-shot prompt increased similarities between children and LLM text\nto a minor extent, but still failed to replicate lexical and semantic patterns.\nThe findings contribute to our understanding of how LLMs approximate child\nlanguage through multimodal prompting (text + image) and give insights into\ntheir use in psycholinguistic research and education while raising important\nquestions about the appropriateness of LLM-generated language in child-directed\neducational tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The role of large language models (LLMs) in education is increasing, yet\nlittle attention has been paid to whether LLM-generated text resembles child\nlanguage. This study evaluates how LLMs replicate child-like language by\ncomparing LLM-generated texts to a collection of German children's descriptions\nof picture stories. We generated two LLM-based corpora using the same picture\nstories and two prompt types: zero-shot and few-shot prompts specifying a\ngeneral age from the children corpus. We conducted a comparative analysis\nacross psycholinguistic text properties, including word frequency, lexical\nrichness, sentence and word length, part-of-speech tags, and semantic\nsimilarity with word embeddings. The results show that LLM-generated texts are\nlonger but less lexically rich, rely more on high-frequency words, and\nunder-represent nouns. Semantic vector space analysis revealed low similarity,\nhighlighting differences between the two corpora on the level of corpus\nsemantics. Few-shot prompt increased similarities between children and LLM text\nto a minor extent, but still failed to replicate lexical and semantic patterns.\nThe findings contribute to our understanding of how LLMs approximate child\nlanguage through multimodal prompting (text + image) and give insights into\ntheir use in psycholinguistic research and education while raising important\nquestions about the appropriateness of LLM-generated language in child-directed\neducational tools."
                },
                "authors": [
                    {
                        "name": "Hanna Woloszyn"
                    },
                    {
                        "name": "Benjamin Gagl"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Gagl"
                },
                "author": "Benjamin Gagl",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13769v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13769v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13764v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13764v1",
                "updated": "2025-08-19T12:00:59Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    12,
                    0,
                    59,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T12:00:59Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    12,
                    0,
                    59,
                    1,
                    231,
                    0
                ],
                "title": "Novel Probes of Quark-Gluon Plasma Evolution in Heavy-Ion Collisions\n  With the ATLAS Detector: From Structure of Colliding Nuclei to Collective\n  Expansion of Medium",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Novel Probes of Quark-Gluon Plasma Evolution in Heavy-Ion Collisions\n  With the ATLAS Detector: From Structure of Colliding Nuclei to Collective\n  Expansion of Medium"
                },
                "summary": "Understanding the properties of the quark-gluon plasma (QGP) offers insights\ninto the strong interaction and the conditions of the early universe.Since the\nQGP cannot be observed directly, its properties must be inferred from the\nparticles emitted as it cools. This dissertation introduces a suite of novel\nprobes based on event-by-event multi-particle correlations allowing us to work\nbackward from final-state particles to constrain the collective evolution,\ntransport properties, and initial conditions of the QGP. The studies utilize\n$^{208}\\mathrm{Pb}+{}^{208}\\mathrm{Pb}$ and\n$^{129}\\mathrm{Xe}+{}^{129}\\mathrm{Xe}$ collisions recorded by ATLAS detector\nat the LHC. First, evidence for the collective nature of the QGP's radial\nexpansion is provided using a transverse momentum\n($p_{\\mathrm{T}}$)-differential observable, $v_0(p_{\\mathrm{T}})$. This\nobservable exhibits genuine long-range correlations and factorizes into a\nsingle-particle property. Its strong sensitivity to bulk viscosity providing\nmore direct constraints on this medium property than traditional measures.\nSecond, the sources of event-wise fluctuations in radial flow within the\ninitial state are disentangled by analyzing higher moments of the event-wise\nmean-$p_{\\mathrm{T}}$ distribution. The average of this distribution is shown\nto constrain the speed of sound in the QGP. Finally, nuclear deformations,\nwhich control overlap area geometry, are constrained by comparing correlations\nbetween the anisotropic flow, $v_{n}$ and mean-$p_{\\mathrm{T}}$ in spherical\nPb+Pb and deformed Xe+Xe collisions. This comparison provides the first\nexperimental evidence for triaxial deformation in $^{129}$Xe. Together, these\nthree complementary analyses significantly improve our understanding of the\ninitial conditions, transport properties, and collective behavior of the QGP,\nestablishing new avenues for precision studies of QGP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the properties of the quark-gluon plasma (QGP) offers insights\ninto the strong interaction and the conditions of the early universe.Since the\nQGP cannot be observed directly, its properties must be inferred from the\nparticles emitted as it cools. This dissertation introduces a suite of novel\nprobes based on event-by-event multi-particle correlations allowing us to work\nbackward from final-state particles to constrain the collective evolution,\ntransport properties, and initial conditions of the QGP. The studies utilize\n$^{208}\\mathrm{Pb}+{}^{208}\\mathrm{Pb}$ and\n$^{129}\\mathrm{Xe}+{}^{129}\\mathrm{Xe}$ collisions recorded by ATLAS detector\nat the LHC. First, evidence for the collective nature of the QGP's radial\nexpansion is provided using a transverse momentum\n($p_{\\mathrm{T}}$)-differential observable, $v_0(p_{\\mathrm{T}})$. This\nobservable exhibits genuine long-range correlations and factorizes into a\nsingle-particle property. Its strong sensitivity to bulk viscosity providing\nmore direct constraints on this medium property than traditional measures.\nSecond, the sources of event-wise fluctuations in radial flow within the\ninitial state are disentangled by analyzing higher moments of the event-wise\nmean-$p_{\\mathrm{T}}$ distribution. The average of this distribution is shown\nto constrain the speed of sound in the QGP. Finally, nuclear deformations,\nwhich control overlap area geometry, are constrained by comparing correlations\nbetween the anisotropic flow, $v_{n}$ and mean-$p_{\\mathrm{T}}$ in spherical\nPb+Pb and deformed Xe+Xe collisions. This comparison provides the first\nexperimental evidence for triaxial deformation in $^{129}$Xe. Together, these\nthree complementary analyses significantly improve our understanding of the\ninitial conditions, transport properties, and collective behavior of the QGP,\nestablishing new avenues for precision studies of QGP."
                },
                "authors": [
                    {
                        "name": "Somadutta Bhatta"
                    }
                ],
                "author_detail": {
                    "name": "Somadutta Bhatta"
                },
                "author": "Somadutta Bhatta",
                "arxiv_comment": "Ph.D. Thesis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13764v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13764v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "nucl-ex",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "nucl-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13762v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13762v1",
                "updated": "2025-08-19T11:58:46Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    11,
                    58,
                    46,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T11:58:46Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    11,
                    58,
                    46,
                    1,
                    231,
                    0
                ],
                "title": "Deep Biomechanically-Guided Interpolation for Keypoint-Based Brain Shift\n  Registration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Biomechanically-Guided Interpolation for Keypoint-Based Brain Shift\n  Registration"
                },
                "summary": "Accurate compensation of brain shift is critical for maintaining the\nreliability of neuronavigation during neurosurgery. While keypoint-based\nregistration methods offer robustness to large deformations and topological\nchanges, they typically rely on simple geometric interpolators that ignore\ntissue biomechanics to create dense displacement fields. In this work, we\npropose a novel deep learning framework that estimates dense, physically\nplausible brain deformations from sparse matched keypoints. We first generate a\nlarge dataset of synthetic brain deformations using biomechanical simulations.\nThen, a residual 3D U-Net is trained to refine standard interpolation estimates\ninto biomechanically guided deformations. Experiments on a large set of\nsimulated displacement fields demonstrate that our method significantly\noutperforms classical interpolators, reducing by half the mean square error\nwhile introducing negligible computational overhead at inference time. Code\navailable at:\n\\href{https://github.com/tiago-assis/Deep-Biomechanical-Interpolator}{https://github.com/tiago-assis/Deep-Biomechanical-Interpolator}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate compensation of brain shift is critical for maintaining the\nreliability of neuronavigation during neurosurgery. While keypoint-based\nregistration methods offer robustness to large deformations and topological\nchanges, they typically rely on simple geometric interpolators that ignore\ntissue biomechanics to create dense displacement fields. In this work, we\npropose a novel deep learning framework that estimates dense, physically\nplausible brain deformations from sparse matched keypoints. We first generate a\nlarge dataset of synthetic brain deformations using biomechanical simulations.\nThen, a residual 3D U-Net is trained to refine standard interpolation estimates\ninto biomechanically guided deformations. Experiments on a large set of\nsimulated displacement fields demonstrate that our method significantly\noutperforms classical interpolators, reducing by half the mean square error\nwhile introducing negligible computational overhead at inference time. Code\navailable at:\n\\href{https://github.com/tiago-assis/Deep-Biomechanical-Interpolator}{https://github.com/tiago-assis/Deep-Biomechanical-Interpolator}."
                },
                "authors": [
                    {
                        "name": "Tiago Assis"
                    },
                    {
                        "name": "Ines P. Machado"
                    },
                    {
                        "name": "Benjamin Zwick"
                    },
                    {
                        "name": "Nuno C. Garcia"
                    },
                    {
                        "name": "Reuben Dorent"
                    }
                ],
                "author_detail": {
                    "name": "Reuben Dorent"
                },
                "author": "Reuben Dorent",
                "arxiv_comment": "Accepted at COLlaborative Intelligence and Autonomy in Image-guided\n  Surgery (COLAS) Workshop - MICCAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13762v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13762v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13761v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13761v1",
                "updated": "2025-08-19T11:57:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    11,
                    57,
                    58,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T11:57:58Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    11,
                    57,
                    58,
                    1,
                    231,
                    0
                ],
                "title": "Narrowing the discovery space of the cosmological 21-cm signal using\n  multi-wavelength constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Narrowing the discovery space of the cosmological 21-cm signal using\n  multi-wavelength constraints"
                },
                "summary": "The cosmic 21-cm signal is a promising probe of the early Universe, owing to\nits sensitivity to the thermal state of the neutral intergalactic medium (IGM)\nand properties of the first luminous sources. In this work, we constrain the\n21-cm signal and infer IGM properties by leveraging multi-wavelength synergies.\nThis builds on our earlier study, where we developed the synergy between\nhigh-redshift UV luminosity functions (UVLFs), cosmic X-ray and radio\nbackgrounds (CXB and CRB), 21-cm global signal non-detection from SARAS~3, and\n21-cm power spectrum upper limits from HERA, to constrain the astrophysical\nproperties of Population II galaxies. Through a combination of CXB and HERA\ndata, we find the IGM kinetic temperature to be $T_\\text{K}(z=15)\\lesssim\n7.7~\\text{K}$, $2.5~\\text{K} \\lesssim T_\\text{K}(z=10) \\lesssim 66~\\text{K}$,\nand $20~\\text{K}\\lesssim T_\\text{K}(z=6) \\lesssim 2078~\\text{K}$ at 95\\%\ncredible interval (C.I.). Similarly, CRB and HERA data place an upper limit on\nthe radio emission efficiency of galaxies, giving $T_\\text{rad}(z=15) \\lesssim\n47~\\text{K}$, $T_\\text{rad}(z=10)\\lesssim 51~\\text{K}$, and\n$T_\\text{rad}(z=6)\\lesssim 101~\\text{K}$. These constraints, strengthened by\nthe inclusion of UVLFs from HST and JWST, allow us to place the first\n\\textit{lower bound} on the cosmic 21-cm signal. We infer an absorption trough\nof depth ${-201~\\text{mK}\\lesssim T_\\text{21,min} \\lesssim -68~\\text{mK}}$ at\n$z_\\text{min}\\approx10-16$, and a power spectrum of $8.7~\\text{mK}^2 <\n\\Delta_{21}^2(z=15) < 197~\\text{mK}^2$ at $k=0.35~h\\text{Mpc}^{-1}$. Our\nresults provide promising predictions for upcoming 21-cm experiments, and\nhighlight the power of multi-wavelength synergies in constraining the early\nUniverse.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The cosmic 21-cm signal is a promising probe of the early Universe, owing to\nits sensitivity to the thermal state of the neutral intergalactic medium (IGM)\nand properties of the first luminous sources. In this work, we constrain the\n21-cm signal and infer IGM properties by leveraging multi-wavelength synergies.\nThis builds on our earlier study, where we developed the synergy between\nhigh-redshift UV luminosity functions (UVLFs), cosmic X-ray and radio\nbackgrounds (CXB and CRB), 21-cm global signal non-detection from SARAS~3, and\n21-cm power spectrum upper limits from HERA, to constrain the astrophysical\nproperties of Population II galaxies. Through a combination of CXB and HERA\ndata, we find the IGM kinetic temperature to be $T_\\text{K}(z=15)\\lesssim\n7.7~\\text{K}$, $2.5~\\text{K} \\lesssim T_\\text{K}(z=10) \\lesssim 66~\\text{K}$,\nand $20~\\text{K}\\lesssim T_\\text{K}(z=6) \\lesssim 2078~\\text{K}$ at 95\\%\ncredible interval (C.I.). Similarly, CRB and HERA data place an upper limit on\nthe radio emission efficiency of galaxies, giving $T_\\text{rad}(z=15) \\lesssim\n47~\\text{K}$, $T_\\text{rad}(z=10)\\lesssim 51~\\text{K}$, and\n$T_\\text{rad}(z=6)\\lesssim 101~\\text{K}$. These constraints, strengthened by\nthe inclusion of UVLFs from HST and JWST, allow us to place the first\n\\textit{lower bound} on the cosmic 21-cm signal. We infer an absorption trough\nof depth ${-201~\\text{mK}\\lesssim T_\\text{21,min} \\lesssim -68~\\text{mK}}$ at\n$z_\\text{min}\\approx10-16$, and a power spectrum of $8.7~\\text{mK}^2 <\n\\Delta_{21}^2(z=15) < 197~\\text{mK}^2$ at $k=0.35~h\\text{Mpc}^{-1}$. Our\nresults provide promising predictions for upcoming 21-cm experiments, and\nhighlight the power of multi-wavelength synergies in constraining the early\nUniverse."
                },
                "authors": [
                    {
                        "name": "Jiten Dhandha"
                    },
                    {
                        "name": "Anastasia Fialkov"
                    },
                    {
                        "name": "Thomas Gessey-Jones"
                    },
                    {
                        "name": "Harry T. J. Bevins"
                    },
                    {
                        "name": "Sandro Tacchella"
                    },
                    {
                        "name": "Simon Pochinda"
                    },
                    {
                        "name": "Eloy de Lera Acedo"
                    },
                    {
                        "name": "Saurabh Singh"
                    },
                    {
                        "name": "Rennan Barkana"
                    }
                ],
                "author_detail": {
                    "name": "Rennan Barkana"
                },
                "author": "Rennan Barkana",
                "arxiv_comment": "18 pages, 9 figures, 5 tables. Submitted to MNRAS. All comments\n  welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13761v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13761v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07571v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07571v2",
                "updated": "2025-08-19T11:54:07Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    11,
                    54,
                    7,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-11T03:05:36Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    3,
                    5,
                    36,
                    0,
                    223,
                    0
                ],
                "title": "Towards Theoretical Understanding of Transformer Test-Time Computing:\n  Investigation on In-Context Linear Regression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Theoretical Understanding of Transformer Test-Time Computing:\n  Investigation on In-Context Linear Regression"
                },
                "summary": "Using more test-time computation during language model inference, such as\ngenerating more intermediate thoughts or sampling multiple candidate answers,\nhas proven effective in significantly improving model performance. This paper\ntakes an initial step toward bridging the gap between practical language model\ninference and theoretical transformer analysis by incorporating randomness and\nsampling. We focus on in-context linear regression with continuous/binary\ncoefficients, where our framework simulates language model decoding through\nnoise injection and binary coefficient sampling. Through this framework, we\nprovide detailed analyses of widely adopted inference techniques. Supported by\nempirical results, our theoretical framework and analysis demonstrate the\npotential for offering new insights into understanding inference behaviors in\nreal-world language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using more test-time computation during language model inference, such as\ngenerating more intermediate thoughts or sampling multiple candidate answers,\nhas proven effective in significantly improving model performance. This paper\ntakes an initial step toward bridging the gap between practical language model\ninference and theoretical transformer analysis by incorporating randomness and\nsampling. We focus on in-context linear regression with continuous/binary\ncoefficients, where our framework simulates language model decoding through\nnoise injection and binary coefficient sampling. Through this framework, we\nprovide detailed analyses of widely adopted inference techniques. Supported by\nempirical results, our theoretical framework and analysis demonstrate the\npotential for offering new insights into understanding inference behaviors in\nreal-world language models."
                },
                "authors": [
                    {
                        "name": "Xingwu Chen"
                    },
                    {
                        "name": "Miao Lu"
                    },
                    {
                        "name": "Beining Wu"
                    },
                    {
                        "name": "Difan Zou"
                    }
                ],
                "author_detail": {
                    "name": "Difan Zou"
                },
                "author": "Difan Zou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07571v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07571v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13755v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13755v1",
                "updated": "2025-08-19T11:51:40Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    11,
                    51,
                    40,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T11:51:40Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    11,
                    51,
                    40,
                    1,
                    231,
                    0
                ],
                "title": "Depth-Breadth Synergy in RLVR: Unlocking LLM Reasoning Gains with\n  Adaptive Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Depth-Breadth Synergy in RLVR: Unlocking LLM Reasoning Gains with\n  Adaptive Exploration"
                },
                "summary": "Reinforcement Learning with Verifiable Reward (RLVR) has emerged as a\npowerful paradigm for unlocking reasoning capabilities in large language\nmodels, yet its full potential is hindered by two under-explored dimensions:\nDepth-the hardest problem a model can sample; Breadth-the number of instances\nconsumed in a single iteration. We dissect the popular GRPO algorithm and\nreveal a systematic bias: the cumulative-advantage disproportionately weights\nsamples with medium accuracy, while down-weighting the low-accuracy instances\nthat are crucial for pushing reasoning boundaries. To rectify the depth\nneglect, we introduce Difficulty Adaptive Rollout Sampling (DARS), which\nre-weights hard problems through targeted multi-stage rollouts, thereby\nincreasing the number of positive rollouts for hard problems. Empirically,\nnaively enlarging rollout size only accelerates convergence and even hurts\nPass@K. Our DARS, in contrast, delivers consistent Pass@K gains without extra\ninference cost at convergence. Just as we adaptively expanded the depth of\nexploration, we now ask whether aggressively scaling the breadth of training\ndata can further amplify reasoning gains. To this end, we intensely scale batch\nsize and replace PPO's mini-batch iterations with full-batch updates over\nmultiple epochs. Increasing breadth significantly enhances Pass@1 performance.\nLarge-breadth training sustains high token-level entropy, indicating continued\nexploration and reduced gradient noise. We further present DARS-B, which\naugments DARS with large breadth, and demonstrate simultaneous gains in Pass@K\nand Pass@1. The results confirm that breadth and adaptive exploration across\ndepth operate as orthogonal dimensions in RLVR, which are key to unleashing the\nreasoning power of RLVR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning with Verifiable Reward (RLVR) has emerged as a\npowerful paradigm for unlocking reasoning capabilities in large language\nmodels, yet its full potential is hindered by two under-explored dimensions:\nDepth-the hardest problem a model can sample; Breadth-the number of instances\nconsumed in a single iteration. We dissect the popular GRPO algorithm and\nreveal a systematic bias: the cumulative-advantage disproportionately weights\nsamples with medium accuracy, while down-weighting the low-accuracy instances\nthat are crucial for pushing reasoning boundaries. To rectify the depth\nneglect, we introduce Difficulty Adaptive Rollout Sampling (DARS), which\nre-weights hard problems through targeted multi-stage rollouts, thereby\nincreasing the number of positive rollouts for hard problems. Empirically,\nnaively enlarging rollout size only accelerates convergence and even hurts\nPass@K. Our DARS, in contrast, delivers consistent Pass@K gains without extra\ninference cost at convergence. Just as we adaptively expanded the depth of\nexploration, we now ask whether aggressively scaling the breadth of training\ndata can further amplify reasoning gains. To this end, we intensely scale batch\nsize and replace PPO's mini-batch iterations with full-batch updates over\nmultiple epochs. Increasing breadth significantly enhances Pass@1 performance.\nLarge-breadth training sustains high token-level entropy, indicating continued\nexploration and reduced gradient noise. We further present DARS-B, which\naugments DARS with large breadth, and demonstrate simultaneous gains in Pass@K\nand Pass@1. The results confirm that breadth and adaptive exploration across\ndepth operate as orthogonal dimensions in RLVR, which are key to unleashing the\nreasoning power of RLVR."
                },
                "authors": [
                    {
                        "name": "Zhicheng Yang"
                    },
                    {
                        "name": "Zhijiang Guo"
                    },
                    {
                        "name": "Yinya Huang"
                    },
                    {
                        "name": "Yongxin Wang"
                    },
                    {
                        "name": "Dongchun Xie"
                    },
                    {
                        "name": "Yiwei Wang"
                    },
                    {
                        "name": "Xiaodan Liang"
                    },
                    {
                        "name": "Jing Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jing Tang"
                },
                "author": "Jing Tang",
                "arxiv_comment": "11pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13755v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13755v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13754v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13754v1",
                "updated": "2025-08-19T11:51:15Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    11,
                    51,
                    15,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T11:51:15Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    11,
                    51,
                    15,
                    1,
                    231,
                    0
                ],
                "title": "Expertise-aware Multi-LLM Recruitment and Collaboration for Medical\n  Decision-Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Expertise-aware Multi-LLM Recruitment and Collaboration for Medical\n  Decision-Making"
                },
                "summary": "Medical Decision-Making (MDM) is a complex process requiring substantial\ndomain-specific expertise to effectively synthesize heterogeneous and\ncomplicated clinical information. While recent advancements in Large Language\nModels (LLMs) show promise in supporting MDM, single-LLM approaches are limited\nby their parametric knowledge constraints and static training corpora, failing\nto robustly integrate the clinical information. To address this challenge, we\npropose the Expertise-aware Multi-LLM Recruitment and Collaboration (EMRC)\nframework to enhance the accuracy and reliability of MDM systems. It operates\nin two stages: (i) expertise-aware agent recruitment and (ii) confidence- and\nadversarial-driven multi-agent collaboration. Specifically, in the first stage,\nwe use a publicly available corpus to construct an LLM expertise table for\ncapturing expertise-specific strengths of multiple LLMs across medical\ndepartment categories and query difficulty levels. This table enables the\nsubsequent dynamic selection of the optimal LLMs to act as medical expert\nagents for each medical query during the inference phase. In the second stage,\nwe employ selected agents to generate responses with self-assessed confidence\nscores, which are then integrated through the confidence fusion and adversarial\nvalidation to improve diagnostic reliability. We evaluate our EMRC framework on\nthree public MDM datasets, where the results demonstrate that our EMRC\noutperforms state-of-the-art single- and multi-LLM methods, achieving superior\ndiagnostic performance. For instance, on the MMLU-Pro-Health dataset, our EMRC\nachieves 74.45% accuracy, representing a 2.69% improvement over the\nbest-performing closed-source model GPT- 4-0613, which demonstrates the\neffectiveness of our expertise-aware agent recruitment strategy and the agent\ncomplementarity in leveraging each LLM's specialized capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical Decision-Making (MDM) is a complex process requiring substantial\ndomain-specific expertise to effectively synthesize heterogeneous and\ncomplicated clinical information. While recent advancements in Large Language\nModels (LLMs) show promise in supporting MDM, single-LLM approaches are limited\nby their parametric knowledge constraints and static training corpora, failing\nto robustly integrate the clinical information. To address this challenge, we\npropose the Expertise-aware Multi-LLM Recruitment and Collaboration (EMRC)\nframework to enhance the accuracy and reliability of MDM systems. It operates\nin two stages: (i) expertise-aware agent recruitment and (ii) confidence- and\nadversarial-driven multi-agent collaboration. Specifically, in the first stage,\nwe use a publicly available corpus to construct an LLM expertise table for\ncapturing expertise-specific strengths of multiple LLMs across medical\ndepartment categories and query difficulty levels. This table enables the\nsubsequent dynamic selection of the optimal LLMs to act as medical expert\nagents for each medical query during the inference phase. In the second stage,\nwe employ selected agents to generate responses with self-assessed confidence\nscores, which are then integrated through the confidence fusion and adversarial\nvalidation to improve diagnostic reliability. We evaluate our EMRC framework on\nthree public MDM datasets, where the results demonstrate that our EMRC\noutperforms state-of-the-art single- and multi-LLM methods, achieving superior\ndiagnostic performance. For instance, on the MMLU-Pro-Health dataset, our EMRC\nachieves 74.45% accuracy, representing a 2.69% improvement over the\nbest-performing closed-source model GPT- 4-0613, which demonstrates the\neffectiveness of our expertise-aware agent recruitment strategy and the agent\ncomplementarity in leveraging each LLM's specialized capabilities."
                },
                "authors": [
                    {
                        "name": "Liuxin Bao"
                    },
                    {
                        "name": "Zhihao Peng"
                    },
                    {
                        "name": "Xiaofei Zhou"
                    },
                    {
                        "name": "Runmin Cong"
                    },
                    {
                        "name": "Jiyong Zhang"
                    },
                    {
                        "name": "Yixuan Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Yixuan Yuan"
                },
                "author": "Yixuan Yuan",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13754v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13754v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14226v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14226v2",
                "updated": "2025-08-19T11:43:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    11,
                    43,
                    9,
                    1,
                    231,
                    0
                ],
                "published": "2025-05-20T11:35:25Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    11,
                    35,
                    25,
                    1,
                    140,
                    0
                ],
                "title": "\"Haet Bhasha aur Diskrimineshun\": Phonetic Perturbations in Code-Mixed\n  Hinglish to Red-Team LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"Haet Bhasha aur Diskrimineshun\": Phonetic Perturbations in Code-Mixed\n  Hinglish to Red-Team LLMs"
                },
                "summary": "Recently released LLMs have strong multilingual \\& multimodal capabilities.\nModel vulnerabilities are exposed using audits and red-teaming efforts.\nExisting efforts have focused primarily on the English language; thus, models\ncontinue to be susceptible to multilingual jailbreaking strategies, especially\nfor multimodal contexts. In this study, we introduce a novel strategy that\nleverages code-mixing and phonetic perturbations to jailbreak LLMs for both\ntext and image generation tasks. We also introduce \\textit{two new} jailbreak\nstrategies that show higher effectiveness than baselines. Our work presents a\nmethod to effectively bypass safety filters in LLMs while maintaining\ninterpretability by applying phonetic misspellings to sensitive words in\ncode-mixed prompts. We achieve a 99\\% Attack Success Rate for text generation\nand 78\\% for image generation, with Attack Relevance Rate of 100\\% for text\ngeneration and 95\\% for image generation for the phonetically perturbed\ncode-mixed prompts. Our interpretability experiments reveal that phonetic\nperturbations impact word tokenization, leading to jailbreak success. Our study\nmotivates increasing the focus towards more generalizable safety alignment for\nmultilingual multimodal models, especially in real-world settings wherein\nprompts can have misspelt words. \\textit{\\textbf{Warning: This paper contains\nexamples of potentially harmful and offensive content.}}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently released LLMs have strong multilingual \\& multimodal capabilities.\nModel vulnerabilities are exposed using audits and red-teaming efforts.\nExisting efforts have focused primarily on the English language; thus, models\ncontinue to be susceptible to multilingual jailbreaking strategies, especially\nfor multimodal contexts. In this study, we introduce a novel strategy that\nleverages code-mixing and phonetic perturbations to jailbreak LLMs for both\ntext and image generation tasks. We also introduce \\textit{two new} jailbreak\nstrategies that show higher effectiveness than baselines. Our work presents a\nmethod to effectively bypass safety filters in LLMs while maintaining\ninterpretability by applying phonetic misspellings to sensitive words in\ncode-mixed prompts. We achieve a 99\\% Attack Success Rate for text generation\nand 78\\% for image generation, with Attack Relevance Rate of 100\\% for text\ngeneration and 95\\% for image generation for the phonetically perturbed\ncode-mixed prompts. Our interpretability experiments reveal that phonetic\nperturbations impact word tokenization, leading to jailbreak success. Our study\nmotivates increasing the focus towards more generalizable safety alignment for\nmultilingual multimodal models, especially in real-world settings wherein\nprompts can have misspelt words. \\textit{\\textbf{Warning: This paper contains\nexamples of potentially harmful and offensive content.}}"
                },
                "authors": [
                    {
                        "name": "Darpan Aswal"
                    },
                    {
                        "name": "Siddharth D Jaiswal"
                    }
                ],
                "author_detail": {
                    "name": "Siddharth D Jaiswal"
                },
                "author": "Siddharth D Jaiswal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14226v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14226v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.12800v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.12800v2",
                "updated": "2025-08-19T11:40:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    11,
                    40,
                    33,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-18T10:23:10Z",
                "published_parsed": [
                    2025,
                    8,
                    18,
                    10,
                    23,
                    10,
                    0,
                    230,
                    0
                ],
                "title": "Atom-Searcher: Enhancing Agentic Deep Research via Fine-Grained Atomic\n  Thought Reward",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Atom-Searcher: Enhancing Agentic Deep Research via Fine-Grained Atomic\n  Thought Reward"
                },
                "summary": "Large language models (LLMs) exhibit remarkable problem-solving abilities,\nbut struggle with complex tasks due to static internal knowledge.\nRetrieval-Augmented Generation (RAG) enhances access to external information,\nyet remains limited in multi-hop reasoning and strategic search due to rigid\nworkflows. Recent advancements in agentic deep research empower LLMs to\nautonomously reason, search, and synthesize information. However, current\napproaches relying on outcome-based reinforcement learning (RL) face critical\nissues such as conflicting gradients and reward sparsity, limiting performance\ngains and training efficiency. To address these, we first propose Atomic\nThought, a novel LLM thinking paradigm that decomposes reasoning into\nfine-grained functional units. These units are supervised by Reasoning Reward\nModels (RRMs), which provide Atomic Thought Rewards (ATR) for fine-grained\nguidance. Building on this, we propose Atom-Searcher, a novel RL framework for\nagentic deep research that integrates Atomic Thought and ATR. Atom-Searcher\nuses a curriculum-inspired reward schedule, prioritizing process-level ATR\nearly and transitioning to outcome rewards, accelerating convergence on\neffective reasoning paths. Experiments on seven benchmarks show consistent\nimprovements over the state-of-the-art. Key advantages include: (1)\nAtom-Searcher scales computation at test-time. (2) Atomic Thought provides\nsupervision anchors for RRMs, bridging deep research tasks and RRMs. (3)\nAtom-Searcher exhibits more interpretable, human-like reasoning patterns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit remarkable problem-solving abilities,\nbut struggle with complex tasks due to static internal knowledge.\nRetrieval-Augmented Generation (RAG) enhances access to external information,\nyet remains limited in multi-hop reasoning and strategic search due to rigid\nworkflows. Recent advancements in agentic deep research empower LLMs to\nautonomously reason, search, and synthesize information. However, current\napproaches relying on outcome-based reinforcement learning (RL) face critical\nissues such as conflicting gradients and reward sparsity, limiting performance\ngains and training efficiency. To address these, we first propose Atomic\nThought, a novel LLM thinking paradigm that decomposes reasoning into\nfine-grained functional units. These units are supervised by Reasoning Reward\nModels (RRMs), which provide Atomic Thought Rewards (ATR) for fine-grained\nguidance. Building on this, we propose Atom-Searcher, a novel RL framework for\nagentic deep research that integrates Atomic Thought and ATR. Atom-Searcher\nuses a curriculum-inspired reward schedule, prioritizing process-level ATR\nearly and transitioning to outcome rewards, accelerating convergence on\neffective reasoning paths. Experiments on seven benchmarks show consistent\nimprovements over the state-of-the-art. Key advantages include: (1)\nAtom-Searcher scales computation at test-time. (2) Atomic Thought provides\nsupervision anchors for RRMs, bridging deep research tasks and RRMs. (3)\nAtom-Searcher exhibits more interpretable, human-like reasoning patterns."
                },
                "authors": [
                    {
                        "name": "Yong Deng"
                    },
                    {
                        "name": "Guoqing Wang"
                    },
                    {
                        "name": "Zhenzhe Ying"
                    },
                    {
                        "name": "Xiaofeng Wu"
                    },
                    {
                        "name": "Jinzhen Lin"
                    },
                    {
                        "name": "Wenwen Xiong"
                    },
                    {
                        "name": "Yuqin Dai"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Zhanwei Zhang"
                    },
                    {
                        "name": "Qiwen Wang"
                    },
                    {
                        "name": "Yang Qin"
                    },
                    {
                        "name": "Changhua Meng"
                    }
                ],
                "author_detail": {
                    "name": "Changhua Meng"
                },
                "author": "Changhua Meng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.12800v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.12800v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13250v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13250v2",
                "updated": "2025-08-19T11:40:15Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    11,
                    40,
                    15,
                    1,
                    231,
                    0
                ],
                "published": "2025-03-17T15:06:14Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    6,
                    14,
                    0,
                    76,
                    0
                ],
                "title": "MindEye-OmniAssist: A Gaze-Driven LLM-Enhanced Assistive Robot System\n  for Implicit Intention Recognition and Task Execution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MindEye-OmniAssist: A Gaze-Driven LLM-Enhanced Assistive Robot System\n  for Implicit Intention Recognition and Task Execution"
                },
                "summary": "A promising effective human-robot interaction in assistive robotic systems is\ngaze-based control. However, current gaze-based assistive systems mainly help\nusers with basic grasping actions, offering limited support. Moreover, the\nrestricted intent recognition capability constrains the assistive system's\nability to provide diverse assistance functions. In this paper, we propose an\nopen implicit intention recognition framework powered by Large Language Model\n(LLM) and Vision Foundation Model (VFM), which can process gaze input and\nrecognize user intents that are not confined to predefined or specific\nscenarios. Furthermore, we implement a gaze-driven LLM-enhanced assistive robot\nsystem (MindEye-OmniAssist) that recognizes user's intentions through gaze and\nassists in completing task. To achieve this, the system utilizes open\nvocabulary object detector, intention recognition network and LLM to infer\ntheir full intentions. By integrating eye movement feedback and LLM, it\ngenerates action sequences to assist the user in completing tasks. Real-world\nexperiments have been conducted for assistive tasks, and the system achieved an\noverall success rate of 41/55 across various undefined tasks. Preliminary\nresults show that the proposed method holds the potential to provide a more\nuser-friendly human-computer interaction interface and significantly enhance\nthe versatility and effectiveness of assistive systems by supporting more\ncomplex and diverse task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A promising effective human-robot interaction in assistive robotic systems is\ngaze-based control. However, current gaze-based assistive systems mainly help\nusers with basic grasping actions, offering limited support. Moreover, the\nrestricted intent recognition capability constrains the assistive system's\nability to provide diverse assistance functions. In this paper, we propose an\nopen implicit intention recognition framework powered by Large Language Model\n(LLM) and Vision Foundation Model (VFM), which can process gaze input and\nrecognize user intents that are not confined to predefined or specific\nscenarios. Furthermore, we implement a gaze-driven LLM-enhanced assistive robot\nsystem (MindEye-OmniAssist) that recognizes user's intentions through gaze and\nassists in completing task. To achieve this, the system utilizes open\nvocabulary object detector, intention recognition network and LLM to infer\ntheir full intentions. By integrating eye movement feedback and LLM, it\ngenerates action sequences to assist the user in completing tasks. Real-world\nexperiments have been conducted for assistive tasks, and the system achieved an\noverall success rate of 41/55 across various undefined tasks. Preliminary\nresults show that the proposed method holds the potential to provide a more\nuser-friendly human-computer interaction interface and significantly enhance\nthe versatility and effectiveness of assistive systems by supporting more\ncomplex and diverse task."
                },
                "authors": [
                    {
                        "name": "Zejia Zhang"
                    },
                    {
                        "name": "Bo Yang"
                    },
                    {
                        "name": "Xinxing Chen"
                    },
                    {
                        "name": "Weizhuang Shi"
                    },
                    {
                        "name": "Haoyuan Wang"
                    },
                    {
                        "name": "Wei Luo"
                    },
                    {
                        "name": "Jian Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Huang"
                },
                "author": "Jian Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13250v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13250v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13744v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13744v1",
                "updated": "2025-08-19T11:31:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    11,
                    31,
                    39,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T11:31:39Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    11,
                    31,
                    39,
                    1,
                    231,
                    0
                ],
                "title": "Mitigating Cross-Image Information Leakage in LVLMs for Multi-Image\n  Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Cross-Image Information Leakage in LVLMs for Multi-Image\n  Tasks"
                },
                "summary": "Large Vision-Language Models (LVLMs) demonstrate strong performance on\nsingle-image tasks. However, we observe that their performance degrades\nsignificantly when handling multi-image inputs. This occurs because visual cues\nfrom different images become entangled in the model's output. We refer to this\nphenomenon as cross-image information leakage. To address this issue, we\npropose FOCUS, a training-free and architecture-agnostic decoding strategy that\nmitigates cross-image information leakage during inference. FOCUS sequentially\nmasks all but one image with random noise, guiding the model to focus on the\nsingle clean image. We repeat this process across all target images to obtain\nlogits under partially masked contexts. These logits are aggregated and then\ncontrastively refined using a noise-only reference input, which suppresses the\nleakage and yields more accurate outputs. FOCUS consistently improves\nperformance across four multi-image benchmarks and diverse LVLM families. This\ndemonstrates that FOCUS offers a general and practical solution for enhancing\nmulti-image reasoning without additional training or architectural\nmodifications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Vision-Language Models (LVLMs) demonstrate strong performance on\nsingle-image tasks. However, we observe that their performance degrades\nsignificantly when handling multi-image inputs. This occurs because visual cues\nfrom different images become entangled in the model's output. We refer to this\nphenomenon as cross-image information leakage. To address this issue, we\npropose FOCUS, a training-free and architecture-agnostic decoding strategy that\nmitigates cross-image information leakage during inference. FOCUS sequentially\nmasks all but one image with random noise, guiding the model to focus on the\nsingle clean image. We repeat this process across all target images to obtain\nlogits under partially masked contexts. These logits are aggregated and then\ncontrastively refined using a noise-only reference input, which suppresses the\nleakage and yields more accurate outputs. FOCUS consistently improves\nperformance across four multi-image benchmarks and diverse LVLM families. This\ndemonstrates that FOCUS offers a general and practical solution for enhancing\nmulti-image reasoning without additional training or architectural\nmodifications."
                },
                "authors": [
                    {
                        "name": "Yeji Park"
                    },
                    {
                        "name": "Minyoung Lee"
                    },
                    {
                        "name": "Sanghyuk Chun"
                    },
                    {
                        "name": "Junsuk Choe"
                    }
                ],
                "author_detail": {
                    "name": "Junsuk Choe"
                },
                "author": "Junsuk Choe",
                "arxiv_comment": "Source code is available at https://github.com/yejipark-m/FOCUS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13744v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13744v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13743v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13743v1",
                "updated": "2025-08-19T11:30:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    11,
                    30,
                    52,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T11:30:52Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    11,
                    30,
                    52,
                    1,
                    231,
                    0
                ],
                "title": "Sycophancy under Pressure: Evaluating and Mitigating Sycophantic Bias\n  via Adversarial Dialogues in Scientific QA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sycophancy under Pressure: Evaluating and Mitigating Sycophantic Bias\n  via Adversarial Dialogues in Scientific QA"
                },
                "summary": "Large language models (LLMs), while increasingly used in domains requiring\nfactual rigor, often display a troubling behavior: sycophancy, the tendency to\nalign with user beliefs regardless of correctness. This tendency is reinforced\nby preference-based alignment techniques that optimize for user satisfaction\nbut can undermine truthfulness. While relatively benign in casual dialogue,\nsycophancy poses serious risks in high-stakes settings such as scientific\nquestion answering (QA), where model outputs may shape collaborative reasoning,\ndecision-making, and knowledge formation. Despite its importance, this\nphenomenon remains underexamined in factual QA contexts. We address this gap by\nintroducing a unified evaluation framework to quantify the impact of\nsycophantic context on model behavior in scientific QA, measuring how much\nuser-imposed social pressure distorts model outputs. The framework incorporates\nadversarial prompting setups and targeted metrics, such as misleading\nresistance and sycophancy resistance, that capture a model's ability to\nmaintain factual consistency under misleading cues. Systematic evaluations\nacross open-source and proprietary models reveal pervasive sycophantic\ntendencies, driven more by alignment strategy than by model size. To mitigate\nthis issue, we propose Pressure-Tune, a lightweight post-training method that\nfine-tunes models on synthetic adversarial dialogues paired with\nchain-of-thought rationales. These rationales reject user misinformation while\nreinforcing factual commitments. Experiments on challenging scientific QA\nbenchmarks show that Pressure-Tune significantly enhances sycophancy resistance\nwithout compromising accuracy or responsiveness to valid feedback, offering a\npractical pathway toward more truthful and principled model behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), while increasingly used in domains requiring\nfactual rigor, often display a troubling behavior: sycophancy, the tendency to\nalign with user beliefs regardless of correctness. This tendency is reinforced\nby preference-based alignment techniques that optimize for user satisfaction\nbut can undermine truthfulness. While relatively benign in casual dialogue,\nsycophancy poses serious risks in high-stakes settings such as scientific\nquestion answering (QA), where model outputs may shape collaborative reasoning,\ndecision-making, and knowledge formation. Despite its importance, this\nphenomenon remains underexamined in factual QA contexts. We address this gap by\nintroducing a unified evaluation framework to quantify the impact of\nsycophantic context on model behavior in scientific QA, measuring how much\nuser-imposed social pressure distorts model outputs. The framework incorporates\nadversarial prompting setups and targeted metrics, such as misleading\nresistance and sycophancy resistance, that capture a model's ability to\nmaintain factual consistency under misleading cues. Systematic evaluations\nacross open-source and proprietary models reveal pervasive sycophantic\ntendencies, driven more by alignment strategy than by model size. To mitigate\nthis issue, we propose Pressure-Tune, a lightweight post-training method that\nfine-tunes models on synthetic adversarial dialogues paired with\nchain-of-thought rationales. These rationales reject user misinformation while\nreinforcing factual commitments. Experiments on challenging scientific QA\nbenchmarks show that Pressure-Tune significantly enhances sycophancy resistance\nwithout compromising accuracy or responsiveness to valid feedback, offering a\npractical pathway toward more truthful and principled model behavior."
                },
                "authors": [
                    {
                        "name": "Kaiwei Zhang"
                    },
                    {
                        "name": "Qi Jia"
                    },
                    {
                        "name": "Zijian Chen"
                    },
                    {
                        "name": "Wei Sun"
                    },
                    {
                        "name": "Xiangyang Zhu"
                    },
                    {
                        "name": "Chunyi Li"
                    },
                    {
                        "name": "Dandan Zhu"
                    },
                    {
                        "name": "Guangtao Zhai"
                    }
                ],
                "author_detail": {
                    "name": "Guangtao Zhai"
                },
                "author": "Guangtao Zhai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13743v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13739v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13739v1",
                "updated": "2025-08-19T11:23:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    11,
                    23,
                    9,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T11:23:09Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    11,
                    23,
                    9,
                    1,
                    231,
                    0
                ],
                "title": "Enhancing Targeted Adversarial Attacks on Large Vision-Language Models\n  through Intermediate Projector Guidance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Targeted Adversarial Attacks on Large Vision-Language Models\n  through Intermediate Projector Guidance"
                },
                "summary": "Targeted adversarial attacks are essential for proactively identifying\nsecurity flaws in Vision-Language Models before real-world deployment. However,\ncurrent methods perturb images to maximize global similarity with the target\ntext or reference image at the encoder level, collapsing rich visual semantics\ninto a single global vector. This limits attack granularity, hindering\nfine-grained manipulations such as modifying a car while preserving its\nbackground. Furthermore, these methods largely overlook the projector module, a\ncritical semantic bridge between the visual encoder and the language model in\nVLMs, thereby failing to disrupt the full vision-language alignment pipeline\nwithin VLMs and limiting attack effectiveness. To address these issues, we\npropose the Intermediate Projector Guided Attack (IPGA), the first method to\nattack using the intermediate stage of the projector module, specifically the\nwidely adopted Q-Former, which transforms global image embeddings into\nfine-grained visual features. This enables more precise control over\nadversarial perturbations by operating on semantically meaningful visual tokens\nrather than a single global representation. Specifically, IPGA leverages the\nQ-Former pretrained solely on the first vision-language alignment stage,\nwithout LLM fine-tuning, which improves both attack effectiveness and\ntransferability across diverse VLMs. Furthermore, we propose Residual Query\nAlignment (RQA) to preserve unrelated visual content, thereby yielding more\ncontrolled and precise adversarial manipulations. Extensive experiments show\nthat our attack method consistently outperforms existing methods in both\nstandard global image captioning tasks and fine-grained visual\nquestion-answering tasks in black-box environment. Additionally, IPGA\nsuccessfully transfers to multiple commercial VLMs, including Google Gemini and\nOpenAI GPT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Targeted adversarial attacks are essential for proactively identifying\nsecurity flaws in Vision-Language Models before real-world deployment. However,\ncurrent methods perturb images to maximize global similarity with the target\ntext or reference image at the encoder level, collapsing rich visual semantics\ninto a single global vector. This limits attack granularity, hindering\nfine-grained manipulations such as modifying a car while preserving its\nbackground. Furthermore, these methods largely overlook the projector module, a\ncritical semantic bridge between the visual encoder and the language model in\nVLMs, thereby failing to disrupt the full vision-language alignment pipeline\nwithin VLMs and limiting attack effectiveness. To address these issues, we\npropose the Intermediate Projector Guided Attack (IPGA), the first method to\nattack using the intermediate stage of the projector module, specifically the\nwidely adopted Q-Former, which transforms global image embeddings into\nfine-grained visual features. This enables more precise control over\nadversarial perturbations by operating on semantically meaningful visual tokens\nrather than a single global representation. Specifically, IPGA leverages the\nQ-Former pretrained solely on the first vision-language alignment stage,\nwithout LLM fine-tuning, which improves both attack effectiveness and\ntransferability across diverse VLMs. Furthermore, we propose Residual Query\nAlignment (RQA) to preserve unrelated visual content, thereby yielding more\ncontrolled and precise adversarial manipulations. Extensive experiments show\nthat our attack method consistently outperforms existing methods in both\nstandard global image captioning tasks and fine-grained visual\nquestion-answering tasks in black-box environment. Additionally, IPGA\nsuccessfully transfers to multiple commercial VLMs, including Google Gemini and\nOpenAI GPT."
                },
                "authors": [
                    {
                        "name": "Yiming Cao"
                    },
                    {
                        "name": "Yanjie Li"
                    },
                    {
                        "name": "Kaisheng Liang"
                    },
                    {
                        "name": "Yuni Lai"
                    },
                    {
                        "name": "Bin Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Bin Xiao"
                },
                "author": "Bin Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13739v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13739v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06189v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06189v2",
                "updated": "2025-08-19T11:14:11Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    11,
                    14,
                    11,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-08T10:12:00Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    10,
                    12,
                    0,
                    4,
                    220,
                    0
                ],
                "title": "MA-CBP: A Criminal Behavior Prediction Framework Based on Multi-Agent\n  Asynchronous Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MA-CBP: A Criminal Behavior Prediction Framework Based on Multi-Agent\n  Asynchronous Collaboration"
                },
                "summary": "With the acceleration of urbanization, criminal behavior in public scenes\nposes an increasingly serious threat to social security. Traditional anomaly\ndetection methods based on feature recognition struggle to capture high-level\nbehavioral semantics from historical information, while generative approaches\nbased on Large Language Models (LLMs) often fail to meet real-time\nrequirements. To address these challenges, we propose MA-CBP, a criminal\nbehavior prediction framework based on multi-agent asynchronous collaboration.\nThis framework transforms real-time video streams into frame-level semantic\ndescriptions, constructs causally consistent historical summaries, and fuses\nadjacent image frames to perform joint reasoning over long- and short-term\ncontexts. The resulting behavioral decisions include key elements such as event\nsubjects, locations, and causes, enabling early warning of potential criminal\nactivity. In addition, we construct a high-quality criminal behavior dataset\nthat provides multi-scale language supervision, including frame-level,\nsummary-level, and event-level semantic annotations. Experimental results\ndemonstrate that our method achieves superior performance on multiple datasets\nand offers a promising solution for risk warning in urban public safety\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the acceleration of urbanization, criminal behavior in public scenes\nposes an increasingly serious threat to social security. Traditional anomaly\ndetection methods based on feature recognition struggle to capture high-level\nbehavioral semantics from historical information, while generative approaches\nbased on Large Language Models (LLMs) often fail to meet real-time\nrequirements. To address these challenges, we propose MA-CBP, a criminal\nbehavior prediction framework based on multi-agent asynchronous collaboration.\nThis framework transforms real-time video streams into frame-level semantic\ndescriptions, constructs causally consistent historical summaries, and fuses\nadjacent image frames to perform joint reasoning over long- and short-term\ncontexts. The resulting behavioral decisions include key elements such as event\nsubjects, locations, and causes, enabling early warning of potential criminal\nactivity. In addition, we construct a high-quality criminal behavior dataset\nthat provides multi-scale language supervision, including frame-level,\nsummary-level, and event-level semantic annotations. Experimental results\ndemonstrate that our method achieves superior performance on multiple datasets\nand offers a promising solution for risk warning in urban public safety\nscenarios."
                },
                "authors": [
                    {
                        "name": "Cheng Liu"
                    },
                    {
                        "name": "Daou Zhang"
                    },
                    {
                        "name": "Tingxu Liu"
                    },
                    {
                        "name": "Yuhan Wang"
                    },
                    {
                        "name": "Jinyang Chen"
                    },
                    {
                        "name": "Yuexuan Li"
                    },
                    {
                        "name": "Xinying Xiao"
                    },
                    {
                        "name": "Chenbo Xin"
                    },
                    {
                        "name": "Ziru Wang"
                    },
                    {
                        "name": "Weichao Wu"
                    }
                ],
                "author_detail": {
                    "name": "Weichao Wu"
                },
                "author": "Weichao Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06189v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06189v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13732v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13732v1",
                "updated": "2025-08-19T11:10:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    11,
                    10,
                    56,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T11:10:56Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    11,
                    10,
                    56,
                    1,
                    231,
                    0
                ],
                "title": "Self-Organizing Agent Network for LLM-based Workflow Automation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Organizing Agent Network for LLM-based Workflow Automation"
                },
                "summary": "Recent multi-agent frameworks built upon large language models (LLMs) have\ndemonstrated remarkable capabilities in complex task planning. However, in\nreal-world enterprise environments, business workflows are typically composed\nthrough modularization and reuse of numerous subprocesses, resulting in\nintricate workflows characterized by lengthy and deeply nested execution paths.\nSuch complexity poses significant challenges for LLM-driven orchestration, as\nextended reasoning chains and state-space explosions severely impact planning\neffectiveness and the proper sequencing of tool invocations. Therefore,\ndeveloping an orchestration method with controllable structures capable of\nhandling multi-layer nesting becomes a critical issue. To address this, we\npropose a novel structure-driven orchestration framework Self-Organizing Agent\nNetwork (SOAN). SOAN incrementally builds a formalized agent network by\nidentifying and encapsulating structural units as independent agents, enhancing\nmodularity and clarity in orchestration. Extensive evaluations were performed\nusing multiple benchmarks as well as a real-world enterprise workflow dataset.\nExperimental results demonstrate that SOAN significantly outperforms\nstate-of-the-art methods in terms of adaptability, fault tolerance, and\nexecution efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent multi-agent frameworks built upon large language models (LLMs) have\ndemonstrated remarkable capabilities in complex task planning. However, in\nreal-world enterprise environments, business workflows are typically composed\nthrough modularization and reuse of numerous subprocesses, resulting in\nintricate workflows characterized by lengthy and deeply nested execution paths.\nSuch complexity poses significant challenges for LLM-driven orchestration, as\nextended reasoning chains and state-space explosions severely impact planning\neffectiveness and the proper sequencing of tool invocations. Therefore,\ndeveloping an orchestration method with controllable structures capable of\nhandling multi-layer nesting becomes a critical issue. To address this, we\npropose a novel structure-driven orchestration framework Self-Organizing Agent\nNetwork (SOAN). SOAN incrementally builds a formalized agent network by\nidentifying and encapsulating structural units as independent agents, enhancing\nmodularity and clarity in orchestration. Extensive evaluations were performed\nusing multiple benchmarks as well as a real-world enterprise workflow dataset.\nExperimental results demonstrate that SOAN significantly outperforms\nstate-of-the-art methods in terms of adaptability, fault tolerance, and\nexecution efficiency."
                },
                "authors": [
                    {
                        "name": "Yiming Xiong"
                    },
                    {
                        "name": "Jian Wang"
                    },
                    {
                        "name": "Bing Li"
                    },
                    {
                        "name": "Yuhan Zhu"
                    },
                    {
                        "name": "Yuqi Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yuqi Zhao"
                },
                "author": "Yuqi Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13732v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13732v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13729v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13729v1",
                "updated": "2025-08-19T11:00:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    11,
                    0,
                    47,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T11:00:47Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    11,
                    0,
                    47,
                    1,
                    231,
                    0
                ],
                "title": "Prediction is not Explanation: Revisiting the Explanatory Capacity of\n  Mapping Embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prediction is not Explanation: Revisiting the Explanatory Capacity of\n  Mapping Embeddings"
                },
                "summary": "Understanding what knowledge is implicitly encoded in deep learning models is\nessential for improving the interpretability of AI systems. This paper examines\ncommon methods to explain the knowledge encoded in word embeddings, which are\ncore elements of large language models (LLMs). These methods typically involve\nmapping embeddings onto collections of human-interpretable semantic features,\nknown as feature norms. Prior work assumes that accurately predicting these\nsemantic features from the word embeddings implies that the embeddings contain\nthe corresponding knowledge. We challenge this assumption by demonstrating that\nprediction accuracy alone does not reliably indicate genuine feature-based\ninterpretability.\n  We show that these methods can successfully predict even random information,\nconcluding that the results are predominantly determined by an algorithmic\nupper bound rather than meaningful semantic representation in the word\nembeddings. Consequently, comparisons between datasets based solely on\nprediction performance do not reliably indicate which dataset is better\ncaptured by the word embeddings. Our analysis illustrates that such mappings\nprimarily reflect geometric similarity within vector spaces rather than\nindicating the genuine emergence of semantic properties.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding what knowledge is implicitly encoded in deep learning models is\nessential for improving the interpretability of AI systems. This paper examines\ncommon methods to explain the knowledge encoded in word embeddings, which are\ncore elements of large language models (LLMs). These methods typically involve\nmapping embeddings onto collections of human-interpretable semantic features,\nknown as feature norms. Prior work assumes that accurately predicting these\nsemantic features from the word embeddings implies that the embeddings contain\nthe corresponding knowledge. We challenge this assumption by demonstrating that\nprediction accuracy alone does not reliably indicate genuine feature-based\ninterpretability.\n  We show that these methods can successfully predict even random information,\nconcluding that the results are predominantly determined by an algorithmic\nupper bound rather than meaningful semantic representation in the word\nembeddings. Consequently, comparisons between datasets based solely on\nprediction performance do not reliably indicate which dataset is better\ncaptured by the word embeddings. Our analysis illustrates that such mappings\nprimarily reflect geometric similarity within vector spaces rather than\nindicating the genuine emergence of semantic properties."
                },
                "authors": [
                    {
                        "name": "Hanna Herasimchyk"
                    },
                    {
                        "name": "Alhassan Abdelhalim"
                    },
                    {
                        "name": "Sören Laue"
                    },
                    {
                        "name": "Michaela Regneri"
                    }
                ],
                "author_detail": {
                    "name": "Michaela Regneri"
                },
                "author": "Michaela Regneri",
                "arxiv_comment": "10 pages, 6 Figures. Published at ECAI 2025 in a version without the\n  Appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13729v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13729v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19244v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19244v2",
                "updated": "2025-08-19T10:54:02Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    10,
                    54,
                    2,
                    1,
                    231,
                    0
                ],
                "published": "2024-11-28T16:32:02Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    16,
                    32,
                    2,
                    3,
                    333,
                    0
                ],
                "title": "Consolidating and Developing Benchmarking Datasets for the Nepali\n  Natural Language Understanding Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Consolidating and Developing Benchmarking Datasets for the Nepali\n  Natural Language Understanding Tasks"
                },
                "summary": "The Nepali language has distinct linguistic features, especially its complex\nscript (Devanagari script), morphology, and various dialects,which pose a\nunique challenge for Natural Language Understanding (NLU) tasks. While the\nNepali Language Understanding Evaluation (Nep-gLUE) benchmark provides a\nfoundation for evaluating models, it remains limited in scope, covering four\ntasks. This restricts their utility for comprehensive assessments of Natural\nLanguage Processing (NLP) models. To address this limitation, we introduce\ntwelve new datasets, creating a new benchmark, the Nepali /Language\nUnderstanding Evaluation (NLUE) benchmark for evaluating the performance of\nmodels across a diverse set of Natural Language Understanding (NLU) tasks. The\nadded tasks include Single-Sentence Classification, Similarity and Paraphrase\nTasks, Natural Language Inference (NLI), and General Masked Evaluation Task\n(GMET). Through extensive experiments, we demonstrate that existing top models\nstruggle with the added complexity of these tasks. We also find that the best\nmultilingual model outperforms the best monolingual models across most tasks,\nhighlighting the need for more robust solutions tailored to the Nepali\nlanguage. This expanded benchmark sets a new standard for evaluating,\ncomparing, and advancing models, contributing significantly to the broader goal\nof advancing NLP research for low-resource languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Nepali language has distinct linguistic features, especially its complex\nscript (Devanagari script), morphology, and various dialects,which pose a\nunique challenge for Natural Language Understanding (NLU) tasks. While the\nNepali Language Understanding Evaluation (Nep-gLUE) benchmark provides a\nfoundation for evaluating models, it remains limited in scope, covering four\ntasks. This restricts their utility for comprehensive assessments of Natural\nLanguage Processing (NLP) models. To address this limitation, we introduce\ntwelve new datasets, creating a new benchmark, the Nepali /Language\nUnderstanding Evaluation (NLUE) benchmark for evaluating the performance of\nmodels across a diverse set of Natural Language Understanding (NLU) tasks. The\nadded tasks include Single-Sentence Classification, Similarity and Paraphrase\nTasks, Natural Language Inference (NLI), and General Masked Evaluation Task\n(GMET). Through extensive experiments, we demonstrate that existing top models\nstruggle with the added complexity of these tasks. We also find that the best\nmultilingual model outperforms the best monolingual models across most tasks,\nhighlighting the need for more robust solutions tailored to the Nepali\nlanguage. This expanded benchmark sets a new standard for evaluating,\ncomparing, and advancing models, contributing significantly to the broader goal\nof advancing NLP research for low-resource languages."
                },
                "authors": [
                    {
                        "name": "Jinu Nyachhyon"
                    },
                    {
                        "name": "Mridul Sharma"
                    },
                    {
                        "name": "Prajwal Thapa"
                    },
                    {
                        "name": "Bal Krishna Bal"
                    }
                ],
                "author_detail": {
                    "name": "Bal Krishna Bal"
                },
                "author": "Bal Krishna Bal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19244v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19244v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13727v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13727v1",
                "updated": "2025-08-19T10:53:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    10,
                    53,
                    5,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T10:53:05Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    10,
                    53,
                    5,
                    1,
                    231,
                    0
                ],
                "title": "Simulation of Impact-induced seismic shaking on asteroid (25143) Itokawa\n  to address its resurfacing process",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulation of Impact-induced seismic shaking on asteroid (25143) Itokawa\n  to address its resurfacing process"
                },
                "summary": "The surface of asteroid (25143) Itokawa shows both fresh and mature terrains,\ndespite its short space weathering timescale of approximately 1000 years, as\ninferred from recent studies. Seismic shaking triggered by the impact that\nformed the 8-meter Kamoi crater has been proposed as a possible explanation for\nthe diversity. This study aims to examine whether the seismic shaking induced\nby the impact could account for the observed spatial variations in space\nweathering and further constrain the internal structure of Itokawa. Assuming\nthat the Kamoi crater was formed by a recent impact, we conducted\nthree-dimensional seismic wave propagation simulations and applied a simplified\nlandslide model to estimate surface accelerations and boulder displacements.\nOur results show that even a low-energy case (1 % of the nominal seismic\nenergy) produces surface accelerations sufficient to destabilize the surface\nmaterials. The simulated boulder displacements are consistent with the observed\ndistribution of space weathering degrees even on the opposite hemisphere. We\nestimate the seismic diffusivity to be 1000-2000 m2 s-1 and the seismic\nefficiency to be in the range of 5.0 x 10-8 to 5.0 x 10-7, implying that\nItokawa's interior contains blocks tens of meters across and acts as a strongly\nscattering medium.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The surface of asteroid (25143) Itokawa shows both fresh and mature terrains,\ndespite its short space weathering timescale of approximately 1000 years, as\ninferred from recent studies. Seismic shaking triggered by the impact that\nformed the 8-meter Kamoi crater has been proposed as a possible explanation for\nthe diversity. This study aims to examine whether the seismic shaking induced\nby the impact could account for the observed spatial variations in space\nweathering and further constrain the internal structure of Itokawa. Assuming\nthat the Kamoi crater was formed by a recent impact, we conducted\nthree-dimensional seismic wave propagation simulations and applied a simplified\nlandslide model to estimate surface accelerations and boulder displacements.\nOur results show that even a low-energy case (1 % of the nominal seismic\nenergy) produces surface accelerations sufficient to destabilize the surface\nmaterials. The simulated boulder displacements are consistent with the observed\ndistribution of space weathering degrees even on the opposite hemisphere. We\nestimate the seismic diffusivity to be 1000-2000 m2 s-1 and the seismic\nefficiency to be in the range of 5.0 x 10-8 to 5.0 x 10-7, implying that\nItokawa's interior contains blocks tens of meters across and acts as a strongly\nscattering medium."
                },
                "authors": [
                    {
                        "name": "Sunho Jin"
                    },
                    {
                        "name": "Masateru Ishiguro"
                    }
                ],
                "author_detail": {
                    "name": "Masateru Ishiguro"
                },
                "author": "Masateru Ishiguro",
                "arxiv_comment": "12 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13727v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13727v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13721v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13721v1",
                "updated": "2025-08-19T10:37:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    10,
                    37,
                    20,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T10:37:20Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    10,
                    37,
                    20,
                    1,
                    231,
                    0
                ],
                "title": "CausalPlan: Empowering Efficient LLM Multi-Agent Collaboration Through\n  Causality-Driven Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CausalPlan: Empowering Efficient LLM Multi-Agent Collaboration Through\n  Causality-Driven Planning"
                },
                "summary": "Large language model (LLM) agents-especially smaller, open-source\nmodels-often produce causally invalid or incoherent actions in collaborative\ntasks due to their reliance on surface-level correlations rather than grounded\ncausal reasoning. This limitation undermines their performance in terms of\ncoordination and planning in dynamic environments. We address this challenge\nwith CausalPlan, a two-phase framework that integrates explicit structural\ncausal reasoning into the LLM planning process. At the core of CausalPlan is\nthe Structural Causal Action (SCA) model, which learns a causal graph from\nagent trajectories to capture how prior actions and current environment states\ninfluence future decisions. This structure is then used to guide action\nselection by assigning causal scores to LLM-generated proposals, reweighting\nthem accordingly, or falling back to causally grounded alternatives when\nneeded. By embedding this causal knowledge directly into the decision loop,\nCausalPlan constrains planning to intervention-consistent behaviours without\nrequiring fine-tuning of the LLM itself. We evaluate CausalPlan on the\nOvercooked-AI benchmark across five multi-agent coordination tasks and four\nLLMs of varying sizes: Gemma-7B, Llama-8B, Qwen-14B, and Llama-70B.\nExperimental results show that CausalPlan consistently reduces invalid actions\nand improves collaboration in both AI-AI and human-AI settings, outperforming\nstrong reinforcement learning baselines. Our findings highlight the value of\ncausality-driven planning for deploying efficient, interpretable, and\ngeneralisable multi-agent LLM systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) agents-especially smaller, open-source\nmodels-often produce causally invalid or incoherent actions in collaborative\ntasks due to their reliance on surface-level correlations rather than grounded\ncausal reasoning. This limitation undermines their performance in terms of\ncoordination and planning in dynamic environments. We address this challenge\nwith CausalPlan, a two-phase framework that integrates explicit structural\ncausal reasoning into the LLM planning process. At the core of CausalPlan is\nthe Structural Causal Action (SCA) model, which learns a causal graph from\nagent trajectories to capture how prior actions and current environment states\ninfluence future decisions. This structure is then used to guide action\nselection by assigning causal scores to LLM-generated proposals, reweighting\nthem accordingly, or falling back to causally grounded alternatives when\nneeded. By embedding this causal knowledge directly into the decision loop,\nCausalPlan constrains planning to intervention-consistent behaviours without\nrequiring fine-tuning of the LLM itself. We evaluate CausalPlan on the\nOvercooked-AI benchmark across five multi-agent coordination tasks and four\nLLMs of varying sizes: Gemma-7B, Llama-8B, Qwen-14B, and Llama-70B.\nExperimental results show that CausalPlan consistently reduces invalid actions\nand improves collaboration in both AI-AI and human-AI settings, outperforming\nstrong reinforcement learning baselines. Our findings highlight the value of\ncausality-driven planning for deploying efficient, interpretable, and\ngeneralisable multi-agent LLM systems."
                },
                "authors": [
                    {
                        "name": "Minh Hoang Nguyen"
                    },
                    {
                        "name": "Van Dai Do"
                    },
                    {
                        "name": "Dung Nguyen"
                    },
                    {
                        "name": "Thin Nguyen"
                    },
                    {
                        "name": "Hung Le"
                    }
                ],
                "author_detail": {
                    "name": "Hung Le"
                },
                "author": "Hung Le",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13721v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13721v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13718v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13718v1",
                "updated": "2025-08-19T10:28:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    10,
                    28,
                    53,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T10:28:53Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    10,
                    28,
                    53,
                    1,
                    231,
                    0
                ],
                "title": "Generics and Default Reasoning in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generics and Default Reasoning in Large Language Models"
                },
                "summary": "This paper evaluates the capabilities of 28 large language models (LLMs) to\nreason with 20 defeasible reasoning patterns involving generic generalizations\n(e.g., 'Birds fly', 'Ravens are black') central to non-monotonic logic.\nGenerics are of special interest to linguists, philosophers, logicians, and\ncognitive scientists because of their complex exception-permitting behaviour\nand their centrality to default reasoning, cognition, and concept acquisition.\nWe find that while several frontier models handle many default reasoning\nproblems well, performance varies widely across models and prompting styles.\nFew-shot prompting modestly improves performance for some models, but\nchain-of-thought (CoT) prompting often leads to serious performance degradation\n(mean accuracy drop -11.14%, SD 15.74% in models performing above 75% accuracy\nin zero-shot condition, temperature 0). Most models either struggle to\ndistinguish between defeasible and deductive inference or misinterpret generics\nas universal statements. These findings underscore both the promise and limits\nof current LLMs for default reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper evaluates the capabilities of 28 large language models (LLMs) to\nreason with 20 defeasible reasoning patterns involving generic generalizations\n(e.g., 'Birds fly', 'Ravens are black') central to non-monotonic logic.\nGenerics are of special interest to linguists, philosophers, logicians, and\ncognitive scientists because of their complex exception-permitting behaviour\nand their centrality to default reasoning, cognition, and concept acquisition.\nWe find that while several frontier models handle many default reasoning\nproblems well, performance varies widely across models and prompting styles.\nFew-shot prompting modestly improves performance for some models, but\nchain-of-thought (CoT) prompting often leads to serious performance degradation\n(mean accuracy drop -11.14%, SD 15.74% in models performing above 75% accuracy\nin zero-shot condition, temperature 0). Most models either struggle to\ndistinguish between defeasible and deductive inference or misinterpret generics\nas universal statements. These findings underscore both the promise and limits\nof current LLMs for default reasoning."
                },
                "authors": [
                    {
                        "name": "James Ravi Kirkpatrick"
                    },
                    {
                        "name": "Rachel Katharine Sterken"
                    }
                ],
                "author_detail": {
                    "name": "Rachel Katharine Sterken"
                },
                "author": "Rachel Katharine Sterken",
                "arxiv_comment": "33 pages, 26 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13718v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13718v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11290v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11290v2",
                "updated": "2025-08-19T09:59:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    9,
                    59,
                    5,
                    1,
                    231,
                    0
                ],
                "published": "2024-06-17T07:52:42Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    7,
                    52,
                    42,
                    0,
                    169,
                    0
                ],
                "title": "Iterative Utility Judgment Framework via LLMs Inspired by Relevance in\n  Philosophy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Iterative Utility Judgment Framework via LLMs Inspired by Relevance in\n  Philosophy"
                },
                "summary": "Relevance and utility are two frequently used measures to evaluate the\neffectiveness of an information retrieval (IR) system. Relevance emphasizes the\naboutness of a result to a query, while utility refers to the result's\nusefulness or value to an information seeker. In Retrieval-Augmented Generation\n(RAG), high-utility results should be prioritized to feed to LLMs due to their\nlimited input bandwidth. Re-examining RAG's three core components -- relevance\nranking derived from retrieval models, utility judgments, and answer generation\n-- aligns with Schutz's philosophical system of relevances, which encompasses\nthree types of relevance representing different levels of human cognition that\nenhance each other. These three RAG components also reflect three cognitive\nlevels for LLMs in question-answering. Therefore, we propose an Iterative\nutiliTy judgmEnt fraMework (ITEM) to promote each step in RAG. We conducted\nextensive experiments on retrieval (TREC DL, WebAP), utility judgment task\n(GTI-NQ), and factoid question-answering (NQ) datasets. Experimental results\ndemonstrate significant improvements of ITEM in utility judgments, ranking, and\nanswer generation upon representative baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Relevance and utility are two frequently used measures to evaluate the\neffectiveness of an information retrieval (IR) system. Relevance emphasizes the\naboutness of a result to a query, while utility refers to the result's\nusefulness or value to an information seeker. In Retrieval-Augmented Generation\n(RAG), high-utility results should be prioritized to feed to LLMs due to their\nlimited input bandwidth. Re-examining RAG's three core components -- relevance\nranking derived from retrieval models, utility judgments, and answer generation\n-- aligns with Schutz's philosophical system of relevances, which encompasses\nthree types of relevance representing different levels of human cognition that\nenhance each other. These three RAG components also reflect three cognitive\nlevels for LLMs in question-answering. Therefore, we propose an Iterative\nutiliTy judgmEnt fraMework (ITEM) to promote each step in RAG. We conducted\nextensive experiments on retrieval (TREC DL, WebAP), utility judgment task\n(GTI-NQ), and factoid question-answering (NQ) datasets. Experimental results\ndemonstrate significant improvements of ITEM in utility judgments, ranking, and\nanswer generation upon representative baselines."
                },
                "authors": [
                    {
                        "name": "Hengran Zhang"
                    },
                    {
                        "name": "Keping Bi"
                    },
                    {
                        "name": "Jiafeng Guo"
                    },
                    {
                        "name": "Xueqi Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xueqi Cheng"
                },
                "author": "Xueqi Cheng",
                "arxiv_comment": "22 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11290v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11290v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13697v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13697v1",
                "updated": "2025-08-19T09:58:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    9,
                    58,
                    24,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T09:58:24Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    9,
                    58,
                    24,
                    1,
                    231,
                    0
                ],
                "title": "The DeepLog Neurosymbolic Machine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The DeepLog Neurosymbolic Machine"
                },
                "summary": "We contribute a theoretical and operational framework for neurosymbolic AI\ncalled DeepLog. DeepLog introduces building blocks and primitives for\nneurosymbolic AI that make abstraction of commonly used representations and\ncomputational mechanisms used in neurosymbolic AI. DeepLog can represent and\nemulate a wide range of neurosymbolic systems. It consists of two key\ncomponents. The first is the DeepLog language for specifying neurosymbolic\nmodels and inference tasks. This language consists of an annotated neural\nextension of grounded first-order logic, and makes abstraction of the type of\nlogic, e.g. boolean, fuzzy or probabilistic, and whether logic is used in the\narchitecture or in the loss function. The second DeepLog component is situated\nat the computational level and uses extended algebraic circuits as\ncomputational graphs. Together these two components are to be considered as a\nneurosymbolic abstract machine, with the DeepLog language as the intermediate\nlevel of abstraction and the circuits level as the computational one. DeepLog\nis implemented in software, relies on the latest insights in implementing\nalgebraic circuits on GPUs, and is declarative in that it is easy to obtain\ndifferent neurosymbolic models by making different choices for the underlying\nalgebraic structures and logics. The generality and efficiency of the DeepLog\nneurosymbolic machine is demonstrated through an experimental comparison\nbetween 1) different fuzzy and probabilistic logics, 2) between using logic in\nthe architecture or in the loss function, and 3) between a standalone CPU-based\nimplementation of a neurosymbolic AI system and a DeepLog GPU-based one.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We contribute a theoretical and operational framework for neurosymbolic AI\ncalled DeepLog. DeepLog introduces building blocks and primitives for\nneurosymbolic AI that make abstraction of commonly used representations and\ncomputational mechanisms used in neurosymbolic AI. DeepLog can represent and\nemulate a wide range of neurosymbolic systems. It consists of two key\ncomponents. The first is the DeepLog language for specifying neurosymbolic\nmodels and inference tasks. This language consists of an annotated neural\nextension of grounded first-order logic, and makes abstraction of the type of\nlogic, e.g. boolean, fuzzy or probabilistic, and whether logic is used in the\narchitecture or in the loss function. The second DeepLog component is situated\nat the computational level and uses extended algebraic circuits as\ncomputational graphs. Together these two components are to be considered as a\nneurosymbolic abstract machine, with the DeepLog language as the intermediate\nlevel of abstraction and the circuits level as the computational one. DeepLog\nis implemented in software, relies on the latest insights in implementing\nalgebraic circuits on GPUs, and is declarative in that it is easy to obtain\ndifferent neurosymbolic models by making different choices for the underlying\nalgebraic structures and logics. The generality and efficiency of the DeepLog\nneurosymbolic machine is demonstrated through an experimental comparison\nbetween 1) different fuzzy and probabilistic logics, 2) between using logic in\nthe architecture or in the loss function, and 3) between a standalone CPU-based\nimplementation of a neurosymbolic AI system and a DeepLog GPU-based one."
                },
                "authors": [
                    {
                        "name": "Vincent Derkinderen"
                    },
                    {
                        "name": "Robin Manhaeve"
                    },
                    {
                        "name": "Rik Adriaensen"
                    },
                    {
                        "name": "Lucas Van Praet"
                    },
                    {
                        "name": "Lennert De Smet"
                    },
                    {
                        "name": "Giuseppe Marra"
                    },
                    {
                        "name": "Luc De Raedt"
                    }
                ],
                "author_detail": {
                    "name": "Luc De Raedt"
                },
                "author": "Luc De Raedt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13697v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13697v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13695v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13695v1",
                "updated": "2025-08-19T09:55:49Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    9,
                    55,
                    49,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T09:55:49Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    9,
                    55,
                    49,
                    1,
                    231,
                    0
                ],
                "title": "A Bayesian approach to time-domain Photonic Doppler Velocimetry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Bayesian approach to time-domain Photonic Doppler Velocimetry"
                },
                "summary": "Photonic Doppler Velocimetry (PDV) is an established technique for measuring\nthe velocities of fast-moving surfaces in high-energy-density experiments. In\nthe standard approach to PDV analysis, a short-time Fourier transform (STFT) is\nused to generate a spectrogram from which the velocity history of the target is\ninferred. The user chooses the form, duration and separation of the window\nfunction. Here we present a Bayesian approach to infer the velocity directly\nfrom the PDV oscilloscope trace, without using the spectrogram for analysis.\nThis is clearly a difficult inference problem due to the highly-periodic nature\nof the data, but we find that with carefully chosen prior distributions for the\nmodel parameters we can accurately recover the injected velocity from synthetic\ndata. We validate this method using PDV data collected at the STAR two-stage\nlight gas gun at Sandia National Laboratories, recovering shock-front\nvelocities in quartz that are consistent with those inferred using the\nSTFT-based approach, and are interpolated across regions of low signal-to-noise\ndata. Although this method does not rely on the same user choices as the STFT,\nwe caution that it can be prone to misspecification if the chosen model is not\nsufficient to capture the velocity behavior. Analysis using posterior\npredictive checks can be used to establish if a better model is required,\nalthough more complex models come with additional computational cost, often\ntaking more than several hours to converge when sampling the Bayesian\nposterior. We therefore recommend it be viewed as a complementary method to\nthat of the STFT-based approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Photonic Doppler Velocimetry (PDV) is an established technique for measuring\nthe velocities of fast-moving surfaces in high-energy-density experiments. In\nthe standard approach to PDV analysis, a short-time Fourier transform (STFT) is\nused to generate a spectrogram from which the velocity history of the target is\ninferred. The user chooses the form, duration and separation of the window\nfunction. Here we present a Bayesian approach to infer the velocity directly\nfrom the PDV oscilloscope trace, without using the spectrogram for analysis.\nThis is clearly a difficult inference problem due to the highly-periodic nature\nof the data, but we find that with carefully chosen prior distributions for the\nmodel parameters we can accurately recover the injected velocity from synthetic\ndata. We validate this method using PDV data collected at the STAR two-stage\nlight gas gun at Sandia National Laboratories, recovering shock-front\nvelocities in quartz that are consistent with those inferred using the\nSTFT-based approach, and are interpolated across regions of low signal-to-noise\ndata. Although this method does not rely on the same user choices as the STFT,\nwe caution that it can be prone to misspecification if the chosen model is not\nsufficient to capture the velocity behavior. Analysis using posterior\npredictive checks can be used to establish if a better model is required,\nalthough more complex models come with additional computational cost, often\ntaking more than several hours to converge when sampling the Bayesian\nposterior. We therefore recommend it be viewed as a complementary method to\nthat of the STFT-based approach."
                },
                "authors": [
                    {
                        "name": "J. R. Allison"
                    },
                    {
                        "name": "R. Bordas"
                    },
                    {
                        "name": "J. Read"
                    },
                    {
                        "name": "G. Burdiak"
                    },
                    {
                        "name": "V. Beltrán"
                    },
                    {
                        "name": "N. Joiner"
                    },
                    {
                        "name": "H. Doyle"
                    },
                    {
                        "name": "N. Hawker"
                    },
                    {
                        "name": "J. Skidmore"
                    },
                    {
                        "name": "T. Ao"
                    },
                    {
                        "name": "A. Porwitzky"
                    },
                    {
                        "name": "D. Dolan"
                    },
                    {
                        "name": "B. Farfan"
                    },
                    {
                        "name": "C. Johnson"
                    },
                    {
                        "name": "A. Hansen"
                    }
                ],
                "author_detail": {
                    "name": "A. Hansen"
                },
                "arxiv_affiliation": "Sandia National Laboratories",
                "author": "A. Hansen",
                "arxiv_doi": "10.1063/5.0267409",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1063/5.0267409",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.13695v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13695v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14 pages, 9 figures, manuscript accepted on 23rd July 2025 for\n  publication in Review of Scientific Instruments",
                "arxiv_journal_ref": "Rev. Sci. Instrum. 96, 085203 (2025)",
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04715v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04715v7",
                "updated": "2025-08-19T09:45:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    9,
                    45,
                    25,
                    1,
                    231,
                    0
                ],
                "published": "2025-03-06T18:58:29Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    18,
                    58,
                    29,
                    3,
                    65,
                    0
                ],
                "title": "Predictable Scale: Part I, Step Law -- Optimal Hyperparameter Scaling\n  Law in Large Language Model Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predictable Scale: Part I, Step Law -- Optimal Hyperparameter Scaling\n  Law in Large Language Model Pretraining"
                },
                "summary": "The impressive capabilities of Large Language Models (LLMs) across diverse\ntasks are now well established, yet their effective deployment necessitates\ncareful hyperparameter optimization. Although existing methods have explored\nthe influence of hyperparameters on model performance, a principled and\ngeneralizable framework across model architectures and data recipes remains\nabsent. In this study, we conduct an unprecedented empirical investigation\ntraining over 3,700 LLMs from scratch across 100 trillion tokens, consuming\nnearly one million NVIDIA H800 GPU hours to establish a universal Scaling Law\nfor hyperparameter optimization in LLM Pre-training, called Step Law. We\nempirically observe that, under fixed model size ($N$) and dataset size ($D$),\nthe hyperparameter landscape exhibits convexity with a broad optimum,\nsubstantially reducing the complexity of hyperparameter search. Building on\nthis insight, we formally define and empirically validate the Step Law: The\noptimal learning rate follows a power-law relationship with $N$ and $D$, while\nthe optimal batch size is primarily influenced by $D$ and remains largely\ninvariant to $N$.Notably, our estimated optima deviate from the global best\nperformance found via exhaustive search by merely 0.094\\% on the test set. To\nour best known, Step Law is the first that unifies different model shapes and\nstructures, such as Mixture-of-Experts models and dense transformers, as well\nas establishes optimal hyperparameter scaling laws across diverse data recipes.\nWe contribute a universal, plug-and-play optimal hyperparameter tool for the\ncommunity, which is expected to advance efficient LLM training at scale. All\nexperimental code, data and checkpoints are publicly available at\nhttps://github.com/step-law/steplaw",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The impressive capabilities of Large Language Models (LLMs) across diverse\ntasks are now well established, yet their effective deployment necessitates\ncareful hyperparameter optimization. Although existing methods have explored\nthe influence of hyperparameters on model performance, a principled and\ngeneralizable framework across model architectures and data recipes remains\nabsent. In this study, we conduct an unprecedented empirical investigation\ntraining over 3,700 LLMs from scratch across 100 trillion tokens, consuming\nnearly one million NVIDIA H800 GPU hours to establish a universal Scaling Law\nfor hyperparameter optimization in LLM Pre-training, called Step Law. We\nempirically observe that, under fixed model size ($N$) and dataset size ($D$),\nthe hyperparameter landscape exhibits convexity with a broad optimum,\nsubstantially reducing the complexity of hyperparameter search. Building on\nthis insight, we formally define and empirically validate the Step Law: The\noptimal learning rate follows a power-law relationship with $N$ and $D$, while\nthe optimal batch size is primarily influenced by $D$ and remains largely\ninvariant to $N$.Notably, our estimated optima deviate from the global best\nperformance found via exhaustive search by merely 0.094\\% on the test set. To\nour best known, Step Law is the first that unifies different model shapes and\nstructures, such as Mixture-of-Experts models and dense transformers, as well\nas establishes optimal hyperparameter scaling laws across diverse data recipes.\nWe contribute a universal, plug-and-play optimal hyperparameter tool for the\ncommunity, which is expected to advance efficient LLM training at scale. All\nexperimental code, data and checkpoints are publicly available at\nhttps://github.com/step-law/steplaw"
                },
                "authors": [
                    {
                        "name": "Houyi Li"
                    },
                    {
                        "name": "Wenzhen Zheng"
                    },
                    {
                        "name": "Qiufeng Wang"
                    },
                    {
                        "name": "Hanshan Zhang"
                    },
                    {
                        "name": "Zili Wang"
                    },
                    {
                        "name": "Shijie Xuyang"
                    },
                    {
                        "name": "Yuantao Fan"
                    },
                    {
                        "name": "Zhenyu Ding"
                    },
                    {
                        "name": "Haoying Wang"
                    },
                    {
                        "name": "Ning Ding"
                    },
                    {
                        "name": "Shuigeng Zhou"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Daxin Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Daxin Jiang"
                },
                "author": "Daxin Jiang",
                "arxiv_comment": "22 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04715v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04715v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.2; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05844v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05844v3",
                "updated": "2025-08-19T09:35:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    9,
                    35,
                    53,
                    1,
                    231,
                    0
                ],
                "published": "2024-11-06T15:32:28Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    15,
                    32,
                    28,
                    2,
                    311,
                    0
                ],
                "title": "LEGO-GraphRAG: Modularizing Graph-based Retrieval-Augmented Generation\n  for Design Space Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LEGO-GraphRAG: Modularizing Graph-based Retrieval-Augmented Generation\n  for Design Space Exploration"
                },
                "summary": "GraphRAG integrates (knowledge) graphs with large language models (LLMs) to\nimprove reasoning accuracy and contextual relevance. Despite its promising\napplications and strong relevance to multiple research communities, such as\ndatabases and natural language processing, GraphRAG currently lacks modular\nworkflow analysis, systematic solution frameworks, and insightful empirical\nstudies. To bridge these gaps, we propose LEGO-GraphRAG, a modular framework\nthat enables: 1) fine-grained decomposition of the GraphRAG workflow, 2)\nsystematic classification of existing techniques and implemented GraphRAG\ninstances, and 3) creation of new GraphRAG instances. Our framework facilitates\ncomprehensive empirical studies of GraphRAG on large-scale real-world graphs\nand diverse query sets, revealing insights into balancing reasoning quality,\nruntime efficiency, and token or GPU cost, that are essential for building\nadvanced GraphRAG systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphRAG integrates (knowledge) graphs with large language models (LLMs) to\nimprove reasoning accuracy and contextual relevance. Despite its promising\napplications and strong relevance to multiple research communities, such as\ndatabases and natural language processing, GraphRAG currently lacks modular\nworkflow analysis, systematic solution frameworks, and insightful empirical\nstudies. To bridge these gaps, we propose LEGO-GraphRAG, a modular framework\nthat enables: 1) fine-grained decomposition of the GraphRAG workflow, 2)\nsystematic classification of existing techniques and implemented GraphRAG\ninstances, and 3) creation of new GraphRAG instances. Our framework facilitates\ncomprehensive empirical studies of GraphRAG on large-scale real-world graphs\nand diverse query sets, revealing insights into balancing reasoning quality,\nruntime efficiency, and token or GPU cost, that are essential for building\nadvanced GraphRAG systems."
                },
                "authors": [
                    {
                        "name": "Yukun Cao"
                    },
                    {
                        "name": "Zengyi Gao"
                    },
                    {
                        "name": "Zhiyang Li"
                    },
                    {
                        "name": "Xike Xie"
                    },
                    {
                        "name": "S. Kevin Zhou"
                    },
                    {
                        "name": "Jianliang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jianliang Xu"
                },
                "author": "Jianliang Xu",
                "arxiv_doi": "10.14778/3748191.3748194",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.14778/3748191.3748194",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.05844v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05844v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "VLDB'2025 [Experiment, Analysis & Benchmark]",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05216v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05216v3",
                "updated": "2025-08-19T09:35:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    9,
                    35,
                    52,
                    1,
                    231,
                    0
                ],
                "published": "2025-04-07T16:03:59Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    16,
                    3,
                    59,
                    0,
                    97,
                    0
                ],
                "title": "Unleashing the Power of LLMs in Dense Retrieval with Query Likelihood\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing the Power of LLMs in Dense Retrieval with Query Likelihood\n  Modeling"
                },
                "summary": "Dense retrieval is a crucial task in Information Retrieval (IR), serving as\nthe basis for downstream tasks such as re-ranking and augmenting generation.\nRecently, large language models (LLMs) have demonstrated impressive semantic\nunderstanding capabilities, making them attractive to researchers focusing on\ndense retrieval. While LLMs, as decoder-style generative models, excel in\nlanguage generation, they often fall short in modeling global information due\nto a lack of attention to subsequent tokens. Drawing inspiration from the\nclassical word-based language modeling approach for IR, specifically the query\nlikelihood (QL) model, we aim to leverage the generative strengths of LLMs\nthrough QL maximization. Rather than employing QL estimation for document\nranking, we propose an auxiliary task of QL maximization to enhance the\nbackbone for subsequent contrastive learning of the retriever. We introduce our\nmodel, LLM-QL, which incorporates two key components: Attention Block (AB) and\nDocument Corruption (DC). AB blocks the attention of predictive tokens to the\ndocument tokens before the document's ending token, while DC corrupts a\ndocument by masking a portion of its tokens during prediction. Evaluations on\nthe in-domain (MS MARCO) and out-of-domain dataset (BEIR) indicate LLM-QL's\nsuperiority over other LLM-based retrievers. Furthermore, comprehensive\nanalyses also validate the efficacy of LLM-QL and its components.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dense retrieval is a crucial task in Information Retrieval (IR), serving as\nthe basis for downstream tasks such as re-ranking and augmenting generation.\nRecently, large language models (LLMs) have demonstrated impressive semantic\nunderstanding capabilities, making them attractive to researchers focusing on\ndense retrieval. While LLMs, as decoder-style generative models, excel in\nlanguage generation, they often fall short in modeling global information due\nto a lack of attention to subsequent tokens. Drawing inspiration from the\nclassical word-based language modeling approach for IR, specifically the query\nlikelihood (QL) model, we aim to leverage the generative strengths of LLMs\nthrough QL maximization. Rather than employing QL estimation for document\nranking, we propose an auxiliary task of QL maximization to enhance the\nbackbone for subsequent contrastive learning of the retriever. We introduce our\nmodel, LLM-QL, which incorporates two key components: Attention Block (AB) and\nDocument Corruption (DC). AB blocks the attention of predictive tokens to the\ndocument tokens before the document's ending token, while DC corrupts a\ndocument by masking a portion of its tokens during prediction. Evaluations on\nthe in-domain (MS MARCO) and out-of-domain dataset (BEIR) indicate LLM-QL's\nsuperiority over other LLM-based retrievers. Furthermore, comprehensive\nanalyses also validate the efficacy of LLM-QL and its components."
                },
                "authors": [
                    {
                        "name": "Hengran Zhang"
                    },
                    {
                        "name": "Keping Bi"
                    },
                    {
                        "name": "Jiafeng Guo"
                    },
                    {
                        "name": "Xiaojie Sun"
                    },
                    {
                        "name": "Shihao Liu"
                    },
                    {
                        "name": "Daiting Shi"
                    },
                    {
                        "name": "Dawei Yin"
                    },
                    {
                        "name": "Xueqi Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xueqi Cheng"
                },
                "author": "Xueqi Cheng",
                "arxiv_comment": "12 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05216v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05216v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13092v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13092v2",
                "updated": "2025-08-19T09:32:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    9,
                    32,
                    33,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-18T17:05:18Z",
                "published_parsed": [
                    2025,
                    8,
                    18,
                    17,
                    5,
                    18,
                    0,
                    230,
                    0
                ],
                "title": "VerilogLAVD: LLM-Aided Rule Generation for Vulnerability Detection in\n  Verilog",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VerilogLAVD: LLM-Aided Rule Generation for Vulnerability Detection in\n  Verilog"
                },
                "summary": "Timely detection of hardware vulnerabilities during the early design stage is\ncritical for reducing remediation costs. Existing early detection techniques\noften require specialized security expertise, limiting their usability. Recent\nefforts have explored the use of large language models (LLMs) for Verilog\nvulnerability detection. However, LLMs struggle to capture the structure in\nVerilog code, resulting in inconsistent detection results. To this end, we\npropose VerilogLAVD, the first LLM-aided graph traversal rule generation\napproach for Verilog vulnerability detection. Our approach introduces the\nVerilog Property Graph (VeriPG), a unified representation of Verilog code. It\ncombines syntactic features extracted from the abstract syntax tree (AST) with\nsemantic information derived from control flow and data dependency graphs. We\nleverage LLMs to generate VeriPG-based detection rules from Common Weakness\nEnumeration (CWE) descriptions. These rules guide the rule executor that\ntraversal VeriPG for potential vulnerabilities. To evaluate VerilogLAVD, we\nbuild a dataset collected from open-source repositories and synthesized data.\nIn our empirical evaluation on 77 Verilog designs encompassing 12 CWE types,\nVerilogLAVD achieves an F1-score of 0.54. Compared to the LLM-only and LLM with\nexternal knowledge baselines, VerilogLAVD improves F1-score by 0.31 and 0.27,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Timely detection of hardware vulnerabilities during the early design stage is\ncritical for reducing remediation costs. Existing early detection techniques\noften require specialized security expertise, limiting their usability. Recent\nefforts have explored the use of large language models (LLMs) for Verilog\nvulnerability detection. However, LLMs struggle to capture the structure in\nVerilog code, resulting in inconsistent detection results. To this end, we\npropose VerilogLAVD, the first LLM-aided graph traversal rule generation\napproach for Verilog vulnerability detection. Our approach introduces the\nVerilog Property Graph (VeriPG), a unified representation of Verilog code. It\ncombines syntactic features extracted from the abstract syntax tree (AST) with\nsemantic information derived from control flow and data dependency graphs. We\nleverage LLMs to generate VeriPG-based detection rules from Common Weakness\nEnumeration (CWE) descriptions. These rules guide the rule executor that\ntraversal VeriPG for potential vulnerabilities. To evaluate VerilogLAVD, we\nbuild a dataset collected from open-source repositories and synthesized data.\nIn our empirical evaluation on 77 Verilog designs encompassing 12 CWE types,\nVerilogLAVD achieves an F1-score of 0.54. Compared to the LLM-only and LLM with\nexternal knowledge baselines, VerilogLAVD improves F1-score by 0.31 and 0.27,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Xiang Long"
                    },
                    {
                        "name": "Yingjie Xia"
                    },
                    {
                        "name": "Xiyuan Chen"
                    },
                    {
                        "name": "Li Kuang"
                    }
                ],
                "author_detail": {
                    "name": "Li Kuang"
                },
                "author": "Li Kuang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13092v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13092v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13678v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13678v1",
                "updated": "2025-08-19T09:27:46Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    9,
                    27,
                    46,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T09:27:46Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    9,
                    27,
                    46,
                    1,
                    231,
                    0
                ],
                "title": "Neuro-Symbolic Artificial Intelligence: Towards Improving the Reasoning\n  Abilities of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neuro-Symbolic Artificial Intelligence: Towards Improving the Reasoning\n  Abilities of Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have shown promising results across various\ntasks, yet their reasoning capabilities remain a fundamental challenge.\nDeveloping AI systems with strong reasoning capabilities is regarded as a\ncrucial milestone in the pursuit of Artificial General Intelligence (AGI) and\nhas garnered considerable attention from both academia and industry. Various\ntechniques have been explored to enhance the reasoning capabilities of LLMs,\nwith neuro-symbolic approaches being a particularly promising way. This paper\ncomprehensively reviews recent developments in neuro-symbolic approaches for\nenhancing LLM reasoning. We first present a formalization of reasoning tasks\nand give a brief introduction to the neurosymbolic learning paradigm. Then, we\ndiscuss neuro-symbolic methods for improving the reasoning capabilities of LLMs\nfrom three perspectives: Symbolic->LLM, LLM->Symbolic, and LLM+Symbolic.\nFinally, we discuss several key challenges and promising future directions. We\nhave also released a GitHub repository including papers and resources related\nto this survey: https://github.com/LAMDASZ-ML/Awesome-LLM-Reasoning-with-NeSy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown promising results across various\ntasks, yet their reasoning capabilities remain a fundamental challenge.\nDeveloping AI systems with strong reasoning capabilities is regarded as a\ncrucial milestone in the pursuit of Artificial General Intelligence (AGI) and\nhas garnered considerable attention from both academia and industry. Various\ntechniques have been explored to enhance the reasoning capabilities of LLMs,\nwith neuro-symbolic approaches being a particularly promising way. This paper\ncomprehensively reviews recent developments in neuro-symbolic approaches for\nenhancing LLM reasoning. We first present a formalization of reasoning tasks\nand give a brief introduction to the neurosymbolic learning paradigm. Then, we\ndiscuss neuro-symbolic methods for improving the reasoning capabilities of LLMs\nfrom three perspectives: Symbolic->LLM, LLM->Symbolic, and LLM+Symbolic.\nFinally, we discuss several key challenges and promising future directions. We\nhave also released a GitHub repository including papers and resources related\nto this survey: https://github.com/LAMDASZ-ML/Awesome-LLM-Reasoning-with-NeSy."
                },
                "authors": [
                    {
                        "name": "Xiao-Wen Yang"
                    },
                    {
                        "name": "Jie-Jing Shao"
                    },
                    {
                        "name": "Lan-Zhe Guo"
                    },
                    {
                        "name": "Bo-Wen Zhang"
                    },
                    {
                        "name": "Zhi Zhou"
                    },
                    {
                        "name": "Lin-Han Jia"
                    },
                    {
                        "name": "Wang-Zhou Dai"
                    },
                    {
                        "name": "Yu-Feng Li"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Feng Li"
                },
                "author": "Yu-Feng Li",
                "arxiv_comment": "9 pages, 3 figures, IJCAI 2025 Survey Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13678v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13678v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09105v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09105v2",
                "updated": "2025-08-19T09:23:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    9,
                    23,
                    56,
                    1,
                    231,
                    0
                ],
                "published": "2024-12-12T09:35:47Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    9,
                    35,
                    47,
                    3,
                    347,
                    0
                ],
                "title": "ResFlow: Fine-tuning Residual Optical Flow for Event-based High Temporal\n  Resolution Motion Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ResFlow: Fine-tuning Residual Optical Flow for Event-based High Temporal\n  Resolution Motion Estimation"
                },
                "summary": "Event cameras hold significant promise for high-temporal-resolution (HTR)\nmotion estimation. However, estimating event-based HTR optical flow faces two\nkey challenges: the absence of HTR ground-truth data and the intrinsic sparsity\nof event data. Most existing approaches rely on the flow accumulation paradigms\nto indirectly supervise intermediate flows, often resulting in accumulation\nerrors and optimization difficulties. To address these challenges, we propose a\nresidual-based paradigm for estimating HTR optical flow with event data. Our\napproach separates HTR flow estimation into two stages: global linear motion\nestimation and HTR residual flow refinement. The residual paradigm effectively\nmitigates the impacts of event sparsity on optimization and is compatible with\nany LTR algorithm. Next, to address the challenge posed by the absence of HTR\nground truth, we incorporate novel learning strategies. Specifically, we\ninitially employ a shared refiner to estimate the residual flows, enabling both\nLTR supervision and HTR inference. Subsequently, we introduce regional noise to\nsimulate the residual patterns of intermediate flows, facilitating the\nadaptation from LTR supervision to HTR inference. Additionally, we show that\nthe noise-based strategy supports in-domain self-supervised training.\nComprehensive experimental results demonstrate that our approach achieves\nstate-of-the-art accuracy in both LTR and HTR metrics, highlighting its\neffectiveness and superiority.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Event cameras hold significant promise for high-temporal-resolution (HTR)\nmotion estimation. However, estimating event-based HTR optical flow faces two\nkey challenges: the absence of HTR ground-truth data and the intrinsic sparsity\nof event data. Most existing approaches rely on the flow accumulation paradigms\nto indirectly supervise intermediate flows, often resulting in accumulation\nerrors and optimization difficulties. To address these challenges, we propose a\nresidual-based paradigm for estimating HTR optical flow with event data. Our\napproach separates HTR flow estimation into two stages: global linear motion\nestimation and HTR residual flow refinement. The residual paradigm effectively\nmitigates the impacts of event sparsity on optimization and is compatible with\nany LTR algorithm. Next, to address the challenge posed by the absence of HTR\nground truth, we incorporate novel learning strategies. Specifically, we\ninitially employ a shared refiner to estimate the residual flows, enabling both\nLTR supervision and HTR inference. Subsequently, we introduce regional noise to\nsimulate the residual patterns of intermediate flows, facilitating the\nadaptation from LTR supervision to HTR inference. Additionally, we show that\nthe noise-based strategy supports in-domain self-supervised training.\nComprehensive experimental results demonstrate that our approach achieves\nstate-of-the-art accuracy in both LTR and HTR metrics, highlighting its\neffectiveness and superiority."
                },
                "authors": [
                    {
                        "name": "Qianang Zhou"
                    },
                    {
                        "name": "Zhiyu Zhu"
                    },
                    {
                        "name": "Junhui Hou"
                    },
                    {
                        "name": "Yongjian Deng"
                    },
                    {
                        "name": "Youfu Li"
                    },
                    {
                        "name": "Junlin Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Junlin Xiong"
                },
                "author": "Junlin Xiong",
                "arxiv_comment": "12 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09105v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09105v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13669v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13669v1",
                "updated": "2025-08-19T09:16:15Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    9,
                    16,
                    15,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T09:16:15Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    9,
                    16,
                    15,
                    1,
                    231,
                    0
                ],
                "title": "DeH4R: A Decoupled and Hybrid Method for Road Network Graph Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeH4R: A Decoupled and Hybrid Method for Road Network Graph Extraction"
                },
                "summary": "The automated extraction of complete and precise road network graphs from\nremote sensing imagery remains a critical challenge in geospatial computer\nvision. Segmentation-based approaches, while effective in pixel-level\nrecognition, struggle to maintain topology fidelity after vectorization\npostprocessing. Graph-growing methods build more topologically faithful graphs\nbut suffer from computationally prohibitive iterative ROI cropping.\nGraph-generating methods first predict global static candidate road network\nvertices, and then infer possible edges between vertices. They achieve fast\ntopology-aware inference, but limits the dynamic insertion of vertices. To\naddress these challenges, we propose DeH4R, a novel hybrid model that combines\ngraph-generating efficiency and graph-growing dynamics. This is achieved by\ndecoupling the task into candidate vertex detection, adjacent vertex\nprediction, initial graph contruction, and graph expansion. This architectural\ninnovation enables dynamic vertex (edge) insertions while retaining fast\ninference speed and enhancing both topology fidelity and spatial consistency.\nComprehensive evaluations on CityScale and SpaceNet benchmarks demonstrate\nstate-of-the-art (SOTA) performance. DeH4R outperforms the prior SOTA\ngraph-growing method RNGDet++ by 4.62 APLS and 10.18 IoU on CityScale, while\nbeing approximately 10 $\\times$ faster. The code will be made publicly\navailable at https://github.com/7777777FAN/DeH4R.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The automated extraction of complete and precise road network graphs from\nremote sensing imagery remains a critical challenge in geospatial computer\nvision. Segmentation-based approaches, while effective in pixel-level\nrecognition, struggle to maintain topology fidelity after vectorization\npostprocessing. Graph-growing methods build more topologically faithful graphs\nbut suffer from computationally prohibitive iterative ROI cropping.\nGraph-generating methods first predict global static candidate road network\nvertices, and then infer possible edges between vertices. They achieve fast\ntopology-aware inference, but limits the dynamic insertion of vertices. To\naddress these challenges, we propose DeH4R, a novel hybrid model that combines\ngraph-generating efficiency and graph-growing dynamics. This is achieved by\ndecoupling the task into candidate vertex detection, adjacent vertex\nprediction, initial graph contruction, and graph expansion. This architectural\ninnovation enables dynamic vertex (edge) insertions while retaining fast\ninference speed and enhancing both topology fidelity and spatial consistency.\nComprehensive evaluations on CityScale and SpaceNet benchmarks demonstrate\nstate-of-the-art (SOTA) performance. DeH4R outperforms the prior SOTA\ngraph-growing method RNGDet++ by 4.62 APLS and 10.18 IoU on CityScale, while\nbeing approximately 10 $\\times$ faster. The code will be made publicly\navailable at https://github.com/7777777FAN/DeH4R."
                },
                "authors": [
                    {
                        "name": "Dengxian Gong"
                    },
                    {
                        "name": "Shunping Ji"
                    }
                ],
                "author_detail": {
                    "name": "Shunping Ji"
                },
                "author": "Shunping Ji",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13669v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13669v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13666v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13666v1",
                "updated": "2025-08-19T09:13:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    9,
                    13,
                    48,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T09:13:48Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    9,
                    13,
                    48,
                    1,
                    231,
                    0
                ],
                "title": "The Hidden Cost of Readability: How Code Formatting Silently Consumes\n  Your LLM Budget",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Hidden Cost of Readability: How Code Formatting Silently Consumes\n  Your LLM Budget"
                },
                "summary": "Source code is usually formatted with elements like indentation and newlines\nto improve readability for human developers. However, these visual aids do not\nseem to be beneficial for large language models (LLMs) in the same way since\nthe code is processed as a linear sequence of tokens. Furthermore, these\nadditional tokens can lead to increased computational costs and longer response\ntimes for LLMs. If such formatting elements are non-essential to LLMs, we can\nreduce such costs by removing them from the code. To figure out the role played\nby formatting elements, we conduct a comprehensive empirical study to evaluate\nthe impact of code formatting on LLM performance and efficiency. Through\nlarge-scale experiments on Fill-in-the-Middle Code Completion tasks across four\nprogramming languages (Java, Python, C++, C\\#) and ten LLMs-including both\ncommercial and open-source models-we systematically analyze token count and\nperformance when formatting elements are removed. Key findings indicate that\nLLMs can maintain performance across formatted code and unformatted code,\nachieving an average input token reduction of 24.5\\% with negligible output\ntoken reductions. This makes code format removal a practical optimization\nstrategy for improving LLM efficiency. Further exploration reveals that both\nprompting and fine-tuning LLMs can lead to significant reductions (up to\n36.1\\%) in output code length without compromising correctness. To facilitate\npractical applications, we develop a bidirectional code transformation tool for\nformat processing, which can be seamlessly integrated into existing LLM\ninference workflows, ensuring both human readability and LLM efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Source code is usually formatted with elements like indentation and newlines\nto improve readability for human developers. However, these visual aids do not\nseem to be beneficial for large language models (LLMs) in the same way since\nthe code is processed as a linear sequence of tokens. Furthermore, these\nadditional tokens can lead to increased computational costs and longer response\ntimes for LLMs. If such formatting elements are non-essential to LLMs, we can\nreduce such costs by removing them from the code. To figure out the role played\nby formatting elements, we conduct a comprehensive empirical study to evaluate\nthe impact of code formatting on LLM performance and efficiency. Through\nlarge-scale experiments on Fill-in-the-Middle Code Completion tasks across four\nprogramming languages (Java, Python, C++, C\\#) and ten LLMs-including both\ncommercial and open-source models-we systematically analyze token count and\nperformance when formatting elements are removed. Key findings indicate that\nLLMs can maintain performance across formatted code and unformatted code,\nachieving an average input token reduction of 24.5\\% with negligible output\ntoken reductions. This makes code format removal a practical optimization\nstrategy for improving LLM efficiency. Further exploration reveals that both\nprompting and fine-tuning LLMs can lead to significant reductions (up to\n36.1\\%) in output code length without compromising correctness. To facilitate\npractical applications, we develop a bidirectional code transformation tool for\nformat processing, which can be seamlessly integrated into existing LLM\ninference workflows, ensuring both human readability and LLM efficiency."
                },
                "authors": [
                    {
                        "name": "Dangfeng Pan"
                    },
                    {
                        "name": "Zhensu Sun"
                    },
                    {
                        "name": "Cenyuan Zhang"
                    },
                    {
                        "name": "David Lo"
                    },
                    {
                        "name": "Xiaoning Du"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoning Du"
                },
                "author": "Xiaoning Du",
                "arxiv_comment": "Accepted by ICSE'26 (First Cycle)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13666v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13666v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00838v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00838v2",
                "updated": "2025-08-19T09:13:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    9,
                    13,
                    25,
                    1,
                    231,
                    0
                ],
                "published": "2025-01-01T13:40:09Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    13,
                    40,
                    9,
                    2,
                    1,
                    0
                ],
                "title": "Spatially-guided Temporal Aggregation for Robust Event-RGB Optical Flow\n  Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatially-guided Temporal Aggregation for Robust Event-RGB Optical Flow\n  Estimation"
                },
                "summary": "Current optical flow methods exploit the stable appearance of frame (or RGB)\ndata to establish robust correspondences across time. Event cameras, on the\nother hand, provide high-temporal-resolution motion cues and excel in\nchallenging scenarios. These complementary characteristics underscore the\npotential of integrating frame and event data for optical flow estimation.\nHowever, most cross-modal approaches fail to fully utilize the complementary\nadvantages, relying instead on simply stacking information. This study\nintroduces a novel approach that uses a spatially dense modality to guide the\naggregation of the temporally dense event modality, achieving effective\ncross-modal fusion. Specifically, we propose an event-enhanced frame\nrepresentation that preserves the rich texture of frames and the basic\nstructure of events. We use the enhanced representation as the guiding modality\nand employ events to capture temporally dense motion information. The robust\nmotion features derived from the guiding modality direct the aggregation of\nmotion information from events. To further enhance fusion, we propose a\ntransformer-based module that complements sparse event motion features with\nspatially rich frame information and enhances global information propagation.\nAdditionally, a mix-fusion encoder is designed to extract comprehensive\nspatiotemporal contextual features from both modalities. Extensive experiments\non the MVSEC and DSEC-Flow datasets demonstrate the effectiveness of our\nframework. Leveraging the complementary strengths of frames and events, our\nmethod achieves leading performance on the DSEC-Flow dataset. Compared to the\nevent-only model, frame guidance improves accuracy by 10\\%. Furthermore, it\noutperforms the state-of-the-art fusion-based method with a 4\\% accuracy gain\nand a 45\\% reduction in inference time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current optical flow methods exploit the stable appearance of frame (or RGB)\ndata to establish robust correspondences across time. Event cameras, on the\nother hand, provide high-temporal-resolution motion cues and excel in\nchallenging scenarios. These complementary characteristics underscore the\npotential of integrating frame and event data for optical flow estimation.\nHowever, most cross-modal approaches fail to fully utilize the complementary\nadvantages, relying instead on simply stacking information. This study\nintroduces a novel approach that uses a spatially dense modality to guide the\naggregation of the temporally dense event modality, achieving effective\ncross-modal fusion. Specifically, we propose an event-enhanced frame\nrepresentation that preserves the rich texture of frames and the basic\nstructure of events. We use the enhanced representation as the guiding modality\nand employ events to capture temporally dense motion information. The robust\nmotion features derived from the guiding modality direct the aggregation of\nmotion information from events. To further enhance fusion, we propose a\ntransformer-based module that complements sparse event motion features with\nspatially rich frame information and enhances global information propagation.\nAdditionally, a mix-fusion encoder is designed to extract comprehensive\nspatiotemporal contextual features from both modalities. Extensive experiments\non the MVSEC and DSEC-Flow datasets demonstrate the effectiveness of our\nframework. Leveraging the complementary strengths of frames and events, our\nmethod achieves leading performance on the DSEC-Flow dataset. Compared to the\nevent-only model, frame guidance improves accuracy by 10\\%. Furthermore, it\noutperforms the state-of-the-art fusion-based method with a 4\\% accuracy gain\nand a 45\\% reduction in inference time."
                },
                "authors": [
                    {
                        "name": "Qianang Zhou"
                    },
                    {
                        "name": "Junhui Hou"
                    },
                    {
                        "name": "Meiyi Yang"
                    },
                    {
                        "name": "Yongjian Deng"
                    },
                    {
                        "name": "Youfu Li"
                    },
                    {
                        "name": "Junlin Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Junlin Xiong"
                },
                "author": "Junlin Xiong",
                "arxiv_comment": "11 pages, 8 figures, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00838v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00838v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.12992v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.12992v2",
                "updated": "2025-08-19T09:08:17Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    9,
                    8,
                    17,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-18T15:12:27Z",
                "published_parsed": [
                    2025,
                    8,
                    18,
                    15,
                    12,
                    27,
                    0,
                    230,
                    0
                ],
                "title": "MAGNeT: Multimodal Adaptive Gaussian Networks for Intent Inference in\n  Moving Target Selection across Complex Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAGNeT: Multimodal Adaptive Gaussian Networks for Intent Inference in\n  Moving Target Selection across Complex Scenarios"
                },
                "summary": "Moving target selection in multimedia interactive systems faces unprecedented\nchallenges as users increasingly interact across diverse and dynamic\ncontexts-from live streaming in moving vehicles to VR gaming in varying\nenvironments. Existing approaches rely on probabilistic models that relate\nendpoint distribution to target properties such as size and speed. However,\nthese methods require substantial training data for each new context and lack\ntransferability across scenarios, limiting their practical deployment in\ndiverse multimedia environments where rich multimodal contextual information is\nreadily available. This paper introduces MAGNeT (Multimodal Adaptive Gaussian\nNetworks), which addresses these problems by combining classical statistical\nmodeling with a context-aware multimodal method. MAGNeT dynamically fuses\npre-fitted Ternary-Gaussian models from various scenarios based on real-time\ncontextual cues, enabling effective adaptation with minimal training data while\npreserving model interpretability. We conduct experiments on self-constructed\n2D and 3D moving target selection datasets under in-vehicle vibration\nconditions. Extensive experiments demonstrate that MAGNeT achieves lower error\nrates with few-shot samples by applying context-aware fusion of Gaussian\nexperts from multi-factor conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Moving target selection in multimedia interactive systems faces unprecedented\nchallenges as users increasingly interact across diverse and dynamic\ncontexts-from live streaming in moving vehicles to VR gaming in varying\nenvironments. Existing approaches rely on probabilistic models that relate\nendpoint distribution to target properties such as size and speed. However,\nthese methods require substantial training data for each new context and lack\ntransferability across scenarios, limiting their practical deployment in\ndiverse multimedia environments where rich multimodal contextual information is\nreadily available. This paper introduces MAGNeT (Multimodal Adaptive Gaussian\nNetworks), which addresses these problems by combining classical statistical\nmodeling with a context-aware multimodal method. MAGNeT dynamically fuses\npre-fitted Ternary-Gaussian models from various scenarios based on real-time\ncontextual cues, enabling effective adaptation with minimal training data while\npreserving model interpretability. We conduct experiments on self-constructed\n2D and 3D moving target selection datasets under in-vehicle vibration\nconditions. Extensive experiments demonstrate that MAGNeT achieves lower error\nrates with few-shot samples by applying context-aware fusion of Gaussian\nexperts from multi-factor conditions."
                },
                "authors": [
                    {
                        "name": "Xiangxian Li"
                    },
                    {
                        "name": "Yawen Zheng"
                    },
                    {
                        "name": "Baiqiao Zhang"
                    },
                    {
                        "name": "Yijia Ma"
                    },
                    {
                        "name": "Xianhui Cao"
                    },
                    {
                        "name": "Juan Liu"
                    },
                    {
                        "name": "Yulong Bian"
                    },
                    {
                        "name": "Jin Huang"
                    },
                    {
                        "name": "Chenglei Yang"
                    }
                ],
                "author_detail": {
                    "name": "Chenglei Yang"
                },
                "author": "Chenglei Yang",
                "arxiv_comment": "Accepted by ACM MM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.12992v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.12992v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09190v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09190v2",
                "updated": "2025-08-19T09:06:26Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    9,
                    6,
                    26,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-08T03:20:25Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    3,
                    20,
                    25,
                    4,
                    220,
                    0
                ],
                "title": "Fine-Grained Safety Neurons with Training-Free Continual Projection to\n  Reduce LLM Fine Tuning Risks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Grained Safety Neurons with Training-Free Continual Projection to\n  Reduce LLM Fine Tuning Risks"
                },
                "summary": "Fine-tuning as service injects domain-specific knowledge into large language\nmodels (LLMs), while challenging the original alignment mechanisms and\nintroducing safety risks. A series of defense strategies have been proposed for\nthe alignment, fine-tuning, and post-fine-tuning phases, where most\npost-fine-tuning defenses rely on coarse-grained safety layer mapping. These\nmethods lack a comprehensive consideration of both safety layers and\nfine-grained neurons, limiting their ability to efficiently balance safety and\nutility. To address this, we propose the Fine-Grained Safety Neurons (FGSN)\nwith Training-Free Continual Projection method to reduce the fine-tuning safety\nrisks. FGSN inherently integrates the multi-scale interactions between safety\nlayers and neurons, localizing sparser and more precise fine-grained safety\nneurons while minimizing interference with downstream task neurons. We then\nproject the safety neuron parameters onto safety directions, improving model\nsafety while aligning more closely with human preferences. Extensive\nexperiments across multiple fine-tuned LLM models demonstrate that our method\nsignificantly reduce harmfulness scores and attack success rates with minimal\nparameter modifications, while preserving the model's utility. Furthermore, by\nintroducing a task-specific, multi-dimensional heterogeneous safety neuron\ncluster optimization mechanism, we achieve continual defense and generalization\ncapability against unforeseen emerging safety concerns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning as service injects domain-specific knowledge into large language\nmodels (LLMs), while challenging the original alignment mechanisms and\nintroducing safety risks. A series of defense strategies have been proposed for\nthe alignment, fine-tuning, and post-fine-tuning phases, where most\npost-fine-tuning defenses rely on coarse-grained safety layer mapping. These\nmethods lack a comprehensive consideration of both safety layers and\nfine-grained neurons, limiting their ability to efficiently balance safety and\nutility. To address this, we propose the Fine-Grained Safety Neurons (FGSN)\nwith Training-Free Continual Projection method to reduce the fine-tuning safety\nrisks. FGSN inherently integrates the multi-scale interactions between safety\nlayers and neurons, localizing sparser and more precise fine-grained safety\nneurons while minimizing interference with downstream task neurons. We then\nproject the safety neuron parameters onto safety directions, improving model\nsafety while aligning more closely with human preferences. Extensive\nexperiments across multiple fine-tuned LLM models demonstrate that our method\nsignificantly reduce harmfulness scores and attack success rates with minimal\nparameter modifications, while preserving the model's utility. Furthermore, by\nintroducing a task-specific, multi-dimensional heterogeneous safety neuron\ncluster optimization mechanism, we achieve continual defense and generalization\ncapability against unforeseen emerging safety concerns."
                },
                "authors": [
                    {
                        "name": "Bing Han"
                    },
                    {
                        "name": "Feifei Zhao"
                    },
                    {
                        "name": "Dongcheng Zhao"
                    },
                    {
                        "name": "Guobin Shen"
                    },
                    {
                        "name": "Ping Wu"
                    },
                    {
                        "name": "Yu Shi"
                    },
                    {
                        "name": "Yi Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Yi Zeng"
                },
                "author": "Yi Zeng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09190v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09190v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13654v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13654v2",
                "updated": "2025-08-20T06:41:59Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    6,
                    41,
                    59,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-19T09:04:13Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    9,
                    4,
                    13,
                    1,
                    231,
                    0
                ],
                "title": "Input Time Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Input Time Scaling"
                },
                "summary": "Current Large Language Models (LLMs) are usually post-trained on large-scale\ncarefully curated datasets (data & training scaling) and doing reasoning in\ntest time (inference time scaling). In this work, we present a new scaling\nparadigm, Input Time Scaling, to complement previous scaling methods by putting\nresources on queries (input time). During training and testing, we combine\nmeta-knowledge from LLMs to refine inputs with different strategies. We also\nfind a new phenomenon, training-testing co-design there. We need to apply query\nstrategies during both training and testing. Only applying strategies on\ntraining or testing would seriously degrade the performance. We are also\nsurprised to find that seemingly low data quality datasets can gain high\nperformance. Adding irrelevant information to the queries, randomly selecting\nexamples from a minimally filtered dataset, can even perform the best. These\nfindings contradict the widely held inductive bias, \"garbage in, garbage out\".\nCurating datasets with seemingly high-quality data can even potentially limit\nthe performance ceiling. In addition, models trained on more data with similar\nquality (15k VS 1k) perform worse, simple dataset size scaling should also be\ncarefully inspected. The good news is that our findings are compatible with the\nLess is More phenomenon. A small set of examples is enough to evoke high-level\nreasoning ability. With experiments on models trained on Qwen2.5-32B-Instruct,\nwe are able to reach SOTA performance among 32B models on AIME24(76.7%) and\nAIME25(76.7%) pass@1. We can further achieve AIME24(76.7%) and AIME25(80%) with\na majority vote of three models. Starting from DeepSeek-R1-Distill-Qwen-32B,\nthe best result would be 86.7% on AIME24 and 76.7% on AIME25. To facilitate\nreproducibility and further research, we are working on open-source our\ndatasets, data pipelines, evaluation results, and checkpoints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current Large Language Models (LLMs) are usually post-trained on large-scale\ncarefully curated datasets (data & training scaling) and doing reasoning in\ntest time (inference time scaling). In this work, we present a new scaling\nparadigm, Input Time Scaling, to complement previous scaling methods by putting\nresources on queries (input time). During training and testing, we combine\nmeta-knowledge from LLMs to refine inputs with different strategies. We also\nfind a new phenomenon, training-testing co-design there. We need to apply query\nstrategies during both training and testing. Only applying strategies on\ntraining or testing would seriously degrade the performance. We are also\nsurprised to find that seemingly low data quality datasets can gain high\nperformance. Adding irrelevant information to the queries, randomly selecting\nexamples from a minimally filtered dataset, can even perform the best. These\nfindings contradict the widely held inductive bias, \"garbage in, garbage out\".\nCurating datasets with seemingly high-quality data can even potentially limit\nthe performance ceiling. In addition, models trained on more data with similar\nquality (15k VS 1k) perform worse, simple dataset size scaling should also be\ncarefully inspected. The good news is that our findings are compatible with the\nLess is More phenomenon. A small set of examples is enough to evoke high-level\nreasoning ability. With experiments on models trained on Qwen2.5-32B-Instruct,\nwe are able to reach SOTA performance among 32B models on AIME24(76.7%) and\nAIME25(76.7%) pass@1. We can further achieve AIME24(76.7%) and AIME25(80%) with\na majority vote of three models. Starting from DeepSeek-R1-Distill-Qwen-32B,\nthe best result would be 86.7% on AIME24 and 76.7% on AIME25. To facilitate\nreproducibility and further research, we are working on open-source our\ndatasets, data pipelines, evaluation results, and checkpoints."
                },
                "authors": [
                    {
                        "name": "Rapheal Huang"
                    },
                    {
                        "name": "Weilong Guo"
                    }
                ],
                "author_detail": {
                    "name": "Weilong Guo"
                },
                "arxiv_affiliation": "Yuming",
                "author": "Weilong Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13654v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13654v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13650v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13650v1",
                "updated": "2025-08-19T09:01:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    9,
                    1,
                    22,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T09:01:22Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    9,
                    1,
                    22,
                    1,
                    231,
                    0
                ],
                "title": "CRISP: Persistent Concept Unlearning via Sparse Autoencoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CRISP: Persistent Concept Unlearning via Sparse Autoencoders"
                },
                "summary": "As large language models (LLMs) are increasingly deployed in real-world\napplications, the need to selectively remove unwanted knowledge while\npreserving model utility has become paramount. Recent work has explored sparse\nautoencoders (SAEs) to perform precise interventions on monosemantic features.\nHowever, most SAE-based methods operate at inference time, which does not\ncreate persistent changes in the model's parameters. Such interventions can be\nbypassed or reversed by malicious actors with parameter access. We introduce\nCRISP, a parameter-efficient method for persistent concept unlearning using\nSAEs. CRISP automatically identifies salient SAE features across multiple\nlayers and suppresses their activations. We experiment with two LLMs and show\nthat our method outperforms prior approaches on safety-critical unlearning\ntasks from the WMDP benchmark, successfully removing harmful knowledge while\npreserving general and in-domain capabilities. Feature-level analysis reveals\nthat CRISP achieves semantically coherent separation between target and benign\nconcepts, allowing precise suppression of the target features.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) are increasingly deployed in real-world\napplications, the need to selectively remove unwanted knowledge while\npreserving model utility has become paramount. Recent work has explored sparse\nautoencoders (SAEs) to perform precise interventions on monosemantic features.\nHowever, most SAE-based methods operate at inference time, which does not\ncreate persistent changes in the model's parameters. Such interventions can be\nbypassed or reversed by malicious actors with parameter access. We introduce\nCRISP, a parameter-efficient method for persistent concept unlearning using\nSAEs. CRISP automatically identifies salient SAE features across multiple\nlayers and suppresses their activations. We experiment with two LLMs and show\nthat our method outperforms prior approaches on safety-critical unlearning\ntasks from the WMDP benchmark, successfully removing harmful knowledge while\npreserving general and in-domain capabilities. Feature-level analysis reveals\nthat CRISP achieves semantically coherent separation between target and benign\nconcepts, allowing precise suppression of the target features."
                },
                "authors": [
                    {
                        "name": "Tomer Ashuach"
                    },
                    {
                        "name": "Dana Arad"
                    },
                    {
                        "name": "Aaron Mueller"
                    },
                    {
                        "name": "Martin Tutek"
                    },
                    {
                        "name": "Yonatan Belinkov"
                    }
                ],
                "author_detail": {
                    "name": "Yonatan Belinkov"
                },
                "author": "Yonatan Belinkov",
                "arxiv_comment": "18 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13650v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13650v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13635v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13635v1",
                "updated": "2025-08-19T08:48:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    8,
                    48,
                    5,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T08:48:05Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    8,
                    48,
                    5,
                    1,
                    231,
                    0
                ],
                "title": "Interpreting the Interpreter: Can We Model post-ECB Conferences\n  Volatility with LLM Agents?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpreting the Interpreter: Can We Model post-ECB Conferences\n  Volatility with LLM Agents?"
                },
                "summary": "This paper develops a novel method to simulate financial market reactions to\nEuropean Central Bank (ECB) press conferences using a Large Language Model\n(LLM). We create a behavioral, agent-based simulation of 30 synthetic traders,\neach with distinct risk preferences, cognitive biases, and interpretive styles.\nThese agents forecast Euro interest rate swap levels at 3-month, 2-year, and\n10-year maturities, with the variation across forecasts serving as a measure of\nmarket uncertainty or disagreement. We evaluate three prompting strategies,\nnaive, few-shot (enriched with historical data), and an advanced iterative\n'LLM-as-a-Judge' framework, to assess the effect of prompt design on predictive\nperformance. Even the naive approach generates a strong correlation (roughly\n0.5) between synthetic disagreement and actual market outcomes, particularly\nfor longer-term maturities. The LLM-as-a-Judge framework further improves\naccuracy at the first iteration. These results demonstrate that LLM-driven\nsimulations can capture interpretive uncertainty beyond traditional measures,\nproviding central banks with a practical tool to anticipate market reactions,\nrefine communication strategies, and enhance financial stability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper develops a novel method to simulate financial market reactions to\nEuropean Central Bank (ECB) press conferences using a Large Language Model\n(LLM). We create a behavioral, agent-based simulation of 30 synthetic traders,\neach with distinct risk preferences, cognitive biases, and interpretive styles.\nThese agents forecast Euro interest rate swap levels at 3-month, 2-year, and\n10-year maturities, with the variation across forecasts serving as a measure of\nmarket uncertainty or disagreement. We evaluate three prompting strategies,\nnaive, few-shot (enriched with historical data), and an advanced iterative\n'LLM-as-a-Judge' framework, to assess the effect of prompt design on predictive\nperformance. Even the naive approach generates a strong correlation (roughly\n0.5) between synthetic disagreement and actual market outcomes, particularly\nfor longer-term maturities. The LLM-as-a-Judge framework further improves\naccuracy at the first iteration. These results demonstrate that LLM-driven\nsimulations can capture interpretive uncertainty beyond traditional measures,\nproviding central banks with a practical tool to anticipate market reactions,\nrefine communication strategies, and enhance financial stability."
                },
                "authors": [
                    {
                        "name": "Umberto Collodel"
                    }
                ],
                "author_detail": {
                    "name": "Umberto Collodel"
                },
                "author": "Umberto Collodel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13635v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13635v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06445v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06445v2",
                "updated": "2025-08-19T08:46:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    8,
                    46,
                    37,
                    1,
                    231,
                    0
                ],
                "published": "2025-04-08T21:33:00Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    21,
                    33,
                    0,
                    1,
                    98,
                    0
                ],
                "title": "Hydrodynamic Modelling of Early Peaks in Type Ibc Supernovae with\n  Circumstellar Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hydrodynamic Modelling of Early Peaks in Type Ibc Supernovae with\n  Circumstellar Interaction"
                },
                "summary": "Recent high-cadence transient surveys have uncovered a subclass of Type Ibc\nsupernovae (SNe) that exhibit an early, blue peak lasting a few days before the\nmain, radioactively powered peak. Since progenitors of Type Ibc SNe are\ntypically compact and lack an extended envelope, this early peak is commonly\nattributed to the presence of circumstellar matter (CSM) surrounding the\nprogenitor star. As such, these SNe provide a unique opportunity to constrain\nthe pre-explosion activity of Type Ibc SN progenitors. We present the first\nsystematic study of this Type Ibc SN population that incorporates hydrodynamic\nmodelling. We simulated Type Ibc SNe exploding within CSM using the multi-group\nradiation-hydrodynamics code \\texttt{STELLA}, exploring a range of SN and CSM\nproperties. By comparing the theoretical multi-band light curves to a sample of\nseven Type Ibc SNe with early peaks, we constrained their CSM properties.\nAssuming a wind-like density distribution of CSM, we found CSM masses of\n$10^{-2} - 10^{-1} \\ \\Msun$ and CSM radii of $(1 - 5) \\times 10^3 \\ \\Rsun$.\nWhile the masses were roughly consistent with a previous estimate obtained\nusing an analytical model, the radii were significantly different, likely due\nto a simplified assumption on blackbody temperature used in analytical models.\nWe infer that the progenitors could have created CSM via late-time binary mass\ntransfer or pulsational pair instability. We also estimate that, in the planned\n\\textit{ULTRASAT} high-cadence survey, $\\sim 30$ early peaks similar to those\nin this paper from Type Ibc SNe will be observed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent high-cadence transient surveys have uncovered a subclass of Type Ibc\nsupernovae (SNe) that exhibit an early, blue peak lasting a few days before the\nmain, radioactively powered peak. Since progenitors of Type Ibc SNe are\ntypically compact and lack an extended envelope, this early peak is commonly\nattributed to the presence of circumstellar matter (CSM) surrounding the\nprogenitor star. As such, these SNe provide a unique opportunity to constrain\nthe pre-explosion activity of Type Ibc SN progenitors. We present the first\nsystematic study of this Type Ibc SN population that incorporates hydrodynamic\nmodelling. We simulated Type Ibc SNe exploding within CSM using the multi-group\nradiation-hydrodynamics code \\texttt{STELLA}, exploring a range of SN and CSM\nproperties. By comparing the theoretical multi-band light curves to a sample of\nseven Type Ibc SNe with early peaks, we constrained their CSM properties.\nAssuming a wind-like density distribution of CSM, we found CSM masses of\n$10^{-2} - 10^{-1} \\ \\Msun$ and CSM radii of $(1 - 5) \\times 10^3 \\ \\Rsun$.\nWhile the masses were roughly consistent with a previous estimate obtained\nusing an analytical model, the radii were significantly different, likely due\nto a simplified assumption on blackbody temperature used in analytical models.\nWe infer that the progenitors could have created CSM via late-time binary mass\ntransfer or pulsational pair instability. We also estimate that, in the planned\n\\textit{ULTRASAT} high-cadence survey, $\\sim 30$ early peaks similar to those\nin this paper from Type Ibc SNe will be observed."
                },
                "authors": [
                    {
                        "name": "Ryotaro Chiba"
                    },
                    {
                        "name": "Takashi J. Moriya"
                    }
                ],
                "author_detail": {
                    "name": "Takashi J. Moriya"
                },
                "author": "Takashi J. Moriya",
                "arxiv_comment": "10 pages, 9 figures, 1 table, accepted for publication in MNRAS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06445v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06445v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13628v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13628v2",
                "updated": "2025-08-20T12:14:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    12,
                    14,
                    16,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-19T08:41:49Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    8,
                    41,
                    49,
                    1,
                    231,
                    0
                ],
                "title": "DiffIER: Optimizing Diffusion Models with Iterative Error Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiffIER: Optimizing Diffusion Models with Iterative Error Reduction"
                },
                "summary": "Diffusion models have demonstrated remarkable capabilities in generating\nhigh-quality samples and enhancing performance across diverse domains through\nClassifier-Free Guidance (CFG). However, the quality of generated samples is\nhighly sensitive to the selection of the guidance weight. In this work, we\nidentify a critical ``training-inference gap'' and we argue that it is the\npresence of this gap that undermines the performance of conditional generation\nand renders outputs highly sensitive to the guidance weight. We quantify this\ngap by measuring the accumulated error during the inference stage and establish\na correlation between the selection of guidance weight and minimizing this gap.\nFurthermore, to mitigate this gap, we propose DiffIER, an optimization-based\nmethod for high-quality generation. We demonstrate that the accumulated error\ncan be effectively reduced by an iterative error minimization at each step\nduring inference. By introducing this novel plug-and-play optimization\nframework, we enable the optimization of errors at every single inference step\nand enhance generation quality. Empirical results demonstrate that our proposed\nmethod outperforms baseline approaches in conditional generation tasks.\nFurthermore, the method achieves consistent success in text-to-image\ngeneration, image super-resolution, and text-to-speech generation, underscoring\nits versatility and potential for broad applications in future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have demonstrated remarkable capabilities in generating\nhigh-quality samples and enhancing performance across diverse domains through\nClassifier-Free Guidance (CFG). However, the quality of generated samples is\nhighly sensitive to the selection of the guidance weight. In this work, we\nidentify a critical ``training-inference gap'' and we argue that it is the\npresence of this gap that undermines the performance of conditional generation\nand renders outputs highly sensitive to the guidance weight. We quantify this\ngap by measuring the accumulated error during the inference stage and establish\na correlation between the selection of guidance weight and minimizing this gap.\nFurthermore, to mitigate this gap, we propose DiffIER, an optimization-based\nmethod for high-quality generation. We demonstrate that the accumulated error\ncan be effectively reduced by an iterative error minimization at each step\nduring inference. By introducing this novel plug-and-play optimization\nframework, we enable the optimization of errors at every single inference step\nand enhance generation quality. Empirical results demonstrate that our proposed\nmethod outperforms baseline approaches in conditional generation tasks.\nFurthermore, the method achieves consistent success in text-to-image\ngeneration, image super-resolution, and text-to-speech generation, underscoring\nits versatility and potential for broad applications in future research."
                },
                "authors": [
                    {
                        "name": "Ao Chen"
                    },
                    {
                        "name": "Lihe Ding"
                    },
                    {
                        "name": "Tianfan Xue"
                    }
                ],
                "author_detail": {
                    "name": "Tianfan Xue"
                },
                "author": "Tianfan Xue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13628v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13628v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04107v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04107v3",
                "updated": "2025-08-19T08:35:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    8,
                    35,
                    4,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-06T06:06:52Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    6,
                    6,
                    52,
                    2,
                    218,
                    0
                ],
                "title": "Unlocking the Potential of MLLMs in Referring Expression Segmentation\n  via a Light-weight Mask Decoder",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking the Potential of MLLMs in Referring Expression Segmentation\n  via a Light-weight Mask Decoder"
                },
                "summary": "Reference Expression Segmentation (RES) aims to segment image regions\nspecified by referring expressions and has become popular with the rise of\nmultimodal large models (MLLMs). While MLLMs excel in semantic understanding,\ntheir token-generation paradigm struggles with pixel-level dense prediction.\nExisting RES methods either couple MLLMs with the parameter-heavy Segment\nAnything Model (SAM) with 632M network parameters or adopt SAM-free lightweight\npipelines that sacrifice accuracy. To address the trade-off between performance\nand cost, we specifically propose MLLMSeg, a novel framework that fully\nexploits the inherent visual detail features encoded in the MLLM vision encoder\nwithout introducing an extra visual encoder. Besides, we propose a\ndetail-enhanced and semantic-consistent feature fusion module (DSFF) that fully\nintegrates the detail-related visual feature with the semantic-related feature\noutput by the large language model (LLM) of MLLM. Finally, we establish a\nlight-weight mask decoder with only 34M network parameters that optimally\nleverages detailed spatial features from the visual encoder and semantic\nfeatures from the LLM to achieve precise mask prediction. Extensive experiments\ndemonstrate that our method generally surpasses both SAM-based and SAM-free\ncompetitors, striking a better balance between performance and cost. Code is\navailable at https://github.com/jcwang0602/MLLMSeg.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reference Expression Segmentation (RES) aims to segment image regions\nspecified by referring expressions and has become popular with the rise of\nmultimodal large models (MLLMs). While MLLMs excel in semantic understanding,\ntheir token-generation paradigm struggles with pixel-level dense prediction.\nExisting RES methods either couple MLLMs with the parameter-heavy Segment\nAnything Model (SAM) with 632M network parameters or adopt SAM-free lightweight\npipelines that sacrifice accuracy. To address the trade-off between performance\nand cost, we specifically propose MLLMSeg, a novel framework that fully\nexploits the inherent visual detail features encoded in the MLLM vision encoder\nwithout introducing an extra visual encoder. Besides, we propose a\ndetail-enhanced and semantic-consistent feature fusion module (DSFF) that fully\nintegrates the detail-related visual feature with the semantic-related feature\noutput by the large language model (LLM) of MLLM. Finally, we establish a\nlight-weight mask decoder with only 34M network parameters that optimally\nleverages detailed spatial features from the visual encoder and semantic\nfeatures from the LLM to achieve precise mask prediction. Extensive experiments\ndemonstrate that our method generally surpasses both SAM-based and SAM-free\ncompetitors, striking a better balance between performance and cost. Code is\navailable at https://github.com/jcwang0602/MLLMSeg."
                },
                "authors": [
                    {
                        "name": "Jingchao Wang"
                    },
                    {
                        "name": "Zhijian Wu"
                    },
                    {
                        "name": "Dingjiang Huang"
                    },
                    {
                        "name": "Yefeng Zheng"
                    },
                    {
                        "name": "Hong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hong Wang"
                },
                "author": "Hong Wang",
                "arxiv_comment": "9 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04107v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04107v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17642v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17642v2",
                "updated": "2025-08-19T08:29:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    8,
                    29,
                    16,
                    1,
                    231,
                    0
                ],
                "published": "2025-06-21T08:51:53Z",
                "published_parsed": [
                    2025,
                    6,
                    21,
                    8,
                    51,
                    53,
                    5,
                    172,
                    0
                ],
                "title": "May the Feedback Be with You! Unlocking the Power of Feedback-Driven\n  Deep Learning Framework Fuzzing via LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "May the Feedback Be with You! Unlocking the Power of Feedback-Driven\n  Deep Learning Framework Fuzzing via LLMs"
                },
                "summary": "Deep Learning (DL) frameworks have served as fundamental components in DL\nsystems over the last decade. However, bugs in DL frameworks could lead to\ncatastrophic consequences in critical scenarios. A simple yet effective way to\nfind bugs in DL frameworks is fuzz testing (Fuzzing). Existing approaches focus\non test generation, leaving execution results with high semantic value (e.g.,\ncoverage information, bug reports, and exception logs) in the wild, which can\nserve as multiple types of feedback. To fill this gap, we propose FUEL to\neffectively utilize the feedback information, which comprises two Large\nLanguage Models (LLMs): analysis LLM and generation LLM. Specifically, analysis\nLLM infers analysis summaries from feedback information, while the generation\nLLM creates tests guided by these summaries. Furthermore, based on multiple\nfeedback guidance, we design two additional components: (i) a feedback-aware\nsimulated annealing algorithm to select operators for test generation,\nenriching test diversity. (ii) a program self-repair strategy to automatically\nrepair invalid tests, enhancing test validity. We evaluate FUEL on the two most\npopular DL frameworks, and experiment results show that FUEL can improve line\ncode coverage of PyTorch and TensorFlow by 9.15% and 14.70% over\nstate-of-the-art baselines (e.g., TitanFuzz and WhiteFox). By the time of\nsubmission, FUEL has detected 104 previously unknown bugs for PyTorch and\nTensorFlow, with 93 confirmed as new bugs, 49 already fixed, and 5 assigned CVE\nIDs. Our artifact is available at https://github.com/NJU-iSE/FUEL",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning (DL) frameworks have served as fundamental components in DL\nsystems over the last decade. However, bugs in DL frameworks could lead to\ncatastrophic consequences in critical scenarios. A simple yet effective way to\nfind bugs in DL frameworks is fuzz testing (Fuzzing). Existing approaches focus\non test generation, leaving execution results with high semantic value (e.g.,\ncoverage information, bug reports, and exception logs) in the wild, which can\nserve as multiple types of feedback. To fill this gap, we propose FUEL to\neffectively utilize the feedback information, which comprises two Large\nLanguage Models (LLMs): analysis LLM and generation LLM. Specifically, analysis\nLLM infers analysis summaries from feedback information, while the generation\nLLM creates tests guided by these summaries. Furthermore, based on multiple\nfeedback guidance, we design two additional components: (i) a feedback-aware\nsimulated annealing algorithm to select operators for test generation,\nenriching test diversity. (ii) a program self-repair strategy to automatically\nrepair invalid tests, enhancing test validity. We evaluate FUEL on the two most\npopular DL frameworks, and experiment results show that FUEL can improve line\ncode coverage of PyTorch and TensorFlow by 9.15% and 14.70% over\nstate-of-the-art baselines (e.g., TitanFuzz and WhiteFox). By the time of\nsubmission, FUEL has detected 104 previously unknown bugs for PyTorch and\nTensorFlow, with 93 confirmed as new bugs, 49 already fixed, and 5 assigned CVE\nIDs. Our artifact is available at https://github.com/NJU-iSE/FUEL"
                },
                "authors": [
                    {
                        "name": "Shaoyu Yang"
                    },
                    {
                        "name": "Chunrong Fang"
                    },
                    {
                        "name": "Haifeng Lin"
                    },
                    {
                        "name": "Xiang Chen"
                    },
                    {
                        "name": "Zhenyu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhenyu Chen"
                },
                "author": "Zhenyu Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17642v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17642v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19046v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19046v2",
                "updated": "2025-08-19T08:25:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    8,
                    25,
                    34,
                    1,
                    231,
                    0
                ],
                "published": "2025-07-25T08:07:17Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    8,
                    7,
                    17,
                    4,
                    206,
                    0
                ],
                "title": "Dynamics-Informed Reservoir Computing with Visibility Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamics-Informed Reservoir Computing with Visibility Graphs"
                },
                "summary": "Accurate prediction of complex and nonlinear time series remains a\nchallenging problem across engineering and scientific disciplines. Reservoir\ncomputing (RC) offers a computationally efficient alternative to traditional\ndeep learning by training only the read-out layer while employing a randomly\nstructured and fixed reservoir network. Despite its advantages, the largely\nrandom reservoir graph architecture often results in suboptimal and oversized\nnetworks with poorly understood dynamics. Addressing this issue, we propose a\nnovel Dynamics-Informed Reservoir Computing (DyRC) framework that\nsystematically infers the reservoir network structure directly from the input\ntraining sequence. This work proposes to employ the visibility graph (VG)\ntechnique, which converts time series data into networks by representing\nmeasurement points as nodes linked by mutual visibility. The reservoir network\nis constructed by directly adopting the VG network from a training data\nsequence, leveraging the parameter-free visibility graph approach to avoid\nexpensive hyperparameter tuning. This process results in a reservoir that is\ndirectly informed by the specific dynamics of the prediction task under study.\nWe assess the DyRC-VG method through prediction tasks involving the canonical\nnonlinear Duffing oscillator, evaluating prediction accuracy and consistency.\nCompared to an Erd\\H{o}s-R\\'enyi (ER) graph of the same size, spectral radius,\nand fixed density, we observe higher prediction quality and more consistent\nperformance over repeated implementations in the DyRC-VG. An ER graph with\ndensity matched to the DyRC-VG can in some conditions outperform both\napproaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate prediction of complex and nonlinear time series remains a\nchallenging problem across engineering and scientific disciplines. Reservoir\ncomputing (RC) offers a computationally efficient alternative to traditional\ndeep learning by training only the read-out layer while employing a randomly\nstructured and fixed reservoir network. Despite its advantages, the largely\nrandom reservoir graph architecture often results in suboptimal and oversized\nnetworks with poorly understood dynamics. Addressing this issue, we propose a\nnovel Dynamics-Informed Reservoir Computing (DyRC) framework that\nsystematically infers the reservoir network structure directly from the input\ntraining sequence. This work proposes to employ the visibility graph (VG)\ntechnique, which converts time series data into networks by representing\nmeasurement points as nodes linked by mutual visibility. The reservoir network\nis constructed by directly adopting the VG network from a training data\nsequence, leveraging the parameter-free visibility graph approach to avoid\nexpensive hyperparameter tuning. This process results in a reservoir that is\ndirectly informed by the specific dynamics of the prediction task under study.\nWe assess the DyRC-VG method through prediction tasks involving the canonical\nnonlinear Duffing oscillator, evaluating prediction accuracy and consistency.\nCompared to an Erd\\H{o}s-R\\'enyi (ER) graph of the same size, spectral radius,\nand fixed density, we observe higher prediction quality and more consistent\nperformance over repeated implementations in the DyRC-VG. An ER graph with\ndensity matched to the DyRC-VG can in some conditions outperform both\napproaches."
                },
                "authors": [
                    {
                        "name": "Charlotte Geier"
                    },
                    {
                        "name": "Rasha Shanaz"
                    },
                    {
                        "name": "Merten Stender"
                    }
                ],
                "author_detail": {
                    "name": "Merten Stender"
                },
                "arxiv_affiliation": "Chair of Cyber-Physical Systems in Mechanical Engineering, Technische Universität Berlin, Germany",
                "author": "Merten Stender",
                "arxiv_comment": "7 pages, 5 figures. The following article has been submitted to by\n  Chaos: An Interdisciplinary Journal of Nonlinear Science",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19046v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19046v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04680v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04680v2",
                "updated": "2025-08-19T08:22:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    8,
                    22,
                    48,
                    1,
                    231,
                    0
                ],
                "published": "2025-07-07T05:56:19Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    5,
                    56,
                    19,
                    0,
                    188,
                    0
                ],
                "title": "Identify, Isolate, and Purge: Mitigating Hallucinations in LVLMs via\n  Self-Evolving Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identify, Isolate, and Purge: Mitigating Hallucinations in LVLMs via\n  Self-Evolving Distillation"
                },
                "summary": "Large Vision-Language Models (LVLMs) have demonstrated remarkable\nadvancements in numerous areas such as multimedia. However, hallucination\nissues significantly limit their credibility and application potential.\nExisting mitigation methods typically rely on external tools or the comparison\nof multi-round inference, which significantly increase inference time. In this\npaper, we propose \\textbf{SE}lf-\\textbf{E}volving \\textbf{D}istillation\n(\\textbf{SEED}), which identifies hallucinations within the inner knowledge of\nLVLMs, isolates and purges them, and then distills the purified knowledge back\ninto the model, enabling self-evolution. Furthermore, we identified that\ntraditional distillation methods are prone to inducing void spaces in the\noutput space of LVLMs. To address this issue, we propose a Mode-Seeking\nEvolving approach, which performs distillation to capture the dominant modes of\nthe purified knowledge distribution, thereby avoiding the chaotic results that\ncould emerge from void spaces. Moreover, we introduce a Hallucination\nElimination Adapter, which corrects the dark knowledge of the original model by\nlearning purified knowledge. Extensive experiments on multiple benchmarks\nvalidate the superiority of our SEED, demonstrating substantial improvements in\nmitigating hallucinations for representative LVLM models such as LLaVA-1.5 and\nInternVL2. Remarkably, the F1 score of LLaVA-1.5 on the hallucination\nevaluation metric POPE-Random improved from 81.3 to 88.3.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Vision-Language Models (LVLMs) have demonstrated remarkable\nadvancements in numerous areas such as multimedia. However, hallucination\nissues significantly limit their credibility and application potential.\nExisting mitigation methods typically rely on external tools or the comparison\nof multi-round inference, which significantly increase inference time. In this\npaper, we propose \\textbf{SE}lf-\\textbf{E}volving \\textbf{D}istillation\n(\\textbf{SEED}), which identifies hallucinations within the inner knowledge of\nLVLMs, isolates and purges them, and then distills the purified knowledge back\ninto the model, enabling self-evolution. Furthermore, we identified that\ntraditional distillation methods are prone to inducing void spaces in the\noutput space of LVLMs. To address this issue, we propose a Mode-Seeking\nEvolving approach, which performs distillation to capture the dominant modes of\nthe purified knowledge distribution, thereby avoiding the chaotic results that\ncould emerge from void spaces. Moreover, we introduce a Hallucination\nElimination Adapter, which corrects the dark knowledge of the original model by\nlearning purified knowledge. Extensive experiments on multiple benchmarks\nvalidate the superiority of our SEED, demonstrating substantial improvements in\nmitigating hallucinations for representative LVLM models such as LLaVA-1.5 and\nInternVL2. Remarkably, the F1 score of LLaVA-1.5 on the hallucination\nevaluation metric POPE-Random improved from 81.3 to 88.3."
                },
                "authors": [
                    {
                        "name": "Wenhao Li"
                    },
                    {
                        "name": "Xiu Su"
                    },
                    {
                        "name": "Jingyi Wu"
                    },
                    {
                        "name": "Feng Yang"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Yi Chen"
                    },
                    {
                        "name": "Shan You"
                    },
                    {
                        "name": "Chang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chang Xu"
                },
                "author": "Chang Xu",
                "arxiv_comment": "In Figure 2, the correlation coefficient and the scatter plot do not\n  match. I calculated this correlation using two sets of settings. I used the\n  scatter plot from setting A, but accidentally wrote the correlation\n  coefficient, r, from setting B",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04680v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04680v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13607v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13607v1",
                "updated": "2025-08-19T08:13:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    8,
                    13,
                    34,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T08:13:34Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    8,
                    13,
                    34,
                    1,
                    231,
                    0
                ],
                "title": "Bounding Causal Effects and Counterfactuals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bounding Causal Effects and Counterfactuals"
                },
                "summary": "Causal inference often hinges on strong assumptions - such as no unmeasured\nconfounding or perfect compliance - that are rarely satisfied in practice.\nPartial identification offers a principled alternative: instead of relying on\nunverifiable assumptions to estimate causal effects precisely, it derives\nbounds that reflect the uncertainty inherent in the data. Despite its\ntheoretical appeal, partial identification remains underutilized in applied\nwork, in part due to the fragmented nature of existing methods and the lack of\npractical guidance. This thesis addresses these challenges by systematically\ncomparing a diverse set of bounding algorithms across multiple causal\nscenarios. We implement, extend, and unify state-of-the-art methods - including\nsymbolic, optimization-based, and information-theoretic approaches - within a\ncommon evaluation framework. In particular, we propose an extension of a\nrecently introduced entropy-bounded method, making it applicable to\ncounterfactual queries such as the Probability of Necessity and Sufficiency\n(PNS). Our empirical study spans thousands of randomized simulations involving\nboth discrete and continuous data-generating processes. We assess each method\nin terms of bound tightness, computational efficiency, and robustness to\nassumption violations. To support practitioners, we distill our findings into a\npractical decision tree for algorithm selection and train a machine learning\nmodel to predict the best-performing method based on observable data\ncharacteristics.\n  All implementations are released as part of an open-source Python package,\nCausalBoundingEngine, which enables users to apply and compare bounding methods\nthrough a unified interface.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal inference often hinges on strong assumptions - such as no unmeasured\nconfounding or perfect compliance - that are rarely satisfied in practice.\nPartial identification offers a principled alternative: instead of relying on\nunverifiable assumptions to estimate causal effects precisely, it derives\nbounds that reflect the uncertainty inherent in the data. Despite its\ntheoretical appeal, partial identification remains underutilized in applied\nwork, in part due to the fragmented nature of existing methods and the lack of\npractical guidance. This thesis addresses these challenges by systematically\ncomparing a diverse set of bounding algorithms across multiple causal\nscenarios. We implement, extend, and unify state-of-the-art methods - including\nsymbolic, optimization-based, and information-theoretic approaches - within a\ncommon evaluation framework. In particular, we propose an extension of a\nrecently introduced entropy-bounded method, making it applicable to\ncounterfactual queries such as the Probability of Necessity and Sufficiency\n(PNS). Our empirical study spans thousands of randomized simulations involving\nboth discrete and continuous data-generating processes. We assess each method\nin terms of bound tightness, computational efficiency, and robustness to\nassumption violations. To support practitioners, we distill our findings into a\npractical decision tree for algorithm selection and train a machine learning\nmodel to predict the best-performing method based on observable data\ncharacteristics.\n  All implementations are released as part of an open-source Python package,\nCausalBoundingEngine, which enables users to apply and compare bounding methods\nthrough a unified interface."
                },
                "authors": [
                    {
                        "name": "Tobias Maringgele"
                    }
                ],
                "author_detail": {
                    "name": "Tobias Maringgele"
                },
                "author": "Tobias Maringgele",
                "arxiv_comment": "Bachelor's thesis, Technical University of Munich, 2025. 102 pages,\n  20 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13607v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13607v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62A01 (Foundations of statistics), 68T01 (Artificial intelligence,\n  general)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.3; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13606v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13606v1",
                "updated": "2025-08-19T08:12:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    8,
                    12,
                    45,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T08:12:45Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    8,
                    12,
                    45,
                    1,
                    231,
                    0
                ],
                "title": "AdaDocVQA: Adaptive Framework for Long Document Visual Question\n  Answering in Low-Resource Settings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaDocVQA: Adaptive Framework for Long Document Visual Question\n  Answering in Low-Resource Settings"
                },
                "summary": "Document Visual Question Answering (Document VQA) faces significant\nchallenges when processing long documents in low-resource environments due to\ncontext limitations and insufficient training data. This paper presents\nAdaDocVQA, a unified adaptive framework addressing these challenges through\nthree core innovations: a hybrid text retrieval architecture for effective\ndocument segmentation, an intelligent data augmentation pipeline that\nautomatically generates high-quality reasoning question-answer pairs with\nmulti-level verification, and adaptive ensemble inference with dynamic\nconfiguration generation and early stopping mechanisms. Experiments on Japanese\ndocument VQA benchmarks demonstrate substantial improvements with 83.04\\%\naccuracy on Yes/No questions, 52.66\\% on factual questions, and 44.12\\% on\nnumerical questions in JDocQA, and 59\\% accuracy on LAVA dataset. Ablation\nstudies confirm meaningful contributions from each component, and our framework\nestablishes new state-of-the-art results for Japanese document VQA while\nproviding a scalable foundation for other low-resource languages and\nspecialized domains. Our code available at:\nhttps://github.com/Haoxuanli-Thu/AdaDocVQA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Document Visual Question Answering (Document VQA) faces significant\nchallenges when processing long documents in low-resource environments due to\ncontext limitations and insufficient training data. This paper presents\nAdaDocVQA, a unified adaptive framework addressing these challenges through\nthree core innovations: a hybrid text retrieval architecture for effective\ndocument segmentation, an intelligent data augmentation pipeline that\nautomatically generates high-quality reasoning question-answer pairs with\nmulti-level verification, and adaptive ensemble inference with dynamic\nconfiguration generation and early stopping mechanisms. Experiments on Japanese\ndocument VQA benchmarks demonstrate substantial improvements with 83.04\\%\naccuracy on Yes/No questions, 52.66\\% on factual questions, and 44.12\\% on\nnumerical questions in JDocQA, and 59\\% accuracy on LAVA dataset. Ablation\nstudies confirm meaningful contributions from each component, and our framework\nestablishes new state-of-the-art results for Japanese document VQA while\nproviding a scalable foundation for other low-resource languages and\nspecialized domains. Our code available at:\nhttps://github.com/Haoxuanli-Thu/AdaDocVQA."
                },
                "authors": [
                    {
                        "name": "Haoxuan Li"
                    },
                    {
                        "name": "Wei Song"
                    },
                    {
                        "name": "Aofan Liu"
                    },
                    {
                        "name": "Peiwu Qin"
                    }
                ],
                "author_detail": {
                    "name": "Peiwu Qin"
                },
                "author": "Peiwu Qin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13606v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13606v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2506.15006v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15006v2",
                "updated": "2025-08-19T17:56:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    17,
                    56,
                    18,
                    1,
                    231,
                    0
                ],
                "published": "2025-06-17T22:29:37Z",
                "published_parsed": [
                    2025,
                    6,
                    17,
                    22,
                    29,
                    37,
                    1,
                    168,
                    0
                ],
                "title": "Scaling Intelligence: Designing Data Centers for Next-Gen Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Intelligence: Designing Data Centers for Next-Gen Language\n  Models"
                },
                "summary": "The explosive growth of Large Language Models (LLMs), such as GPT-4 with 1.8\ntrillion parameters, demands a fundamental rethinking of data center\narchitecture to ensure scalability, efficiency, and cost-effectiveness. Our\nwork provides a comprehensive co-design framework that jointly explores FLOPS,\nHBM bandwidth and capacity, multiple network topologies (two-tier vs. FullFlat\noptical), the size of the scale-out domain, and popular\nparallelism/optimization strategies used in LLMs. We introduce and evaluate\nFullFlat network architectures, which provide uniform high-bandwidth,\nlow-latency connectivity between all nodes, and demonstrate their\ntransformative impact on performance and scalability. Through detailed\nsensitivity analyses, we quantify the benefits of overlapping compute and\ncommunication, leveraging hardware-accelerated collectives, widening the\nscale-out domain, and increasing memory capacity. Our study spans both sparse\n(mixture of experts) and dense transformer-based LLMs, revealing how system\ndesign choices affect Model FLOPS Utilization (MFU = Model FLOPS per token *\nObserved tokens per second / Peak FLOPS of the hardware) and overall\nthroughput. For the co-design study, we utilized an analytical performance\nmodeling tool capable of predicting LLM runtime within 10% of real-world\nmeasurements. Our findings offer actionable insights and a practical roadmap\nfor designing AI data centers that can efficiently support trillion-parameter\nmodels, reduce optimization complexity, and sustain the rapid evolution of AI\ncapabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The explosive growth of Large Language Models (LLMs), such as GPT-4 with 1.8\ntrillion parameters, demands a fundamental rethinking of data center\narchitecture to ensure scalability, efficiency, and cost-effectiveness. Our\nwork provides a comprehensive co-design framework that jointly explores FLOPS,\nHBM bandwidth and capacity, multiple network topologies (two-tier vs. FullFlat\noptical), the size of the scale-out domain, and popular\nparallelism/optimization strategies used in LLMs. We introduce and evaluate\nFullFlat network architectures, which provide uniform high-bandwidth,\nlow-latency connectivity between all nodes, and demonstrate their\ntransformative impact on performance and scalability. Through detailed\nsensitivity analyses, we quantify the benefits of overlapping compute and\ncommunication, leveraging hardware-accelerated collectives, widening the\nscale-out domain, and increasing memory capacity. Our study spans both sparse\n(mixture of experts) and dense transformer-based LLMs, revealing how system\ndesign choices affect Model FLOPS Utilization (MFU = Model FLOPS per token *\nObserved tokens per second / Peak FLOPS of the hardware) and overall\nthroughput. For the co-design study, we utilized an analytical performance\nmodeling tool capable of predicting LLM runtime within 10% of real-world\nmeasurements. Our findings offer actionable insights and a practical roadmap\nfor designing AI data centers that can efficiently support trillion-parameter\nmodels, reduce optimization complexity, and sustain the rapid evolution of AI\ncapabilities."
                },
                "authors": [
                    {
                        "name": "Jesmin Jahan Tithi"
                    },
                    {
                        "name": "Hanjiang Wu"
                    },
                    {
                        "name": "Avishaii Abuhatzera"
                    },
                    {
                        "name": "Fabrizio Petrini"
                    }
                ],
                "author_detail": {
                    "name": "Fabrizio Petrini"
                },
                "author": "Fabrizio Petrini",
                "arxiv_comment": "14 pages, submitted to SC25 for review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15006v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15006v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16438v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16438v2",
                "updated": "2025-08-19T17:56:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    17,
                    56,
                    16,
                    1,
                    231,
                    0
                ],
                "published": "2025-04-23T05:57:20Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    5,
                    57,
                    20,
                    2,
                    113,
                    0
                ],
                "title": "POPri: Private Federated Learning using Preference-Optimized Synthetic\n  Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "POPri: Private Federated Learning using Preference-Optimized Synthetic\n  Data"
                },
                "summary": "In practical settings, differentially private Federated learning (DP-FL) is\nthe dominant method for training models from private, on-device client data.\nRecent work has suggested that DP-FL may be enhanced or outperformed by methods\nthat use DP synthetic data (Wu et al., 2024; Hou et al., 2024). The primary\nalgorithms for generating DP synthetic data for FL applications require careful\nprompt engineering based on public information and/or iterative private client\nfeedback. Our key insight is that the private client feedback collected by\nprior DP synthetic data methods (Hou et al., 2024; Xie et al., 2024) can be\nviewed as an RL (reinforcement learning) reward. Our algorithm, Policy\nOptimization for Private Data (POPri) harnesses client feedback using policy\noptimization algorithms such as Direct Preference Optimization (DPO) to\nfine-tune LLMs to generate high-quality DP synthetic data. To evaluate POPri,\nwe release LargeFedBench, a new federated text benchmark for uncontaminated LLM\nevaluations on federated client data. POPri substantially improves the utility\nof DP synthetic data relative to prior work on LargeFedBench datasets and an\nexisting benchmark from Xie et al. (2024). POPri closes the gap between\nnext-token prediction accuracy in the fully-private and non-private settings by\nup to 58%, compared to 28% for prior synthetic data methods, and 3% for\nstate-of-the-art DP federated learning methods. The code and data are available\nat https://github.com/meiyuw/POPri.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In practical settings, differentially private Federated learning (DP-FL) is\nthe dominant method for training models from private, on-device client data.\nRecent work has suggested that DP-FL may be enhanced or outperformed by methods\nthat use DP synthetic data (Wu et al., 2024; Hou et al., 2024). The primary\nalgorithms for generating DP synthetic data for FL applications require careful\nprompt engineering based on public information and/or iterative private client\nfeedback. Our key insight is that the private client feedback collected by\nprior DP synthetic data methods (Hou et al., 2024; Xie et al., 2024) can be\nviewed as an RL (reinforcement learning) reward. Our algorithm, Policy\nOptimization for Private Data (POPri) harnesses client feedback using policy\noptimization algorithms such as Direct Preference Optimization (DPO) to\nfine-tune LLMs to generate high-quality DP synthetic data. To evaluate POPri,\nwe release LargeFedBench, a new federated text benchmark for uncontaminated LLM\nevaluations on federated client data. POPri substantially improves the utility\nof DP synthetic data relative to prior work on LargeFedBench datasets and an\nexisting benchmark from Xie et al. (2024). POPri closes the gap between\nnext-token prediction accuracy in the fully-private and non-private settings by\nup to 58%, compared to 28% for prior synthetic data methods, and 3% for\nstate-of-the-art DP federated learning methods. The code and data are available\nat https://github.com/meiyuw/POPri."
                },
                "authors": [
                    {
                        "name": "Charlie Hou"
                    },
                    {
                        "name": "Mei-Yu Wang"
                    },
                    {
                        "name": "Yige Zhu"
                    },
                    {
                        "name": "Daniel Lazar"
                    },
                    {
                        "name": "Giulia Fanti"
                    }
                ],
                "author_detail": {
                    "name": "Giulia Fanti"
                },
                "author": "Giulia Fanti",
                "arxiv_comment": "ICML 2025 camera-ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16438v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16438v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14032v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14032v1",
                "updated": "2025-08-19T17:54:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    17,
                    54,
                    56,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T17:54:56Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    17,
                    54,
                    56,
                    1,
                    231,
                    0
                ],
                "title": "The Promise of Large Language Models in Digital Health: Evidence from\n  Sentiment Analysis in Online Health Communities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Promise of Large Language Models in Digital Health: Evidence from\n  Sentiment Analysis in Online Health Communities"
                },
                "summary": "Digital health analytics face critical challenges nowadays. The sophisticated\nanalysis of patient-generated health content, which contains complex emotional\nand medical contexts, requires scarce domain expertise, while traditional ML\napproaches are constrained by data shortage and privacy limitations in\nhealthcare settings. Online Health Communities (OHCs) exemplify these\nchallenges with mixed-sentiment posts, clinical terminology, and implicit\nemotional expressions that demand specialised knowledge for accurate Sentiment\nAnalysis (SA). To address these challenges, this study explores how Large\nLanguage Models (LLMs) can integrate expert knowledge through in-context\nlearning for SA, providing a scalable solution for sophisticated health data\nanalysis. Specifically, we develop a structured codebook that systematically\nencodes expert interpretation guidelines, enabling LLMs to apply\ndomain-specific knowledge through targeted prompting rather than extensive\ntraining. Six GPT models validated alongside DeepSeek and LLaMA 3.1 are\ncompared with pre-trained language models (BioBERT variants) and lexicon-based\nmethods, using 400 expert-annotated posts from two OHCs. LLMs achieve superior\nperformance while demonstrating expert-level agreement. This high agreement,\nwith no statistically significant difference from inter-expert agreement\nlevels, suggests knowledge integration beyond surface-level pattern\nrecognition. The consistent performance across diverse LLM models, supported by\nin-context learning, offers a promising solution for digital health analytics.\nThis approach addresses the critical challenge of expert knowledge shortage in\ndigital health research, enabling real-time, expert-quality analysis for\npatient monitoring, intervention assessment, and evidence-based health\nstrategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital health analytics face critical challenges nowadays. The sophisticated\nanalysis of patient-generated health content, which contains complex emotional\nand medical contexts, requires scarce domain expertise, while traditional ML\napproaches are constrained by data shortage and privacy limitations in\nhealthcare settings. Online Health Communities (OHCs) exemplify these\nchallenges with mixed-sentiment posts, clinical terminology, and implicit\nemotional expressions that demand specialised knowledge for accurate Sentiment\nAnalysis (SA). To address these challenges, this study explores how Large\nLanguage Models (LLMs) can integrate expert knowledge through in-context\nlearning for SA, providing a scalable solution for sophisticated health data\nanalysis. Specifically, we develop a structured codebook that systematically\nencodes expert interpretation guidelines, enabling LLMs to apply\ndomain-specific knowledge through targeted prompting rather than extensive\ntraining. Six GPT models validated alongside DeepSeek and LLaMA 3.1 are\ncompared with pre-trained language models (BioBERT variants) and lexicon-based\nmethods, using 400 expert-annotated posts from two OHCs. LLMs achieve superior\nperformance while demonstrating expert-level agreement. This high agreement,\nwith no statistically significant difference from inter-expert agreement\nlevels, suggests knowledge integration beyond surface-level pattern\nrecognition. The consistent performance across diverse LLM models, supported by\nin-context learning, offers a promising solution for digital health analytics.\nThis approach addresses the critical challenge of expert knowledge shortage in\ndigital health research, enabling real-time, expert-quality analysis for\npatient monitoring, intervention assessment, and evidence-based health\nstrategies."
                },
                "authors": [
                    {
                        "name": "Xiancheng Li"
                    },
                    {
                        "name": "Georgios D. Karampatakis"
                    },
                    {
                        "name": "Helen E. Wood"
                    },
                    {
                        "name": "Chris J. Griffiths"
                    },
                    {
                        "name": "Borislava Mihaylova"
                    },
                    {
                        "name": "Neil S. Coulson"
                    },
                    {
                        "name": "Alessio Pasinato"
                    },
                    {
                        "name": "Pietro Panzarasa"
                    },
                    {
                        "name": "Marco Viviani"
                    },
                    {
                        "name": "Anna De Simoni"
                    }
                ],
                "author_detail": {
                    "name": "Anna De Simoni"
                },
                "author": "Anna De Simoni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14032v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14032v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14031v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14031v1",
                "updated": "2025-08-19T17:53:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    17,
                    53,
                    35,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T17:53:35Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    17,
                    53,
                    35,
                    1,
                    231,
                    0
                ],
                "title": "Unintended Misalignment from Agentic Fine-Tuning: Risks and Mitigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unintended Misalignment from Agentic Fine-Tuning: Risks and Mitigation"
                },
                "summary": "Beyond simple text generation, Large Language Models (LLMs) have evolved into\nagentic systems capable of planning and interacting with external tools to\nsolve complex tasks. This evolution involves fine-tuning LLMs on agent-specific\ntasks to enhance their proficiency. However, safety concerns are frequently\noverlooked during this fine-tuning process. In this work, we show that aligned\nLLMs can become unintentionally misaligned, leading to a higher likelihood of\nexecuting harmful tasks and a reduced tendency to refuse them when fine-tuned\nto execute agentic tasks. To address these safety challenges, we propose Prefix\nINjection Guard (PING), a simple yet effective method that prepends\nautomatically generated natural language prefixes to agent responses, guiding\nthem to refuse harmful requests while preserving performance on benign tasks.\nSpecifically, we introduce an iterative approach that alternates between (1)\ngenerating candidate prefixes and (2) selecting those that optimize both task\nperformance and refusal behavior. Experimental results demonstrate that PING\nsignificantly enhances the safety of fine-tuned LLM agents without sacrificing\ntheir effectiveness. PING consistently outperforms existing prompting\napproaches across diverse benchmarks in both web navigation and code generation\ntasks. Our analysis of internal hidden states via linear probes reveals that\nprefix tokens are crucial for behavior modification, explaining the performance\ngains. WARNING: This paper contains contents that are unethical or offensive in\nnature.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond simple text generation, Large Language Models (LLMs) have evolved into\nagentic systems capable of planning and interacting with external tools to\nsolve complex tasks. This evolution involves fine-tuning LLMs on agent-specific\ntasks to enhance their proficiency. However, safety concerns are frequently\noverlooked during this fine-tuning process. In this work, we show that aligned\nLLMs can become unintentionally misaligned, leading to a higher likelihood of\nexecuting harmful tasks and a reduced tendency to refuse them when fine-tuned\nto execute agentic tasks. To address these safety challenges, we propose Prefix\nINjection Guard (PING), a simple yet effective method that prepends\nautomatically generated natural language prefixes to agent responses, guiding\nthem to refuse harmful requests while preserving performance on benign tasks.\nSpecifically, we introduce an iterative approach that alternates between (1)\ngenerating candidate prefixes and (2) selecting those that optimize both task\nperformance and refusal behavior. Experimental results demonstrate that PING\nsignificantly enhances the safety of fine-tuned LLM agents without sacrificing\ntheir effectiveness. PING consistently outperforms existing prompting\napproaches across diverse benchmarks in both web navigation and code generation\ntasks. Our analysis of internal hidden states via linear probes reveals that\nprefix tokens are crucial for behavior modification, explaining the performance\ngains. WARNING: This paper contains contents that are unethical or offensive in\nnature."
                },
                "authors": [
                    {
                        "name": "Dongyoon Hahm"
                    },
                    {
                        "name": "Taywon Min"
                    },
                    {
                        "name": "Woogyeol Jin"
                    },
                    {
                        "name": "Kimin Lee"
                    }
                ],
                "author_detail": {
                    "name": "Kimin Lee"
                },
                "author": "Kimin Lee",
                "arxiv_comment": "Source code: https://github.com/HahmDY/prefix_injection_guard",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14031v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14031v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14029v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14029v2",
                "updated": "2025-08-20T01:21:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    1,
                    21,
                    25,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-19T17:42:45Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    17,
                    42,
                    45,
                    1,
                    231,
                    0
                ],
                "title": "Beyond Pass@1: Self-Play with Variational Problem Synthesis Sustains\n  RLVR",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Pass@1: Self-Play with Variational Problem Synthesis Sustains\n  RLVR"
                },
                "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as\na key paradigm for post-training Large Language Models (LLMs), particularly for\ncomplex reasoning tasks. However, vanilla RLVR training has been shown to\nimprove Pass@1 performance at the expense of policy entropy, leading to reduced\ngeneration diversity and limiting the Pass@k performance, which typically\nrepresents the upper bound of LLM reasoning capability. In this paper, we\nsystematically analyze the policy's generation diversity from the perspective\nof training problems and find that augmenting and updating training problems\nhelps mitigate entropy collapse during training. Based on these observations,\nwe propose an online Self-play with Variational problem Synthesis (SvS)\nstrategy for RLVR training, which uses the policy's correct solutions to\nsynthesize variational problems while ensuring their reference answers remain\nidentical to the originals. This self-improving strategy effectively maintains\npolicy entropy during training and substantially improves Pass@k compared with\nstandard RLVR, sustaining prolonged improvements and achieving absolute gains\nof 18.3% and 22.8% in Pass@32 performance on the competition-level AIME24 and\nAIME25 benchmarks. Experiments on 12 reasoning benchmarks across varying model\nsizes from 3B to 32B consistently demonstrate the generalizability and\nrobustness of SvS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as\na key paradigm for post-training Large Language Models (LLMs), particularly for\ncomplex reasoning tasks. However, vanilla RLVR training has been shown to\nimprove Pass@1 performance at the expense of policy entropy, leading to reduced\ngeneration diversity and limiting the Pass@k performance, which typically\nrepresents the upper bound of LLM reasoning capability. In this paper, we\nsystematically analyze the policy's generation diversity from the perspective\nof training problems and find that augmenting and updating training problems\nhelps mitigate entropy collapse during training. Based on these observations,\nwe propose an online Self-play with Variational problem Synthesis (SvS)\nstrategy for RLVR training, which uses the policy's correct solutions to\nsynthesize variational problems while ensuring their reference answers remain\nidentical to the originals. This self-improving strategy effectively maintains\npolicy entropy during training and substantially improves Pass@k compared with\nstandard RLVR, sustaining prolonged improvements and achieving absolute gains\nof 18.3% and 22.8% in Pass@32 performance on the competition-level AIME24 and\nAIME25 benchmarks. Experiments on 12 reasoning benchmarks across varying model\nsizes from 3B to 32B consistently demonstrate the generalizability and\nrobustness of SvS."
                },
                "authors": [
                    {
                        "name": "Xiao Liang"
                    },
                    {
                        "name": "Zhongzhi Li"
                    },
                    {
                        "name": "Yeyun Gong"
                    },
                    {
                        "name": "Yelong Shen"
                    },
                    {
                        "name": "Ying Nian Wu"
                    },
                    {
                        "name": "Zhijiang Guo"
                    },
                    {
                        "name": "Weizhu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Weizhu Chen"
                },
                "author": "Weizhu Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14029v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14029v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14025v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14025v1",
                "updated": "2025-08-19T17:31:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    17,
                    31,
                    42,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T17:31:42Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    17,
                    31,
                    42,
                    1,
                    231,
                    0
                ],
                "title": "Ask Good Questions for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ask Good Questions for Large Language Models"
                },
                "summary": "Recent advances in large language models (LLMs) have significantly improved\nthe performance of dialog systems, yet current approaches often fail to provide\naccurate guidance of topic due to their inability to discern user confusion in\nrelated concepts. To address this, we introduce the Ask-Good-Question (AGQ)\nframework, which features an improved Concept-Enhanced Item Response Theory\n(CEIRT) model to better identify users' knowledge levels. Our contributions\ninclude applying the CEIRT model along with LLMs to directly generate guiding\nquestions based on the inspiring text, greatly improving information retrieval\nefficiency during the question & answer process. Through comparisons with other\nbaseline methods, our approach outperforms by significantly enhencing the\nusers' information retrieval experiences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have significantly improved\nthe performance of dialog systems, yet current approaches often fail to provide\naccurate guidance of topic due to their inability to discern user confusion in\nrelated concepts. To address this, we introduce the Ask-Good-Question (AGQ)\nframework, which features an improved Concept-Enhanced Item Response Theory\n(CEIRT) model to better identify users' knowledge levels. Our contributions\ninclude applying the CEIRT model along with LLMs to directly generate guiding\nquestions based on the inspiring text, greatly improving information retrieval\nefficiency during the question & answer process. Through comparisons with other\nbaseline methods, our approach outperforms by significantly enhencing the\nusers' information retrieval experiences."
                },
                "authors": [
                    {
                        "name": "Qi Wu"
                    },
                    {
                        "name": "Zhongqi Lu"
                    }
                ],
                "author_detail": {
                    "name": "Zhongqi Lu"
                },
                "author": "Zhongqi Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14025v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14025v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12152v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12152v2",
                "updated": "2025-08-19T17:29:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    17,
                    29,
                    28,
                    1,
                    231,
                    0
                ],
                "published": "2025-01-21T14:02:39Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    14,
                    2,
                    39,
                    1,
                    21,
                    0
                ],
                "title": "Contextualizing Recommendation Explanations with LLMs: A User Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contextualizing Recommendation Explanations with LLMs: A User Study"
                },
                "summary": "Large language models (LLMs) are increasingly prevalent in recommender\nsystems, where LLMs can be used to generate personalized recommendations. Here,\nwe examine how different LLM-generated explanations for movie recommendations\naffect users' perceptions of cognitive, affective, and utilitarian needs and\nconsumption intentions. In a pre-registered, between-subject online experiment\n(N=759) and follow-up interviews (N=30), we compare (a) LLM-generated generic\nexplanations, and (b) LLM-generated contextualized explanations. Our findings\nshow that contextualized explanations (i.e., explanations that incorporate\nusers' past behaviors) effectively meet users' cognitive needs while increasing\nusers' intentions to watch recommended movies. However, adding explanations\noffers limited benefits in meeting users' utilitarian and affective needs,\nraising concerns about the proper design and implications of LLM-generated\nexplanations. Qualitative insights from interviews reveal that referencing\nusers' past preferences enhances trust and understanding but can feel excessive\nif overused. Furthermore, users with more active and positive engagement with\nthe recommender system and movie-watching get substantial gains from\ncontextualized explanations. Overall, our research clarifies how LLM-generated\nrecommendations influence users' motivations and behaviors, providing valuable\ninsights for the future development of user-centric recommender systems, a key\nelement in social media platforms and online ecosystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly prevalent in recommender\nsystems, where LLMs can be used to generate personalized recommendations. Here,\nwe examine how different LLM-generated explanations for movie recommendations\naffect users' perceptions of cognitive, affective, and utilitarian needs and\nconsumption intentions. In a pre-registered, between-subject online experiment\n(N=759) and follow-up interviews (N=30), we compare (a) LLM-generated generic\nexplanations, and (b) LLM-generated contextualized explanations. Our findings\nshow that contextualized explanations (i.e., explanations that incorporate\nusers' past behaviors) effectively meet users' cognitive needs while increasing\nusers' intentions to watch recommended movies. However, adding explanations\noffers limited benefits in meeting users' utilitarian and affective needs,\nraising concerns about the proper design and implications of LLM-generated\nexplanations. Qualitative insights from interviews reveal that referencing\nusers' past preferences enhances trust and understanding but can feel excessive\nif overused. Furthermore, users with more active and positive engagement with\nthe recommender system and movie-watching get substantial gains from\ncontextualized explanations. Overall, our research clarifies how LLM-generated\nrecommendations influence users' motivations and behaviors, providing valuable\ninsights for the future development of user-centric recommender systems, a key\nelement in social media platforms and online ecosystems."
                },
                "authors": [
                    {
                        "name": "Yuanjun Feng"
                    },
                    {
                        "name": "Stefan Feuerriegel"
                    },
                    {
                        "name": "Yash Raj Shrestha"
                    }
                ],
                "author_detail": {
                    "name": "Yash Raj Shrestha"
                },
                "author": "Yash Raj Shrestha",
                "arxiv_comment": "Accepted to the International AAAI Conference on Web and Social Media\n  (ICWSM 2026)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12152v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12152v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19061v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19061v3",
                "updated": "2025-08-20T14:24:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    14,
                    24,
                    25,
                    2,
                    232,
                    0
                ],
                "published": "2025-04-27T00:39:12Z",
                "published_parsed": [
                    2025,
                    4,
                    27,
                    0,
                    39,
                    12,
                    6,
                    117,
                    0
                ],
                "title": "Hallucinations and Key Information Extraction in Medical Texts: A\n  Comprehensive Assessment of Open-Source Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucinations and Key Information Extraction in Medical Texts: A\n  Comprehensive Assessment of Open-Source Large Language Models"
                },
                "summary": "Clinical summarization is crucial in healthcare as it distills complex\nmedical data into digestible information, enhancing patient understanding and\ncare management. Large language models (LLMs) have shown significant potential\nin automating and improving the accuracy of such summarizations due to their\nadvanced natural language understanding capabilities. These models are\nparticularly applicable in the context of summarizing medical/clinical texts,\nwhere precise and concise information transfer is essential. In this paper, we\ninvestigate the effectiveness of open-source LLMs in extracting key events from\ndischarge reports, including admission reasons, major in-hospital events, and\ncritical follow-up actions. In addition, we also assess the prevalence of\nvarious types of hallucinations in the summaries produced by these models.\nDetecting hallucinations is vital as it directly influences the reliability of\nthe information, potentially affecting patient care and treatment outcomes. We\nconduct comprehensive simulations to rigorously evaluate the performance of\nthese models, further probing the accuracy and fidelity of the extracted\ncontent in clinical summarization. Our results reveal that while the LLMs\n(e.g., Qwen2.5 and DeepSeek-v2) perform quite well in capturing admission\nreasons and hospitalization events, they are generally less consistent when it\ncomes to identifying follow-up recommendations, highlighting broader challenges\nin leveraging LLMs for comprehensive summarization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clinical summarization is crucial in healthcare as it distills complex\nmedical data into digestible information, enhancing patient understanding and\ncare management. Large language models (LLMs) have shown significant potential\nin automating and improving the accuracy of such summarizations due to their\nadvanced natural language understanding capabilities. These models are\nparticularly applicable in the context of summarizing medical/clinical texts,\nwhere precise and concise information transfer is essential. In this paper, we\ninvestigate the effectiveness of open-source LLMs in extracting key events from\ndischarge reports, including admission reasons, major in-hospital events, and\ncritical follow-up actions. In addition, we also assess the prevalence of\nvarious types of hallucinations in the summaries produced by these models.\nDetecting hallucinations is vital as it directly influences the reliability of\nthe information, potentially affecting patient care and treatment outcomes. We\nconduct comprehensive simulations to rigorously evaluate the performance of\nthese models, further probing the accuracy and fidelity of the extracted\ncontent in clinical summarization. Our results reveal that while the LLMs\n(e.g., Qwen2.5 and DeepSeek-v2) perform quite well in capturing admission\nreasons and hospitalization events, they are generally less consistent when it\ncomes to identifying follow-up recommendations, highlighting broader challenges\nin leveraging LLMs for comprehensive summarization."
                },
                "authors": [
                    {
                        "name": "Anindya Bijoy Das"
                    },
                    {
                        "name": "Shibbir Ahmed"
                    },
                    {
                        "name": "Shahnewaz Karim Sakib"
                    }
                ],
                "author_detail": {
                    "name": "Shahnewaz Karim Sakib"
                },
                "author": "Shahnewaz Karim Sakib",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19061v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19061v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13993v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13993v1",
                "updated": "2025-08-19T16:33:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    16,
                    33,
                    55,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T16:33:55Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    16,
                    33,
                    55,
                    1,
                    231,
                    0
                ],
                "title": "Chunks as Arms: Multi-Armed Bandit-Guided Sampling for Long-Context LLM\n  Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chunks as Arms: Multi-Armed Bandit-Guided Sampling for Long-Context LLM\n  Preference Optimization"
                },
                "summary": "Long-context modeling is critical for a wide range of real-world tasks,\nincluding long-context question answering, summarization, and complex reasoning\ntasks. Recent studies have explored fine-tuning Large Language Models (LLMs)\nwith synthetic data to enhance their long-context capabilities. However, the\neffectiveness of such approaches is often limited by the low diversity and\nfactual inconsistencies in the generated data. To address these challenges, we\npropose LongMab-PO, a novel framework that leverages a Multi-Armed Bandit (MAB)\nrollout strategy to identify the most informative chunks from the given long\ncontext for sampling high-quality and diverse responses and constructing\npreference data pairs for Direct Preference Optimization (DPO) training.\nSpecifically, we treat context chunks as arms of MAB, select chunks based on\ntheir expected reward scores to input into LLMs to generate responses, and\niteratively update these scores based on reward feedback. This exploration and\nexploitation process enables the model to focus on the most relevant context\nsegments, thereby generating and collecting high-quality and diverse responses.\nFinally, we collect these generated responses from the rollout process and\napply the DPO method to further optimize the LLM. Experimental results show\nthat LongMab-PO significantly improves the diversity and quality of preference\ndata pairs, achieving state-of-the-art performance on long-context reasoning\nbenchmarks. All code and data will be released on\nhttps://github.com/NEUIR/LongMab-PO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context modeling is critical for a wide range of real-world tasks,\nincluding long-context question answering, summarization, and complex reasoning\ntasks. Recent studies have explored fine-tuning Large Language Models (LLMs)\nwith synthetic data to enhance their long-context capabilities. However, the\neffectiveness of such approaches is often limited by the low diversity and\nfactual inconsistencies in the generated data. To address these challenges, we\npropose LongMab-PO, a novel framework that leverages a Multi-Armed Bandit (MAB)\nrollout strategy to identify the most informative chunks from the given long\ncontext for sampling high-quality and diverse responses and constructing\npreference data pairs for Direct Preference Optimization (DPO) training.\nSpecifically, we treat context chunks as arms of MAB, select chunks based on\ntheir expected reward scores to input into LLMs to generate responses, and\niteratively update these scores based on reward feedback. This exploration and\nexploitation process enables the model to focus on the most relevant context\nsegments, thereby generating and collecting high-quality and diverse responses.\nFinally, we collect these generated responses from the rollout process and\napply the DPO method to further optimize the LLM. Experimental results show\nthat LongMab-PO significantly improves the diversity and quality of preference\ndata pairs, achieving state-of-the-art performance on long-context reasoning\nbenchmarks. All code and data will be released on\nhttps://github.com/NEUIR/LongMab-PO."
                },
                "authors": [
                    {
                        "name": "Shaohua Duan"
                    },
                    {
                        "name": "Xinze Li"
                    },
                    {
                        "name": "Zhenghao Liu"
                    },
                    {
                        "name": "Xiaoyuan Yi"
                    },
                    {
                        "name": "Yukun Yan"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Yu Gu"
                    },
                    {
                        "name": "Ge Yu"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13993v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13993v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13984v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13984v1",
                "updated": "2025-08-19T16:21:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    16,
                    21,
                    44,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T16:21:44Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    16,
                    21,
                    44,
                    1,
                    231,
                    0
                ],
                "title": "The AI-Fraud Diamond: A Novel Lens for Auditing Algorithmic Deception",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The AI-Fraud Diamond: A Novel Lens for Auditing Algorithmic Deception"
                },
                "summary": "As artificial intelligence (AI) systems become increasingly integral to\norganizational processes, they introduce new forms of fraud that are often\nsubtle, systemic, and concealed within technical complexity. This paper\nintroduces the AI-Fraud Diamond, an extension of the traditional Fraud Triangle\nthat adds technical opacity as a fourth condition alongside pressure,\nopportunity, and rationalization. Unlike traditional fraud, AI-enabled\ndeception may not involve clear human intent but can arise from system-level\nfeatures such as opaque model behavior, flawed training data, or unregulated\ndeployment practices. The paper develops a taxonomy of AI-fraud across five\ncategories: input data manipulation, model exploitation, algorithmic decision\nmanipulation, synthetic misinformation, and ethics-based fraud. To assess the\nrelevance and applicability of the AI-Fraud Diamond, the study draws on expert\ninterviews with auditors from two of the Big Four consulting firms. The\nfindings underscore the challenges auditors face when addressing fraud in\nopaque and automated environments, including limited technical expertise,\ninsufficient cross-disciplinary collaboration, and constrained access to\ninternal system processes. These conditions hinder fraud detection and reduce\naccountability. The paper argues for a shift in audit methodology-from\noutcome-based checks to a more diagnostic approach focused on identifying\nsystemic vulnerabilities. Ultimately, the work lays a foundation for future\nempirical research and audit innovation in a rapidly evolving AI governance\nlandscape.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As artificial intelligence (AI) systems become increasingly integral to\norganizational processes, they introduce new forms of fraud that are often\nsubtle, systemic, and concealed within technical complexity. This paper\nintroduces the AI-Fraud Diamond, an extension of the traditional Fraud Triangle\nthat adds technical opacity as a fourth condition alongside pressure,\nopportunity, and rationalization. Unlike traditional fraud, AI-enabled\ndeception may not involve clear human intent but can arise from system-level\nfeatures such as opaque model behavior, flawed training data, or unregulated\ndeployment practices. The paper develops a taxonomy of AI-fraud across five\ncategories: input data manipulation, model exploitation, algorithmic decision\nmanipulation, synthetic misinformation, and ethics-based fraud. To assess the\nrelevance and applicability of the AI-Fraud Diamond, the study draws on expert\ninterviews with auditors from two of the Big Four consulting firms. The\nfindings underscore the challenges auditors face when addressing fraud in\nopaque and automated environments, including limited technical expertise,\ninsufficient cross-disciplinary collaboration, and constrained access to\ninternal system processes. These conditions hinder fraud detection and reduce\naccountability. The paper argues for a shift in audit methodology-from\noutcome-based checks to a more diagnostic approach focused on identifying\nsystemic vulnerabilities. Ultimately, the work lays a foundation for future\nempirical research and audit innovation in a rapidly evolving AI governance\nlandscape."
                },
                "authors": [
                    {
                        "name": "Benjamin Zweers"
                    },
                    {
                        "name": "Diptish Dey"
                    },
                    {
                        "name": "Debarati Bhaumik"
                    }
                ],
                "author_detail": {
                    "name": "Debarati Bhaumik"
                },
                "author": "Debarati Bhaumik",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13984v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13984v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13443v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13443v2",
                "updated": "2025-08-19T16:13:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    16,
                    13,
                    16,
                    1,
                    231,
                    0
                ],
                "published": "2025-04-18T03:49:53Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    3,
                    49,
                    53,
                    4,
                    108,
                    0
                ],
                "title": "Trust, but verify",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trust, but verify"
                },
                "summary": "Decentralized AI agent networks, such as Gaia, allows individuals to run\ncustomized LLMs on their own computers and then provide services to the public.\nHowever, in order to maintain service quality, the network must verify that\nindividual nodes are running their designated LLMs. In this paper, we\ndemonstrate that in a cluster of mostly honest nodes, we can detect nodes that\nrun unauthorized or incorrect LLM through social consensus of its peers. We\nwill discuss the algorithm and experimental data from the Gaia network. We will\nalso discuss the intersubjective validation system, implemented as an\nEigenLayer AVS to introduce financial incentives and penalties to encourage\nhonest behavior from LLM nodes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralized AI agent networks, such as Gaia, allows individuals to run\ncustomized LLMs on their own computers and then provide services to the public.\nHowever, in order to maintain service quality, the network must verify that\nindividual nodes are running their designated LLMs. In this paper, we\ndemonstrate that in a cluster of mostly honest nodes, we can detect nodes that\nrun unauthorized or incorrect LLM through social consensus of its peers. We\nwill discuss the algorithm and experimental data from the Gaia network. We will\nalso discuss the intersubjective validation system, implemented as an\nEigenLayer AVS to introduce financial incentives and penalties to encourage\nhonest behavior from LLM nodes."
                },
                "authors": [
                    {
                        "name": "Michael J. Yuan"
                    },
                    {
                        "name": "Carlos Lospoy"
                    },
                    {
                        "name": "Sydney Lai"
                    },
                    {
                        "name": "James Snewin"
                    },
                    {
                        "name": "Ju Long"
                    }
                ],
                "author_detail": {
                    "name": "Ju Long"
                },
                "author": "Ju Long",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13443v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13443v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22884v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22884v2",
                "updated": "2025-08-19T16:13:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    16,
                    13,
                    9,
                    1,
                    231,
                    0
                ],
                "published": "2025-03-28T21:21:35Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    21,
                    21,
                    35,
                    4,
                    87,
                    0
                ],
                "title": "AutoComPose: Automatic Generation of Pose Transition Descriptions for\n  Composed Pose Retrieval Using Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoComPose: Automatic Generation of Pose Transition Descriptions for\n  Composed Pose Retrieval Using Multimodal LLMs"
                },
                "summary": "Composed pose retrieval (CPR) enables users to search for human poses by\nspecifying a reference pose and a transition description, but progress in this\nfield is hindered by the scarcity and inconsistency of annotated pose\ntransitions. Existing CPR datasets rely on costly human annotations or\nheuristic-based rule generation, both of which limit scalability and diversity.\nIn this work, we introduce AutoComPose, the first framework that leverages\nmultimodal large language models (MLLMs) to automatically generate rich and\nstructured pose transition descriptions. Our method enhances annotation quality\nby structuring transitions into fine-grained body part movements and\nintroducing mirrored/swapped variations, while a cyclic consistency constraint\nensures logical coherence between forward and reverse transitions. To advance\nCPR research, we construct and release two dedicated benchmarks, AIST-CPR and\nPoseFixCPR, supplementing prior datasets with enhanced attributes. Extensive\nexperiments demonstrate that training retrieval models with AutoComPose yields\nsuperior performance over human-annotated and heuristic-based methods,\nsignificantly reducing annotation costs while improving retrieval quality. Our\nwork pioneers the automatic annotation of pose transitions, establishing a\nscalable foundation for future CPR research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Composed pose retrieval (CPR) enables users to search for human poses by\nspecifying a reference pose and a transition description, but progress in this\nfield is hindered by the scarcity and inconsistency of annotated pose\ntransitions. Existing CPR datasets rely on costly human annotations or\nheuristic-based rule generation, both of which limit scalability and diversity.\nIn this work, we introduce AutoComPose, the first framework that leverages\nmultimodal large language models (MLLMs) to automatically generate rich and\nstructured pose transition descriptions. Our method enhances annotation quality\nby structuring transitions into fine-grained body part movements and\nintroducing mirrored/swapped variations, while a cyclic consistency constraint\nensures logical coherence between forward and reverse transitions. To advance\nCPR research, we construct and release two dedicated benchmarks, AIST-CPR and\nPoseFixCPR, supplementing prior datasets with enhanced attributes. Extensive\nexperiments demonstrate that training retrieval models with AutoComPose yields\nsuperior performance over human-annotated and heuristic-based methods,\nsignificantly reducing annotation costs while improving retrieval quality. Our\nwork pioneers the automatic annotation of pose transitions, establishing a\nscalable foundation for future CPR research."
                },
                "authors": [
                    {
                        "name": "Yi-Ting Shen"
                    },
                    {
                        "name": "Sungmin Eum"
                    },
                    {
                        "name": "Doheon Lee"
                    },
                    {
                        "name": "Rohit Shete"
                    },
                    {
                        "name": "Chiao-Yi Wang"
                    },
                    {
                        "name": "Heesung Kwon"
                    },
                    {
                        "name": "Shuvra S. Bhattacharyya"
                    }
                ],
                "author_detail": {
                    "name": "Shuvra S. Bhattacharyya"
                },
                "author": "Shuvra S. Bhattacharyya",
                "arxiv_comment": "ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22884v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22884v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13975v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13975v1",
                "updated": "2025-08-19T16:12:51Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    16,
                    12,
                    51,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T16:12:51Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    16,
                    12,
                    51,
                    1,
                    231,
                    0
                ],
                "title": "ChronoLLM: Customizing Language Models for Physics-Based Simulation Code\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChronoLLM: Customizing Language Models for Physics-Based Simulation Code\n  Generation"
                },
                "summary": "This contribution is concerned with the following issue: can pretrained large\nlanguage models (LLMs) be refined and customized to the point where they become\nvirtual assistants helping experts with the effective use of a simulation tool?\nIn this case study, the ``simulation tool'' considered is PyChrono, an open\nsource multi-physics dynamics engine for multibody systems. We present a\nframework for refining and customizing both open- and closed-source LLMs to\nharness the power of AI in generating scripts that perform PyChrono virtual\nexperiments. We refine and customize several classes of LLMs through a process\nthat leads to a quantifiable improvement in the quality of the generated\nPyChrono simulation scripts. These scripts can range from simple\nsingle-pendulum simulations to complex virtual experiments involving full\nvehicles on deformable terrain. While the generated scripts are rarely perfect,\nthey often serve as strong starting points for the user to modify and improve\non. Additionally, the LLM can answer specific API questions about the\nsimulator, or recommend modeling approaches. The framework discussed is general\nand can be applied to lower the entry barrier for simulation tools associated\nwith other application domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This contribution is concerned with the following issue: can pretrained large\nlanguage models (LLMs) be refined and customized to the point where they become\nvirtual assistants helping experts with the effective use of a simulation tool?\nIn this case study, the ``simulation tool'' considered is PyChrono, an open\nsource multi-physics dynamics engine for multibody systems. We present a\nframework for refining and customizing both open- and closed-source LLMs to\nharness the power of AI in generating scripts that perform PyChrono virtual\nexperiments. We refine and customize several classes of LLMs through a process\nthat leads to a quantifiable improvement in the quality of the generated\nPyChrono simulation scripts. These scripts can range from simple\nsingle-pendulum simulations to complex virtual experiments involving full\nvehicles on deformable terrain. While the generated scripts are rarely perfect,\nthey often serve as strong starting points for the user to modify and improve\non. Additionally, the LLM can answer specific API questions about the\nsimulator, or recommend modeling approaches. The framework discussed is general\nand can be applied to lower the entry barrier for simulation tools associated\nwith other application domains."
                },
                "authors": [
                    {
                        "name": "Jingquan Wang"
                    },
                    {
                        "name": "Andrew Negrut"
                    },
                    {
                        "name": "Harry Zhang"
                    },
                    {
                        "name": "Khailanii Slaton"
                    },
                    {
                        "name": "Shu Wang"
                    },
                    {
                        "name": "Radu Serban"
                    },
                    {
                        "name": "Jinlong Wu"
                    },
                    {
                        "name": "Dan Negrut"
                    }
                ],
                "author_detail": {
                    "name": "Dan Negrut"
                },
                "author": "Dan Negrut",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13975v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13975v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05985v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05985v2",
                "updated": "2025-08-19T16:01:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    16,
                    1,
                    10,
                    1,
                    231,
                    0
                ],
                "published": "2025-01-10T14:17:48Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    14,
                    17,
                    48,
                    4,
                    10,
                    0
                ],
                "title": "Exploring LLMs for Automated Generation and Adaptation of Questionnaires",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring LLMs for Automated Generation and Adaptation of Questionnaires"
                },
                "summary": "Effective questionnaire design improves the validity of the results, but\ncreating and adapting questionnaires across contexts is challenging due to\nresource constraints and limited expert access. Recently, the emergence of LLMs\nhas led researchers to explore their potential in survey research. In this\nwork, we focus on the suitability of LLMs in assisting the generation and\nadaptation of questionnaires. We introduce a novel pipeline that leverages LLMs\nto create new questionnaires, pretest with a target audience to determine\npotential issues and adapt existing standardized questionnaires for different\ncontexts. We evaluated our pipeline for creation and adaptation through two\nstudies on Prolific, involving 238 participants from the US and 118\nparticipants from South Africa. Our findings show that participants found\nLLM-generated text clearer, LLM-pretested text more specific, and LLM-adapted\nquestions slightly clearer and less biased than traditional ones. Our work\nopens new opportunities for LLM-driven questionnaire support in survey\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective questionnaire design improves the validity of the results, but\ncreating and adapting questionnaires across contexts is challenging due to\nresource constraints and limited expert access. Recently, the emergence of LLMs\nhas led researchers to explore their potential in survey research. In this\nwork, we focus on the suitability of LLMs in assisting the generation and\nadaptation of questionnaires. We introduce a novel pipeline that leverages LLMs\nto create new questionnaires, pretest with a target audience to determine\npotential issues and adapt existing standardized questionnaires for different\ncontexts. We evaluated our pipeline for creation and adaptation through two\nstudies on Prolific, involving 238 participants from the US and 118\nparticipants from South Africa. Our findings show that participants found\nLLM-generated text clearer, LLM-pretested text more specific, and LLM-adapted\nquestions slightly clearer and less biased than traditional ones. Our work\nopens new opportunities for LLM-driven questionnaire support in survey\nresearch."
                },
                "authors": [
                    {
                        "name": "Divya Mani Adhikari"
                    },
                    {
                        "name": "Alexander Hartland"
                    },
                    {
                        "name": "Ingmar Weber"
                    },
                    {
                        "name": "Vikram Kamath Cannanure"
                    }
                ],
                "author_detail": {
                    "name": "Vikram Kamath Cannanure"
                },
                "author": "Vikram Kamath Cannanure",
                "arxiv_doi": "10.1145/3719160.3736606",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3719160.3736606",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.05985v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05985v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in the Proceedings of the 7th ACM Conference on\n  Conversational User Interfaces (CUI '25)",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13962v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13962v1",
                "updated": "2025-08-19T15:54:51Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    15,
                    54,
                    51,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T15:54:51Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    15,
                    54,
                    51,
                    1,
                    231,
                    0
                ],
                "title": "Learning to Use AI for Learning: How Can We Effectively Teach and\n  Measure Prompting Literacy for K-12 Students?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Use AI for Learning: How Can We Effectively Teach and\n  Measure Prompting Literacy for K-12 Students?"
                },
                "summary": "As Artificial Intelligence (AI) becomes increasingly integrated into daily\nlife, there is a growing need to equip the next generation with the ability to\napply, interact with, evaluate, and collaborate with AI systems responsibly.\nPrior research highlights the urgent demand from K-12 educators to teach\nstudents the ethical and effective use of AI for learning. To address this\nneed, we designed an Large-Language Model (LLM)-based module to teach prompting\nliteracy. This includes scenario-based deliberate practice activities with\ndirect interaction with intelligent LLM agents, aiming to foster secondary\nschool students' responsible engagement with AI chatbots. We conducted two\niterations of classroom deployment in 11 authentic secondary education\nclassrooms, and evaluated 1) AI-based auto-grader's capability; 2) students'\nprompting performance and confidence changes towards using AI for learning; and\n3) the quality of learning and assessment materials. Results indicated that the\nAI-based auto-grader could grade student-written prompts with satisfactory\nquality. In addition, the instructional materials supported students in\nimproving their prompting skills through practice and led to positive shifts in\ntheir perceptions of using AI for learning. Furthermore, data from Study 1\ninformed assessment revisions in Study 2. Analyses of item difficulty and\ndiscrimination in Study 2 showed that True/False and open-ended questions could\nmeasure prompting literacy more effectively than multiple-choice questions for\nour target learners. These promising outcomes highlight the potential for\nbroader deployment and highlight the need for broader studies to assess\nlearning effectiveness and assessment design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Artificial Intelligence (AI) becomes increasingly integrated into daily\nlife, there is a growing need to equip the next generation with the ability to\napply, interact with, evaluate, and collaborate with AI systems responsibly.\nPrior research highlights the urgent demand from K-12 educators to teach\nstudents the ethical and effective use of AI for learning. To address this\nneed, we designed an Large-Language Model (LLM)-based module to teach prompting\nliteracy. This includes scenario-based deliberate practice activities with\ndirect interaction with intelligent LLM agents, aiming to foster secondary\nschool students' responsible engagement with AI chatbots. We conducted two\niterations of classroom deployment in 11 authentic secondary education\nclassrooms, and evaluated 1) AI-based auto-grader's capability; 2) students'\nprompting performance and confidence changes towards using AI for learning; and\n3) the quality of learning and assessment materials. Results indicated that the\nAI-based auto-grader could grade student-written prompts with satisfactory\nquality. In addition, the instructional materials supported students in\nimproving their prompting skills through practice and led to positive shifts in\ntheir perceptions of using AI for learning. Furthermore, data from Study 1\ninformed assessment revisions in Study 2. Analyses of item difficulty and\ndiscrimination in Study 2 showed that True/False and open-ended questions could\nmeasure prompting literacy more effectively than multiple-choice questions for\nour target learners. These promising outcomes highlight the potential for\nbroader deployment and highlight the need for broader studies to assess\nlearning effectiveness and assessment design."
                },
                "authors": [
                    {
                        "name": "Ruiwei Xiao"
                    },
                    {
                        "name": "Xinying Hou"
                    },
                    {
                        "name": "Ying-Jui Tseng"
                    },
                    {
                        "name": "Hsuan Nieu"
                    },
                    {
                        "name": "Guanze Liao"
                    },
                    {
                        "name": "John Stamper"
                    },
                    {
                        "name": "Kenneth R. Koedinger"
                    }
                ],
                "author_detail": {
                    "name": "Kenneth R. Koedinger"
                },
                "author": "Kenneth R. Koedinger",
                "arxiv_comment": "7 pages + 2 pages references; under review for an [anonymized\n  according to the conference policy] conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13962v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13962v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24688v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24688v3",
                "updated": "2025-08-19T15:45:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    15,
                    45,
                    20,
                    1,
                    231,
                    0
                ],
                "published": "2025-05-30T15:11:52Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    15,
                    11,
                    52,
                    4,
                    150,
                    0
                ],
                "title": "Soft Reasoning: Navigating Solution Spaces in Large Language Models\n  through Controlled Embedding Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Soft Reasoning: Navigating Solution Spaces in Large Language Models\n  through Controlled Embedding Exploration"
                },
                "summary": "Large Language Models (LLMs) struggle with complex reasoning due to limited\ndiversity and inefficient search. We propose Soft Reasoning, an embedding-based\nsearch framework that optimises the embedding of the first token to guide\ngeneration. It combines (1) embedding perturbation for controlled exploration\nand (2) Bayesian optimisation to refine embeddings via a verifier-guided\nobjective, balancing exploration and exploitation. This approach improves\nreasoning accuracy and coherence while avoiding reliance on heuristic search.\nExperiments demonstrate superior correctness with minimal computation, making\nit a scalable, model-agnostic solution. The code is released at\nhttps://github.com/alickzhu/Soft-Reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) struggle with complex reasoning due to limited\ndiversity and inefficient search. We propose Soft Reasoning, an embedding-based\nsearch framework that optimises the embedding of the first token to guide\ngeneration. It combines (1) embedding perturbation for controlled exploration\nand (2) Bayesian optimisation to refine embeddings via a verifier-guided\nobjective, balancing exploration and exploitation. This approach improves\nreasoning accuracy and coherence while avoiding reliance on heuristic search.\nExperiments demonstrate superior correctness with minimal computation, making\nit a scalable, model-agnostic solution. The code is released at\nhttps://github.com/alickzhu/Soft-Reasoning."
                },
                "authors": [
                    {
                        "name": "Qinglin Zhu"
                    },
                    {
                        "name": "Runcong Zhao"
                    },
                    {
                        "name": "Hanqi Yan"
                    },
                    {
                        "name": "Yulan He"
                    },
                    {
                        "name": "Yudong Chen"
                    },
                    {
                        "name": "Lin Gui"
                    }
                ],
                "author_detail": {
                    "name": "Lin Gui"
                },
                "author": "Lin Gui",
                "arxiv_comment": "Accepted as a Spotlight at ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24688v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24688v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13953v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13953v1",
                "updated": "2025-08-19T15:44:27Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    15,
                    44,
                    27,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T15:44:27Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    15,
                    44,
                    27,
                    1,
                    231,
                    0
                ],
                "title": "ReviewGraph: A Knowledge Graph Embedding Based Framework for Review\n  Rating Prediction with Sentiment Features",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReviewGraph: A Knowledge Graph Embedding Based Framework for Review\n  Rating Prediction with Sentiment Features"
                },
                "summary": "In the hospitality industry, understanding the factors that drive customer\nreview ratings is critical for improving guest satisfaction and business\nperformance. This work proposes ReviewGraph for Review Rating Prediction (RRP),\na novel framework that transforms textual customer reviews into knowledge\ngraphs by extracting (subject, predicate, object) triples and associating\nsentiment scores. Using graph embeddings (Node2Vec) and sentiment features, the\nframework predicts review rating scores through machine learning classifiers.\nWe compare ReviewGraph performance with traditional NLP baselines (such as Bag\nof Words, TF-IDF, and Word2Vec) and large language models (LLMs), evaluating\nthem in the HotelRec dataset. In comparison to the state of the art literature,\nour proposed model performs similar to their best performing model but with\nlower computational cost (without ensemble).\n  While ReviewGraph achieves comparable predictive performance to LLMs and\noutperforms baselines on agreement-based metrics such as Cohen's Kappa, it\noffers additional advantages in interpretability, visual exploration, and\npotential integration into Retrieval-Augmented Generation (RAG) systems. This\nwork highlights the potential of graph-based representations for enhancing\nreview analytics and lays the groundwork for future research integrating\nadvanced graph neural networks and fine-tuned LLM-based extraction methods. We\nwill share ReviewGraph output and platform open-sourced on our GitHub page\nhttps://github.com/aaronlifenghan/ReviewGraph",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the hospitality industry, understanding the factors that drive customer\nreview ratings is critical for improving guest satisfaction and business\nperformance. This work proposes ReviewGraph for Review Rating Prediction (RRP),\na novel framework that transforms textual customer reviews into knowledge\ngraphs by extracting (subject, predicate, object) triples and associating\nsentiment scores. Using graph embeddings (Node2Vec) and sentiment features, the\nframework predicts review rating scores through machine learning classifiers.\nWe compare ReviewGraph performance with traditional NLP baselines (such as Bag\nof Words, TF-IDF, and Word2Vec) and large language models (LLMs), evaluating\nthem in the HotelRec dataset. In comparison to the state of the art literature,\nour proposed model performs similar to their best performing model but with\nlower computational cost (without ensemble).\n  While ReviewGraph achieves comparable predictive performance to LLMs and\noutperforms baselines on agreement-based metrics such as Cohen's Kappa, it\noffers additional advantages in interpretability, visual exploration, and\npotential integration into Retrieval-Augmented Generation (RAG) systems. This\nwork highlights the potential of graph-based representations for enhancing\nreview analytics and lays the groundwork for future research integrating\nadvanced graph neural networks and fine-tuned LLM-based extraction methods. We\nwill share ReviewGraph output and platform open-sourced on our GitHub page\nhttps://github.com/aaronlifenghan/ReviewGraph"
                },
                "authors": [
                    {
                        "name": "A. J. W. de Vink"
                    },
                    {
                        "name": "Natalia Amat-Lefort"
                    },
                    {
                        "name": "Lifeng Han"
                    }
                ],
                "author_detail": {
                    "name": "Lifeng Han"
                },
                "author": "Lifeng Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13953v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13953v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13948v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13948v1",
                "updated": "2025-08-19T15:37:29Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    15,
                    37,
                    29,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T15:37:29Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    15,
                    37,
                    29,
                    1,
                    231,
                    0
                ],
                "title": "Prompt Orchestration Markup Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt Orchestration Markup Language"
                },
                "summary": "Large Language Models (LLMs) require sophisticated prompting, yet current\npractices face challenges in structure, data integration, format sensitivity,\nand tooling. Existing methods lack comprehensive solutions for organizing\ncomplex prompts involving diverse data types (documents, tables, images) or\nmanaging presentation variations systematically. To address these gaps, we\nintroduce POML (Prompt Orchestration Markup Language). POML employs\ncomponent-based markup for logical structure (roles, tasks, examples),\nspecialized tags for seamless data integration, and a CSS-like styling system\nto decouple content from presentation, reducing formatting sensitivity. It\nincludes templating for dynamic prompts and a comprehensive developer toolkit\n(IDE support, SDKs) to improve version control and collaboration. We validate\nPOML through two case studies demonstrating its impact on complex application\nintegration (PomLink) and accuracy performance (TableQA), as well as a user\nstudy assessing its effectiveness in real-world development scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) require sophisticated prompting, yet current\npractices face challenges in structure, data integration, format sensitivity,\nand tooling. Existing methods lack comprehensive solutions for organizing\ncomplex prompts involving diverse data types (documents, tables, images) or\nmanaging presentation variations systematically. To address these gaps, we\nintroduce POML (Prompt Orchestration Markup Language). POML employs\ncomponent-based markup for logical structure (roles, tasks, examples),\nspecialized tags for seamless data integration, and a CSS-like styling system\nto decouple content from presentation, reducing formatting sensitivity. It\nincludes templating for dynamic prompts and a comprehensive developer toolkit\n(IDE support, SDKs) to improve version control and collaboration. We validate\nPOML through two case studies demonstrating its impact on complex application\nintegration (PomLink) and accuracy performance (TableQA), as well as a user\nstudy assessing its effectiveness in real-world development scenarios."
                },
                "authors": [
                    {
                        "name": "Yuge Zhang"
                    },
                    {
                        "name": "Nan Chen"
                    },
                    {
                        "name": "Jiahang Xu"
                    },
                    {
                        "name": "Yuqing Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yuqing Yang"
                },
                "author": "Yuqing Yang",
                "arxiv_comment": "All findings in this paper are derived from a POML snapshot as of\n  February 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13948v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13948v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13943v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13943v1",
                "updated": "2025-08-19T15:31:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    15,
                    31,
                    37,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T15:31:37Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    15,
                    31,
                    37,
                    1,
                    231,
                    0
                ],
                "title": "LLM-Powered Virtual Patient Agents for Interactive Clinical Skills\n  Training with Automated Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Powered Virtual Patient Agents for Interactive Clinical Skills\n  Training with Automated Feedback"
                },
                "summary": "Objective Structured Clinical Examinations (OSCEs) are essential for medical\ntraining, but they require significant resources, including professional actors\nand expert medical feedback. Although Large Language Models (LLMs) have\nintroduced text-based virtual patients for communication practice, these\nsimulations often lack the capability for richer, non-textual interactions.\nThis paper presents a novel framework that significantly enhances LLM-based\nsimulated patients by equipping them with action spaces, thereby enabling more\nrealistic and dynamic patient behaviors that extend beyond text. Furthermore,\nour system incorporates virtual tutors that provide students with instant,\npersonalized feedback on their performance at any time during these simulated\nencounters. We have conducted a rigorous evaluation of the framework's\nreal-time performance, including system latency and component accuracy.\nPreliminary evaluations with medical experts assessed the naturalness and\ncoherence of the simulated patients, as well as the usefulness and\nappropriateness of the virtual tutor's assessments. This innovative system\nprovides medical students with a low-cost, accessible platform for personalized\nOSCE preparation at home.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Objective Structured Clinical Examinations (OSCEs) are essential for medical\ntraining, but they require significant resources, including professional actors\nand expert medical feedback. Although Large Language Models (LLMs) have\nintroduced text-based virtual patients for communication practice, these\nsimulations often lack the capability for richer, non-textual interactions.\nThis paper presents a novel framework that significantly enhances LLM-based\nsimulated patients by equipping them with action spaces, thereby enabling more\nrealistic and dynamic patient behaviors that extend beyond text. Furthermore,\nour system incorporates virtual tutors that provide students with instant,\npersonalized feedback on their performance at any time during these simulated\nencounters. We have conducted a rigorous evaluation of the framework's\nreal-time performance, including system latency and component accuracy.\nPreliminary evaluations with medical experts assessed the naturalness and\ncoherence of the simulated patients, as well as the usefulness and\nappropriateness of the virtual tutor's assessments. This innovative system\nprovides medical students with a low-cost, accessible platform for personalized\nOSCE preparation at home."
                },
                "authors": [
                    {
                        "name": "Henrik Voigt"
                    },
                    {
                        "name": "Yurina Sugamiya"
                    },
                    {
                        "name": "Kai Lawonn"
                    },
                    {
                        "name": "Sina Zarrieß"
                    },
                    {
                        "name": "Atsuo Takanishi"
                    }
                ],
                "author_detail": {
                    "name": "Atsuo Takanishi"
                },
                "author": "Atsuo Takanishi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13943v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13943v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13942v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13942v1",
                "updated": "2025-08-19T15:31:23Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    15,
                    31,
                    23,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T15:31:23Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    15,
                    31,
                    23,
                    1,
                    231,
                    0
                ],
                "title": "The Collaboration Paradox: Why Generative AI Requires Both Strategic\n  Intelligence and Operational Stability in Supply Chain Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Collaboration Paradox: Why Generative AI Requires Both Strategic\n  Intelligence and Operational Stability in Supply Chain Management"
                },
                "summary": "The rise of autonomous, AI-driven agents in economic settings raises critical\nquestions about their emergent strategic behavior. This paper investigates\nthese dynamics in the cooperative context of a multi-echelon supply chain, a\nsystem famously prone to instabilities like the bullwhip effect. We conduct\ncomputational experiments with generative AI agents, powered by Large Language\nModels (LLMs), within a controlled supply chain simulation designed to isolate\ntheir behavioral tendencies. Our central finding is the \"collaboration\nparadox\": a novel, catastrophic failure mode where theoretically superior\ncollaborative AI agents, designed with Vendor-Managed Inventory (VMI)\nprinciples, perform even worse than non-AI baselines. We demonstrate that this\nparadox arises from an operational flaw where agents hoard inventory, starving\nthe system. We then show that resilience is only achieved through a synthesis\nof two distinct layers: high-level, AI-driven proactive policy-setting to\nestablish robust operational targets, and a low-level, collaborative execution\nprotocol with proactive downstream replenishment to maintain stability. Our\nfinal framework, which implements this synthesis, can autonomously generate,\nevaluate, and quantify a portfolio of viable strategic choices. The work\nprovides a crucial insight into the emergent behaviors of collaborative AI\nagents and offers a blueprint for designing stable, effective AI-driven systems\nfor business analytics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of autonomous, AI-driven agents in economic settings raises critical\nquestions about their emergent strategic behavior. This paper investigates\nthese dynamics in the cooperative context of a multi-echelon supply chain, a\nsystem famously prone to instabilities like the bullwhip effect. We conduct\ncomputational experiments with generative AI agents, powered by Large Language\nModels (LLMs), within a controlled supply chain simulation designed to isolate\ntheir behavioral tendencies. Our central finding is the \"collaboration\nparadox\": a novel, catastrophic failure mode where theoretically superior\ncollaborative AI agents, designed with Vendor-Managed Inventory (VMI)\nprinciples, perform even worse than non-AI baselines. We demonstrate that this\nparadox arises from an operational flaw where agents hoard inventory, starving\nthe system. We then show that resilience is only achieved through a synthesis\nof two distinct layers: high-level, AI-driven proactive policy-setting to\nestablish robust operational targets, and a low-level, collaborative execution\nprotocol with proactive downstream replenishment to maintain stability. Our\nfinal framework, which implements this synthesis, can autonomously generate,\nevaluate, and quantify a portfolio of viable strategic choices. The work\nprovides a crucial insight into the emergent behaviors of collaborative AI\nagents and offers a blueprint for designing stable, effective AI-driven systems\nfor business analytics."
                },
                "authors": [
                    {
                        "name": "Soumyadeep Dhar"
                    }
                ],
                "author_detail": {
                    "name": "Soumyadeep Dhar"
                },
                "author": "Soumyadeep Dhar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13942v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13942v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13937v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13937v1",
                "updated": "2025-08-19T15:27:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    15,
                    27,
                    19,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T15:27:19Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    15,
                    27,
                    19,
                    1,
                    231,
                    0
                ],
                "title": "Evaluating Particle Filtering for RSS-Based Target Localization under\n  Varying Noise Levels and Sensor Geometries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Particle Filtering for RSS-Based Target Localization under\n  Varying Noise Levels and Sensor Geometries"
                },
                "summary": "Target localization is a critical task in various applications, such as\nsearch and rescue, surveillance, and wireless sensor networks. When a target\nemits a radio frequency (RF) signal, spatially distributed sensors can collect\nsignal measurements to estimate the target's location. Among various\nmeasurement modalities, received signal strength (RSS) is particularly\nattractive due to its low cost, low power consumption, and ease of deployment.\nWhile particle filtering has previously been applied to RSS-based target\nlocalization, few studies have systematically analyzed its performance under\nvarying sensor geometries and RSS noise levels. This paper addresses this gap\nby designing and evaluating a particle filtering algorithm for localizing a\nstationary target. The proposed method is compared with a conventional\nRSS-based trilateration approach across different sensor configurations and\nnoise conditions. Simulation results indicate that particle filtering provides\nmore accurate target localization than trilateration, particularly in scenarios\nwith unfavorable sensor geometries and high RSS noise.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Target localization is a critical task in various applications, such as\nsearch and rescue, surveillance, and wireless sensor networks. When a target\nemits a radio frequency (RF) signal, spatially distributed sensors can collect\nsignal measurements to estimate the target's location. Among various\nmeasurement modalities, received signal strength (RSS) is particularly\nattractive due to its low cost, low power consumption, and ease of deployment.\nWhile particle filtering has previously been applied to RSS-based target\nlocalization, few studies have systematically analyzed its performance under\nvarying sensor geometries and RSS noise levels. This paper addresses this gap\nby designing and evaluating a particle filtering algorithm for localizing a\nstationary target. The proposed method is compared with a conventional\nRSS-based trilateration approach across different sensor configurations and\nnoise conditions. Simulation results indicate that particle filtering provides\nmore accurate target localization than trilateration, particularly in scenarios\nwith unfavorable sensor geometries and high RSS noise."
                },
                "authors": [
                    {
                        "name": "Halim Lee"
                    },
                    {
                        "name": "Jongmin Park"
                    },
                    {
                        "name": "Kwansik Park"
                    }
                ],
                "author_detail": {
                    "name": "Kwansik Park"
                },
                "author": "Kwansik Park",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13937v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13937v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13930v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13930v1",
                "updated": "2025-08-19T15:23:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    15,
                    23,
                    18,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T15:23:18Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    15,
                    23,
                    18,
                    1,
                    231,
                    0
                ],
                "title": "InPars+: Supercharging Synthetic Data Generation for Information\n  Retrieval Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InPars+: Supercharging Synthetic Data Generation for Information\n  Retrieval Systems"
                },
                "summary": "This work revisits and extends synthetic query generation pipelines for\nNeural Information Retrieval (NIR) by leveraging the InPars Toolkit, a\nreproducible, end-to-end framework for generating training data using large\nlanguage models (LLMs). We first assess the reproducibility of the original\nInPars, InPars-V2, and Promptagator pipelines on the SciFact benchmark and\nvalidate their effectiveness using open-source reranker and generator models.\nBuilding on this foundation, we introduce two key extensions to the pipeline:\n(1) fine-tuning a query generator LLM via Contrastive Preference Optimization\n(CPO) to improve the signal quality in generated queries, and (2) replacing\nstatic prompt templates with dynamic, Chain-of-Thought (CoT) optimized prompts\nusing the DSPy framework. Our results show that both extensions reduce the need\nfor aggressive filtering while improving retrieval performance. All code,\nmodels, and synthetic datasets are publicly released to support further\nresearch at: \\href{https://github.com/danilotpnta/IR2-project}{this https URL}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work revisits and extends synthetic query generation pipelines for\nNeural Information Retrieval (NIR) by leveraging the InPars Toolkit, a\nreproducible, end-to-end framework for generating training data using large\nlanguage models (LLMs). We first assess the reproducibility of the original\nInPars, InPars-V2, and Promptagator pipelines on the SciFact benchmark and\nvalidate their effectiveness using open-source reranker and generator models.\nBuilding on this foundation, we introduce two key extensions to the pipeline:\n(1) fine-tuning a query generator LLM via Contrastive Preference Optimization\n(CPO) to improve the signal quality in generated queries, and (2) replacing\nstatic prompt templates with dynamic, Chain-of-Thought (CoT) optimized prompts\nusing the DSPy framework. Our results show that both extensions reduce the need\nfor aggressive filtering while improving retrieval performance. All code,\nmodels, and synthetic datasets are publicly released to support further\nresearch at: \\href{https://github.com/danilotpnta/IR2-project}{this https URL}."
                },
                "authors": [
                    {
                        "name": "Matey Krastev"
                    },
                    {
                        "name": "Miklos Hamar"
                    },
                    {
                        "name": "Danilo Toapanta"
                    },
                    {
                        "name": "Jesse Brouwers"
                    },
                    {
                        "name": "Yibin Lei"
                    }
                ],
                "author_detail": {
                    "name": "Yibin Lei"
                },
                "author": "Yibin Lei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13930v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13930v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13920v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13920v1",
                "updated": "2025-08-19T15:17:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    15,
                    17,
                    31,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T15:17:31Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    15,
                    17,
                    31,
                    1,
                    231,
                    0
                ],
                "title": "LLMind 2.0: Distributed IoT Automation with Natural Language M2M\n  Communication and Lightweight LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMind 2.0: Distributed IoT Automation with Natural Language M2M\n  Communication and Lightweight LLM Agents"
                },
                "summary": "Recent advances in large language models (LLMs) have sparked interest in\ntheir application to IoT and automation systems, particularly for facilitating\ndevice management through natural language instructions. However, existing\ncentralized approaches face significant scalability challenges when managing\nand coordinating the collaboration between IoT devices of diverse capabilities\nin large-scale heterogeneous IoT systems. This paper introduces LLMind 2.0, a\ndistributed IoT automation framework that addresses the scalability challenges\nthrough lightweight LLM-empowered device agents via natural language-based\nmachine-to-machine (M2M) communication. Unlike previous LLM-controlled\nautomation systems that rely on a centralized coordinator to generate\ndevice-specific code to be executed on individual devices, LLMind 2.0\ndistributes intelligence across individual devices through lightweight LLMs\nembedded in IoT devices. The central coordinator translates human instructions\ninto simple subtasks described in natural human language, which are then\nprocessed by device-specific agents to generate device-specific code locally at\nthe associated devices. This approach transcends device heterogeneity barriers\nby using natural language as a unified communication medium, enabling seamless\ncollaboration between devices from different manufacturers. The system\nincorporates several key innovations: a Retrieval-Augmented Generation (RAG)\nmechanism for accurate subtask-to-API mapping, fine-tuned lightweight LLMs for\nreliable code generation, and a finite state machine-based task execution\nframework. Experimental validation in multi-robot warehouse scenarios and\nreal-world WiFi network deployments demonstrates significant improvements in\nscalability, reliability, and privacy protection compared to the centralized\napproach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have sparked interest in\ntheir application to IoT and automation systems, particularly for facilitating\ndevice management through natural language instructions. However, existing\ncentralized approaches face significant scalability challenges when managing\nand coordinating the collaboration between IoT devices of diverse capabilities\nin large-scale heterogeneous IoT systems. This paper introduces LLMind 2.0, a\ndistributed IoT automation framework that addresses the scalability challenges\nthrough lightweight LLM-empowered device agents via natural language-based\nmachine-to-machine (M2M) communication. Unlike previous LLM-controlled\nautomation systems that rely on a centralized coordinator to generate\ndevice-specific code to be executed on individual devices, LLMind 2.0\ndistributes intelligence across individual devices through lightweight LLMs\nembedded in IoT devices. The central coordinator translates human instructions\ninto simple subtasks described in natural human language, which are then\nprocessed by device-specific agents to generate device-specific code locally at\nthe associated devices. This approach transcends device heterogeneity barriers\nby using natural language as a unified communication medium, enabling seamless\ncollaboration between devices from different manufacturers. The system\nincorporates several key innovations: a Retrieval-Augmented Generation (RAG)\nmechanism for accurate subtask-to-API mapping, fine-tuned lightweight LLMs for\nreliable code generation, and a finite state machine-based task execution\nframework. Experimental validation in multi-robot warehouse scenarios and\nreal-world WiFi network deployments demonstrates significant improvements in\nscalability, reliability, and privacy protection compared to the centralized\napproach."
                },
                "authors": [
                    {
                        "name": "Yuyang Du"
                    },
                    {
                        "name": "Qun Yang"
                    },
                    {
                        "name": "Liujianfu Wang"
                    },
                    {
                        "name": "Jingqi Lin"
                    },
                    {
                        "name": "Hongwei Cui"
                    },
                    {
                        "name": "Soung Chang Liew"
                    }
                ],
                "author_detail": {
                    "name": "Soung Chang Liew"
                },
                "author": "Soung Chang Liew",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13920v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13920v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13915v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13915v1",
                "updated": "2025-08-19T15:14:49Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    15,
                    14,
                    49,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T15:14:49Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    15,
                    14,
                    49,
                    1,
                    231,
                    0
                ],
                "title": "Structured Agentic Workflows for Financial Time-Series Modeling with\n  LLMs and Reflective Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structured Agentic Workflows for Financial Time-Series Modeling with\n  LLMs and Reflective Feedback"
                },
                "summary": "Time-series data is central to decision-making in financial markets, yet\nbuilding high-performing, interpretable, and auditable models remains a major\nchallenge. While Automated Machine Learning (AutoML) frameworks streamline\nmodel development, they often lack adaptability and responsiveness to\ndomain-specific needs and evolving objectives. Concurrently, Large Language\nModels (LLMs) have enabled agentic systems capable of reasoning, memory\nmanagement, and dynamic code generation, offering a path toward more flexible\nworkflow automation. In this paper, we introduce \\textsf{TS-Agent}, a modular\nagentic framework designed to automate and enhance time-series modeling\nworkflows for financial applications. The agent formalizes the pipeline as a\nstructured, iterative decision process across three stages: model selection,\ncode refinement, and fine-tuning, guided by contextual reasoning and\nexperimental feedback. Central to our architecture is a planner agent equipped\nwith structured knowledge banks, curated libraries of models and refinement\nstrategies, which guide exploration, while improving interpretability and\nreducing error propagation. \\textsf{TS-Agent} supports adaptive learning,\nrobust debugging, and transparent auditing, key requirements for high-stakes\nenvironments such as financial services. Empirical evaluations on diverse\nfinancial forecasting and synthetic data generation tasks demonstrate that\n\\textsf{TS-Agent} consistently outperforms state-of-the-art AutoML and agentic\nbaselines, achieving superior accuracy, robustness, and decision traceability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-series data is central to decision-making in financial markets, yet\nbuilding high-performing, interpretable, and auditable models remains a major\nchallenge. While Automated Machine Learning (AutoML) frameworks streamline\nmodel development, they often lack adaptability and responsiveness to\ndomain-specific needs and evolving objectives. Concurrently, Large Language\nModels (LLMs) have enabled agentic systems capable of reasoning, memory\nmanagement, and dynamic code generation, offering a path toward more flexible\nworkflow automation. In this paper, we introduce \\textsf{TS-Agent}, a modular\nagentic framework designed to automate and enhance time-series modeling\nworkflows for financial applications. The agent formalizes the pipeline as a\nstructured, iterative decision process across three stages: model selection,\ncode refinement, and fine-tuning, guided by contextual reasoning and\nexperimental feedback. Central to our architecture is a planner agent equipped\nwith structured knowledge banks, curated libraries of models and refinement\nstrategies, which guide exploration, while improving interpretability and\nreducing error propagation. \\textsf{TS-Agent} supports adaptive learning,\nrobust debugging, and transparent auditing, key requirements for high-stakes\nenvironments such as financial services. Empirical evaluations on diverse\nfinancial forecasting and synthetic data generation tasks demonstrate that\n\\textsf{TS-Agent} consistently outperforms state-of-the-art AutoML and agentic\nbaselines, achieving superior accuracy, robustness, and decision traceability."
                },
                "authors": [
                    {
                        "name": "Yihao Ang"
                    },
                    {
                        "name": "Yifan Bao"
                    },
                    {
                        "name": "Lei Jiang"
                    },
                    {
                        "name": "Jiajie Tao"
                    },
                    {
                        "name": "Anthony K. H. Tung"
                    },
                    {
                        "name": "Lukasz Szpruch"
                    },
                    {
                        "name": "Hao Ni"
                    }
                ],
                "author_detail": {
                    "name": "Hao Ni"
                },
                "author": "Hao Ni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13915v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13915v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13908v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13908v1",
                "updated": "2025-08-19T15:07:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    15,
                    7,
                    21,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T15:07:21Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    15,
                    7,
                    21,
                    1,
                    231,
                    0
                ],
                "title": "Translating the Force Concept Inventory in the age of AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Translating the Force Concept Inventory in the age of AI"
                },
                "summary": "We present a study that translates the Force Concept Inventory (FCI) using\nOpenAI GPT-4o and assess the specific difficulties of translating a\nscientific-focused topic using Large Language Models (LLMs). The FCI is a\nphysics exam meant to evaluate outcomes of a student cohort before and after\ninstruction in Newtonian physics. We examine the problem-solving ability of the\nLLM in both the translated document and the translation back into English,\ndetailing the language-dependent issues that complicate the translation. While\nChatGPT performs remarkably well on answering the questions in both the\ntranslated language as well as the back-translation into English, problems\narise with language-specific nuances and formatting. Pitfalls include words or\nphrases that lack one-to-one matching terms in another language, especially\ndiscipline-specific scientific terms, or outright mistranslations. Depending on\nthe context, these translations can result in a critical change in the physical\nmeaning of the problem. Additionally, issues with question numbering and\nlettering are found in some languages. The issues around the translations of\nnumbering and lettering provide insight into the abilities of the LLM and\nsuggest that it is not simply relying upon FCI questions that may have been\npart of the LLM training data to provide answers. These findings underscore\nthat while LLMs can accelerate multilingual access to educational tools,\ncareful review is still needed to ensure fidelity and clarity in translated\nassessments. LLMs provide a new opportunity to expand educational tools and\nassessments. At the same time, there are unique challenges using LLMs to\nfacilitate translations that this case study examines in detail.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a study that translates the Force Concept Inventory (FCI) using\nOpenAI GPT-4o and assess the specific difficulties of translating a\nscientific-focused topic using Large Language Models (LLMs). The FCI is a\nphysics exam meant to evaluate outcomes of a student cohort before and after\ninstruction in Newtonian physics. We examine the problem-solving ability of the\nLLM in both the translated document and the translation back into English,\ndetailing the language-dependent issues that complicate the translation. While\nChatGPT performs remarkably well on answering the questions in both the\ntranslated language as well as the back-translation into English, problems\narise with language-specific nuances and formatting. Pitfalls include words or\nphrases that lack one-to-one matching terms in another language, especially\ndiscipline-specific scientific terms, or outright mistranslations. Depending on\nthe context, these translations can result in a critical change in the physical\nmeaning of the problem. Additionally, issues with question numbering and\nlettering are found in some languages. The issues around the translations of\nnumbering and lettering provide insight into the abilities of the LLM and\nsuggest that it is not simply relying upon FCI questions that may have been\npart of the LLM training data to provide answers. These findings underscore\nthat while LLMs can accelerate multilingual access to educational tools,\ncareful review is still needed to ensure fidelity and clarity in translated\nassessments. LLMs provide a new opportunity to expand educational tools and\nassessments. At the same time, there are unique challenges using LLMs to\nfacilitate translations that this case study examines in detail."
                },
                "authors": [
                    {
                        "name": "Marina Babayeva"
                    },
                    {
                        "name": "Justin Dunlap"
                    },
                    {
                        "name": "Marie Snětinová"
                    },
                    {
                        "name": "Ralf Widenhorn"
                    }
                ],
                "author_detail": {
                    "name": "Ralf Widenhorn"
                },
                "author": "Ralf Widenhorn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13908v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13908v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ed-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ed-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13905v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13905v1",
                "updated": "2025-08-19T15:06:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    15,
                    6,
                    4,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T15:06:04Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    15,
                    6,
                    4,
                    1,
                    231,
                    0
                ],
                "title": "Automated Energy-Aware Time-Series Model Deployment on Embedded FPGAs\n  for Resilient Combined Sewer Overflow Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Energy-Aware Time-Series Model Deployment on Embedded FPGAs\n  for Resilient Combined Sewer Overflow Management"
                },
                "summary": "Extreme weather events, intensified by climate change, increasingly challenge\naging combined sewer systems, raising the risk of untreated wastewater\noverflow. Accurate forecasting of sewer overflow basin filling levels can\nprovide actionable insights for early intervention, helping mitigating\nuncontrolled discharge. In recent years, AI-based forecasting methods have\noffered scalable alternatives to traditional physics-based models, but their\nreliance on cloud computing limits their reliability during communication\noutages. To address this, we propose an end-to-end forecasting framework that\nenables energy-efficient inference directly on edge devices. Our solution\nintegrates lightweight Transformer and Long Short-Term Memory (LSTM) models,\ncompressed via integer-only quantization for efficient on-device execution.\nMoreover, an automated hardware-aware deployment pipeline is used to search for\noptimal model configurations by jointly minimizing prediction error and energy\nconsumption on an AMD Spartan-7 XC7S15 FPGA. Evaluated on real-world sewer\ndata, the selected 8-bit Transformer model, trained on 24 hours of historical\nmeasurements, achieves high accuracy (MSE 0.0376) at an energy cost of 0.370 mJ\nper inference. In contrast, the optimal 8-bit LSTM model requires significantly\nless energy (0.009 mJ, over 40x lower) but yields 14.89% worse accuracy (MSE\n0.0432) and much longer training time. This trade-off highlights the need to\nalign model selection with deployment priorities, favoring LSTM for ultra-low\nenergy consumption or Transformer for higher predictive accuracy. In general,\nour work enables local, energy-efficient forecasting, contributing to more\nresilient combined sewer systems. All code can be found in the GitHub\nRepository (https://github.com/tianheng-ling/EdgeOverflowForecast).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extreme weather events, intensified by climate change, increasingly challenge\naging combined sewer systems, raising the risk of untreated wastewater\noverflow. Accurate forecasting of sewer overflow basin filling levels can\nprovide actionable insights for early intervention, helping mitigating\nuncontrolled discharge. In recent years, AI-based forecasting methods have\noffered scalable alternatives to traditional physics-based models, but their\nreliance on cloud computing limits their reliability during communication\noutages. To address this, we propose an end-to-end forecasting framework that\nenables energy-efficient inference directly on edge devices. Our solution\nintegrates lightweight Transformer and Long Short-Term Memory (LSTM) models,\ncompressed via integer-only quantization for efficient on-device execution.\nMoreover, an automated hardware-aware deployment pipeline is used to search for\noptimal model configurations by jointly minimizing prediction error and energy\nconsumption on an AMD Spartan-7 XC7S15 FPGA. Evaluated on real-world sewer\ndata, the selected 8-bit Transformer model, trained on 24 hours of historical\nmeasurements, achieves high accuracy (MSE 0.0376) at an energy cost of 0.370 mJ\nper inference. In contrast, the optimal 8-bit LSTM model requires significantly\nless energy (0.009 mJ, over 40x lower) but yields 14.89% worse accuracy (MSE\n0.0432) and much longer training time. This trade-off highlights the need to\nalign model selection with deployment priorities, favoring LSTM for ultra-low\nenergy consumption or Transformer for higher predictive accuracy. In general,\nour work enables local, energy-efficient forecasting, contributing to more\nresilient combined sewer systems. All code can be found in the GitHub\nRepository (https://github.com/tianheng-ling/EdgeOverflowForecast)."
                },
                "authors": [
                    {
                        "name": "Tianheng Ling"
                    },
                    {
                        "name": "Vipin Singh"
                    },
                    {
                        "name": "Chao Qian"
                    },
                    {
                        "name": "Felix Biessmann"
                    },
                    {
                        "name": "Gregor Schiele"
                    }
                ],
                "author_detail": {
                    "name": "Gregor Schiele"
                },
                "author": "Gregor Schiele",
                "arxiv_comment": "6 pages, 6 figures, 1 table, accepted by the 11th IEEE International\n  Smart Cities Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13905v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13905v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13889v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13889v1",
                "updated": "2025-08-19T14:53:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    14,
                    53,
                    30,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T14:53:30Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    14,
                    53,
                    30,
                    1,
                    231,
                    0
                ],
                "title": "CARE: Contextual Adaptation of Recommenders for LLM-based Conversational\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CARE: Contextual Adaptation of Recommenders for LLM-based Conversational\n  Recommendation"
                },
                "summary": "We tackle the challenge of integrating large language models (LLMs) with\nexternal recommender systems to enhance domain expertise in conversational\nrecommendation (CRS). Current LLM-based CRS approaches primarily rely on zero-\nor few-shot methods for generating item recommendations based on user queries,\nbut this method faces two significant challenges: (1) without domain-specific\nadaptation, LLMs frequently recommend items not in the target item space,\nresulting in low recommendation accuracy; and (2) LLMs largely rely on dialogue\ncontext for content-based recommendations, neglecting the collaborative\nrelationships among entities or item sequences. To address these limitations,\nwe introduce the CARE (Contextual Adaptation of Recommenders) framework. CARE\ncustomizes LLMs for CRS tasks, and synergizes them with external recommendation\nsystems. CARE (a) integrates external recommender systems as domain experts,\nproducing recommendations through entity-level insights, and (b) enhances those\nrecommendations by leveraging contextual information for more accurate and\nunbiased final recommendations using LLMs. Our results demonstrate that\nincorporating external recommender systems with entity-level information\nsignificantly enhances recommendation accuracy of LLM-based CRS by an average\nof 54% and 25% for ReDial and INSPIRED datasets. The most effective strategy in\nthe CARE framework involves LLMs selecting and reranking candidate items that\nexternal recommenders provide based on contextual insights. Our analysis\nindicates that the CARE framework effectively addresses the identified\nchallenges and mitigates the popularity bias in the external recommender.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We tackle the challenge of integrating large language models (LLMs) with\nexternal recommender systems to enhance domain expertise in conversational\nrecommendation (CRS). Current LLM-based CRS approaches primarily rely on zero-\nor few-shot methods for generating item recommendations based on user queries,\nbut this method faces two significant challenges: (1) without domain-specific\nadaptation, LLMs frequently recommend items not in the target item space,\nresulting in low recommendation accuracy; and (2) LLMs largely rely on dialogue\ncontext for content-based recommendations, neglecting the collaborative\nrelationships among entities or item sequences. To address these limitations,\nwe introduce the CARE (Contextual Adaptation of Recommenders) framework. CARE\ncustomizes LLMs for CRS tasks, and synergizes them with external recommendation\nsystems. CARE (a) integrates external recommender systems as domain experts,\nproducing recommendations through entity-level insights, and (b) enhances those\nrecommendations by leveraging contextual information for more accurate and\nunbiased final recommendations using LLMs. Our results demonstrate that\nincorporating external recommender systems with entity-level information\nsignificantly enhances recommendation accuracy of LLM-based CRS by an average\nof 54% and 25% for ReDial and INSPIRED datasets. The most effective strategy in\nthe CARE framework involves LLMs selecting and reranking candidate items that\nexternal recommenders provide based on contextual insights. Our analysis\nindicates that the CARE framework effectively addresses the identified\nchallenges and mitigates the popularity bias in the external recommender."
                },
                "authors": [
                    {
                        "name": "Chuang Li"
                    },
                    {
                        "name": "Yang Deng"
                    },
                    {
                        "name": "Hengchang Hu"
                    },
                    {
                        "name": "See-Kiong Ng"
                    },
                    {
                        "name": "Min-Yen Kan"
                    },
                    {
                        "name": "Haizhou Li"
                    }
                ],
                "author_detail": {
                    "name": "Haizhou Li"
                },
                "author": "Haizhou Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13889v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13889v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13881v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13881v1",
                "updated": "2025-08-19T14:43:51Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    14,
                    43,
                    51,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T14:43:51Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    14,
                    43,
                    51,
                    1,
                    231,
                    0
                ],
                "title": "Driving Style Recognition Like an Expert Using Semantic Privileged\n  Information from Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Driving Style Recognition Like an Expert Using Semantic Privileged\n  Information from Large Language Models"
                },
                "summary": "Existing driving style recognition systems largely depend on low-level\nsensor-derived features for training, neglecting the rich semantic reasoning\ncapability inherent to human experts. This discrepancy results in a fundamental\nmisalignment between algorithmic classifications and expert judgments. To\nbridge this gap, we propose a novel framework that integrates Semantic\nPrivileged Information (SPI) derived from large language models (LLMs) to align\nrecognition outcomes with human-interpretable reasoning. First, we introduce\nDriBehavGPT, an interactive LLM-based module that generates natural-language\ndescriptions of driving behaviors. These descriptions are then encoded into\nmachine learning-compatible representations via text embedding and\ndimensionality reduction. Finally, we incorporate them as privileged\ninformation into Support Vector Machine Plus (SVM+) for training, enabling the\nmodel to approximate human-like interpretation patterns. Experiments across\ndiverse real-world driving scenarios demonstrate that our SPI-enhanced\nframework outperforms conventional methods, achieving F1-score improvements of\n7.6% (car-following) and 7.9% (lane-changing). Importantly, SPI is exclusively\nused during training, while inference relies solely on sensor data, ensuring\ncomputational efficiency without sacrificing performance. These results\nhighlight the pivotal role of semantic behavioral representations in improving\nrecognition accuracy while advancing interpretable, human-centric driving\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing driving style recognition systems largely depend on low-level\nsensor-derived features for training, neglecting the rich semantic reasoning\ncapability inherent to human experts. This discrepancy results in a fundamental\nmisalignment between algorithmic classifications and expert judgments. To\nbridge this gap, we propose a novel framework that integrates Semantic\nPrivileged Information (SPI) derived from large language models (LLMs) to align\nrecognition outcomes with human-interpretable reasoning. First, we introduce\nDriBehavGPT, an interactive LLM-based module that generates natural-language\ndescriptions of driving behaviors. These descriptions are then encoded into\nmachine learning-compatible representations via text embedding and\ndimensionality reduction. Finally, we incorporate them as privileged\ninformation into Support Vector Machine Plus (SVM+) for training, enabling the\nmodel to approximate human-like interpretation patterns. Experiments across\ndiverse real-world driving scenarios demonstrate that our SPI-enhanced\nframework outperforms conventional methods, achieving F1-score improvements of\n7.6% (car-following) and 7.9% (lane-changing). Importantly, SPI is exclusively\nused during training, while inference relies solely on sensor data, ensuring\ncomputational efficiency without sacrificing performance. These results\nhighlight the pivotal role of semantic behavioral representations in improving\nrecognition accuracy while advancing interpretable, human-centric driving\nsystems."
                },
                "authors": [
                    {
                        "name": "Zhaokun Chen"
                    },
                    {
                        "name": "Chaopeng Zhang"
                    },
                    {
                        "name": "Xiaohan Li"
                    },
                    {
                        "name": "Wenshuo Wang"
                    },
                    {
                        "name": "Gentiane Venture"
                    },
                    {
                        "name": "Junqiang Xi"
                    }
                ],
                "author_detail": {
                    "name": "Junqiang Xi"
                },
                "author": "Junqiang Xi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13881v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13881v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13876v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13876v1",
                "updated": "2025-08-19T14:42:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    14,
                    42,
                    18,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T14:42:18Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    14,
                    42,
                    18,
                    1,
                    231,
                    0
                ],
                "title": "Improved Generalized Planning with LLMs through Strategy Refinement and\n  Reflection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improved Generalized Planning with LLMs through Strategy Refinement and\n  Reflection"
                },
                "summary": "LLMs have recently been used to generate Python programs representing\ngeneralized plans in PDDL planning, i.e., plans that generalize across the\ntasks of a given PDDL domain. Previous work proposed a framework consisting of\nthree steps: the LLM first generates a summary and then a strategy for the\ndomain, both in natural language, and then implements that strategy as a Python\nprogram, that gets debugged on example planning tasks. In that work, only one\nstrategy is generated and passed directly to the program generation. If the\nstrategy is incorrect, its implementation will therefore result in an incorrect\ngeneralized plan. Here, we introduce an approach that generates the strategy in\nthe form of pseudocode and enables automatic debugging of the pseudocode, hence\nallowing us to identify and fix errors prior to the generation of the\ngeneralized plan itself. Additionally, we extend the Python debugging phase\nwith a reflection step prompting the LLM to pinpoint the reason for the\nobserved plan failure. Finally, we take inspiration from LLM code generation to\nproduce several program variants and pick the best one. Running experiments on\n17 benchmark domains, we show that these extensions substantially improve (and\nnever deteriorate) the quality of the generalized plans. In 12 of the domains,\nour best Python programs solve all tasks that can be generated with the\nrespective instance generator.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs have recently been used to generate Python programs representing\ngeneralized plans in PDDL planning, i.e., plans that generalize across the\ntasks of a given PDDL domain. Previous work proposed a framework consisting of\nthree steps: the LLM first generates a summary and then a strategy for the\ndomain, both in natural language, and then implements that strategy as a Python\nprogram, that gets debugged on example planning tasks. In that work, only one\nstrategy is generated and passed directly to the program generation. If the\nstrategy is incorrect, its implementation will therefore result in an incorrect\ngeneralized plan. Here, we introduce an approach that generates the strategy in\nthe form of pseudocode and enables automatic debugging of the pseudocode, hence\nallowing us to identify and fix errors prior to the generation of the\ngeneralized plan itself. Additionally, we extend the Python debugging phase\nwith a reflection step prompting the LLM to pinpoint the reason for the\nobserved plan failure. Finally, we take inspiration from LLM code generation to\nproduce several program variants and pick the best one. Running experiments on\n17 benchmark domains, we show that these extensions substantially improve (and\nnever deteriorate) the quality of the generalized plans. In 12 of the domains,\nour best Python programs solve all tasks that can be generated with the\nrespective instance generator."
                },
                "authors": [
                    {
                        "name": "Katharina Stein"
                    },
                    {
                        "name": "Nils Hodel"
                    },
                    {
                        "name": "Daniel Fišer"
                    },
                    {
                        "name": "Jörg Hoffmann"
                    },
                    {
                        "name": "Michael Katz"
                    },
                    {
                        "name": "Alexander Koller"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Koller"
                },
                "author": "Alexander Koller",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13876v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13876v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13877v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13877v1",
                "updated": "2025-08-19T14:42:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    14,
                    42,
                    18,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T14:42:18Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    14,
                    42,
                    18,
                    1,
                    231,
                    0
                ],
                "title": "Toward Deployable Multi-Robot Collaboration via a Symbolically-Guided\n  Decision Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward Deployable Multi-Robot Collaboration via a Symbolically-Guided\n  Decision Transformer"
                },
                "summary": "Reinforcement learning (RL) has demonstrated great potential in robotic\noperations. However, its data-intensive nature and reliance on the Markov\nDecision Process (MDP) assumption limit its practical deployment in real-world\nscenarios involving complex dynamics and long-term temporal dependencies, such\nas multi-robot manipulation. Decision Transformers (DTs) have emerged as a\npromising offline alternative by leveraging causal transformers for sequence\nmodeling in RL tasks. However, their applications to multi-robot manipulations\nstill remain underexplored. To address this gap, we propose a novel framework,\nSymbolically-Guided Decision Transformer (SGDT), which integrates a\nneuro-symbolic mechanism with a causal transformer to enable deployable\nmulti-robot collaboration. In the proposed SGDT framework, a neuro-symbolic\nplanner generates a high-level task-oriented plan composed of symbolic\nsubgoals. Guided by these subgoals, a goal-conditioned decision transformer\n(GCDT) performs low-level sequential decision-making for multi-robot\nmanipulation. This hierarchical architecture enables structured, interpretable,\nand generalizable decision making in complex multi-robot collaboration tasks.\nWe evaluate the performance of SGDT across a range of task scenarios, including\nzero-shot and few-shot scenarios. To our knowledge, this is the first work to\nexplore DT-based technology for multi-robot manipulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) has demonstrated great potential in robotic\noperations. However, its data-intensive nature and reliance on the Markov\nDecision Process (MDP) assumption limit its practical deployment in real-world\nscenarios involving complex dynamics and long-term temporal dependencies, such\nas multi-robot manipulation. Decision Transformers (DTs) have emerged as a\npromising offline alternative by leveraging causal transformers for sequence\nmodeling in RL tasks. However, their applications to multi-robot manipulations\nstill remain underexplored. To address this gap, we propose a novel framework,\nSymbolically-Guided Decision Transformer (SGDT), which integrates a\nneuro-symbolic mechanism with a causal transformer to enable deployable\nmulti-robot collaboration. In the proposed SGDT framework, a neuro-symbolic\nplanner generates a high-level task-oriented plan composed of symbolic\nsubgoals. Guided by these subgoals, a goal-conditioned decision transformer\n(GCDT) performs low-level sequential decision-making for multi-robot\nmanipulation. This hierarchical architecture enables structured, interpretable,\nand generalizable decision making in complex multi-robot collaboration tasks.\nWe evaluate the performance of SGDT across a range of task scenarios, including\nzero-shot and few-shot scenarios. To our knowledge, this is the first work to\nexplore DT-based technology for multi-robot manipulation."
                },
                "authors": [
                    {
                        "name": "Rathnam Vidushika Rasanji"
                    },
                    {
                        "name": "Jin Wei-Kocsis"
                    },
                    {
                        "name": "Jiansong Zhang"
                    },
                    {
                        "name": "Dongming Gan"
                    },
                    {
                        "name": "Ragu Athinarayanan"
                    },
                    {
                        "name": "Paul Asunda"
                    }
                ],
                "author_detail": {
                    "name": "Paul Asunda"
                },
                "author": "Paul Asunda",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13877v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13877v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13839v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13839v1",
                "updated": "2025-08-19T13:59:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    13,
                    59,
                    28,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T13:59:28Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    13,
                    59,
                    28,
                    1,
                    231,
                    0
                ],
                "title": "Distributed Distortion-Aware Robust Optimization for Movable\n  Antenna-aided Cell-Free ISAC Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed Distortion-Aware Robust Optimization for Movable\n  Antenna-aided Cell-Free ISAC Systems"
                },
                "summary": "The cell-free integrated sensing and communication (CF-ISAC) architecture is\na promising enabler for 6G, offering spectrum efficiency and ubiquitous\ncoverage. However, real deployments suffer from hardware impairments,\nespecially nonlinear distortion from power amplifiers (PAs), which degrades\nboth communication and sensing. To address this, we propose a movable antenna\n(MA)-aided CF-ISAC system that mitigates distortion and enhances robustness.\nThe PAs nonlinearities are modeled by a third-order memoryless polynomial,\nwhere the third-order distortion coefficients (3RDCs) vary across access points\n(APs) due to hardware differences, aging, and environmental conditions. We\ndesign a distributed distortion-aware worst-case robust optimization framework\nthat explicitly incorporates uncertainty in 3RDCs. First, we analyze the\nworst-case impact of PA distortion on both the Cramer-Rao lower bound (CRLB)\nand communication rate. Then, to address the resulting non-convexity, we apply\nsuccessive convex approximation (SCA) for estimating the 3RDCs. With these, we\njointly optimize beamforming and MA positions under transmit power and sensing\nconstraints. To efficiently solve this highly non-convex problem, we develop an\nMA-enabled self-attention convolutional graph neural network (SACGNN)\nalgorithm. Simulations demonstrate that our method substantially enhances the\ncommunication-sensing trade-off under distortion and outperforms fixed-position\nantenna baselines in terms of robustness and capacity, thereby highlighting the\nadvantages of MA-aided CF-ISAC systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The cell-free integrated sensing and communication (CF-ISAC) architecture is\na promising enabler for 6G, offering spectrum efficiency and ubiquitous\ncoverage. However, real deployments suffer from hardware impairments,\nespecially nonlinear distortion from power amplifiers (PAs), which degrades\nboth communication and sensing. To address this, we propose a movable antenna\n(MA)-aided CF-ISAC system that mitigates distortion and enhances robustness.\nThe PAs nonlinearities are modeled by a third-order memoryless polynomial,\nwhere the third-order distortion coefficients (3RDCs) vary across access points\n(APs) due to hardware differences, aging, and environmental conditions. We\ndesign a distributed distortion-aware worst-case robust optimization framework\nthat explicitly incorporates uncertainty in 3RDCs. First, we analyze the\nworst-case impact of PA distortion on both the Cramer-Rao lower bound (CRLB)\nand communication rate. Then, to address the resulting non-convexity, we apply\nsuccessive convex approximation (SCA) for estimating the 3RDCs. With these, we\njointly optimize beamforming and MA positions under transmit power and sensing\nconstraints. To efficiently solve this highly non-convex problem, we develop an\nMA-enabled self-attention convolutional graph neural network (SACGNN)\nalgorithm. Simulations demonstrate that our method substantially enhances the\ncommunication-sensing trade-off under distortion and outperforms fixed-position\nantenna baselines in terms of robustness and capacity, thereby highlighting the\nadvantages of MA-aided CF-ISAC systems."
                },
                "authors": [
                    {
                        "name": "Yue Xiu"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Ran Yang"
                    },
                    {
                        "name": "Zheng Dong"
                    },
                    {
                        "name": "Wanting Lyu"
                    },
                    {
                        "name": "Zeyuan Zhang"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Guangyi Liu"
                    },
                    {
                        "name": "Ning Wei"
                    }
                ],
                "author_detail": {
                    "name": "Ning Wei"
                },
                "author": "Ning Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13839v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13839v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07928v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07928v3",
                "updated": "2025-08-19T13:36:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    13,
                    36,
                    20,
                    1,
                    231,
                    0
                ],
                "published": "2025-03-11T00:17:07Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    0,
                    17,
                    7,
                    1,
                    70,
                    0
                ],
                "title": "The StudyChat Dataset: Student Dialogues With ChatGPT in an Artificial\n  Intelligence Course",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The StudyChat Dataset: Student Dialogues With ChatGPT in an Artificial\n  Intelligence Course"
                },
                "summary": "The widespread availability of large language models (LLMs), such as ChatGPT,\nhas significantly impacted education, raising both opportunities and\nchallenges. Students can frequently interact with LLM-powered, interactive\nlearning tools, but their usage patterns need to be monitored and understood.\nWe introduce StudyChat, a publicly available dataset capturing real-world\nstudent interactions with an LLM-powered tutoring chatbot in a semester-long,\nuniversity-level artificial intelligence (AI) course. We deploy a web\napplication that replicates ChatGPTs core functionalities, and use it to log\nstudent interactions with the LLM while working on programming assignments. We\ncollect 16,851 interactions, which we annotate using a dialogue act labeling\nschema inspired by observed interaction patterns and prior research. We analyze\nthese interactions, highlight usage trends, and analyze how specific student\nbehavior correlates with their course outcome. We find that students who prompt\nLLMs for conceptual understanding and coding help tend to perform better on\nassignments and exams. Moreover, students who use LLMs to write reports and\ncircumvent assignment learning objectives have lower outcomes on exams than\nothers. StudyChat serves as a shared resource to facilitate further research on\nthe evolving role of LLMs in education.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread availability of large language models (LLMs), such as ChatGPT,\nhas significantly impacted education, raising both opportunities and\nchallenges. Students can frequently interact with LLM-powered, interactive\nlearning tools, but their usage patterns need to be monitored and understood.\nWe introduce StudyChat, a publicly available dataset capturing real-world\nstudent interactions with an LLM-powered tutoring chatbot in a semester-long,\nuniversity-level artificial intelligence (AI) course. We deploy a web\napplication that replicates ChatGPTs core functionalities, and use it to log\nstudent interactions with the LLM while working on programming assignments. We\ncollect 16,851 interactions, which we annotate using a dialogue act labeling\nschema inspired by observed interaction patterns and prior research. We analyze\nthese interactions, highlight usage trends, and analyze how specific student\nbehavior correlates with their course outcome. We find that students who prompt\nLLMs for conceptual understanding and coding help tend to perform better on\nassignments and exams. Moreover, students who use LLMs to write reports and\ncircumvent assignment learning objectives have lower outcomes on exams than\nothers. StudyChat serves as a shared resource to facilitate further research on\nthe evolving role of LLMs in education."
                },
                "authors": [
                    {
                        "name": "Hunter McNichols"
                    },
                    {
                        "name": "Fareya Ikram"
                    },
                    {
                        "name": "Andrew Lan"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Lan"
                },
                "author": "Andrew Lan",
                "arxiv_comment": "Pre-print v0.3",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07928v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07928v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13825v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13825v1",
                "updated": "2025-08-19T13:36:13Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    13,
                    36,
                    13,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T13:36:13Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    13,
                    36,
                    13,
                    1,
                    231,
                    0
                ],
                "title": "Energy Management and Wake-up for IoT Networks Powered by Energy\n  Harvesting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy Management and Wake-up for IoT Networks Powered by Energy\n  Harvesting"
                },
                "summary": "The rapid growth of the Internet of Things (IoT) presents sustainability\nchallenges such as increased maintenance requirements and overall higher energy\nconsumption. This motivates self-sustainable IoT ecosystems based on Energy\nHarvesting (EH). This paper treats IoT deployments in which IoT devices (IoTDs)\nrely solely on EH to sense and transmit information about events/alarms to a\nbase station (BS). The objective is to effectively manage the duty cycling of\nthe IoTDs to prolong battery life and maximize the relevant data sent to the\nBS. The BS can also wake up specific IoTDs if extra information about an event\nis needed upon initial detection. We propose a K-nearest neighbors (KNN)-based\nduty cycling management to optimize energy efficiency and detection accuracy by\nconsidering spatial correlations among IoTDs' activity and their EH process. We\nevaluate machine learning approaches, including reinforcement learning (RL) and\ndecision transformers (DT), to maximize information captured from events while\nmanaging energy consumption. Significant improvements over the state-ofthe-art\napproaches are obtained in terms of energy saving by all three proposals, KNN,\nRL, and DT. Moreover, the RL-based solution approaches the performance of a\ngenie-aided benchmark as the number of IoTDs increases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of the Internet of Things (IoT) presents sustainability\nchallenges such as increased maintenance requirements and overall higher energy\nconsumption. This motivates self-sustainable IoT ecosystems based on Energy\nHarvesting (EH). This paper treats IoT deployments in which IoT devices (IoTDs)\nrely solely on EH to sense and transmit information about events/alarms to a\nbase station (BS). The objective is to effectively manage the duty cycling of\nthe IoTDs to prolong battery life and maximize the relevant data sent to the\nBS. The BS can also wake up specific IoTDs if extra information about an event\nis needed upon initial detection. We propose a K-nearest neighbors (KNN)-based\nduty cycling management to optimize energy efficiency and detection accuracy by\nconsidering spatial correlations among IoTDs' activity and their EH process. We\nevaluate machine learning approaches, including reinforcement learning (RL) and\ndecision transformers (DT), to maximize information captured from events while\nmanaging energy consumption. Significant improvements over the state-ofthe-art\napproaches are obtained in terms of energy saving by all three proposals, KNN,\nRL, and DT. Moreover, the RL-based solution approaches the performance of a\ngenie-aided benchmark as the number of IoTDs increases."
                },
                "authors": [
                    {
                        "name": "David Ernesto Ruiz-Guirola"
                    },
                    {
                        "name": "Samuel Montejo-Sanchez"
                    },
                    {
                        "name": "Israel Leyva-Mayorga"
                    },
                    {
                        "name": "Zhu Han"
                    },
                    {
                        "name": "Petar Popovski"
                    },
                    {
                        "name": "Onel L. A. Lopez"
                    }
                ],
                "author_detail": {
                    "name": "Onel L. A. Lopez"
                },
                "author": "Onel L. A. Lopez",
                "arxiv_comment": "This work has been partially supported by the Research Council of\n  Finland (Grant 369116 (6G Flagship Programme), Grant 362782), the Finnish\n  Foundation for Technology Promotion, the European Commission through the\n  Horizon Europe/JU SNS project AMBIENT-6G (Grant 101192113), and in Chile, by\n  ANID FONDECYT Regular No.1241977",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13825v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13825v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.06095v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.06095v2",
                "updated": "2025-08-19T13:26:23Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    13,
                    26,
                    23,
                    1,
                    231,
                    0
                ],
                "published": "2025-06-06T13:54:34Z",
                "published_parsed": [
                    2025,
                    6,
                    6,
                    13,
                    54,
                    34,
                    4,
                    157,
                    0
                ],
                "title": "Flexible Operator Fusion for Fast Sparse Transformer with Diverse\n  Masking on GPU",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flexible Operator Fusion for Fast Sparse Transformer with Diverse\n  Masking on GPU"
                },
                "summary": "Large language models are popular around the world due to their powerful\nunderstanding capabilities. As the core component of LLMs, accelerating\nTransformer through parallelization has gradually become a hot research topic.\nMask layers introduce sparsity into Transformer to reduce calculations.\nHowever, previous works rarely focus on the performance optimization of sparse\nTransformer. Moreover, rule-based mechanisms ignore the fusion opportunities of\nmixed-type operators and fail to adapt to various sequence lengths. To address\nthe above problems, we propose STOF, a framework that incorporates\noptimizations for Sparse Transformer via flexible masking and operator fusion\non GPU. We firstly unify the storage format and kernel implementation for the\nmulti-head attention. Then, we map fusion schemes to compilation templates and\ndetermine the optimal parameter setting through a two-stage search engine. The\nexperimental results show that compared to the state-of-the-art work, STOF\nachieves maximum speedups of 1.7x in MHA computation and 1.5x in end-to-end\ninference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models are popular around the world due to their powerful\nunderstanding capabilities. As the core component of LLMs, accelerating\nTransformer through parallelization has gradually become a hot research topic.\nMask layers introduce sparsity into Transformer to reduce calculations.\nHowever, previous works rarely focus on the performance optimization of sparse\nTransformer. Moreover, rule-based mechanisms ignore the fusion opportunities of\nmixed-type operators and fail to adapt to various sequence lengths. To address\nthe above problems, we propose STOF, a framework that incorporates\noptimizations for Sparse Transformer via flexible masking and operator fusion\non GPU. We firstly unify the storage format and kernel implementation for the\nmulti-head attention. Then, we map fusion schemes to compilation templates and\ndetermine the optimal parameter setting through a two-stage search engine. The\nexperimental results show that compared to the state-of-the-art work, STOF\nachieves maximum speedups of 1.7x in MHA computation and 1.5x in end-to-end\ninference."
                },
                "authors": [
                    {
                        "name": "Wenhao Dai"
                    },
                    {
                        "name": "Haodong Deng"
                    },
                    {
                        "name": "Mengfei Rong"
                    },
                    {
                        "name": "Xinyu Yang"
                    },
                    {
                        "name": "Hongyu Liu"
                    },
                    {
                        "name": "Fangxin Liu"
                    },
                    {
                        "name": "Hailong Yang"
                    },
                    {
                        "name": "Qianwen Cao"
                    },
                    {
                        "name": "Qingxiao Sun"
                    }
                ],
                "author_detail": {
                    "name": "Qingxiao Sun"
                },
                "author": "Qingxiao Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.06095v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.06095v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13816v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13816v1",
                "updated": "2025-08-19T13:22:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    13,
                    22,
                    41,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T13:22:41Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    13,
                    22,
                    41,
                    1,
                    231,
                    0
                ],
                "title": "The illusion of a perfect metric: Why evaluating AI's words is harder\n  than it looks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The illusion of a perfect metric: Why evaluating AI's words is harder\n  than it looks"
                },
                "summary": "Evaluating Natural Language Generation (NLG) is crucial for the practical\nadoption of AI, but has been a longstanding research challenge. While human\nevaluation is considered the de-facto standard, it is expensive and lacks\nscalability. Practical applications have driven the development of various\nautomatic evaluation metrics (AEM), designed to compare the model output with\nhuman-written references, generating a score which approximates human judgment.\nOver time, AEMs have evolved from simple lexical comparisons, to semantic\nsimilarity models and, more recently, to LLM-based evaluators. However, it\nseems that no single metric has emerged as a definitive solution, resulting in\nstudies using different ones without fully considering the implications. This\npaper aims to show this by conducting a thorough examination of the\nmethodologies of existing metrics, their documented strengths and limitations,\nvalidation methods, and correlations with human judgment. We identify several\nkey challenges: metrics often capture only specific aspects of text quality,\ntheir effectiveness varies by task and dataset, validation practices remain\nunstructured, and correlations with human judgment are inconsistent.\nImportantly, we find that these challenges persist in the most recent type of\nmetric, LLM-as-a-Judge, as well as in the evaluation of Retrieval Augmented\nGeneration (RAG), an increasingly relevant task in academia and industry. Our\nfindings challenge the quest for the 'perfect metric'. We propose selecting\nmetrics based on task-specific needs and leveraging complementary evaluations\nand advocate that new metrics should focus on enhanced validation\nmethodologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Natural Language Generation (NLG) is crucial for the practical\nadoption of AI, but has been a longstanding research challenge. While human\nevaluation is considered the de-facto standard, it is expensive and lacks\nscalability. Practical applications have driven the development of various\nautomatic evaluation metrics (AEM), designed to compare the model output with\nhuman-written references, generating a score which approximates human judgment.\nOver time, AEMs have evolved from simple lexical comparisons, to semantic\nsimilarity models and, more recently, to LLM-based evaluators. However, it\nseems that no single metric has emerged as a definitive solution, resulting in\nstudies using different ones without fully considering the implications. This\npaper aims to show this by conducting a thorough examination of the\nmethodologies of existing metrics, their documented strengths and limitations,\nvalidation methods, and correlations with human judgment. We identify several\nkey challenges: metrics often capture only specific aspects of text quality,\ntheir effectiveness varies by task and dataset, validation practices remain\nunstructured, and correlations with human judgment are inconsistent.\nImportantly, we find that these challenges persist in the most recent type of\nmetric, LLM-as-a-Judge, as well as in the evaluation of Retrieval Augmented\nGeneration (RAG), an increasingly relevant task in academia and industry. Our\nfindings challenge the quest for the 'perfect metric'. We propose selecting\nmetrics based on task-specific needs and leveraging complementary evaluations\nand advocate that new metrics should focus on enhanced validation\nmethodologies."
                },
                "authors": [
                    {
                        "name": "Maria Paz Oliva"
                    },
                    {
                        "name": "Adriana Correia"
                    },
                    {
                        "name": "Ivan Vankov"
                    },
                    {
                        "name": "Viktor Botev"
                    }
                ],
                "author_detail": {
                    "name": "Viktor Botev"
                },
                "author": "Viktor Botev",
                "arxiv_comment": "11 pages, 1 figure. Accepted to RANLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13816v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13816v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.12769v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.12769v3",
                "updated": "2025-08-20T08:11:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    8,
                    11,
                    10,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-18T09:43:07Z",
                "published_parsed": [
                    2025,
                    8,
                    18,
                    9,
                    43,
                    7,
                    0,
                    230,
                    0
                ],
                "title": "CRED-SQL: Enhancing Real-world Large Scale Database Text-to-SQL Parsing\n  through Cluster Retrieval and Execution Description",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CRED-SQL: Enhancing Real-world Large Scale Database Text-to-SQL Parsing\n  through Cluster Retrieval and Execution Description"
                },
                "summary": "Recent advances in large language models (LLMs) have significantly improved\nthe accuracy of Text-to-SQL systems. However, a critical challenge remains: the\nsemantic mismatch between natural language questions (NLQs) and their\ncorresponding SQL queries. This issue is exacerbated in large-scale databases,\nwhere semantically similar attributes hinder schema linking and semantic drift\nduring SQL generation, ultimately reducing model accuracy. To address these\nchallenges, we introduce CRED-SQL, a framework designed for large-scale\ndatabases that integrates Cluster Retrieval and Execution Description. CRED-SQL\nfirst performs cluster-based large-scale schema retrieval to pinpoint the\ntables and columns most relevant to a given NLQ, alleviating schema mismatch.\nIt then introduces an intermediate natural language representation-Execution\nDescription Language (EDL)-to bridge the gap between NLQs and SQL. This\nreformulation decomposes the task into two stages: Text-to-EDL and EDL-to-SQL,\nleveraging LLMs' strong general reasoning capabilities while reducing semantic\ndeviation. Extensive experiments on two large-scale, cross-domain\nbenchmarks-SpiderUnion and BirdUnion-demonstrate that CRED-SQL achieves new\nstate-of-the-art (SOTA) performance, validating its effectiveness and\nscalability. Our code is available at https://github.com/smduan/CRED-SQL.git",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have significantly improved\nthe accuracy of Text-to-SQL systems. However, a critical challenge remains: the\nsemantic mismatch between natural language questions (NLQs) and their\ncorresponding SQL queries. This issue is exacerbated in large-scale databases,\nwhere semantically similar attributes hinder schema linking and semantic drift\nduring SQL generation, ultimately reducing model accuracy. To address these\nchallenges, we introduce CRED-SQL, a framework designed for large-scale\ndatabases that integrates Cluster Retrieval and Execution Description. CRED-SQL\nfirst performs cluster-based large-scale schema retrieval to pinpoint the\ntables and columns most relevant to a given NLQ, alleviating schema mismatch.\nIt then introduces an intermediate natural language representation-Execution\nDescription Language (EDL)-to bridge the gap between NLQs and SQL. This\nreformulation decomposes the task into two stages: Text-to-EDL and EDL-to-SQL,\nleveraging LLMs' strong general reasoning capabilities while reducing semantic\ndeviation. Extensive experiments on two large-scale, cross-domain\nbenchmarks-SpiderUnion and BirdUnion-demonstrate that CRED-SQL achieves new\nstate-of-the-art (SOTA) performance, validating its effectiveness and\nscalability. Our code is available at https://github.com/smduan/CRED-SQL.git"
                },
                "authors": [
                    {
                        "name": "Shaoming Duan"
                    },
                    {
                        "name": "Zirui Wang"
                    },
                    {
                        "name": "Chuanyi Liu"
                    },
                    {
                        "name": "Zhibin Zhu"
                    },
                    {
                        "name": "Yuhao Zhang"
                    },
                    {
                        "name": "Peiyi Han"
                    },
                    {
                        "name": "Liang Yan"
                    },
                    {
                        "name": "Zewu Peng"
                    }
                ],
                "author_detail": {
                    "name": "Zewu Peng"
                },
                "author": "Zewu Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.12769v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.12769v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13805v1",
                "updated": "2025-08-19T13:12:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    13,
                    12,
                    1,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T13:12:01Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    13,
                    12,
                    1,
                    1,
                    231,
                    0
                ],
                "title": "Prompt-Based One-Shot Exact Length-Controlled Generation with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt-Based One-Shot Exact Length-Controlled Generation with LLMs"
                },
                "summary": "Controlling the length of text produced by large language models (LLMs)\nremains challenging: models frequently overshoot or undershoot explicit length\ninstructions because they cannot reliably keep an internal token count. We\npresent a prompt-based, one-shot strategy that compels an off-the-shelf LLM to\ngenerate exactly a desired number of tokens - words (English) or characters\n(Chinese) - without any fine-tuning or iterative sampling. The prompt appends\ncountdown markers and explicit counting rules so that the model \"writes while\ncounting.\" We evaluate on four settings: open-ended generation (1-1000 tokens),\nXSUM summarization, MT-Bench-LI instruction following, and the LIFEBENCH\nequal-length track. On MT-Bench-LI, strict length compliance with GPT-4.1 leaps\nfrom below 30% under naive prompts to above 95% with our countdown prompt,\nsurpassing the popular draft-then-revise baseline, while judged answer quality\nis preserved. These results show that precise length control can be achieved\nthrough prompt engineering alone, offering a lightweight alternative to\ntraining- or decoding-based methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Controlling the length of text produced by large language models (LLMs)\nremains challenging: models frequently overshoot or undershoot explicit length\ninstructions because they cannot reliably keep an internal token count. We\npresent a prompt-based, one-shot strategy that compels an off-the-shelf LLM to\ngenerate exactly a desired number of tokens - words (English) or characters\n(Chinese) - without any fine-tuning or iterative sampling. The prompt appends\ncountdown markers and explicit counting rules so that the model \"writes while\ncounting.\" We evaluate on four settings: open-ended generation (1-1000 tokens),\nXSUM summarization, MT-Bench-LI instruction following, and the LIFEBENCH\nequal-length track. On MT-Bench-LI, strict length compliance with GPT-4.1 leaps\nfrom below 30% under naive prompts to above 95% with our countdown prompt,\nsurpassing the popular draft-then-revise baseline, while judged answer quality\nis preserved. These results show that precise length control can be achieved\nthrough prompt engineering alone, offering a lightweight alternative to\ntraining- or decoding-based methods."
                },
                "authors": [
                    {
                        "name": "Juncheng Xie"
                    },
                    {
                        "name": "Hung-yi Lee"
                    }
                ],
                "author_detail": {
                    "name": "Hung-yi Lee"
                },
                "author": "Hung-yi Lee",
                "arxiv_comment": "18 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13804v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13804v1",
                "updated": "2025-08-19T13:05:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    13,
                    5,
                    48,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T13:05:48Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    13,
                    5,
                    48,
                    1,
                    231,
                    0
                ],
                "title": "Beyond Human Judgment: A Bayesian Evaluation of LLMs' Moral Values\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Human Judgment: A Bayesian Evaluation of LLMs' Moral Values\n  Understanding"
                },
                "summary": "How do large language models understand moral dimensions compared to humans?\n  This first large-scale Bayesian evaluation of market-leading language models\nprovides the answer. In contrast to prior work using deterministic ground truth\n(majority or inclusion rules), we model annotator disagreements to capture both\naleatoric uncertainty (inherent human disagreement) and epistemic uncertainty\n(model domain sensitivity). We evaluate top language models (Claude Sonnet 4,\nDeepSeek-V3, Llama 4 Maverick) across 250K+ annotations from ~700 annotators on\n100K+ texts spanning social media, news, and forums.\n  Our GPU-optimized Bayesian framework processed 1M+ model queries, revealing\nthat AI models typically rank among the top 25\\% of human annotators, achieving\nmuch better-than-average balanced accuracy. Importantly, we find that AI\nproduces far fewer false negatives than humans, highlighting their more\nsensitive moral detection capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How do large language models understand moral dimensions compared to humans?\n  This first large-scale Bayesian evaluation of market-leading language models\nprovides the answer. In contrast to prior work using deterministic ground truth\n(majority or inclusion rules), we model annotator disagreements to capture both\naleatoric uncertainty (inherent human disagreement) and epistemic uncertainty\n(model domain sensitivity). We evaluate top language models (Claude Sonnet 4,\nDeepSeek-V3, Llama 4 Maverick) across 250K+ annotations from ~700 annotators on\n100K+ texts spanning social media, news, and forums.\n  Our GPU-optimized Bayesian framework processed 1M+ model queries, revealing\nthat AI models typically rank among the top 25\\% of human annotators, achieving\nmuch better-than-average balanced accuracy. Importantly, we find that AI\nproduces far fewer false negatives than humans, highlighting their more\nsensitive moral detection capabilities."
                },
                "authors": [
                    {
                        "name": "Maciej Skorski"
                    },
                    {
                        "name": "Alina Landowska"
                    }
                ],
                "author_detail": {
                    "name": "Alina Landowska"
                },
                "author": "Alina Landowska",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13804v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13804v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50, 62F15, 62P25",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; K.4.1; J.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13798v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13798v1",
                "updated": "2025-08-19T12:57:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    12,
                    57,
                    45,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T12:57:45Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    12,
                    57,
                    45,
                    1,
                    231,
                    0
                ],
                "title": "TracSum: A New Benchmark for Aspect-Based Summarization with\n  Sentence-Level Traceability in Medical Domain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TracSum: A New Benchmark for Aspect-Based Summarization with\n  Sentence-Level Traceability in Medical Domain"
                },
                "summary": "While document summarization with LLMs has enhanced access to textual\ninformation, concerns about the factual accuracy of these summaries persist,\nespecially in the medical domain. Tracing evidence from which summaries are\nderived enables users to assess their accuracy, thereby alleviating this\nconcern. In this paper, we introduce TracSum, a novel benchmark for traceable,\naspect-based summarization, in which generated summaries are paired with\nsentence-level citations, enabling users to trace back to the original context.\nFirst, we annotate 500 medical abstracts for seven key medical aspects,\nyielding 3.5K summary-citation pairs. We then propose a fine-grained evaluation\nframework for this new task, designed to assess the completeness and\nconsistency of generated content using four metrics. Finally, we introduce a\nsummarization pipeline, Track-Then-Sum, which serves as a baseline method for\ncomparison. In experiments, we evaluate both this baseline and a set of LLMs on\nTracSum, and conduct a human evaluation to assess the evaluation results. The\nfindings demonstrate that TracSum can serve as an effective benchmark for\ntraceable, aspect-based summarization tasks. We also observe that explicitly\nperforming sentence-level tracking prior to summarization enhances generation\naccuracy, while incorporating the full context further improves completeness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While document summarization with LLMs has enhanced access to textual\ninformation, concerns about the factual accuracy of these summaries persist,\nespecially in the medical domain. Tracing evidence from which summaries are\nderived enables users to assess their accuracy, thereby alleviating this\nconcern. In this paper, we introduce TracSum, a novel benchmark for traceable,\naspect-based summarization, in which generated summaries are paired with\nsentence-level citations, enabling users to trace back to the original context.\nFirst, we annotate 500 medical abstracts for seven key medical aspects,\nyielding 3.5K summary-citation pairs. We then propose a fine-grained evaluation\nframework for this new task, designed to assess the completeness and\nconsistency of generated content using four metrics. Finally, we introduce a\nsummarization pipeline, Track-Then-Sum, which serves as a baseline method for\ncomparison. In experiments, we evaluate both this baseline and a set of LLMs on\nTracSum, and conduct a human evaluation to assess the evaluation results. The\nfindings demonstrate that TracSum can serve as an effective benchmark for\ntraceable, aspect-based summarization tasks. We also observe that explicitly\nperforming sentence-level tracking prior to summarization enhances generation\naccuracy, while incorporating the full context further improves completeness."
                },
                "authors": [
                    {
                        "name": "Bohao Chu"
                    },
                    {
                        "name": "Meijie Li"
                    },
                    {
                        "name": "Sameh Frihat"
                    },
                    {
                        "name": "Chengyu Gu"
                    },
                    {
                        "name": "Georg Lodde"
                    },
                    {
                        "name": "Elisabeth Livingstone"
                    },
                    {
                        "name": "Norbert Fuhr"
                    }
                ],
                "author_detail": {
                    "name": "Norbert Fuhr"
                },
                "author": "Norbert Fuhr",
                "arxiv_comment": "8 main pages, 12 appendix pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13798v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13798v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13792v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13792v1",
                "updated": "2025-08-19T12:52:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    12,
                    52,
                    16,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T12:52:16Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    12,
                    52,
                    16,
                    1,
                    231,
                    0
                ],
                "title": "VisionLaw: Inferring Interpretable Intrinsic Dynamics from Visual\n  Observations via Bilevel Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VisionLaw: Inferring Interpretable Intrinsic Dynamics from Visual\n  Observations via Bilevel Optimization"
                },
                "summary": "The intrinsic dynamics of an object governs its physical behavior in the real\nworld, playing a critical role in enabling physically plausible interactive\nsimulation with 3D assets. Existing methods have attempted to infer the\nintrinsic dynamics of objects from visual observations, but generally face two\nmajor challenges: one line of work relies on manually defined constitutive\npriors, making it difficult to generalize to complex scenarios; the other\nmodels intrinsic dynamics using neural networks, resulting in limited\ninterpretability and poor generalization. To address these challenges, we\npropose VisionLaw, a bilevel optimization framework that infers interpretable\nexpressions of intrinsic dynamics from visual observations. At the upper level,\nwe introduce an LLMs-driven decoupled constitutive evolution strategy, where\nLLMs are prompted as a knowledgeable physics expert to generate and revise\nconstitutive laws, with a built-in decoupling mechanism that substantially\nreduces the search complexity of LLMs. At the lower level, we introduce a\nvision-guided constitutive evaluation mechanism, which utilizes visual\nsimulation to evaluate the consistency between the generated constitutive law\nand the underlying intrinsic dynamics, thereby guiding the upper-level\nevolution. Experiments on both synthetic and real-world datasets demonstrate\nthat VisionLaw can effectively infer interpretable intrinsic dynamics from\nvisual observations. It significantly outperforms existing state-of-the-art\nmethods and exhibits strong generalization for interactive simulation in novel\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The intrinsic dynamics of an object governs its physical behavior in the real\nworld, playing a critical role in enabling physically plausible interactive\nsimulation with 3D assets. Existing methods have attempted to infer the\nintrinsic dynamics of objects from visual observations, but generally face two\nmajor challenges: one line of work relies on manually defined constitutive\npriors, making it difficult to generalize to complex scenarios; the other\nmodels intrinsic dynamics using neural networks, resulting in limited\ninterpretability and poor generalization. To address these challenges, we\npropose VisionLaw, a bilevel optimization framework that infers interpretable\nexpressions of intrinsic dynamics from visual observations. At the upper level,\nwe introduce an LLMs-driven decoupled constitutive evolution strategy, where\nLLMs are prompted as a knowledgeable physics expert to generate and revise\nconstitutive laws, with a built-in decoupling mechanism that substantially\nreduces the search complexity of LLMs. At the lower level, we introduce a\nvision-guided constitutive evaluation mechanism, which utilizes visual\nsimulation to evaluate the consistency between the generated constitutive law\nand the underlying intrinsic dynamics, thereby guiding the upper-level\nevolution. Experiments on both synthetic and real-world datasets demonstrate\nthat VisionLaw can effectively infer interpretable intrinsic dynamics from\nvisual observations. It significantly outperforms existing state-of-the-art\nmethods and exhibits strong generalization for interactive simulation in novel\nscenarios."
                },
                "authors": [
                    {
                        "name": "Jiajing Lin"
                    },
                    {
                        "name": "Shu Jiang"
                    },
                    {
                        "name": "Qingyuan Zeng"
                    },
                    {
                        "name": "Zhenzhong Wang"
                    },
                    {
                        "name": "Min Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Min Jiang"
                },
                "author": "Min Jiang",
                "arxiv_comment": "9 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13792v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13792v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13787v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13787v1",
                "updated": "2025-08-19T12:43:49Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    12,
                    43,
                    49,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T12:43:49Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    12,
                    43,
                    49,
                    1,
                    231,
                    0
                ],
                "title": "BetaWeb: Towards a Blockchain-enabled Trustworthy Agentic Web",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BetaWeb: Towards a Blockchain-enabled Trustworthy Agentic Web"
                },
                "summary": "The rapid development of large language models (LLMs) has significantly\npropelled the development of artificial intelligence (AI) agents, which are\nincreasingly evolving into diverse autonomous entities, advancing the LLM-based\nmulti-agent systems (LaMAS). However, current agentic ecosystems remain\nfragmented and closed. Establishing an interconnected and scalable paradigm for\nAgentic AI has become a critical prerequisite. Although Agentic Web proposes an\nopen architecture to break the ecosystem barriers, its implementation still\nfaces core challenges such as privacy protection, data management, and value\nmeasurement. Existing centralized or semi-centralized paradigms suffer from\ninherent limitations, making them inadequate for supporting large-scale,\nheterogeneous, and cross-domain autonomous interactions. To address these\nchallenges, this paper introduces the blockchain-enabled trustworthy Agentic\nWeb (BetaWeb). By leveraging the inherent strengths of blockchain, BetaWeb not\nonly offers a trustworthy and scalable infrastructure for LaMAS but also has\nthe potential to advance the Web paradigm from Web3 (centered on data\nownership) towards Web3.5, which emphasizes ownership of agent capabilities and\nthe monetization of intelligence. Beyond a systematic examination of the\nBetaWeb framework, this paper presents a five-stage evolutionary roadmap,\noutlining the path of LaMAS from passive execution to advanced collaboration\nand autonomous governance. We also conduct a comparative analysis of existing\nproducts and discuss key challenges of BetaWeb from multiple perspectives.\nUltimately, we argue that deep integration between blockchain and LaMAS can lay\nthe foundation for a resilient, trustworthy, and sustainably incentivized\ndigital ecosystem. A summary of the enabling technologies for each stage is\navailable at https://github.com/MatZaharia/BetaWeb.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of large language models (LLMs) has significantly\npropelled the development of artificial intelligence (AI) agents, which are\nincreasingly evolving into diverse autonomous entities, advancing the LLM-based\nmulti-agent systems (LaMAS). However, current agentic ecosystems remain\nfragmented and closed. Establishing an interconnected and scalable paradigm for\nAgentic AI has become a critical prerequisite. Although Agentic Web proposes an\nopen architecture to break the ecosystem barriers, its implementation still\nfaces core challenges such as privacy protection, data management, and value\nmeasurement. Existing centralized or semi-centralized paradigms suffer from\ninherent limitations, making them inadequate for supporting large-scale,\nheterogeneous, and cross-domain autonomous interactions. To address these\nchallenges, this paper introduces the blockchain-enabled trustworthy Agentic\nWeb (BetaWeb). By leveraging the inherent strengths of blockchain, BetaWeb not\nonly offers a trustworthy and scalable infrastructure for LaMAS but also has\nthe potential to advance the Web paradigm from Web3 (centered on data\nownership) towards Web3.5, which emphasizes ownership of agent capabilities and\nthe monetization of intelligence. Beyond a systematic examination of the\nBetaWeb framework, this paper presents a five-stage evolutionary roadmap,\noutlining the path of LaMAS from passive execution to advanced collaboration\nand autonomous governance. We also conduct a comparative analysis of existing\nproducts and discuss key challenges of BetaWeb from multiple perspectives.\nUltimately, we argue that deep integration between blockchain and LaMAS can lay\nthe foundation for a resilient, trustworthy, and sustainably incentivized\ndigital ecosystem. A summary of the enabling technologies for each stage is\navailable at https://github.com/MatZaharia/BetaWeb."
                },
                "authors": [
                    {
                        "name": "Zihan Guo"
                    },
                    {
                        "name": "Yuanjian Zhou"
                    },
                    {
                        "name": "Chenyi Wang"
                    },
                    {
                        "name": "Linlin You"
                    },
                    {
                        "name": "Minjie Bian"
                    },
                    {
                        "name": "Weinan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Weinan Zhang"
                },
                "author": "Weinan Zhang",
                "arxiv_comment": "A technical report with 21 pages, 3 figures, and 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13787v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13787v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13774v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13774v1",
                "updated": "2025-08-19T12:21:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    12,
                    21,
                    21,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T12:21:21Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    12,
                    21,
                    21,
                    1,
                    231,
                    0
                ],
                "title": "Agentic DraCor and the Art of Docstring Engineering: Evaluating\n  MCP-empowered LLM Usage of the DraCor API",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic DraCor and the Art of Docstring Engineering: Evaluating\n  MCP-empowered LLM Usage of the DraCor API"
                },
                "summary": "This paper reports on the implementation and evaluation of a Model Context\nProtocol (MCP) server for DraCor, enabling Large Language Models (LLM) to\nautonomously interact with the DraCor API. We conducted experiments focusing on\ntool selection and application by the LLM, employing a qualitative approach\nthat includes systematic observation of prompts to understand how LLMs behave\nwhen using MCP tools, evaluating \"Tool Correctness\", \"Tool-Calling Efficiency\",\nand \"Tool-Use Reliability\". Our findings highlight the importance of \"Docstring\nEngineering\", defined as reflexively crafting tool documentation to optimize\nLLM-tool interaction. Our experiments demonstrate both the promise of agentic\nAI for research in Computational Literary Studies and the essential\ninfrastructure development needs for reliable Digital Humanities\ninfrastructures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper reports on the implementation and evaluation of a Model Context\nProtocol (MCP) server for DraCor, enabling Large Language Models (LLM) to\nautonomously interact with the DraCor API. We conducted experiments focusing on\ntool selection and application by the LLM, employing a qualitative approach\nthat includes systematic observation of prompts to understand how LLMs behave\nwhen using MCP tools, evaluating \"Tool Correctness\", \"Tool-Calling Efficiency\",\nand \"Tool-Use Reliability\". Our findings highlight the importance of \"Docstring\nEngineering\", defined as reflexively crafting tool documentation to optimize\nLLM-tool interaction. Our experiments demonstrate both the promise of agentic\nAI for research in Computational Literary Studies and the essential\ninfrastructure development needs for reliable Digital Humanities\ninfrastructures."
                },
                "authors": [
                    {
                        "name": "Peer Trilcke"
                    },
                    {
                        "name": "Ingo Börner"
                    },
                    {
                        "name": "Henny Sluyter-Gäthje"
                    },
                    {
                        "name": "Daniil Skorinkin"
                    },
                    {
                        "name": "Frank Fischer"
                    },
                    {
                        "name": "Carsten Milling"
                    }
                ],
                "author_detail": {
                    "name": "Carsten Milling"
                },
                "author": "Carsten Milling",
                "arxiv_comment": "Preprint, submitted to the 2nd Workshop on Computational Drama\n  Analysis at DraCor Summit 2025, September 03, 2025, Berlin, Germany",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13774v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13774v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "J.5; I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08549v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08549v2",
                "updated": "2025-08-19T12:21:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    12,
                    21,
                    20,
                    1,
                    231,
                    0
                ],
                "published": "2025-03-11T15:36:38Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    15,
                    36,
                    38,
                    1,
                    70,
                    0
                ],
                "title": "GoAI: Enhancing AI Students' Learning Paths and Idea Generation via\n  Graph of AI Ideas",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GoAI: Enhancing AI Students' Learning Paths and Idea Generation via\n  Graph of AI Ideas"
                },
                "summary": "With the rapid advancement of artificial intelligence technology, AI students\nare confronted with a significant \"information-to-innovation\" gap: they must\nnavigate through the rapidly expanding body of literature, trace the\ndevelopment of a specific research field, and synthesize various techniques\ninto feasible innovative concepts. An additional critical step for students is\nto identify the necessary prerequisite knowledge and learning paths. Although\nmany approaches based on large language models (LLMs) can summarize the content\nof papers and trace the development of a field through citations, these methods\noften overlook the prerequisite knowledge involved in the papers and the rich\nsemantic information embedded in the citation relationships between papers.\nSuch information reveals how methods are interrelated, built upon, extended, or\nchallenged. To address these limitations, we propose GoAI, a tool for\nconstructing educational knowledge graphs from AI research papers that\nleverages these graphs to plan personalized learning paths and support creative\nideation. The nodes in the knowledge graph we have built include papers and the\nprerequisite knowledge, such as concepts, skills, and tools, that they involve;\nthe edges record the semantic information of citations. When a student queries\na specific paper, a beam search-based path search method can trace the current\ndevelopment trends of the field from the queried paper and plan a learning path\ntoward cutting-edge objectives. The integrated Idea Studio guides students to\nclarify problem statements, compare alternative designs, and provide formative\nfeedback on novelty, clarity, feasibility, and alignment with learning\nobjectives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid advancement of artificial intelligence technology, AI students\nare confronted with a significant \"information-to-innovation\" gap: they must\nnavigate through the rapidly expanding body of literature, trace the\ndevelopment of a specific research field, and synthesize various techniques\ninto feasible innovative concepts. An additional critical step for students is\nto identify the necessary prerequisite knowledge and learning paths. Although\nmany approaches based on large language models (LLMs) can summarize the content\nof papers and trace the development of a field through citations, these methods\noften overlook the prerequisite knowledge involved in the papers and the rich\nsemantic information embedded in the citation relationships between papers.\nSuch information reveals how methods are interrelated, built upon, extended, or\nchallenged. To address these limitations, we propose GoAI, a tool for\nconstructing educational knowledge graphs from AI research papers that\nleverages these graphs to plan personalized learning paths and support creative\nideation. The nodes in the knowledge graph we have built include papers and the\nprerequisite knowledge, such as concepts, skills, and tools, that they involve;\nthe edges record the semantic information of citations. When a student queries\na specific paper, a beam search-based path search method can trace the current\ndevelopment trends of the field from the queried paper and plan a learning path\ntoward cutting-edge objectives. The integrated Idea Studio guides students to\nclarify problem statements, compare alternative designs, and provide formative\nfeedback on novelty, clarity, feasibility, and alignment with learning\nobjectives."
                },
                "authors": [
                    {
                        "name": "Xian Gao"
                    },
                    {
                        "name": "Zongyun Zhang"
                    },
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Yuzhuo Fu"
                    }
                ],
                "author_detail": {
                    "name": "Yuzhuo Fu"
                },
                "author": "Yuzhuo Fu",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08549v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08549v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08306v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08306v2",
                "updated": "2025-08-19T12:16:14Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    12,
                    16,
                    14,
                    1,
                    231,
                    0
                ],
                "published": "2025-01-14T18:44:35Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    18,
                    44,
                    35,
                    1,
                    14,
                    0
                ],
                "title": "Environmental Feature Engineering and Statistical Validation for\n  ML-Based Path Loss Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Environmental Feature Engineering and Statistical Validation for\n  ML-Based Path Loss Prediction"
                },
                "summary": "Wireless communications rely on path loss modeling, which is most effective\nwhen it includes the physical details of the propagation environment. Acquiring\nthis data has historically been challenging, but geographic information systems\ndata is becoming increasingly available with higher resolution and accuracy.\nAccess to such details enables propagation models to more accurately predict\ncoverage and account for interference in wireless deployments. Machine\nlearning-based modeling can significantly support this effort, with feature\nbased approaches allowing for accurate, efficient, and scalable propagation\nmodeling. Building on previous work, we introduce an extended set of features\nthat improves prediction accuracy while, most importantly, proving model\ngeneralization through rigorous statistical assessment and the use of test set\nholdouts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wireless communications rely on path loss modeling, which is most effective\nwhen it includes the physical details of the propagation environment. Acquiring\nthis data has historically been challenging, but geographic information systems\ndata is becoming increasingly available with higher resolution and accuracy.\nAccess to such details enables propagation models to more accurately predict\ncoverage and account for interference in wireless deployments. Machine\nlearning-based modeling can significantly support this effort, with feature\nbased approaches allowing for accurate, efficient, and scalable propagation\nmodeling. Building on previous work, we introduce an extended set of features\nthat improves prediction accuracy while, most importantly, proving model\ngeneralization through rigorous statistical assessment and the use of test set\nholdouts."
                },
                "authors": [
                    {
                        "name": "Jonathan Ethier"
                    },
                    {
                        "name": "Mathieu Chateauvert"
                    },
                    {
                        "name": "Ryan G. Dempsey"
                    },
                    {
                        "name": "Alexis Bose"
                    }
                ],
                "author_detail": {
                    "name": "Alexis Bose"
                },
                "author": "Alexis Bose",
                "arxiv_comment": "4 pages, 4 figures, AWPL paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08306v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08306v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13771v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13771v1",
                "updated": "2025-08-19T12:15:50Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    12,
                    15,
                    50,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T12:15:50Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    12,
                    15,
                    50,
                    1,
                    231,
                    0
                ],
                "title": "Joint AP Selection and Power Allocation for Unicast-Multicast Cell-Free\n  Massive MIMO",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint AP Selection and Power Allocation for Unicast-Multicast Cell-Free\n  Massive MIMO"
                },
                "summary": "Joint unicast and multicast transmissions are becoming increasingly important\nin practical wireless systems, such as Internet of Things networks. This paper\ninvestigates a cell-free massive multiple-input multiple-output system that\nsimultaneously supports both transmission types, with multicast serving\nmultiple groups. Exact closed-form expressions for the achievable downlink\nspectral efficiency (SE) of both unicast and multicast users are derived for\nzero-forcing and maximum ratio precoding designs. Accordingly, a weighted sum\nSE (SSE) maximization problem is formulated to jointly optimize the access\npoint (AP) selection and power allocation. The optimization framework accounts\nfor practical constraints, including the maximum transmit power per AP,\nfronthaul capacity limitations between APs and the central processing unit, and\nquality-of-service requirements for all users. The resulting non-convex\noptimization problem is reformulated into a tractable structure, and an\naccelerated projected gradient (APG)-based algorithm is developed to\nefficiently obtain near-optimal solutions. As a performance benchmark, a\nsuccessive convex approximation (SCA)-based algorithm is also implemented.\nSimulation results demonstrate that the proposed joint optimization approach\nsignificantly enhances the SSE across various system setups and precoding\nstrategies. In particular, the APG-based algorithm achieves substantial\ncomplexity reduction while maintaining competitive performance, making it\nwell-suited for large-scale practical deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint unicast and multicast transmissions are becoming increasingly important\nin practical wireless systems, such as Internet of Things networks. This paper\ninvestigates a cell-free massive multiple-input multiple-output system that\nsimultaneously supports both transmission types, with multicast serving\nmultiple groups. Exact closed-form expressions for the achievable downlink\nspectral efficiency (SE) of both unicast and multicast users are derived for\nzero-forcing and maximum ratio precoding designs. Accordingly, a weighted sum\nSE (SSE) maximization problem is formulated to jointly optimize the access\npoint (AP) selection and power allocation. The optimization framework accounts\nfor practical constraints, including the maximum transmit power per AP,\nfronthaul capacity limitations between APs and the central processing unit, and\nquality-of-service requirements for all users. The resulting non-convex\noptimization problem is reformulated into a tractable structure, and an\naccelerated projected gradient (APG)-based algorithm is developed to\nefficiently obtain near-optimal solutions. As a performance benchmark, a\nsuccessive convex approximation (SCA)-based algorithm is also implemented.\nSimulation results demonstrate that the proposed joint optimization approach\nsignificantly enhances the SSE across various system setups and precoding\nstrategies. In particular, the APG-based algorithm achieves substantial\ncomplexity reduction while maintaining competitive performance, making it\nwell-suited for large-scale practical deployments."
                },
                "authors": [
                    {
                        "name": "Mustafa S. Abbas"
                    },
                    {
                        "name": "Zahra Mobini"
                    },
                    {
                        "name": "Hien Quoc Ngo"
                    },
                    {
                        "name": "Hyundong Shin"
                    },
                    {
                        "name": "Michail Matthaiou"
                    }
                ],
                "author_detail": {
                    "name": "Michail Matthaiou"
                },
                "author": "Michail Matthaiou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13771v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13771v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13769v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13769v1",
                "updated": "2025-08-19T12:13:54Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    12,
                    13,
                    54,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T12:13:54Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    12,
                    13,
                    54,
                    1,
                    231,
                    0
                ],
                "title": "Can Large Language Models (LLMs) Describe Pictures Like Children? A\n  Comparative Corpus Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models (LLMs) Describe Pictures Like Children? A\n  Comparative Corpus Study"
                },
                "summary": "The role of large language models (LLMs) in education is increasing, yet\nlittle attention has been paid to whether LLM-generated text resembles child\nlanguage. This study evaluates how LLMs replicate child-like language by\ncomparing LLM-generated texts to a collection of German children's descriptions\nof picture stories. We generated two LLM-based corpora using the same picture\nstories and two prompt types: zero-shot and few-shot prompts specifying a\ngeneral age from the children corpus. We conducted a comparative analysis\nacross psycholinguistic text properties, including word frequency, lexical\nrichness, sentence and word length, part-of-speech tags, and semantic\nsimilarity with word embeddings. The results show that LLM-generated texts are\nlonger but less lexically rich, rely more on high-frequency words, and\nunder-represent nouns. Semantic vector space analysis revealed low similarity,\nhighlighting differences between the two corpora on the level of corpus\nsemantics. Few-shot prompt increased similarities between children and LLM text\nto a minor extent, but still failed to replicate lexical and semantic patterns.\nThe findings contribute to our understanding of how LLMs approximate child\nlanguage through multimodal prompting (text + image) and give insights into\ntheir use in psycholinguistic research and education while raising important\nquestions about the appropriateness of LLM-generated language in child-directed\neducational tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The role of large language models (LLMs) in education is increasing, yet\nlittle attention has been paid to whether LLM-generated text resembles child\nlanguage. This study evaluates how LLMs replicate child-like language by\ncomparing LLM-generated texts to a collection of German children's descriptions\nof picture stories. We generated two LLM-based corpora using the same picture\nstories and two prompt types: zero-shot and few-shot prompts specifying a\ngeneral age from the children corpus. We conducted a comparative analysis\nacross psycholinguistic text properties, including word frequency, lexical\nrichness, sentence and word length, part-of-speech tags, and semantic\nsimilarity with word embeddings. The results show that LLM-generated texts are\nlonger but less lexically rich, rely more on high-frequency words, and\nunder-represent nouns. Semantic vector space analysis revealed low similarity,\nhighlighting differences between the two corpora on the level of corpus\nsemantics. Few-shot prompt increased similarities between children and LLM text\nto a minor extent, but still failed to replicate lexical and semantic patterns.\nThe findings contribute to our understanding of how LLMs approximate child\nlanguage through multimodal prompting (text + image) and give insights into\ntheir use in psycholinguistic research and education while raising important\nquestions about the appropriateness of LLM-generated language in child-directed\neducational tools."
                },
                "authors": [
                    {
                        "name": "Hanna Woloszyn"
                    },
                    {
                        "name": "Benjamin Gagl"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Gagl"
                },
                "author": "Benjamin Gagl",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13769v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13769v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13755v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13755v1",
                "updated": "2025-08-19T11:51:40Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    11,
                    51,
                    40,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T11:51:40Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    11,
                    51,
                    40,
                    1,
                    231,
                    0
                ],
                "title": "Depth-Breadth Synergy in RLVR: Unlocking LLM Reasoning Gains with\n  Adaptive Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Depth-Breadth Synergy in RLVR: Unlocking LLM Reasoning Gains with\n  Adaptive Exploration"
                },
                "summary": "Reinforcement Learning with Verifiable Reward (RLVR) has emerged as a\npowerful paradigm for unlocking reasoning capabilities in large language\nmodels, yet its full potential is hindered by two under-explored dimensions:\nDepth-the hardest problem a model can sample; Breadth-the number of instances\nconsumed in a single iteration. We dissect the popular GRPO algorithm and\nreveal a systematic bias: the cumulative-advantage disproportionately weights\nsamples with medium accuracy, while down-weighting the low-accuracy instances\nthat are crucial for pushing reasoning boundaries. To rectify the depth\nneglect, we introduce Difficulty Adaptive Rollout Sampling (DARS), which\nre-weights hard problems through targeted multi-stage rollouts, thereby\nincreasing the number of positive rollouts for hard problems. Empirically,\nnaively enlarging rollout size only accelerates convergence and even hurts\nPass@K. Our DARS, in contrast, delivers consistent Pass@K gains without extra\ninference cost at convergence. Just as we adaptively expanded the depth of\nexploration, we now ask whether aggressively scaling the breadth of training\ndata can further amplify reasoning gains. To this end, we intensely scale batch\nsize and replace PPO's mini-batch iterations with full-batch updates over\nmultiple epochs. Increasing breadth significantly enhances Pass@1 performance.\nLarge-breadth training sustains high token-level entropy, indicating continued\nexploration and reduced gradient noise. We further present DARS-B, which\naugments DARS with large breadth, and demonstrate simultaneous gains in Pass@K\nand Pass@1. The results confirm that breadth and adaptive exploration across\ndepth operate as orthogonal dimensions in RLVR, which are key to unleashing the\nreasoning power of RLVR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning with Verifiable Reward (RLVR) has emerged as a\npowerful paradigm for unlocking reasoning capabilities in large language\nmodels, yet its full potential is hindered by two under-explored dimensions:\nDepth-the hardest problem a model can sample; Breadth-the number of instances\nconsumed in a single iteration. We dissect the popular GRPO algorithm and\nreveal a systematic bias: the cumulative-advantage disproportionately weights\nsamples with medium accuracy, while down-weighting the low-accuracy instances\nthat are crucial for pushing reasoning boundaries. To rectify the depth\nneglect, we introduce Difficulty Adaptive Rollout Sampling (DARS), which\nre-weights hard problems through targeted multi-stage rollouts, thereby\nincreasing the number of positive rollouts for hard problems. Empirically,\nnaively enlarging rollout size only accelerates convergence and even hurts\nPass@K. Our DARS, in contrast, delivers consistent Pass@K gains without extra\ninference cost at convergence. Just as we adaptively expanded the depth of\nexploration, we now ask whether aggressively scaling the breadth of training\ndata can further amplify reasoning gains. To this end, we intensely scale batch\nsize and replace PPO's mini-batch iterations with full-batch updates over\nmultiple epochs. Increasing breadth significantly enhances Pass@1 performance.\nLarge-breadth training sustains high token-level entropy, indicating continued\nexploration and reduced gradient noise. We further present DARS-B, which\naugments DARS with large breadth, and demonstrate simultaneous gains in Pass@K\nand Pass@1. The results confirm that breadth and adaptive exploration across\ndepth operate as orthogonal dimensions in RLVR, which are key to unleashing the\nreasoning power of RLVR."
                },
                "authors": [
                    {
                        "name": "Zhicheng Yang"
                    },
                    {
                        "name": "Zhijiang Guo"
                    },
                    {
                        "name": "Yinya Huang"
                    },
                    {
                        "name": "Yongxin Wang"
                    },
                    {
                        "name": "Dongchun Xie"
                    },
                    {
                        "name": "Yiwei Wang"
                    },
                    {
                        "name": "Xiaodan Liang"
                    },
                    {
                        "name": "Jing Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jing Tang"
                },
                "author": "Jing Tang",
                "arxiv_comment": "11pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13755v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13755v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13754v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13754v1",
                "updated": "2025-08-19T11:51:15Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    11,
                    51,
                    15,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T11:51:15Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    11,
                    51,
                    15,
                    1,
                    231,
                    0
                ],
                "title": "Expertise-aware Multi-LLM Recruitment and Collaboration for Medical\n  Decision-Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Expertise-aware Multi-LLM Recruitment and Collaboration for Medical\n  Decision-Making"
                },
                "summary": "Medical Decision-Making (MDM) is a complex process requiring substantial\ndomain-specific expertise to effectively synthesize heterogeneous and\ncomplicated clinical information. While recent advancements in Large Language\nModels (LLMs) show promise in supporting MDM, single-LLM approaches are limited\nby their parametric knowledge constraints and static training corpora, failing\nto robustly integrate the clinical information. To address this challenge, we\npropose the Expertise-aware Multi-LLM Recruitment and Collaboration (EMRC)\nframework to enhance the accuracy and reliability of MDM systems. It operates\nin two stages: (i) expertise-aware agent recruitment and (ii) confidence- and\nadversarial-driven multi-agent collaboration. Specifically, in the first stage,\nwe use a publicly available corpus to construct an LLM expertise table for\ncapturing expertise-specific strengths of multiple LLMs across medical\ndepartment categories and query difficulty levels. This table enables the\nsubsequent dynamic selection of the optimal LLMs to act as medical expert\nagents for each medical query during the inference phase. In the second stage,\nwe employ selected agents to generate responses with self-assessed confidence\nscores, which are then integrated through the confidence fusion and adversarial\nvalidation to improve diagnostic reliability. We evaluate our EMRC framework on\nthree public MDM datasets, where the results demonstrate that our EMRC\noutperforms state-of-the-art single- and multi-LLM methods, achieving superior\ndiagnostic performance. For instance, on the MMLU-Pro-Health dataset, our EMRC\nachieves 74.45% accuracy, representing a 2.69% improvement over the\nbest-performing closed-source model GPT- 4-0613, which demonstrates the\neffectiveness of our expertise-aware agent recruitment strategy and the agent\ncomplementarity in leveraging each LLM's specialized capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical Decision-Making (MDM) is a complex process requiring substantial\ndomain-specific expertise to effectively synthesize heterogeneous and\ncomplicated clinical information. While recent advancements in Large Language\nModels (LLMs) show promise in supporting MDM, single-LLM approaches are limited\nby their parametric knowledge constraints and static training corpora, failing\nto robustly integrate the clinical information. To address this challenge, we\npropose the Expertise-aware Multi-LLM Recruitment and Collaboration (EMRC)\nframework to enhance the accuracy and reliability of MDM systems. It operates\nin two stages: (i) expertise-aware agent recruitment and (ii) confidence- and\nadversarial-driven multi-agent collaboration. Specifically, in the first stage,\nwe use a publicly available corpus to construct an LLM expertise table for\ncapturing expertise-specific strengths of multiple LLMs across medical\ndepartment categories and query difficulty levels. This table enables the\nsubsequent dynamic selection of the optimal LLMs to act as medical expert\nagents for each medical query during the inference phase. In the second stage,\nwe employ selected agents to generate responses with self-assessed confidence\nscores, which are then integrated through the confidence fusion and adversarial\nvalidation to improve diagnostic reliability. We evaluate our EMRC framework on\nthree public MDM datasets, where the results demonstrate that our EMRC\noutperforms state-of-the-art single- and multi-LLM methods, achieving superior\ndiagnostic performance. For instance, on the MMLU-Pro-Health dataset, our EMRC\nachieves 74.45% accuracy, representing a 2.69% improvement over the\nbest-performing closed-source model GPT- 4-0613, which demonstrates the\neffectiveness of our expertise-aware agent recruitment strategy and the agent\ncomplementarity in leveraging each LLM's specialized capabilities."
                },
                "authors": [
                    {
                        "name": "Liuxin Bao"
                    },
                    {
                        "name": "Zhihao Peng"
                    },
                    {
                        "name": "Xiaofei Zhou"
                    },
                    {
                        "name": "Runmin Cong"
                    },
                    {
                        "name": "Jiyong Zhang"
                    },
                    {
                        "name": "Yixuan Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Yixuan Yuan"
                },
                "author": "Yixuan Yuan",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13754v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13754v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14226v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14226v2",
                "updated": "2025-08-19T11:43:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    11,
                    43,
                    9,
                    1,
                    231,
                    0
                ],
                "published": "2025-05-20T11:35:25Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    11,
                    35,
                    25,
                    1,
                    140,
                    0
                ],
                "title": "\"Haet Bhasha aur Diskrimineshun\": Phonetic Perturbations in Code-Mixed\n  Hinglish to Red-Team LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"Haet Bhasha aur Diskrimineshun\": Phonetic Perturbations in Code-Mixed\n  Hinglish to Red-Team LLMs"
                },
                "summary": "Recently released LLMs have strong multilingual \\& multimodal capabilities.\nModel vulnerabilities are exposed using audits and red-teaming efforts.\nExisting efforts have focused primarily on the English language; thus, models\ncontinue to be susceptible to multilingual jailbreaking strategies, especially\nfor multimodal contexts. In this study, we introduce a novel strategy that\nleverages code-mixing and phonetic perturbations to jailbreak LLMs for both\ntext and image generation tasks. We also introduce \\textit{two new} jailbreak\nstrategies that show higher effectiveness than baselines. Our work presents a\nmethod to effectively bypass safety filters in LLMs while maintaining\ninterpretability by applying phonetic misspellings to sensitive words in\ncode-mixed prompts. We achieve a 99\\% Attack Success Rate for text generation\nand 78\\% for image generation, with Attack Relevance Rate of 100\\% for text\ngeneration and 95\\% for image generation for the phonetically perturbed\ncode-mixed prompts. Our interpretability experiments reveal that phonetic\nperturbations impact word tokenization, leading to jailbreak success. Our study\nmotivates increasing the focus towards more generalizable safety alignment for\nmultilingual multimodal models, especially in real-world settings wherein\nprompts can have misspelt words. \\textit{\\textbf{Warning: This paper contains\nexamples of potentially harmful and offensive content.}}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently released LLMs have strong multilingual \\& multimodal capabilities.\nModel vulnerabilities are exposed using audits and red-teaming efforts.\nExisting efforts have focused primarily on the English language; thus, models\ncontinue to be susceptible to multilingual jailbreaking strategies, especially\nfor multimodal contexts. In this study, we introduce a novel strategy that\nleverages code-mixing and phonetic perturbations to jailbreak LLMs for both\ntext and image generation tasks. We also introduce \\textit{two new} jailbreak\nstrategies that show higher effectiveness than baselines. Our work presents a\nmethod to effectively bypass safety filters in LLMs while maintaining\ninterpretability by applying phonetic misspellings to sensitive words in\ncode-mixed prompts. We achieve a 99\\% Attack Success Rate for text generation\nand 78\\% for image generation, with Attack Relevance Rate of 100\\% for text\ngeneration and 95\\% for image generation for the phonetically perturbed\ncode-mixed prompts. Our interpretability experiments reveal that phonetic\nperturbations impact word tokenization, leading to jailbreak success. Our study\nmotivates increasing the focus towards more generalizable safety alignment for\nmultilingual multimodal models, especially in real-world settings wherein\nprompts can have misspelt words. \\textit{\\textbf{Warning: This paper contains\nexamples of potentially harmful and offensive content.}}"
                },
                "authors": [
                    {
                        "name": "Darpan Aswal"
                    },
                    {
                        "name": "Siddharth D Jaiswal"
                    }
                ],
                "author_detail": {
                    "name": "Siddharth D Jaiswal"
                },
                "author": "Siddharth D Jaiswal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14226v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14226v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.12800v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.12800v2",
                "updated": "2025-08-19T11:40:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    11,
                    40,
                    33,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-18T10:23:10Z",
                "published_parsed": [
                    2025,
                    8,
                    18,
                    10,
                    23,
                    10,
                    0,
                    230,
                    0
                ],
                "title": "Atom-Searcher: Enhancing Agentic Deep Research via Fine-Grained Atomic\n  Thought Reward",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Atom-Searcher: Enhancing Agentic Deep Research via Fine-Grained Atomic\n  Thought Reward"
                },
                "summary": "Large language models (LLMs) exhibit remarkable problem-solving abilities,\nbut struggle with complex tasks due to static internal knowledge.\nRetrieval-Augmented Generation (RAG) enhances access to external information,\nyet remains limited in multi-hop reasoning and strategic search due to rigid\nworkflows. Recent advancements in agentic deep research empower LLMs to\nautonomously reason, search, and synthesize information. However, current\napproaches relying on outcome-based reinforcement learning (RL) face critical\nissues such as conflicting gradients and reward sparsity, limiting performance\ngains and training efficiency. To address these, we first propose Atomic\nThought, a novel LLM thinking paradigm that decomposes reasoning into\nfine-grained functional units. These units are supervised by Reasoning Reward\nModels (RRMs), which provide Atomic Thought Rewards (ATR) for fine-grained\nguidance. Building on this, we propose Atom-Searcher, a novel RL framework for\nagentic deep research that integrates Atomic Thought and ATR. Atom-Searcher\nuses a curriculum-inspired reward schedule, prioritizing process-level ATR\nearly and transitioning to outcome rewards, accelerating convergence on\neffective reasoning paths. Experiments on seven benchmarks show consistent\nimprovements over the state-of-the-art. Key advantages include: (1)\nAtom-Searcher scales computation at test-time. (2) Atomic Thought provides\nsupervision anchors for RRMs, bridging deep research tasks and RRMs. (3)\nAtom-Searcher exhibits more interpretable, human-like reasoning patterns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit remarkable problem-solving abilities,\nbut struggle with complex tasks due to static internal knowledge.\nRetrieval-Augmented Generation (RAG) enhances access to external information,\nyet remains limited in multi-hop reasoning and strategic search due to rigid\nworkflows. Recent advancements in agentic deep research empower LLMs to\nautonomously reason, search, and synthesize information. However, current\napproaches relying on outcome-based reinforcement learning (RL) face critical\nissues such as conflicting gradients and reward sparsity, limiting performance\ngains and training efficiency. To address these, we first propose Atomic\nThought, a novel LLM thinking paradigm that decomposes reasoning into\nfine-grained functional units. These units are supervised by Reasoning Reward\nModels (RRMs), which provide Atomic Thought Rewards (ATR) for fine-grained\nguidance. Building on this, we propose Atom-Searcher, a novel RL framework for\nagentic deep research that integrates Atomic Thought and ATR. Atom-Searcher\nuses a curriculum-inspired reward schedule, prioritizing process-level ATR\nearly and transitioning to outcome rewards, accelerating convergence on\neffective reasoning paths. Experiments on seven benchmarks show consistent\nimprovements over the state-of-the-art. Key advantages include: (1)\nAtom-Searcher scales computation at test-time. (2) Atomic Thought provides\nsupervision anchors for RRMs, bridging deep research tasks and RRMs. (3)\nAtom-Searcher exhibits more interpretable, human-like reasoning patterns."
                },
                "authors": [
                    {
                        "name": "Yong Deng"
                    },
                    {
                        "name": "Guoqing Wang"
                    },
                    {
                        "name": "Zhenzhe Ying"
                    },
                    {
                        "name": "Xiaofeng Wu"
                    },
                    {
                        "name": "Jinzhen Lin"
                    },
                    {
                        "name": "Wenwen Xiong"
                    },
                    {
                        "name": "Yuqin Dai"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Zhanwei Zhang"
                    },
                    {
                        "name": "Qiwen Wang"
                    },
                    {
                        "name": "Yang Qin"
                    },
                    {
                        "name": "Changhua Meng"
                    }
                ],
                "author_detail": {
                    "name": "Changhua Meng"
                },
                "author": "Changhua Meng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.12800v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.12800v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13250v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13250v2",
                "updated": "2025-08-19T11:40:15Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    11,
                    40,
                    15,
                    1,
                    231,
                    0
                ],
                "published": "2025-03-17T15:06:14Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    6,
                    14,
                    0,
                    76,
                    0
                ],
                "title": "MindEye-OmniAssist: A Gaze-Driven LLM-Enhanced Assistive Robot System\n  for Implicit Intention Recognition and Task Execution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MindEye-OmniAssist: A Gaze-Driven LLM-Enhanced Assistive Robot System\n  for Implicit Intention Recognition and Task Execution"
                },
                "summary": "A promising effective human-robot interaction in assistive robotic systems is\ngaze-based control. However, current gaze-based assistive systems mainly help\nusers with basic grasping actions, offering limited support. Moreover, the\nrestricted intent recognition capability constrains the assistive system's\nability to provide diverse assistance functions. In this paper, we propose an\nopen implicit intention recognition framework powered by Large Language Model\n(LLM) and Vision Foundation Model (VFM), which can process gaze input and\nrecognize user intents that are not confined to predefined or specific\nscenarios. Furthermore, we implement a gaze-driven LLM-enhanced assistive robot\nsystem (MindEye-OmniAssist) that recognizes user's intentions through gaze and\nassists in completing task. To achieve this, the system utilizes open\nvocabulary object detector, intention recognition network and LLM to infer\ntheir full intentions. By integrating eye movement feedback and LLM, it\ngenerates action sequences to assist the user in completing tasks. Real-world\nexperiments have been conducted for assistive tasks, and the system achieved an\noverall success rate of 41/55 across various undefined tasks. Preliminary\nresults show that the proposed method holds the potential to provide a more\nuser-friendly human-computer interaction interface and significantly enhance\nthe versatility and effectiveness of assistive systems by supporting more\ncomplex and diverse task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A promising effective human-robot interaction in assistive robotic systems is\ngaze-based control. However, current gaze-based assistive systems mainly help\nusers with basic grasping actions, offering limited support. Moreover, the\nrestricted intent recognition capability constrains the assistive system's\nability to provide diverse assistance functions. In this paper, we propose an\nopen implicit intention recognition framework powered by Large Language Model\n(LLM) and Vision Foundation Model (VFM), which can process gaze input and\nrecognize user intents that are not confined to predefined or specific\nscenarios. Furthermore, we implement a gaze-driven LLM-enhanced assistive robot\nsystem (MindEye-OmniAssist) that recognizes user's intentions through gaze and\nassists in completing task. To achieve this, the system utilizes open\nvocabulary object detector, intention recognition network and LLM to infer\ntheir full intentions. By integrating eye movement feedback and LLM, it\ngenerates action sequences to assist the user in completing tasks. Real-world\nexperiments have been conducted for assistive tasks, and the system achieved an\noverall success rate of 41/55 across various undefined tasks. Preliminary\nresults show that the proposed method holds the potential to provide a more\nuser-friendly human-computer interaction interface and significantly enhance\nthe versatility and effectiveness of assistive systems by supporting more\ncomplex and diverse task."
                },
                "authors": [
                    {
                        "name": "Zejia Zhang"
                    },
                    {
                        "name": "Bo Yang"
                    },
                    {
                        "name": "Xinxing Chen"
                    },
                    {
                        "name": "Weizhuang Shi"
                    },
                    {
                        "name": "Haoyuan Wang"
                    },
                    {
                        "name": "Wei Luo"
                    },
                    {
                        "name": "Jian Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Huang"
                },
                "author": "Jian Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13250v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13250v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13743v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13743v1",
                "updated": "2025-08-19T11:30:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    11,
                    30,
                    52,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T11:30:52Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    11,
                    30,
                    52,
                    1,
                    231,
                    0
                ],
                "title": "Sycophancy under Pressure: Evaluating and Mitigating Sycophantic Bias\n  via Adversarial Dialogues in Scientific QA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sycophancy under Pressure: Evaluating and Mitigating Sycophantic Bias\n  via Adversarial Dialogues in Scientific QA"
                },
                "summary": "Large language models (LLMs), while increasingly used in domains requiring\nfactual rigor, often display a troubling behavior: sycophancy, the tendency to\nalign with user beliefs regardless of correctness. This tendency is reinforced\nby preference-based alignment techniques that optimize for user satisfaction\nbut can undermine truthfulness. While relatively benign in casual dialogue,\nsycophancy poses serious risks in high-stakes settings such as scientific\nquestion answering (QA), where model outputs may shape collaborative reasoning,\ndecision-making, and knowledge formation. Despite its importance, this\nphenomenon remains underexamined in factual QA contexts. We address this gap by\nintroducing a unified evaluation framework to quantify the impact of\nsycophantic context on model behavior in scientific QA, measuring how much\nuser-imposed social pressure distorts model outputs. The framework incorporates\nadversarial prompting setups and targeted metrics, such as misleading\nresistance and sycophancy resistance, that capture a model's ability to\nmaintain factual consistency under misleading cues. Systematic evaluations\nacross open-source and proprietary models reveal pervasive sycophantic\ntendencies, driven more by alignment strategy than by model size. To mitigate\nthis issue, we propose Pressure-Tune, a lightweight post-training method that\nfine-tunes models on synthetic adversarial dialogues paired with\nchain-of-thought rationales. These rationales reject user misinformation while\nreinforcing factual commitments. Experiments on challenging scientific QA\nbenchmarks show that Pressure-Tune significantly enhances sycophancy resistance\nwithout compromising accuracy or responsiveness to valid feedback, offering a\npractical pathway toward more truthful and principled model behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), while increasingly used in domains requiring\nfactual rigor, often display a troubling behavior: sycophancy, the tendency to\nalign with user beliefs regardless of correctness. This tendency is reinforced\nby preference-based alignment techniques that optimize for user satisfaction\nbut can undermine truthfulness. While relatively benign in casual dialogue,\nsycophancy poses serious risks in high-stakes settings such as scientific\nquestion answering (QA), where model outputs may shape collaborative reasoning,\ndecision-making, and knowledge formation. Despite its importance, this\nphenomenon remains underexamined in factual QA contexts. We address this gap by\nintroducing a unified evaluation framework to quantify the impact of\nsycophantic context on model behavior in scientific QA, measuring how much\nuser-imposed social pressure distorts model outputs. The framework incorporates\nadversarial prompting setups and targeted metrics, such as misleading\nresistance and sycophancy resistance, that capture a model's ability to\nmaintain factual consistency under misleading cues. Systematic evaluations\nacross open-source and proprietary models reveal pervasive sycophantic\ntendencies, driven more by alignment strategy than by model size. To mitigate\nthis issue, we propose Pressure-Tune, a lightweight post-training method that\nfine-tunes models on synthetic adversarial dialogues paired with\nchain-of-thought rationales. These rationales reject user misinformation while\nreinforcing factual commitments. Experiments on challenging scientific QA\nbenchmarks show that Pressure-Tune significantly enhances sycophancy resistance\nwithout compromising accuracy or responsiveness to valid feedback, offering a\npractical pathway toward more truthful and principled model behavior."
                },
                "authors": [
                    {
                        "name": "Kaiwei Zhang"
                    },
                    {
                        "name": "Qi Jia"
                    },
                    {
                        "name": "Zijian Chen"
                    },
                    {
                        "name": "Wei Sun"
                    },
                    {
                        "name": "Xiangyang Zhu"
                    },
                    {
                        "name": "Chunyi Li"
                    },
                    {
                        "name": "Dandan Zhu"
                    },
                    {
                        "name": "Guangtao Zhai"
                    }
                ],
                "author_detail": {
                    "name": "Guangtao Zhai"
                },
                "author": "Guangtao Zhai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13743v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13739v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13739v1",
                "updated": "2025-08-19T11:23:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    11,
                    23,
                    9,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T11:23:09Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    11,
                    23,
                    9,
                    1,
                    231,
                    0
                ],
                "title": "Enhancing Targeted Adversarial Attacks on Large Vision-Language Models\n  through Intermediate Projector Guidance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Targeted Adversarial Attacks on Large Vision-Language Models\n  through Intermediate Projector Guidance"
                },
                "summary": "Targeted adversarial attacks are essential for proactively identifying\nsecurity flaws in Vision-Language Models before real-world deployment. However,\ncurrent methods perturb images to maximize global similarity with the target\ntext or reference image at the encoder level, collapsing rich visual semantics\ninto a single global vector. This limits attack granularity, hindering\nfine-grained manipulations such as modifying a car while preserving its\nbackground. Furthermore, these methods largely overlook the projector module, a\ncritical semantic bridge between the visual encoder and the language model in\nVLMs, thereby failing to disrupt the full vision-language alignment pipeline\nwithin VLMs and limiting attack effectiveness. To address these issues, we\npropose the Intermediate Projector Guided Attack (IPGA), the first method to\nattack using the intermediate stage of the projector module, specifically the\nwidely adopted Q-Former, which transforms global image embeddings into\nfine-grained visual features. This enables more precise control over\nadversarial perturbations by operating on semantically meaningful visual tokens\nrather than a single global representation. Specifically, IPGA leverages the\nQ-Former pretrained solely on the first vision-language alignment stage,\nwithout LLM fine-tuning, which improves both attack effectiveness and\ntransferability across diverse VLMs. Furthermore, we propose Residual Query\nAlignment (RQA) to preserve unrelated visual content, thereby yielding more\ncontrolled and precise adversarial manipulations. Extensive experiments show\nthat our attack method consistently outperforms existing methods in both\nstandard global image captioning tasks and fine-grained visual\nquestion-answering tasks in black-box environment. Additionally, IPGA\nsuccessfully transfers to multiple commercial VLMs, including Google Gemini and\nOpenAI GPT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Targeted adversarial attacks are essential for proactively identifying\nsecurity flaws in Vision-Language Models before real-world deployment. However,\ncurrent methods perturb images to maximize global similarity with the target\ntext or reference image at the encoder level, collapsing rich visual semantics\ninto a single global vector. This limits attack granularity, hindering\nfine-grained manipulations such as modifying a car while preserving its\nbackground. Furthermore, these methods largely overlook the projector module, a\ncritical semantic bridge between the visual encoder and the language model in\nVLMs, thereby failing to disrupt the full vision-language alignment pipeline\nwithin VLMs and limiting attack effectiveness. To address these issues, we\npropose the Intermediate Projector Guided Attack (IPGA), the first method to\nattack using the intermediate stage of the projector module, specifically the\nwidely adopted Q-Former, which transforms global image embeddings into\nfine-grained visual features. This enables more precise control over\nadversarial perturbations by operating on semantically meaningful visual tokens\nrather than a single global representation. Specifically, IPGA leverages the\nQ-Former pretrained solely on the first vision-language alignment stage,\nwithout LLM fine-tuning, which improves both attack effectiveness and\ntransferability across diverse VLMs. Furthermore, we propose Residual Query\nAlignment (RQA) to preserve unrelated visual content, thereby yielding more\ncontrolled and precise adversarial manipulations. Extensive experiments show\nthat our attack method consistently outperforms existing methods in both\nstandard global image captioning tasks and fine-grained visual\nquestion-answering tasks in black-box environment. Additionally, IPGA\nsuccessfully transfers to multiple commercial VLMs, including Google Gemini and\nOpenAI GPT."
                },
                "authors": [
                    {
                        "name": "Yiming Cao"
                    },
                    {
                        "name": "Yanjie Li"
                    },
                    {
                        "name": "Kaisheng Liang"
                    },
                    {
                        "name": "Yuni Lai"
                    },
                    {
                        "name": "Bin Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Bin Xiao"
                },
                "author": "Bin Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13739v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13739v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06189v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06189v2",
                "updated": "2025-08-19T11:14:11Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    11,
                    14,
                    11,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-08T10:12:00Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    10,
                    12,
                    0,
                    4,
                    220,
                    0
                ],
                "title": "MA-CBP: A Criminal Behavior Prediction Framework Based on Multi-Agent\n  Asynchronous Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MA-CBP: A Criminal Behavior Prediction Framework Based on Multi-Agent\n  Asynchronous Collaboration"
                },
                "summary": "With the acceleration of urbanization, criminal behavior in public scenes\nposes an increasingly serious threat to social security. Traditional anomaly\ndetection methods based on feature recognition struggle to capture high-level\nbehavioral semantics from historical information, while generative approaches\nbased on Large Language Models (LLMs) often fail to meet real-time\nrequirements. To address these challenges, we propose MA-CBP, a criminal\nbehavior prediction framework based on multi-agent asynchronous collaboration.\nThis framework transforms real-time video streams into frame-level semantic\ndescriptions, constructs causally consistent historical summaries, and fuses\nadjacent image frames to perform joint reasoning over long- and short-term\ncontexts. The resulting behavioral decisions include key elements such as event\nsubjects, locations, and causes, enabling early warning of potential criminal\nactivity. In addition, we construct a high-quality criminal behavior dataset\nthat provides multi-scale language supervision, including frame-level,\nsummary-level, and event-level semantic annotations. Experimental results\ndemonstrate that our method achieves superior performance on multiple datasets\nand offers a promising solution for risk warning in urban public safety\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the acceleration of urbanization, criminal behavior in public scenes\nposes an increasingly serious threat to social security. Traditional anomaly\ndetection methods based on feature recognition struggle to capture high-level\nbehavioral semantics from historical information, while generative approaches\nbased on Large Language Models (LLMs) often fail to meet real-time\nrequirements. To address these challenges, we propose MA-CBP, a criminal\nbehavior prediction framework based on multi-agent asynchronous collaboration.\nThis framework transforms real-time video streams into frame-level semantic\ndescriptions, constructs causally consistent historical summaries, and fuses\nadjacent image frames to perform joint reasoning over long- and short-term\ncontexts. The resulting behavioral decisions include key elements such as event\nsubjects, locations, and causes, enabling early warning of potential criminal\nactivity. In addition, we construct a high-quality criminal behavior dataset\nthat provides multi-scale language supervision, including frame-level,\nsummary-level, and event-level semantic annotations. Experimental results\ndemonstrate that our method achieves superior performance on multiple datasets\nand offers a promising solution for risk warning in urban public safety\nscenarios."
                },
                "authors": [
                    {
                        "name": "Cheng Liu"
                    },
                    {
                        "name": "Daou Zhang"
                    },
                    {
                        "name": "Tingxu Liu"
                    },
                    {
                        "name": "Yuhan Wang"
                    },
                    {
                        "name": "Jinyang Chen"
                    },
                    {
                        "name": "Yuexuan Li"
                    },
                    {
                        "name": "Xinying Xiao"
                    },
                    {
                        "name": "Chenbo Xin"
                    },
                    {
                        "name": "Ziru Wang"
                    },
                    {
                        "name": "Weichao Wu"
                    }
                ],
                "author_detail": {
                    "name": "Weichao Wu"
                },
                "author": "Weichao Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06189v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06189v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13732v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13732v1",
                "updated": "2025-08-19T11:10:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    11,
                    10,
                    56,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T11:10:56Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    11,
                    10,
                    56,
                    1,
                    231,
                    0
                ],
                "title": "Self-Organizing Agent Network for LLM-based Workflow Automation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Organizing Agent Network for LLM-based Workflow Automation"
                },
                "summary": "Recent multi-agent frameworks built upon large language models (LLMs) have\ndemonstrated remarkable capabilities in complex task planning. However, in\nreal-world enterprise environments, business workflows are typically composed\nthrough modularization and reuse of numerous subprocesses, resulting in\nintricate workflows characterized by lengthy and deeply nested execution paths.\nSuch complexity poses significant challenges for LLM-driven orchestration, as\nextended reasoning chains and state-space explosions severely impact planning\neffectiveness and the proper sequencing of tool invocations. Therefore,\ndeveloping an orchestration method with controllable structures capable of\nhandling multi-layer nesting becomes a critical issue. To address this, we\npropose a novel structure-driven orchestration framework Self-Organizing Agent\nNetwork (SOAN). SOAN incrementally builds a formalized agent network by\nidentifying and encapsulating structural units as independent agents, enhancing\nmodularity and clarity in orchestration. Extensive evaluations were performed\nusing multiple benchmarks as well as a real-world enterprise workflow dataset.\nExperimental results demonstrate that SOAN significantly outperforms\nstate-of-the-art methods in terms of adaptability, fault tolerance, and\nexecution efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent multi-agent frameworks built upon large language models (LLMs) have\ndemonstrated remarkable capabilities in complex task planning. However, in\nreal-world enterprise environments, business workflows are typically composed\nthrough modularization and reuse of numerous subprocesses, resulting in\nintricate workflows characterized by lengthy and deeply nested execution paths.\nSuch complexity poses significant challenges for LLM-driven orchestration, as\nextended reasoning chains and state-space explosions severely impact planning\neffectiveness and the proper sequencing of tool invocations. Therefore,\ndeveloping an orchestration method with controllable structures capable of\nhandling multi-layer nesting becomes a critical issue. To address this, we\npropose a novel structure-driven orchestration framework Self-Organizing Agent\nNetwork (SOAN). SOAN incrementally builds a formalized agent network by\nidentifying and encapsulating structural units as independent agents, enhancing\nmodularity and clarity in orchestration. Extensive evaluations were performed\nusing multiple benchmarks as well as a real-world enterprise workflow dataset.\nExperimental results demonstrate that SOAN significantly outperforms\nstate-of-the-art methods in terms of adaptability, fault tolerance, and\nexecution efficiency."
                },
                "authors": [
                    {
                        "name": "Yiming Xiong"
                    },
                    {
                        "name": "Jian Wang"
                    },
                    {
                        "name": "Bing Li"
                    },
                    {
                        "name": "Yuhan Zhu"
                    },
                    {
                        "name": "Yuqi Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yuqi Zhao"
                },
                "author": "Yuqi Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13732v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13732v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13729v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13729v1",
                "updated": "2025-08-19T11:00:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    11,
                    0,
                    47,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T11:00:47Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    11,
                    0,
                    47,
                    1,
                    231,
                    0
                ],
                "title": "Prediction is not Explanation: Revisiting the Explanatory Capacity of\n  Mapping Embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prediction is not Explanation: Revisiting the Explanatory Capacity of\n  Mapping Embeddings"
                },
                "summary": "Understanding what knowledge is implicitly encoded in deep learning models is\nessential for improving the interpretability of AI systems. This paper examines\ncommon methods to explain the knowledge encoded in word embeddings, which are\ncore elements of large language models (LLMs). These methods typically involve\nmapping embeddings onto collections of human-interpretable semantic features,\nknown as feature norms. Prior work assumes that accurately predicting these\nsemantic features from the word embeddings implies that the embeddings contain\nthe corresponding knowledge. We challenge this assumption by demonstrating that\nprediction accuracy alone does not reliably indicate genuine feature-based\ninterpretability.\n  We show that these methods can successfully predict even random information,\nconcluding that the results are predominantly determined by an algorithmic\nupper bound rather than meaningful semantic representation in the word\nembeddings. Consequently, comparisons between datasets based solely on\nprediction performance do not reliably indicate which dataset is better\ncaptured by the word embeddings. Our analysis illustrates that such mappings\nprimarily reflect geometric similarity within vector spaces rather than\nindicating the genuine emergence of semantic properties.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding what knowledge is implicitly encoded in deep learning models is\nessential for improving the interpretability of AI systems. This paper examines\ncommon methods to explain the knowledge encoded in word embeddings, which are\ncore elements of large language models (LLMs). These methods typically involve\nmapping embeddings onto collections of human-interpretable semantic features,\nknown as feature norms. Prior work assumes that accurately predicting these\nsemantic features from the word embeddings implies that the embeddings contain\nthe corresponding knowledge. We challenge this assumption by demonstrating that\nprediction accuracy alone does not reliably indicate genuine feature-based\ninterpretability.\n  We show that these methods can successfully predict even random information,\nconcluding that the results are predominantly determined by an algorithmic\nupper bound rather than meaningful semantic representation in the word\nembeddings. Consequently, comparisons between datasets based solely on\nprediction performance do not reliably indicate which dataset is better\ncaptured by the word embeddings. Our analysis illustrates that such mappings\nprimarily reflect geometric similarity within vector spaces rather than\nindicating the genuine emergence of semantic properties."
                },
                "authors": [
                    {
                        "name": "Hanna Herasimchyk"
                    },
                    {
                        "name": "Alhassan Abdelhalim"
                    },
                    {
                        "name": "Sören Laue"
                    },
                    {
                        "name": "Michaela Regneri"
                    }
                ],
                "author_detail": {
                    "name": "Michaela Regneri"
                },
                "author": "Michaela Regneri",
                "arxiv_comment": "10 pages, 6 Figures. Published at ECAI 2025 in a version without the\n  Appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13729v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13729v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13721v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13721v1",
                "updated": "2025-08-19T10:37:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    10,
                    37,
                    20,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T10:37:20Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    10,
                    37,
                    20,
                    1,
                    231,
                    0
                ],
                "title": "CausalPlan: Empowering Efficient LLM Multi-Agent Collaboration Through\n  Causality-Driven Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CausalPlan: Empowering Efficient LLM Multi-Agent Collaboration Through\n  Causality-Driven Planning"
                },
                "summary": "Large language model (LLM) agents-especially smaller, open-source\nmodels-often produce causally invalid or incoherent actions in collaborative\ntasks due to their reliance on surface-level correlations rather than grounded\ncausal reasoning. This limitation undermines their performance in terms of\ncoordination and planning in dynamic environments. We address this challenge\nwith CausalPlan, a two-phase framework that integrates explicit structural\ncausal reasoning into the LLM planning process. At the core of CausalPlan is\nthe Structural Causal Action (SCA) model, which learns a causal graph from\nagent trajectories to capture how prior actions and current environment states\ninfluence future decisions. This structure is then used to guide action\nselection by assigning causal scores to LLM-generated proposals, reweighting\nthem accordingly, or falling back to causally grounded alternatives when\nneeded. By embedding this causal knowledge directly into the decision loop,\nCausalPlan constrains planning to intervention-consistent behaviours without\nrequiring fine-tuning of the LLM itself. We evaluate CausalPlan on the\nOvercooked-AI benchmark across five multi-agent coordination tasks and four\nLLMs of varying sizes: Gemma-7B, Llama-8B, Qwen-14B, and Llama-70B.\nExperimental results show that CausalPlan consistently reduces invalid actions\nand improves collaboration in both AI-AI and human-AI settings, outperforming\nstrong reinforcement learning baselines. Our findings highlight the value of\ncausality-driven planning for deploying efficient, interpretable, and\ngeneralisable multi-agent LLM systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) agents-especially smaller, open-source\nmodels-often produce causally invalid or incoherent actions in collaborative\ntasks due to their reliance on surface-level correlations rather than grounded\ncausal reasoning. This limitation undermines their performance in terms of\ncoordination and planning in dynamic environments. We address this challenge\nwith CausalPlan, a two-phase framework that integrates explicit structural\ncausal reasoning into the LLM planning process. At the core of CausalPlan is\nthe Structural Causal Action (SCA) model, which learns a causal graph from\nagent trajectories to capture how prior actions and current environment states\ninfluence future decisions. This structure is then used to guide action\nselection by assigning causal scores to LLM-generated proposals, reweighting\nthem accordingly, or falling back to causally grounded alternatives when\nneeded. By embedding this causal knowledge directly into the decision loop,\nCausalPlan constrains planning to intervention-consistent behaviours without\nrequiring fine-tuning of the LLM itself. We evaluate CausalPlan on the\nOvercooked-AI benchmark across five multi-agent coordination tasks and four\nLLMs of varying sizes: Gemma-7B, Llama-8B, Qwen-14B, and Llama-70B.\nExperimental results show that CausalPlan consistently reduces invalid actions\nand improves collaboration in both AI-AI and human-AI settings, outperforming\nstrong reinforcement learning baselines. Our findings highlight the value of\ncausality-driven planning for deploying efficient, interpretable, and\ngeneralisable multi-agent LLM systems."
                },
                "authors": [
                    {
                        "name": "Minh Hoang Nguyen"
                    },
                    {
                        "name": "Van Dai Do"
                    },
                    {
                        "name": "Dung Nguyen"
                    },
                    {
                        "name": "Thin Nguyen"
                    },
                    {
                        "name": "Hung Le"
                    }
                ],
                "author_detail": {
                    "name": "Hung Le"
                },
                "author": "Hung Le",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13721v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13721v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13718v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13718v1",
                "updated": "2025-08-19T10:28:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    10,
                    28,
                    53,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T10:28:53Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    10,
                    28,
                    53,
                    1,
                    231,
                    0
                ],
                "title": "Generics and Default Reasoning in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generics and Default Reasoning in Large Language Models"
                },
                "summary": "This paper evaluates the capabilities of 28 large language models (LLMs) to\nreason with 20 defeasible reasoning patterns involving generic generalizations\n(e.g., 'Birds fly', 'Ravens are black') central to non-monotonic logic.\nGenerics are of special interest to linguists, philosophers, logicians, and\ncognitive scientists because of their complex exception-permitting behaviour\nand their centrality to default reasoning, cognition, and concept acquisition.\nWe find that while several frontier models handle many default reasoning\nproblems well, performance varies widely across models and prompting styles.\nFew-shot prompting modestly improves performance for some models, but\nchain-of-thought (CoT) prompting often leads to serious performance degradation\n(mean accuracy drop -11.14%, SD 15.74% in models performing above 75% accuracy\nin zero-shot condition, temperature 0). Most models either struggle to\ndistinguish between defeasible and deductive inference or misinterpret generics\nas universal statements. These findings underscore both the promise and limits\nof current LLMs for default reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper evaluates the capabilities of 28 large language models (LLMs) to\nreason with 20 defeasible reasoning patterns involving generic generalizations\n(e.g., 'Birds fly', 'Ravens are black') central to non-monotonic logic.\nGenerics are of special interest to linguists, philosophers, logicians, and\ncognitive scientists because of their complex exception-permitting behaviour\nand their centrality to default reasoning, cognition, and concept acquisition.\nWe find that while several frontier models handle many default reasoning\nproblems well, performance varies widely across models and prompting styles.\nFew-shot prompting modestly improves performance for some models, but\nchain-of-thought (CoT) prompting often leads to serious performance degradation\n(mean accuracy drop -11.14%, SD 15.74% in models performing above 75% accuracy\nin zero-shot condition, temperature 0). Most models either struggle to\ndistinguish between defeasible and deductive inference or misinterpret generics\nas universal statements. These findings underscore both the promise and limits\nof current LLMs for default reasoning."
                },
                "authors": [
                    {
                        "name": "James Ravi Kirkpatrick"
                    },
                    {
                        "name": "Rachel Katharine Sterken"
                    }
                ],
                "author_detail": {
                    "name": "Rachel Katharine Sterken"
                },
                "author": "Rachel Katharine Sterken",
                "arxiv_comment": "33 pages, 26 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13718v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13718v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13716v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13716v1",
                "updated": "2025-08-19T10:21:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    10,
                    21,
                    33,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T10:21:33Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    10,
                    21,
                    33,
                    1,
                    231,
                    0
                ],
                "title": "CaPGNN: Optimizing Parallel Graph Neural Network Training with Joint\n  Caching and Resource-Aware Graph Partitioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CaPGNN: Optimizing Parallel Graph Neural Network Training with Joint\n  Caching and Resource-Aware Graph Partitioning"
                },
                "summary": "Graph Neural Networks (GNNs) have shown remarkable capabilities in processing\ngraph-structured data prevalent in various real-world applications. However,\nthe scalability of full-batch GNN training becomes severely limited by high\ncommunication overhead and load imbalance in distributed environments. In this\npaper, we present CaPGNN, a novel framework for efficient parallel full-batch\nGNN training on single-server with multi-GPU, designed specifically to reduce\nredundant inter-GPU communication and balance computational workloads. We\npropose a joint adaptive caching algorithm that leverages both CPU and GPU\nmemory to significantly reduce the repetitive transmission of vertex features\nacross partitions. Additionally, we introduce a resource-aware graph\npartitioning algorithm that adjusts subgraph sizes dynamically according to the\nheterogeneous computational and communication capacities of GPUs. Extensive\nexperiments on large-scale benchmark datasets demonstrate that CaPGNN\neffectively reduces communication costs by up to 96% and accelerates GNN\ntraining by up to 12.7 times compared to state-of-the-art approaches. Our\nresults highlight the potential of adaptive caching and resource-aware\npartitioning to facilitate scalable, efficient, and practical deployment of\nfull-batch GNN training in distributed computing environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) have shown remarkable capabilities in processing\ngraph-structured data prevalent in various real-world applications. However,\nthe scalability of full-batch GNN training becomes severely limited by high\ncommunication overhead and load imbalance in distributed environments. In this\npaper, we present CaPGNN, a novel framework for efficient parallel full-batch\nGNN training on single-server with multi-GPU, designed specifically to reduce\nredundant inter-GPU communication and balance computational workloads. We\npropose a joint adaptive caching algorithm that leverages both CPU and GPU\nmemory to significantly reduce the repetitive transmission of vertex features\nacross partitions. Additionally, we introduce a resource-aware graph\npartitioning algorithm that adjusts subgraph sizes dynamically according to the\nheterogeneous computational and communication capacities of GPUs. Extensive\nexperiments on large-scale benchmark datasets demonstrate that CaPGNN\neffectively reduces communication costs by up to 96% and accelerates GNN\ntraining by up to 12.7 times compared to state-of-the-art approaches. Our\nresults highlight the potential of adaptive caching and resource-aware\npartitioning to facilitate scalable, efficient, and practical deployment of\nfull-batch GNN training in distributed computing environments."
                },
                "authors": [
                    {
                        "name": "Xianfeng Song"
                    },
                    {
                        "name": "Yi Zou"
                    },
                    {
                        "name": "Zheng Shi"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Shi"
                },
                "author": "Zheng Shi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13716v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13716v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11290v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11290v2",
                "updated": "2025-08-19T09:59:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    9,
                    59,
                    5,
                    1,
                    231,
                    0
                ],
                "published": "2024-06-17T07:52:42Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    7,
                    52,
                    42,
                    0,
                    169,
                    0
                ],
                "title": "Iterative Utility Judgment Framework via LLMs Inspired by Relevance in\n  Philosophy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Iterative Utility Judgment Framework via LLMs Inspired by Relevance in\n  Philosophy"
                },
                "summary": "Relevance and utility are two frequently used measures to evaluate the\neffectiveness of an information retrieval (IR) system. Relevance emphasizes the\naboutness of a result to a query, while utility refers to the result's\nusefulness or value to an information seeker. In Retrieval-Augmented Generation\n(RAG), high-utility results should be prioritized to feed to LLMs due to their\nlimited input bandwidth. Re-examining RAG's three core components -- relevance\nranking derived from retrieval models, utility judgments, and answer generation\n-- aligns with Schutz's philosophical system of relevances, which encompasses\nthree types of relevance representing different levels of human cognition that\nenhance each other. These three RAG components also reflect three cognitive\nlevels for LLMs in question-answering. Therefore, we propose an Iterative\nutiliTy judgmEnt fraMework (ITEM) to promote each step in RAG. We conducted\nextensive experiments on retrieval (TREC DL, WebAP), utility judgment task\n(GTI-NQ), and factoid question-answering (NQ) datasets. Experimental results\ndemonstrate significant improvements of ITEM in utility judgments, ranking, and\nanswer generation upon representative baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Relevance and utility are two frequently used measures to evaluate the\neffectiveness of an information retrieval (IR) system. Relevance emphasizes the\naboutness of a result to a query, while utility refers to the result's\nusefulness or value to an information seeker. In Retrieval-Augmented Generation\n(RAG), high-utility results should be prioritized to feed to LLMs due to their\nlimited input bandwidth. Re-examining RAG's three core components -- relevance\nranking derived from retrieval models, utility judgments, and answer generation\n-- aligns with Schutz's philosophical system of relevances, which encompasses\nthree types of relevance representing different levels of human cognition that\nenhance each other. These three RAG components also reflect three cognitive\nlevels for LLMs in question-answering. Therefore, we propose an Iterative\nutiliTy judgmEnt fraMework (ITEM) to promote each step in RAG. We conducted\nextensive experiments on retrieval (TREC DL, WebAP), utility judgment task\n(GTI-NQ), and factoid question-answering (NQ) datasets. Experimental results\ndemonstrate significant improvements of ITEM in utility judgments, ranking, and\nanswer generation upon representative baselines."
                },
                "authors": [
                    {
                        "name": "Hengran Zhang"
                    },
                    {
                        "name": "Keping Bi"
                    },
                    {
                        "name": "Jiafeng Guo"
                    },
                    {
                        "name": "Xueqi Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xueqi Cheng"
                },
                "author": "Xueqi Cheng",
                "arxiv_comment": "22 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11290v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11290v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01457v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01457v2",
                "updated": "2025-08-19T09:55:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    9,
                    55,
                    9,
                    1,
                    231,
                    0
                ],
                "published": "2025-07-02T08:15:33Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    8,
                    15,
                    33,
                    2,
                    183,
                    0
                ],
                "title": "Tensor Program Optimization for the RISC-V Vector Extension Using\n  Probabilistic Programs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tensor Program Optimization for the RISC-V Vector Extension Using\n  Probabilistic Programs"
                },
                "summary": "RISC-V provides a flexible and scalable platform for applications ranging\nfrom embedded devices to high-performance computing clusters. Particularly, its\nRISC-V Vector Extension (RVV) becomes of interest for the acceleration of AI\nworkloads. But writing software that efficiently utilizes the vector units of\nRISC-V CPUs without expert knowledge requires the programmer to rely on the\nautovectorization features of compilers or hand-crafted libraries like\nmuRISCV-NN. Smarter approaches, like autotuning frameworks, have been missing\nthe integration with the RISC-V RVV extension, thus heavily limiting the\nefficient deployment of complex AI workloads. In this paper, we present a\nworkflow based on the TVM compiler to efficiently map AI workloads onto RISC-V\nvector units. Instead of relying on hand-crafted libraries, we integrated the\nRVV extension into TVM's MetaSchedule framework, a probabilistic program\nframework for tensor operation tuning. We implemented different RISC-V SoCs on\nan FPGA and tuned a wide range of AI workloads on them. We found that our\nproposal shows a mean improvement of 46% in execution latency when compared\nagainst the autovectorization feature of GCC, and 29% against muRISCV-NN.\nMoreover, the binary resulting from our proposal has a smaller code memory\nfootprint, making it more suitable for embedded devices. Finally, we also\nevaluated our solution on a commercially available RISC-V SoC implementing the\nRVV 1.0 Vector Extension and found our solution is able to find mappings that\nare 35% faster on average than the ones proposed by LLVM. We open-sourced our\nproposal for the community to expand it to target other RISC-V extensions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RISC-V provides a flexible and scalable platform for applications ranging\nfrom embedded devices to high-performance computing clusters. Particularly, its\nRISC-V Vector Extension (RVV) becomes of interest for the acceleration of AI\nworkloads. But writing software that efficiently utilizes the vector units of\nRISC-V CPUs without expert knowledge requires the programmer to rely on the\nautovectorization features of compilers or hand-crafted libraries like\nmuRISCV-NN. Smarter approaches, like autotuning frameworks, have been missing\nthe integration with the RISC-V RVV extension, thus heavily limiting the\nefficient deployment of complex AI workloads. In this paper, we present a\nworkflow based on the TVM compiler to efficiently map AI workloads onto RISC-V\nvector units. Instead of relying on hand-crafted libraries, we integrated the\nRVV extension into TVM's MetaSchedule framework, a probabilistic program\nframework for tensor operation tuning. We implemented different RISC-V SoCs on\nan FPGA and tuned a wide range of AI workloads on them. We found that our\nproposal shows a mean improvement of 46% in execution latency when compared\nagainst the autovectorization feature of GCC, and 29% against muRISCV-NN.\nMoreover, the binary resulting from our proposal has a smaller code memory\nfootprint, making it more suitable for embedded devices. Finally, we also\nevaluated our solution on a commercially available RISC-V SoC implementing the\nRVV 1.0 Vector Extension and found our solution is able to find mappings that\nare 35% faster on average than the ones proposed by LLVM. We open-sourced our\nproposal for the community to expand it to target other RISC-V extensions."
                },
                "authors": [
                    {
                        "name": "Federico Nicolas Peccia"
                    },
                    {
                        "name": "Frederik Haxel"
                    },
                    {
                        "name": "Oliver Bringmann"
                    }
                ],
                "author_detail": {
                    "name": "Oliver Bringmann"
                },
                "author": "Oliver Bringmann",
                "arxiv_comment": "9 pages, 10 figures, 2 algorithms",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01457v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01457v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13690v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13690v1",
                "updated": "2025-08-19T09:46:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    9,
                    46,
                    1,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T09:46:01Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    9,
                    46,
                    1,
                    1,
                    231,
                    0
                ],
                "title": "Know Me by My Pulse: Toward Practical Continuous Authentication on\n  Wearable Devices via Wrist-Worn PPG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Know Me by My Pulse: Toward Practical Continuous Authentication on\n  Wearable Devices via Wrist-Worn PPG"
                },
                "summary": "Biometric authentication using physiological signals offers a promising path\ntoward secure and user-friendly access control in wearable devices. While\nelectrocardiogram (ECG) signals have shown high discriminability, their\nintrusive sensing requirements and discontinuous acquisition limit\npracticality. Photoplethysmography (PPG), on the other hand, enables\ncontinuous, non-intrusive authentication with seamless integration into\nwrist-worn wearable devices. However, most prior work relies on high-frequency\nPPG (e.g., 75 - 500 Hz) and complex deep models, which incur significant energy\nand computational overhead, impeding deployment in power-constrained real-world\nsystems. In this paper, we present the first real-world implementation and\nevaluation of a continuous authentication system on a smartwatch, We-Be Band,\nusing low-frequency (25 Hz) multi-channel PPG signals. Our method employs a\nBi-LSTM with attention mechanism to extract identity-specific features from\nshort (4 s) windows of 4-channel PPG. Through extensive evaluations on both\npublic datasets (PTTPPG) and our We-Be Dataset (26 subjects), we demonstrate\nstrong classification performance with an average test accuracy of 88.11%,\nmacro F1-score of 0.88, False Acceptance Rate (FAR) of 0.48%, False Rejection\nRate (FRR) of 11.77%, and Equal Error Rate (EER) of 2.76%. Our 25 Hz system\nreduces sensor power consumption by 53% compared to 512 Hz and 19% compared to\n128 Hz setups without compromising performance. We find that sampling at 25 Hz\npreserves authentication accuracy, whereas performance drops sharply at 20 Hz\nwhile offering only trivial additional power savings, underscoring 25 Hz as the\npractical lower bound. Additionally, we find that models trained exclusively on\nresting data fail under motion, while activity-diverse training improves\nrobustness across physiological states.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Biometric authentication using physiological signals offers a promising path\ntoward secure and user-friendly access control in wearable devices. While\nelectrocardiogram (ECG) signals have shown high discriminability, their\nintrusive sensing requirements and discontinuous acquisition limit\npracticality. Photoplethysmography (PPG), on the other hand, enables\ncontinuous, non-intrusive authentication with seamless integration into\nwrist-worn wearable devices. However, most prior work relies on high-frequency\nPPG (e.g., 75 - 500 Hz) and complex deep models, which incur significant energy\nand computational overhead, impeding deployment in power-constrained real-world\nsystems. In this paper, we present the first real-world implementation and\nevaluation of a continuous authentication system on a smartwatch, We-Be Band,\nusing low-frequency (25 Hz) multi-channel PPG signals. Our method employs a\nBi-LSTM with attention mechanism to extract identity-specific features from\nshort (4 s) windows of 4-channel PPG. Through extensive evaluations on both\npublic datasets (PTTPPG) and our We-Be Dataset (26 subjects), we demonstrate\nstrong classification performance with an average test accuracy of 88.11%,\nmacro F1-score of 0.88, False Acceptance Rate (FAR) of 0.48%, False Rejection\nRate (FRR) of 11.77%, and Equal Error Rate (EER) of 2.76%. Our 25 Hz system\nreduces sensor power consumption by 53% compared to 512 Hz and 19% compared to\n128 Hz setups without compromising performance. We find that sampling at 25 Hz\npreserves authentication accuracy, whereas performance drops sharply at 20 Hz\nwhile offering only trivial additional power savings, underscoring 25 Hz as the\npractical lower bound. Additionally, we find that models trained exclusively on\nresting data fail under motion, while activity-diverse training improves\nrobustness across physiological states."
                },
                "authors": [
                    {
                        "name": "Wei Shao"
                    },
                    {
                        "name": "Zequan Liang"
                    },
                    {
                        "name": "Ruoyu Zhang"
                    },
                    {
                        "name": "Ruijie Fang"
                    },
                    {
                        "name": "Ning Miao"
                    },
                    {
                        "name": "Ehsan Kourkchi"
                    },
                    {
                        "name": "Setareh Rafatirad"
                    },
                    {
                        "name": "Houman Homayoun"
                    },
                    {
                        "name": "Chongzhou Fang"
                    }
                ],
                "author_detail": {
                    "name": "Chongzhou Fang"
                },
                "author": "Chongzhou Fang",
                "arxiv_comment": "To be published in Network and Distributed System Security (NDSS)\n  Symposium 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13690v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13690v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04715v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04715v7",
                "updated": "2025-08-19T09:45:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    9,
                    45,
                    25,
                    1,
                    231,
                    0
                ],
                "published": "2025-03-06T18:58:29Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    18,
                    58,
                    29,
                    3,
                    65,
                    0
                ],
                "title": "Predictable Scale: Part I, Step Law -- Optimal Hyperparameter Scaling\n  Law in Large Language Model Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predictable Scale: Part I, Step Law -- Optimal Hyperparameter Scaling\n  Law in Large Language Model Pretraining"
                },
                "summary": "The impressive capabilities of Large Language Models (LLMs) across diverse\ntasks are now well established, yet their effective deployment necessitates\ncareful hyperparameter optimization. Although existing methods have explored\nthe influence of hyperparameters on model performance, a principled and\ngeneralizable framework across model architectures and data recipes remains\nabsent. In this study, we conduct an unprecedented empirical investigation\ntraining over 3,700 LLMs from scratch across 100 trillion tokens, consuming\nnearly one million NVIDIA H800 GPU hours to establish a universal Scaling Law\nfor hyperparameter optimization in LLM Pre-training, called Step Law. We\nempirically observe that, under fixed model size ($N$) and dataset size ($D$),\nthe hyperparameter landscape exhibits convexity with a broad optimum,\nsubstantially reducing the complexity of hyperparameter search. Building on\nthis insight, we formally define and empirically validate the Step Law: The\noptimal learning rate follows a power-law relationship with $N$ and $D$, while\nthe optimal batch size is primarily influenced by $D$ and remains largely\ninvariant to $N$.Notably, our estimated optima deviate from the global best\nperformance found via exhaustive search by merely 0.094\\% on the test set. To\nour best known, Step Law is the first that unifies different model shapes and\nstructures, such as Mixture-of-Experts models and dense transformers, as well\nas establishes optimal hyperparameter scaling laws across diverse data recipes.\nWe contribute a universal, plug-and-play optimal hyperparameter tool for the\ncommunity, which is expected to advance efficient LLM training at scale. All\nexperimental code, data and checkpoints are publicly available at\nhttps://github.com/step-law/steplaw",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The impressive capabilities of Large Language Models (LLMs) across diverse\ntasks are now well established, yet their effective deployment necessitates\ncareful hyperparameter optimization. Although existing methods have explored\nthe influence of hyperparameters on model performance, a principled and\ngeneralizable framework across model architectures and data recipes remains\nabsent. In this study, we conduct an unprecedented empirical investigation\ntraining over 3,700 LLMs from scratch across 100 trillion tokens, consuming\nnearly one million NVIDIA H800 GPU hours to establish a universal Scaling Law\nfor hyperparameter optimization in LLM Pre-training, called Step Law. We\nempirically observe that, under fixed model size ($N$) and dataset size ($D$),\nthe hyperparameter landscape exhibits convexity with a broad optimum,\nsubstantially reducing the complexity of hyperparameter search. Building on\nthis insight, we formally define and empirically validate the Step Law: The\noptimal learning rate follows a power-law relationship with $N$ and $D$, while\nthe optimal batch size is primarily influenced by $D$ and remains largely\ninvariant to $N$.Notably, our estimated optima deviate from the global best\nperformance found via exhaustive search by merely 0.094\\% on the test set. To\nour best known, Step Law is the first that unifies different model shapes and\nstructures, such as Mixture-of-Experts models and dense transformers, as well\nas establishes optimal hyperparameter scaling laws across diverse data recipes.\nWe contribute a universal, plug-and-play optimal hyperparameter tool for the\ncommunity, which is expected to advance efficient LLM training at scale. All\nexperimental code, data and checkpoints are publicly available at\nhttps://github.com/step-law/steplaw"
                },
                "authors": [
                    {
                        "name": "Houyi Li"
                    },
                    {
                        "name": "Wenzhen Zheng"
                    },
                    {
                        "name": "Qiufeng Wang"
                    },
                    {
                        "name": "Hanshan Zhang"
                    },
                    {
                        "name": "Zili Wang"
                    },
                    {
                        "name": "Shijie Xuyang"
                    },
                    {
                        "name": "Yuantao Fan"
                    },
                    {
                        "name": "Zhenyu Ding"
                    },
                    {
                        "name": "Haoying Wang"
                    },
                    {
                        "name": "Ning Ding"
                    },
                    {
                        "name": "Shuigeng Zhou"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Daxin Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Daxin Jiang"
                },
                "author": "Daxin Jiang",
                "arxiv_comment": "22 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04715v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04715v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.2; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05844v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05844v3",
                "updated": "2025-08-19T09:35:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    9,
                    35,
                    53,
                    1,
                    231,
                    0
                ],
                "published": "2024-11-06T15:32:28Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    15,
                    32,
                    28,
                    2,
                    311,
                    0
                ],
                "title": "LEGO-GraphRAG: Modularizing Graph-based Retrieval-Augmented Generation\n  for Design Space Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LEGO-GraphRAG: Modularizing Graph-based Retrieval-Augmented Generation\n  for Design Space Exploration"
                },
                "summary": "GraphRAG integrates (knowledge) graphs with large language models (LLMs) to\nimprove reasoning accuracy and contextual relevance. Despite its promising\napplications and strong relevance to multiple research communities, such as\ndatabases and natural language processing, GraphRAG currently lacks modular\nworkflow analysis, systematic solution frameworks, and insightful empirical\nstudies. To bridge these gaps, we propose LEGO-GraphRAG, a modular framework\nthat enables: 1) fine-grained decomposition of the GraphRAG workflow, 2)\nsystematic classification of existing techniques and implemented GraphRAG\ninstances, and 3) creation of new GraphRAG instances. Our framework facilitates\ncomprehensive empirical studies of GraphRAG on large-scale real-world graphs\nand diverse query sets, revealing insights into balancing reasoning quality,\nruntime efficiency, and token or GPU cost, that are essential for building\nadvanced GraphRAG systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphRAG integrates (knowledge) graphs with large language models (LLMs) to\nimprove reasoning accuracy and contextual relevance. Despite its promising\napplications and strong relevance to multiple research communities, such as\ndatabases and natural language processing, GraphRAG currently lacks modular\nworkflow analysis, systematic solution frameworks, and insightful empirical\nstudies. To bridge these gaps, we propose LEGO-GraphRAG, a modular framework\nthat enables: 1) fine-grained decomposition of the GraphRAG workflow, 2)\nsystematic classification of existing techniques and implemented GraphRAG\ninstances, and 3) creation of new GraphRAG instances. Our framework facilitates\ncomprehensive empirical studies of GraphRAG on large-scale real-world graphs\nand diverse query sets, revealing insights into balancing reasoning quality,\nruntime efficiency, and token or GPU cost, that are essential for building\nadvanced GraphRAG systems."
                },
                "authors": [
                    {
                        "name": "Yukun Cao"
                    },
                    {
                        "name": "Zengyi Gao"
                    },
                    {
                        "name": "Zhiyang Li"
                    },
                    {
                        "name": "Xike Xie"
                    },
                    {
                        "name": "S. Kevin Zhou"
                    },
                    {
                        "name": "Jianliang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jianliang Xu"
                },
                "author": "Jianliang Xu",
                "arxiv_doi": "10.14778/3748191.3748194",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.14778/3748191.3748194",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.05844v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05844v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "VLDB'2025 [Experiment, Analysis & Benchmark]",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05216v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05216v3",
                "updated": "2025-08-19T09:35:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    9,
                    35,
                    52,
                    1,
                    231,
                    0
                ],
                "published": "2025-04-07T16:03:59Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    16,
                    3,
                    59,
                    0,
                    97,
                    0
                ],
                "title": "Unleashing the Power of LLMs in Dense Retrieval with Query Likelihood\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing the Power of LLMs in Dense Retrieval with Query Likelihood\n  Modeling"
                },
                "summary": "Dense retrieval is a crucial task in Information Retrieval (IR), serving as\nthe basis for downstream tasks such as re-ranking and augmenting generation.\nRecently, large language models (LLMs) have demonstrated impressive semantic\nunderstanding capabilities, making them attractive to researchers focusing on\ndense retrieval. While LLMs, as decoder-style generative models, excel in\nlanguage generation, they often fall short in modeling global information due\nto a lack of attention to subsequent tokens. Drawing inspiration from the\nclassical word-based language modeling approach for IR, specifically the query\nlikelihood (QL) model, we aim to leverage the generative strengths of LLMs\nthrough QL maximization. Rather than employing QL estimation for document\nranking, we propose an auxiliary task of QL maximization to enhance the\nbackbone for subsequent contrastive learning of the retriever. We introduce our\nmodel, LLM-QL, which incorporates two key components: Attention Block (AB) and\nDocument Corruption (DC). AB blocks the attention of predictive tokens to the\ndocument tokens before the document's ending token, while DC corrupts a\ndocument by masking a portion of its tokens during prediction. Evaluations on\nthe in-domain (MS MARCO) and out-of-domain dataset (BEIR) indicate LLM-QL's\nsuperiority over other LLM-based retrievers. Furthermore, comprehensive\nanalyses also validate the efficacy of LLM-QL and its components.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dense retrieval is a crucial task in Information Retrieval (IR), serving as\nthe basis for downstream tasks such as re-ranking and augmenting generation.\nRecently, large language models (LLMs) have demonstrated impressive semantic\nunderstanding capabilities, making them attractive to researchers focusing on\ndense retrieval. While LLMs, as decoder-style generative models, excel in\nlanguage generation, they often fall short in modeling global information due\nto a lack of attention to subsequent tokens. Drawing inspiration from the\nclassical word-based language modeling approach for IR, specifically the query\nlikelihood (QL) model, we aim to leverage the generative strengths of LLMs\nthrough QL maximization. Rather than employing QL estimation for document\nranking, we propose an auxiliary task of QL maximization to enhance the\nbackbone for subsequent contrastive learning of the retriever. We introduce our\nmodel, LLM-QL, which incorporates two key components: Attention Block (AB) and\nDocument Corruption (DC). AB blocks the attention of predictive tokens to the\ndocument tokens before the document's ending token, while DC corrupts a\ndocument by masking a portion of its tokens during prediction. Evaluations on\nthe in-domain (MS MARCO) and out-of-domain dataset (BEIR) indicate LLM-QL's\nsuperiority over other LLM-based retrievers. Furthermore, comprehensive\nanalyses also validate the efficacy of LLM-QL and its components."
                },
                "authors": [
                    {
                        "name": "Hengran Zhang"
                    },
                    {
                        "name": "Keping Bi"
                    },
                    {
                        "name": "Jiafeng Guo"
                    },
                    {
                        "name": "Xiaojie Sun"
                    },
                    {
                        "name": "Shihao Liu"
                    },
                    {
                        "name": "Daiting Shi"
                    },
                    {
                        "name": "Dawei Yin"
                    },
                    {
                        "name": "Xueqi Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xueqi Cheng"
                },
                "author": "Xueqi Cheng",
                "arxiv_comment": "12 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05216v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05216v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13092v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13092v2",
                "updated": "2025-08-19T09:32:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    9,
                    32,
                    33,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-18T17:05:18Z",
                "published_parsed": [
                    2025,
                    8,
                    18,
                    17,
                    5,
                    18,
                    0,
                    230,
                    0
                ],
                "title": "VerilogLAVD: LLM-Aided Rule Generation for Vulnerability Detection in\n  Verilog",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VerilogLAVD: LLM-Aided Rule Generation for Vulnerability Detection in\n  Verilog"
                },
                "summary": "Timely detection of hardware vulnerabilities during the early design stage is\ncritical for reducing remediation costs. Existing early detection techniques\noften require specialized security expertise, limiting their usability. Recent\nefforts have explored the use of large language models (LLMs) for Verilog\nvulnerability detection. However, LLMs struggle to capture the structure in\nVerilog code, resulting in inconsistent detection results. To this end, we\npropose VerilogLAVD, the first LLM-aided graph traversal rule generation\napproach for Verilog vulnerability detection. Our approach introduces the\nVerilog Property Graph (VeriPG), a unified representation of Verilog code. It\ncombines syntactic features extracted from the abstract syntax tree (AST) with\nsemantic information derived from control flow and data dependency graphs. We\nleverage LLMs to generate VeriPG-based detection rules from Common Weakness\nEnumeration (CWE) descriptions. These rules guide the rule executor that\ntraversal VeriPG for potential vulnerabilities. To evaluate VerilogLAVD, we\nbuild a dataset collected from open-source repositories and synthesized data.\nIn our empirical evaluation on 77 Verilog designs encompassing 12 CWE types,\nVerilogLAVD achieves an F1-score of 0.54. Compared to the LLM-only and LLM with\nexternal knowledge baselines, VerilogLAVD improves F1-score by 0.31 and 0.27,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Timely detection of hardware vulnerabilities during the early design stage is\ncritical for reducing remediation costs. Existing early detection techniques\noften require specialized security expertise, limiting their usability. Recent\nefforts have explored the use of large language models (LLMs) for Verilog\nvulnerability detection. However, LLMs struggle to capture the structure in\nVerilog code, resulting in inconsistent detection results. To this end, we\npropose VerilogLAVD, the first LLM-aided graph traversal rule generation\napproach for Verilog vulnerability detection. Our approach introduces the\nVerilog Property Graph (VeriPG), a unified representation of Verilog code. It\ncombines syntactic features extracted from the abstract syntax tree (AST) with\nsemantic information derived from control flow and data dependency graphs. We\nleverage LLMs to generate VeriPG-based detection rules from Common Weakness\nEnumeration (CWE) descriptions. These rules guide the rule executor that\ntraversal VeriPG for potential vulnerabilities. To evaluate VerilogLAVD, we\nbuild a dataset collected from open-source repositories and synthesized data.\nIn our empirical evaluation on 77 Verilog designs encompassing 12 CWE types,\nVerilogLAVD achieves an F1-score of 0.54. Compared to the LLM-only and LLM with\nexternal knowledge baselines, VerilogLAVD improves F1-score by 0.31 and 0.27,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Xiang Long"
                    },
                    {
                        "name": "Yingjie Xia"
                    },
                    {
                        "name": "Xiyuan Chen"
                    },
                    {
                        "name": "Li Kuang"
                    }
                ],
                "author_detail": {
                    "name": "Li Kuang"
                },
                "author": "Li Kuang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13092v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13092v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13678v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13678v1",
                "updated": "2025-08-19T09:27:46Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    9,
                    27,
                    46,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T09:27:46Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    9,
                    27,
                    46,
                    1,
                    231,
                    0
                ],
                "title": "Neuro-Symbolic Artificial Intelligence: Towards Improving the Reasoning\n  Abilities of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neuro-Symbolic Artificial Intelligence: Towards Improving the Reasoning\n  Abilities of Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have shown promising results across various\ntasks, yet their reasoning capabilities remain a fundamental challenge.\nDeveloping AI systems with strong reasoning capabilities is regarded as a\ncrucial milestone in the pursuit of Artificial General Intelligence (AGI) and\nhas garnered considerable attention from both academia and industry. Various\ntechniques have been explored to enhance the reasoning capabilities of LLMs,\nwith neuro-symbolic approaches being a particularly promising way. This paper\ncomprehensively reviews recent developments in neuro-symbolic approaches for\nenhancing LLM reasoning. We first present a formalization of reasoning tasks\nand give a brief introduction to the neurosymbolic learning paradigm. Then, we\ndiscuss neuro-symbolic methods for improving the reasoning capabilities of LLMs\nfrom three perspectives: Symbolic->LLM, LLM->Symbolic, and LLM+Symbolic.\nFinally, we discuss several key challenges and promising future directions. We\nhave also released a GitHub repository including papers and resources related\nto this survey: https://github.com/LAMDASZ-ML/Awesome-LLM-Reasoning-with-NeSy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown promising results across various\ntasks, yet their reasoning capabilities remain a fundamental challenge.\nDeveloping AI systems with strong reasoning capabilities is regarded as a\ncrucial milestone in the pursuit of Artificial General Intelligence (AGI) and\nhas garnered considerable attention from both academia and industry. Various\ntechniques have been explored to enhance the reasoning capabilities of LLMs,\nwith neuro-symbolic approaches being a particularly promising way. This paper\ncomprehensively reviews recent developments in neuro-symbolic approaches for\nenhancing LLM reasoning. We first present a formalization of reasoning tasks\nand give a brief introduction to the neurosymbolic learning paradigm. Then, we\ndiscuss neuro-symbolic methods for improving the reasoning capabilities of LLMs\nfrom three perspectives: Symbolic->LLM, LLM->Symbolic, and LLM+Symbolic.\nFinally, we discuss several key challenges and promising future directions. We\nhave also released a GitHub repository including papers and resources related\nto this survey: https://github.com/LAMDASZ-ML/Awesome-LLM-Reasoning-with-NeSy."
                },
                "authors": [
                    {
                        "name": "Xiao-Wen Yang"
                    },
                    {
                        "name": "Jie-Jing Shao"
                    },
                    {
                        "name": "Lan-Zhe Guo"
                    },
                    {
                        "name": "Bo-Wen Zhang"
                    },
                    {
                        "name": "Zhi Zhou"
                    },
                    {
                        "name": "Lin-Han Jia"
                    },
                    {
                        "name": "Wang-Zhou Dai"
                    },
                    {
                        "name": "Yu-Feng Li"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Feng Li"
                },
                "author": "Yu-Feng Li",
                "arxiv_comment": "9 pages, 3 figures, IJCAI 2025 Survey Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13678v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13678v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.11036v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.11036v3",
                "updated": "2025-08-19T09:15:11Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    9,
                    15,
                    11,
                    1,
                    231,
                    0
                ],
                "published": "2023-10-17T07:03:41Z",
                "published_parsed": [
                    2023,
                    10,
                    17,
                    7,
                    3,
                    41,
                    1,
                    290,
                    0
                ],
                "title": "Radio Map Estimation: Empirical Validation and Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Radio Map Estimation: Empirical Validation and Analysis"
                },
                "summary": "Radio maps provide metrics such as the received signal strength at every\nlocation in a geographical region of interest. Extensive research has been\ncarried out in this context, but it relies almost exclusively on synthetic-data\nexperiments. Thus, the practical aspects of the radio map estimation (RME)\nproblem as well as the performance of existing estimators in the real world\nremain unknown. To fill this gap end, this paper puts forth the first\ncomprehensive, rigorous, and reproducible study of RME with real data. The main\ncontributions include (C1) an assessment of the viability of RME based on the\nestimation error that can be achieved, (C2) the analysis of the main phenomena\nand trade-offs involved in RME, including the experimental verification of\ntheoretical findings in the literature, and (C3) a thorough evaluation of a\nwide range of estimators on realworld data. Remarkably, this reveals that the\nperformance gain of existing deep estimators in their pure form may not\ncompensate for their complexity. A simple enhancement (C4) is proposed to\nalleviate this issue. The vast amount of data collected for this study is\npublished along with the developed simulator to enable research on new schemes,\nhopefully bringing RME one step closer to practical deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Radio maps provide metrics such as the received signal strength at every\nlocation in a geographical region of interest. Extensive research has been\ncarried out in this context, but it relies almost exclusively on synthetic-data\nexperiments. Thus, the practical aspects of the radio map estimation (RME)\nproblem as well as the performance of existing estimators in the real world\nremain unknown. To fill this gap end, this paper puts forth the first\ncomprehensive, rigorous, and reproducible study of RME with real data. The main\ncontributions include (C1) an assessment of the viability of RME based on the\nestimation error that can be achieved, (C2) the analysis of the main phenomena\nand trade-offs involved in RME, including the experimental verification of\ntheoretical findings in the literature, and (C3) a thorough evaluation of a\nwide range of estimators on realworld data. Remarkably, this reveals that the\nperformance gain of existing deep estimators in their pure form may not\ncompensate for their complexity. A simple enhancement (C4) is proposed to\nalleviate this issue. The vast amount of data collected for this study is\npublished along with the developed simulator to enable research on new schemes,\nhopefully bringing RME one step closer to practical deployment."
                },
                "authors": [
                    {
                        "name": "Raju Shrestha"
                    },
                    {
                        "name": "Tien Ngoc Ha"
                    },
                    {
                        "name": "Pham Q. Viet"
                    },
                    {
                        "name": "Daniel Romero"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Romero"
                },
                "author": "Daniel Romero",
                "arxiv_comment": "13 pages, Journal version, submitted to the IEEE Transactions on\n  Wireless Communications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.11036v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.11036v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13666v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13666v1",
                "updated": "2025-08-19T09:13:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    9,
                    13,
                    48,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T09:13:48Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    9,
                    13,
                    48,
                    1,
                    231,
                    0
                ],
                "title": "The Hidden Cost of Readability: How Code Formatting Silently Consumes\n  Your LLM Budget",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Hidden Cost of Readability: How Code Formatting Silently Consumes\n  Your LLM Budget"
                },
                "summary": "Source code is usually formatted with elements like indentation and newlines\nto improve readability for human developers. However, these visual aids do not\nseem to be beneficial for large language models (LLMs) in the same way since\nthe code is processed as a linear sequence of tokens. Furthermore, these\nadditional tokens can lead to increased computational costs and longer response\ntimes for LLMs. If such formatting elements are non-essential to LLMs, we can\nreduce such costs by removing them from the code. To figure out the role played\nby formatting elements, we conduct a comprehensive empirical study to evaluate\nthe impact of code formatting on LLM performance and efficiency. Through\nlarge-scale experiments on Fill-in-the-Middle Code Completion tasks across four\nprogramming languages (Java, Python, C++, C\\#) and ten LLMs-including both\ncommercial and open-source models-we systematically analyze token count and\nperformance when formatting elements are removed. Key findings indicate that\nLLMs can maintain performance across formatted code and unformatted code,\nachieving an average input token reduction of 24.5\\% with negligible output\ntoken reductions. This makes code format removal a practical optimization\nstrategy for improving LLM efficiency. Further exploration reveals that both\nprompting and fine-tuning LLMs can lead to significant reductions (up to\n36.1\\%) in output code length without compromising correctness. To facilitate\npractical applications, we develop a bidirectional code transformation tool for\nformat processing, which can be seamlessly integrated into existing LLM\ninference workflows, ensuring both human readability and LLM efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Source code is usually formatted with elements like indentation and newlines\nto improve readability for human developers. However, these visual aids do not\nseem to be beneficial for large language models (LLMs) in the same way since\nthe code is processed as a linear sequence of tokens. Furthermore, these\nadditional tokens can lead to increased computational costs and longer response\ntimes for LLMs. If such formatting elements are non-essential to LLMs, we can\nreduce such costs by removing them from the code. To figure out the role played\nby formatting elements, we conduct a comprehensive empirical study to evaluate\nthe impact of code formatting on LLM performance and efficiency. Through\nlarge-scale experiments on Fill-in-the-Middle Code Completion tasks across four\nprogramming languages (Java, Python, C++, C\\#) and ten LLMs-including both\ncommercial and open-source models-we systematically analyze token count and\nperformance when formatting elements are removed. Key findings indicate that\nLLMs can maintain performance across formatted code and unformatted code,\nachieving an average input token reduction of 24.5\\% with negligible output\ntoken reductions. This makes code format removal a practical optimization\nstrategy for improving LLM efficiency. Further exploration reveals that both\nprompting and fine-tuning LLMs can lead to significant reductions (up to\n36.1\\%) in output code length without compromising correctness. To facilitate\npractical applications, we develop a bidirectional code transformation tool for\nformat processing, which can be seamlessly integrated into existing LLM\ninference workflows, ensuring both human readability and LLM efficiency."
                },
                "authors": [
                    {
                        "name": "Dangfeng Pan"
                    },
                    {
                        "name": "Zhensu Sun"
                    },
                    {
                        "name": "Cenyuan Zhang"
                    },
                    {
                        "name": "David Lo"
                    },
                    {
                        "name": "Xiaoning Du"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoning Du"
                },
                "author": "Xiaoning Du",
                "arxiv_comment": "Accepted by ICSE'26 (First Cycle)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13666v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13666v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.12992v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.12992v2",
                "updated": "2025-08-19T09:08:17Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    9,
                    8,
                    17,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-18T15:12:27Z",
                "published_parsed": [
                    2025,
                    8,
                    18,
                    15,
                    12,
                    27,
                    0,
                    230,
                    0
                ],
                "title": "MAGNeT: Multimodal Adaptive Gaussian Networks for Intent Inference in\n  Moving Target Selection across Complex Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAGNeT: Multimodal Adaptive Gaussian Networks for Intent Inference in\n  Moving Target Selection across Complex Scenarios"
                },
                "summary": "Moving target selection in multimedia interactive systems faces unprecedented\nchallenges as users increasingly interact across diverse and dynamic\ncontexts-from live streaming in moving vehicles to VR gaming in varying\nenvironments. Existing approaches rely on probabilistic models that relate\nendpoint distribution to target properties such as size and speed. However,\nthese methods require substantial training data for each new context and lack\ntransferability across scenarios, limiting their practical deployment in\ndiverse multimedia environments where rich multimodal contextual information is\nreadily available. This paper introduces MAGNeT (Multimodal Adaptive Gaussian\nNetworks), which addresses these problems by combining classical statistical\nmodeling with a context-aware multimodal method. MAGNeT dynamically fuses\npre-fitted Ternary-Gaussian models from various scenarios based on real-time\ncontextual cues, enabling effective adaptation with minimal training data while\npreserving model interpretability. We conduct experiments on self-constructed\n2D and 3D moving target selection datasets under in-vehicle vibration\nconditions. Extensive experiments demonstrate that MAGNeT achieves lower error\nrates with few-shot samples by applying context-aware fusion of Gaussian\nexperts from multi-factor conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Moving target selection in multimedia interactive systems faces unprecedented\nchallenges as users increasingly interact across diverse and dynamic\ncontexts-from live streaming in moving vehicles to VR gaming in varying\nenvironments. Existing approaches rely on probabilistic models that relate\nendpoint distribution to target properties such as size and speed. However,\nthese methods require substantial training data for each new context and lack\ntransferability across scenarios, limiting their practical deployment in\ndiverse multimedia environments where rich multimodal contextual information is\nreadily available. This paper introduces MAGNeT (Multimodal Adaptive Gaussian\nNetworks), which addresses these problems by combining classical statistical\nmodeling with a context-aware multimodal method. MAGNeT dynamically fuses\npre-fitted Ternary-Gaussian models from various scenarios based on real-time\ncontextual cues, enabling effective adaptation with minimal training data while\npreserving model interpretability. We conduct experiments on self-constructed\n2D and 3D moving target selection datasets under in-vehicle vibration\nconditions. Extensive experiments demonstrate that MAGNeT achieves lower error\nrates with few-shot samples by applying context-aware fusion of Gaussian\nexperts from multi-factor conditions."
                },
                "authors": [
                    {
                        "name": "Xiangxian Li"
                    },
                    {
                        "name": "Yawen Zheng"
                    },
                    {
                        "name": "Baiqiao Zhang"
                    },
                    {
                        "name": "Yijia Ma"
                    },
                    {
                        "name": "Xianhui Cao"
                    },
                    {
                        "name": "Juan Liu"
                    },
                    {
                        "name": "Yulong Bian"
                    },
                    {
                        "name": "Jin Huang"
                    },
                    {
                        "name": "Chenglei Yang"
                    }
                ],
                "author_detail": {
                    "name": "Chenglei Yang"
                },
                "author": "Chenglei Yang",
                "arxiv_comment": "Accepted by ACM MM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.12992v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.12992v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09190v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09190v2",
                "updated": "2025-08-19T09:06:26Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    9,
                    6,
                    26,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-08T03:20:25Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    3,
                    20,
                    25,
                    4,
                    220,
                    0
                ],
                "title": "Fine-Grained Safety Neurons with Training-Free Continual Projection to\n  Reduce LLM Fine Tuning Risks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Grained Safety Neurons with Training-Free Continual Projection to\n  Reduce LLM Fine Tuning Risks"
                },
                "summary": "Fine-tuning as service injects domain-specific knowledge into large language\nmodels (LLMs), while challenging the original alignment mechanisms and\nintroducing safety risks. A series of defense strategies have been proposed for\nthe alignment, fine-tuning, and post-fine-tuning phases, where most\npost-fine-tuning defenses rely on coarse-grained safety layer mapping. These\nmethods lack a comprehensive consideration of both safety layers and\nfine-grained neurons, limiting their ability to efficiently balance safety and\nutility. To address this, we propose the Fine-Grained Safety Neurons (FGSN)\nwith Training-Free Continual Projection method to reduce the fine-tuning safety\nrisks. FGSN inherently integrates the multi-scale interactions between safety\nlayers and neurons, localizing sparser and more precise fine-grained safety\nneurons while minimizing interference with downstream task neurons. We then\nproject the safety neuron parameters onto safety directions, improving model\nsafety while aligning more closely with human preferences. Extensive\nexperiments across multiple fine-tuned LLM models demonstrate that our method\nsignificantly reduce harmfulness scores and attack success rates with minimal\nparameter modifications, while preserving the model's utility. Furthermore, by\nintroducing a task-specific, multi-dimensional heterogeneous safety neuron\ncluster optimization mechanism, we achieve continual defense and generalization\ncapability against unforeseen emerging safety concerns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning as service injects domain-specific knowledge into large language\nmodels (LLMs), while challenging the original alignment mechanisms and\nintroducing safety risks. A series of defense strategies have been proposed for\nthe alignment, fine-tuning, and post-fine-tuning phases, where most\npost-fine-tuning defenses rely on coarse-grained safety layer mapping. These\nmethods lack a comprehensive consideration of both safety layers and\nfine-grained neurons, limiting their ability to efficiently balance safety and\nutility. To address this, we propose the Fine-Grained Safety Neurons (FGSN)\nwith Training-Free Continual Projection method to reduce the fine-tuning safety\nrisks. FGSN inherently integrates the multi-scale interactions between safety\nlayers and neurons, localizing sparser and more precise fine-grained safety\nneurons while minimizing interference with downstream task neurons. We then\nproject the safety neuron parameters onto safety directions, improving model\nsafety while aligning more closely with human preferences. Extensive\nexperiments across multiple fine-tuned LLM models demonstrate that our method\nsignificantly reduce harmfulness scores and attack success rates with minimal\nparameter modifications, while preserving the model's utility. Furthermore, by\nintroducing a task-specific, multi-dimensional heterogeneous safety neuron\ncluster optimization mechanism, we achieve continual defense and generalization\ncapability against unforeseen emerging safety concerns."
                },
                "authors": [
                    {
                        "name": "Bing Han"
                    },
                    {
                        "name": "Feifei Zhao"
                    },
                    {
                        "name": "Dongcheng Zhao"
                    },
                    {
                        "name": "Guobin Shen"
                    },
                    {
                        "name": "Ping Wu"
                    },
                    {
                        "name": "Yu Shi"
                    },
                    {
                        "name": "Yi Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Yi Zeng"
                },
                "author": "Yi Zeng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09190v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09190v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13654v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13654v2",
                "updated": "2025-08-20T06:41:59Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    6,
                    41,
                    59,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-19T09:04:13Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    9,
                    4,
                    13,
                    1,
                    231,
                    0
                ],
                "title": "Input Time Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Input Time Scaling"
                },
                "summary": "Current Large Language Models (LLMs) are usually post-trained on large-scale\ncarefully curated datasets (data & training scaling) and doing reasoning in\ntest time (inference time scaling). In this work, we present a new scaling\nparadigm, Input Time Scaling, to complement previous scaling methods by putting\nresources on queries (input time). During training and testing, we combine\nmeta-knowledge from LLMs to refine inputs with different strategies. We also\nfind a new phenomenon, training-testing co-design there. We need to apply query\nstrategies during both training and testing. Only applying strategies on\ntraining or testing would seriously degrade the performance. We are also\nsurprised to find that seemingly low data quality datasets can gain high\nperformance. Adding irrelevant information to the queries, randomly selecting\nexamples from a minimally filtered dataset, can even perform the best. These\nfindings contradict the widely held inductive bias, \"garbage in, garbage out\".\nCurating datasets with seemingly high-quality data can even potentially limit\nthe performance ceiling. In addition, models trained on more data with similar\nquality (15k VS 1k) perform worse, simple dataset size scaling should also be\ncarefully inspected. The good news is that our findings are compatible with the\nLess is More phenomenon. A small set of examples is enough to evoke high-level\nreasoning ability. With experiments on models trained on Qwen2.5-32B-Instruct,\nwe are able to reach SOTA performance among 32B models on AIME24(76.7%) and\nAIME25(76.7%) pass@1. We can further achieve AIME24(76.7%) and AIME25(80%) with\na majority vote of three models. Starting from DeepSeek-R1-Distill-Qwen-32B,\nthe best result would be 86.7% on AIME24 and 76.7% on AIME25. To facilitate\nreproducibility and further research, we are working on open-source our\ndatasets, data pipelines, evaluation results, and checkpoints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current Large Language Models (LLMs) are usually post-trained on large-scale\ncarefully curated datasets (data & training scaling) and doing reasoning in\ntest time (inference time scaling). In this work, we present a new scaling\nparadigm, Input Time Scaling, to complement previous scaling methods by putting\nresources on queries (input time). During training and testing, we combine\nmeta-knowledge from LLMs to refine inputs with different strategies. We also\nfind a new phenomenon, training-testing co-design there. We need to apply query\nstrategies during both training and testing. Only applying strategies on\ntraining or testing would seriously degrade the performance. We are also\nsurprised to find that seemingly low data quality datasets can gain high\nperformance. Adding irrelevant information to the queries, randomly selecting\nexamples from a minimally filtered dataset, can even perform the best. These\nfindings contradict the widely held inductive bias, \"garbage in, garbage out\".\nCurating datasets with seemingly high-quality data can even potentially limit\nthe performance ceiling. In addition, models trained on more data with similar\nquality (15k VS 1k) perform worse, simple dataset size scaling should also be\ncarefully inspected. The good news is that our findings are compatible with the\nLess is More phenomenon. A small set of examples is enough to evoke high-level\nreasoning ability. With experiments on models trained on Qwen2.5-32B-Instruct,\nwe are able to reach SOTA performance among 32B models on AIME24(76.7%) and\nAIME25(76.7%) pass@1. We can further achieve AIME24(76.7%) and AIME25(80%) with\na majority vote of three models. Starting from DeepSeek-R1-Distill-Qwen-32B,\nthe best result would be 86.7% on AIME24 and 76.7% on AIME25. To facilitate\nreproducibility and further research, we are working on open-source our\ndatasets, data pipelines, evaluation results, and checkpoints."
                },
                "authors": [
                    {
                        "name": "Rapheal Huang"
                    },
                    {
                        "name": "Weilong Guo"
                    }
                ],
                "author_detail": {
                    "name": "Weilong Guo"
                },
                "arxiv_affiliation": "Yuming",
                "author": "Weilong Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13654v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13654v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13650v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13650v1",
                "updated": "2025-08-19T09:01:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    9,
                    1,
                    22,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T09:01:22Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    9,
                    1,
                    22,
                    1,
                    231,
                    0
                ],
                "title": "CRISP: Persistent Concept Unlearning via Sparse Autoencoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CRISP: Persistent Concept Unlearning via Sparse Autoencoders"
                },
                "summary": "As large language models (LLMs) are increasingly deployed in real-world\napplications, the need to selectively remove unwanted knowledge while\npreserving model utility has become paramount. Recent work has explored sparse\nautoencoders (SAEs) to perform precise interventions on monosemantic features.\nHowever, most SAE-based methods operate at inference time, which does not\ncreate persistent changes in the model's parameters. Such interventions can be\nbypassed or reversed by malicious actors with parameter access. We introduce\nCRISP, a parameter-efficient method for persistent concept unlearning using\nSAEs. CRISP automatically identifies salient SAE features across multiple\nlayers and suppresses their activations. We experiment with two LLMs and show\nthat our method outperforms prior approaches on safety-critical unlearning\ntasks from the WMDP benchmark, successfully removing harmful knowledge while\npreserving general and in-domain capabilities. Feature-level analysis reveals\nthat CRISP achieves semantically coherent separation between target and benign\nconcepts, allowing precise suppression of the target features.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) are increasingly deployed in real-world\napplications, the need to selectively remove unwanted knowledge while\npreserving model utility has become paramount. Recent work has explored sparse\nautoencoders (SAEs) to perform precise interventions on monosemantic features.\nHowever, most SAE-based methods operate at inference time, which does not\ncreate persistent changes in the model's parameters. Such interventions can be\nbypassed or reversed by malicious actors with parameter access. We introduce\nCRISP, a parameter-efficient method for persistent concept unlearning using\nSAEs. CRISP automatically identifies salient SAE features across multiple\nlayers and suppresses their activations. We experiment with two LLMs and show\nthat our method outperforms prior approaches on safety-critical unlearning\ntasks from the WMDP benchmark, successfully removing harmful knowledge while\npreserving general and in-domain capabilities. Feature-level analysis reveals\nthat CRISP achieves semantically coherent separation between target and benign\nconcepts, allowing precise suppression of the target features."
                },
                "authors": [
                    {
                        "name": "Tomer Ashuach"
                    },
                    {
                        "name": "Dana Arad"
                    },
                    {
                        "name": "Aaron Mueller"
                    },
                    {
                        "name": "Martin Tutek"
                    },
                    {
                        "name": "Yonatan Belinkov"
                    }
                ],
                "author_detail": {
                    "name": "Yonatan Belinkov"
                },
                "author": "Yonatan Belinkov",
                "arxiv_comment": "18 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13650v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13650v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13635v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13635v1",
                "updated": "2025-08-19T08:48:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    8,
                    48,
                    5,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T08:48:05Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    8,
                    48,
                    5,
                    1,
                    231,
                    0
                ],
                "title": "Interpreting the Interpreter: Can We Model post-ECB Conferences\n  Volatility with LLM Agents?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpreting the Interpreter: Can We Model post-ECB Conferences\n  Volatility with LLM Agents?"
                },
                "summary": "This paper develops a novel method to simulate financial market reactions to\nEuropean Central Bank (ECB) press conferences using a Large Language Model\n(LLM). We create a behavioral, agent-based simulation of 30 synthetic traders,\neach with distinct risk preferences, cognitive biases, and interpretive styles.\nThese agents forecast Euro interest rate swap levels at 3-month, 2-year, and\n10-year maturities, with the variation across forecasts serving as a measure of\nmarket uncertainty or disagreement. We evaluate three prompting strategies,\nnaive, few-shot (enriched with historical data), and an advanced iterative\n'LLM-as-a-Judge' framework, to assess the effect of prompt design on predictive\nperformance. Even the naive approach generates a strong correlation (roughly\n0.5) between synthetic disagreement and actual market outcomes, particularly\nfor longer-term maturities. The LLM-as-a-Judge framework further improves\naccuracy at the first iteration. These results demonstrate that LLM-driven\nsimulations can capture interpretive uncertainty beyond traditional measures,\nproviding central banks with a practical tool to anticipate market reactions,\nrefine communication strategies, and enhance financial stability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper develops a novel method to simulate financial market reactions to\nEuropean Central Bank (ECB) press conferences using a Large Language Model\n(LLM). We create a behavioral, agent-based simulation of 30 synthetic traders,\neach with distinct risk preferences, cognitive biases, and interpretive styles.\nThese agents forecast Euro interest rate swap levels at 3-month, 2-year, and\n10-year maturities, with the variation across forecasts serving as a measure of\nmarket uncertainty or disagreement. We evaluate three prompting strategies,\nnaive, few-shot (enriched with historical data), and an advanced iterative\n'LLM-as-a-Judge' framework, to assess the effect of prompt design on predictive\nperformance. Even the naive approach generates a strong correlation (roughly\n0.5) between synthetic disagreement and actual market outcomes, particularly\nfor longer-term maturities. The LLM-as-a-Judge framework further improves\naccuracy at the first iteration. These results demonstrate that LLM-driven\nsimulations can capture interpretive uncertainty beyond traditional measures,\nproviding central banks with a practical tool to anticipate market reactions,\nrefine communication strategies, and enhance financial stability."
                },
                "authors": [
                    {
                        "name": "Umberto Collodel"
                    }
                ],
                "author_detail": {
                    "name": "Umberto Collodel"
                },
                "author": "Umberto Collodel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13635v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13635v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04107v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04107v3",
                "updated": "2025-08-19T08:35:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    8,
                    35,
                    4,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-06T06:06:52Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    6,
                    6,
                    52,
                    2,
                    218,
                    0
                ],
                "title": "Unlocking the Potential of MLLMs in Referring Expression Segmentation\n  via a Light-weight Mask Decoder",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking the Potential of MLLMs in Referring Expression Segmentation\n  via a Light-weight Mask Decoder"
                },
                "summary": "Reference Expression Segmentation (RES) aims to segment image regions\nspecified by referring expressions and has become popular with the rise of\nmultimodal large models (MLLMs). While MLLMs excel in semantic understanding,\ntheir token-generation paradigm struggles with pixel-level dense prediction.\nExisting RES methods either couple MLLMs with the parameter-heavy Segment\nAnything Model (SAM) with 632M network parameters or adopt SAM-free lightweight\npipelines that sacrifice accuracy. To address the trade-off between performance\nand cost, we specifically propose MLLMSeg, a novel framework that fully\nexploits the inherent visual detail features encoded in the MLLM vision encoder\nwithout introducing an extra visual encoder. Besides, we propose a\ndetail-enhanced and semantic-consistent feature fusion module (DSFF) that fully\nintegrates the detail-related visual feature with the semantic-related feature\noutput by the large language model (LLM) of MLLM. Finally, we establish a\nlight-weight mask decoder with only 34M network parameters that optimally\nleverages detailed spatial features from the visual encoder and semantic\nfeatures from the LLM to achieve precise mask prediction. Extensive experiments\ndemonstrate that our method generally surpasses both SAM-based and SAM-free\ncompetitors, striking a better balance between performance and cost. Code is\navailable at https://github.com/jcwang0602/MLLMSeg.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reference Expression Segmentation (RES) aims to segment image regions\nspecified by referring expressions and has become popular with the rise of\nmultimodal large models (MLLMs). While MLLMs excel in semantic understanding,\ntheir token-generation paradigm struggles with pixel-level dense prediction.\nExisting RES methods either couple MLLMs with the parameter-heavy Segment\nAnything Model (SAM) with 632M network parameters or adopt SAM-free lightweight\npipelines that sacrifice accuracy. To address the trade-off between performance\nand cost, we specifically propose MLLMSeg, a novel framework that fully\nexploits the inherent visual detail features encoded in the MLLM vision encoder\nwithout introducing an extra visual encoder. Besides, we propose a\ndetail-enhanced and semantic-consistent feature fusion module (DSFF) that fully\nintegrates the detail-related visual feature with the semantic-related feature\noutput by the large language model (LLM) of MLLM. Finally, we establish a\nlight-weight mask decoder with only 34M network parameters that optimally\nleverages detailed spatial features from the visual encoder and semantic\nfeatures from the LLM to achieve precise mask prediction. Extensive experiments\ndemonstrate that our method generally surpasses both SAM-based and SAM-free\ncompetitors, striking a better balance between performance and cost. Code is\navailable at https://github.com/jcwang0602/MLLMSeg."
                },
                "authors": [
                    {
                        "name": "Jingchao Wang"
                    },
                    {
                        "name": "Zhijian Wu"
                    },
                    {
                        "name": "Dingjiang Huang"
                    },
                    {
                        "name": "Yefeng Zheng"
                    },
                    {
                        "name": "Hong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hong Wang"
                },
                "author": "Hong Wang",
                "arxiv_comment": "9 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04107v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04107v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17642v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17642v2",
                "updated": "2025-08-19T08:29:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    8,
                    29,
                    16,
                    1,
                    231,
                    0
                ],
                "published": "2025-06-21T08:51:53Z",
                "published_parsed": [
                    2025,
                    6,
                    21,
                    8,
                    51,
                    53,
                    5,
                    172,
                    0
                ],
                "title": "May the Feedback Be with You! Unlocking the Power of Feedback-Driven\n  Deep Learning Framework Fuzzing via LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "May the Feedback Be with You! Unlocking the Power of Feedback-Driven\n  Deep Learning Framework Fuzzing via LLMs"
                },
                "summary": "Deep Learning (DL) frameworks have served as fundamental components in DL\nsystems over the last decade. However, bugs in DL frameworks could lead to\ncatastrophic consequences in critical scenarios. A simple yet effective way to\nfind bugs in DL frameworks is fuzz testing (Fuzzing). Existing approaches focus\non test generation, leaving execution results with high semantic value (e.g.,\ncoverage information, bug reports, and exception logs) in the wild, which can\nserve as multiple types of feedback. To fill this gap, we propose FUEL to\neffectively utilize the feedback information, which comprises two Large\nLanguage Models (LLMs): analysis LLM and generation LLM. Specifically, analysis\nLLM infers analysis summaries from feedback information, while the generation\nLLM creates tests guided by these summaries. Furthermore, based on multiple\nfeedback guidance, we design two additional components: (i) a feedback-aware\nsimulated annealing algorithm to select operators for test generation,\nenriching test diversity. (ii) a program self-repair strategy to automatically\nrepair invalid tests, enhancing test validity. We evaluate FUEL on the two most\npopular DL frameworks, and experiment results show that FUEL can improve line\ncode coverage of PyTorch and TensorFlow by 9.15% and 14.70% over\nstate-of-the-art baselines (e.g., TitanFuzz and WhiteFox). By the time of\nsubmission, FUEL has detected 104 previously unknown bugs for PyTorch and\nTensorFlow, with 93 confirmed as new bugs, 49 already fixed, and 5 assigned CVE\nIDs. Our artifact is available at https://github.com/NJU-iSE/FUEL",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning (DL) frameworks have served as fundamental components in DL\nsystems over the last decade. However, bugs in DL frameworks could lead to\ncatastrophic consequences in critical scenarios. A simple yet effective way to\nfind bugs in DL frameworks is fuzz testing (Fuzzing). Existing approaches focus\non test generation, leaving execution results with high semantic value (e.g.,\ncoverage information, bug reports, and exception logs) in the wild, which can\nserve as multiple types of feedback. To fill this gap, we propose FUEL to\neffectively utilize the feedback information, which comprises two Large\nLanguage Models (LLMs): analysis LLM and generation LLM. Specifically, analysis\nLLM infers analysis summaries from feedback information, while the generation\nLLM creates tests guided by these summaries. Furthermore, based on multiple\nfeedback guidance, we design two additional components: (i) a feedback-aware\nsimulated annealing algorithm to select operators for test generation,\nenriching test diversity. (ii) a program self-repair strategy to automatically\nrepair invalid tests, enhancing test validity. We evaluate FUEL on the two most\npopular DL frameworks, and experiment results show that FUEL can improve line\ncode coverage of PyTorch and TensorFlow by 9.15% and 14.70% over\nstate-of-the-art baselines (e.g., TitanFuzz and WhiteFox). By the time of\nsubmission, FUEL has detected 104 previously unknown bugs for PyTorch and\nTensorFlow, with 93 confirmed as new bugs, 49 already fixed, and 5 assigned CVE\nIDs. Our artifact is available at https://github.com/NJU-iSE/FUEL"
                },
                "authors": [
                    {
                        "name": "Shaoyu Yang"
                    },
                    {
                        "name": "Chunrong Fang"
                    },
                    {
                        "name": "Haifeng Lin"
                    },
                    {
                        "name": "Xiang Chen"
                    },
                    {
                        "name": "Zhenyu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhenyu Chen"
                },
                "author": "Zhenyu Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17642v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17642v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13608v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13608v1",
                "updated": "2025-08-19T08:13:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    8,
                    13,
                    53,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T08:13:53Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    8,
                    13,
                    53,
                    1,
                    231,
                    0
                ],
                "title": "Towards safe control parameter tuning in distributed multi-agent systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards safe control parameter tuning in distributed multi-agent systems"
                },
                "summary": "Many safety-critical real-world problems, such as autonomous driving and\ncollaborative robots, are of a distributed multi-agent nature. To optimize the\nperformance of these systems while ensuring safety, we can cast them as\ndistributed optimization problems, where each agent aims to optimize their\nparameters to maximize a coupled reward function subject to coupled\nconstraints. Prior work either studies a centralized setting, does not consider\nsafety, or struggles with sample efficiency. Since we require sample efficiency\nand work with unknown and nonconvex rewards and constraints, we solve this\noptimization problem using safe Bayesian optimization with Gaussian process\nregression. Moreover, we consider nearest-neighbor communication between the\nagents. To capture the behavior of non-neighboring agents, we reformulate the\nstatic global optimization problem as a time-varying local optimization problem\nfor each agent, essentially introducing time as a latent variable. To this end,\nwe propose a custom spatio-temporal kernel to integrate prior knowledge. We\nshow the successful deployment of our algorithm in simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many safety-critical real-world problems, such as autonomous driving and\ncollaborative robots, are of a distributed multi-agent nature. To optimize the\nperformance of these systems while ensuring safety, we can cast them as\ndistributed optimization problems, where each agent aims to optimize their\nparameters to maximize a coupled reward function subject to coupled\nconstraints. Prior work either studies a centralized setting, does not consider\nsafety, or struggles with sample efficiency. Since we require sample efficiency\nand work with unknown and nonconvex rewards and constraints, we solve this\noptimization problem using safe Bayesian optimization with Gaussian process\nregression. Moreover, we consider nearest-neighbor communication between the\nagents. To capture the behavior of non-neighboring agents, we reformulate the\nstatic global optimization problem as a time-varying local optimization problem\nfor each agent, essentially introducing time as a latent variable. To this end,\nwe propose a custom spatio-temporal kernel to integrate prior knowledge. We\nshow the successful deployment of our algorithm in simulations."
                },
                "authors": [
                    {
                        "name": "Abdullah Tokmak"
                    },
                    {
                        "name": "Thomas B. Schön"
                    },
                    {
                        "name": "Dominik Baumann"
                    }
                ],
                "author_detail": {
                    "name": "Dominik Baumann"
                },
                "author": "Dominik Baumann",
                "arxiv_comment": "Accepted to CDC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13608v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13608v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13603v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13603v1",
                "updated": "2025-08-19T08:10:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    8,
                    10,
                    55,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T08:10:55Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    8,
                    10,
                    55,
                    1,
                    231,
                    0
                ],
                "title": "Who Gets the Mic? Investigating Gender Bias in the Speaker Assignment of\n  a Speech-LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Who Gets the Mic? Investigating Gender Bias in the Speaker Assignment of\n  a Speech-LLM"
                },
                "summary": "Similar to text-based Large Language Models (LLMs), Speech-LLMs exhibit\nemergent abilities and context awareness. However, whether these similarities\nextend to gender bias remains an open question. This study proposes a\nmethodology leveraging speaker assignment as an analytic tool for bias\ninvestigation. Unlike text-based models, which encode gendered associations\nimplicitly, Speech-LLMs must produce a gendered voice, making speaker selection\nan explicit bias cue. We evaluate Bark, a Text-to-Speech (TTS) model, analyzing\nits default speaker assignments for textual prompts. If Bark's speaker\nselection systematically aligns with gendered associations, it may reveal\npatterns in its training data or model design. To test this, we construct two\ndatasets: (i) Professions, containing gender-stereotyped occupations, and (ii)\nGender-Colored Words, featuring gendered connotations. While Bark does not\nexhibit systematic bias, it demonstrates gender awareness and has some gender\ninclinations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Similar to text-based Large Language Models (LLMs), Speech-LLMs exhibit\nemergent abilities and context awareness. However, whether these similarities\nextend to gender bias remains an open question. This study proposes a\nmethodology leveraging speaker assignment as an analytic tool for bias\ninvestigation. Unlike text-based models, which encode gendered associations\nimplicitly, Speech-LLMs must produce a gendered voice, making speaker selection\nan explicit bias cue. We evaluate Bark, a Text-to-Speech (TTS) model, analyzing\nits default speaker assignments for textual prompts. If Bark's speaker\nselection systematically aligns with gendered associations, it may reveal\npatterns in its training data or model design. To test this, we construct two\ndatasets: (i) Professions, containing gender-stereotyped occupations, and (ii)\nGender-Colored Words, featuring gendered connotations. While Bark does not\nexhibit systematic bias, it demonstrates gender awareness and has some gender\ninclinations."
                },
                "authors": [
                    {
                        "name": "Dariia Puhach"
                    },
                    {
                        "name": "Amir H. Payberah"
                    },
                    {
                        "name": "Éva Székely"
                    }
                ],
                "author_detail": {
                    "name": "Éva Székely"
                },
                "author": "Éva Székely",
                "arxiv_doi": "10.21437/Interspeech.2025-1402",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.21437/Interspeech.2025-1402",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.13603v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13603v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Interspeech 2025 (2025), 2058-2062",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13593v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13593v1",
                "updated": "2025-08-19T07:59:27Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    7,
                    59,
                    27,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T07:59:27Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    7,
                    59,
                    27,
                    1,
                    231,
                    0
                ],
                "title": "Repeater Swarm-Assisted Cellular Systems: Interaction Stability and\n  Performance Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Repeater Swarm-Assisted Cellular Systems: Interaction Stability and\n  Performance Analysis"
                },
                "summary": "We consider a cellular massive MIMO system where swarms of wireless repeaters\nare deployed to improve coverage. These repeaters are full-duplex relays with\nsmall form factors that receive and instantaneously retransmit signals. They\ncan be deployed in a plug-and-play manner at low cost, while being transparent\nto the network--conceptually they are active channel scatterers with\namplification capabilities. Two fundamental questions need to be addressed in\nrepeater deployments: (I) How can we prevent destructive effects of positive\nfeedback caused by inter-repeater interaction (i.e., each repeater receives and\namplifies signals from others)? (ii) How much performance improvement can be\nachieved given that repeaters also inject noise and may introduce more\ninterference? To answer these questions, we first derive a generalized Nyquist\nstability criterion for the repeater swarm system, and provide an easy-to-check\nstability condition. Then, we study the uplink performance and develop an\nefficient iterative algorithm that jointly optimizes the repeater gains, user\ntransmit powers, and receive combining weights to maximize the weighted sum\nrate while ensuring system stability. Numerical results corroborate our\ntheoretical findings and show that the repeaters can significantly improve the\nsystem performance, both in sub-6 GHz and millimeter-wave bands. The results\nalso warrant careful deployment to fully realize the benefits of repeaters, for\nexample, by ensuring a high probability of line-of-sight links between\nrepeaters and the base station.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a cellular massive MIMO system where swarms of wireless repeaters\nare deployed to improve coverage. These repeaters are full-duplex relays with\nsmall form factors that receive and instantaneously retransmit signals. They\ncan be deployed in a plug-and-play manner at low cost, while being transparent\nto the network--conceptually they are active channel scatterers with\namplification capabilities. Two fundamental questions need to be addressed in\nrepeater deployments: (I) How can we prevent destructive effects of positive\nfeedback caused by inter-repeater interaction (i.e., each repeater receives and\namplifies signals from others)? (ii) How much performance improvement can be\nachieved given that repeaters also inject noise and may introduce more\ninterference? To answer these questions, we first derive a generalized Nyquist\nstability criterion for the repeater swarm system, and provide an easy-to-check\nstability condition. Then, we study the uplink performance and develop an\nefficient iterative algorithm that jointly optimizes the repeater gains, user\ntransmit powers, and receive combining weights to maximize the weighted sum\nrate while ensuring system stability. Numerical results corroborate our\ntheoretical findings and show that the repeaters can significantly improve the\nsystem performance, both in sub-6 GHz and millimeter-wave bands. The results\nalso warrant careful deployment to fully realize the benefits of repeaters, for\nexample, by ensuring a high probability of line-of-sight links between\nrepeaters and the base station."
                },
                "authors": [
                    {
                        "name": "Jianan Bai"
                    },
                    {
                        "name": "Anubhab Chowdhury"
                    },
                    {
                        "name": "Anders Hansson"
                    },
                    {
                        "name": "Erik G. Larsson"
                    }
                ],
                "author_detail": {
                    "name": "Erik G. Larsson"
                },
                "author": "Erik G. Larsson",
                "arxiv_comment": "16 pages, 13 figures. Submitted to IEEE Transactions on Wireless\n  Communications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13593v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13593v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13580v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13580v1",
                "updated": "2025-08-19T07:25:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    7,
                    25,
                    25,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T07:25:25Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    7,
                    25,
                    25,
                    1,
                    231,
                    0
                ],
                "title": "A Comparative Study of Decoding Strategies in Medical Text Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comparative Study of Decoding Strategies in Medical Text Generation"
                },
                "summary": "Large Language Models (LLMs) rely on various decoding strategies to generate\ntext, and these choices can significantly affect output quality. In healthcare,\nwhere accuracy is critical, the impact of decoding strategies remains\nunderexplored. We investigate this effect in five open-ended medical tasks,\nincluding translation, summarization, question answering, dialogue, and image\ncaptioning, evaluating 11 decoding strategies with medically specialized and\ngeneral-purpose LLMs of different sizes. Our results show that deterministic\nstrategies generally outperform stochastic ones: beam search achieves the\nhighest scores, while {\\eta} and top-k sampling perform worst. Slower decoding\nmethods tend to yield better quality. Larger models achieve higher scores\noverall but have longer inference times and are no more robust to decoding.\nSurprisingly, while medical LLMs outperform general ones in two of the five\ntasks, statistical analysis shows no overall performance advantage and reveals\ngreater sensitivity to decoding choice. We further compare multiple evaluation\nmetrics and find that correlations vary by task, with MAUVE showing weak\nagreement with BERTScore and ROUGE, as well as greater sensitivity to the\ndecoding strategy. These results highlight the need for careful selection of\ndecoding methods in medical applications, as their influence can sometimes\nexceed that of model choice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) rely on various decoding strategies to generate\ntext, and these choices can significantly affect output quality. In healthcare,\nwhere accuracy is critical, the impact of decoding strategies remains\nunderexplored. We investigate this effect in five open-ended medical tasks,\nincluding translation, summarization, question answering, dialogue, and image\ncaptioning, evaluating 11 decoding strategies with medically specialized and\ngeneral-purpose LLMs of different sizes. Our results show that deterministic\nstrategies generally outperform stochastic ones: beam search achieves the\nhighest scores, while {\\eta} and top-k sampling perform worst. Slower decoding\nmethods tend to yield better quality. Larger models achieve higher scores\noverall but have longer inference times and are no more robust to decoding.\nSurprisingly, while medical LLMs outperform general ones in two of the five\ntasks, statistical analysis shows no overall performance advantage and reveals\ngreater sensitivity to decoding choice. We further compare multiple evaluation\nmetrics and find that correlations vary by task, with MAUVE showing weak\nagreement with BERTScore and ROUGE, as well as greater sensitivity to the\ndecoding strategy. These results highlight the need for careful selection of\ndecoding methods in medical applications, as their influence can sometimes\nexceed that of model choice."
                },
                "authors": [
                    {
                        "name": "Oriana Presacan"
                    },
                    {
                        "name": "Alireza Nik"
                    },
                    {
                        "name": "Vajira Thambawita"
                    },
                    {
                        "name": "Bogdan Ionescu"
                    },
                    {
                        "name": "Michael Riegler"
                    }
                ],
                "author_detail": {
                    "name": "Michael Riegler"
                },
                "author": "Michael Riegler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13580v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13580v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13579v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13579v1",
                "updated": "2025-08-19T07:24:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    7,
                    24,
                    48,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T07:24:48Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    7,
                    24,
                    48,
                    1,
                    231,
                    0
                ],
                "title": "Toward Better EHR Reasoning in LLMs: Reinforcement Learning with Expert\n  Attention Guidance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward Better EHR Reasoning in LLMs: Reinforcement Learning with Expert\n  Attention Guidance"
                },
                "summary": "Improving large language models (LLMs) for electronic health record (EHR)\nreasoning is essential for enabling accurate and generalizable clinical\npredictions. While LLMs excel at medical text understanding, they underperform\non EHR-based prediction tasks due to challenges in modeling temporally\nstructured, high-dimensional data. Existing approaches often rely on hybrid\nparadigms, where LLMs serve merely as frozen prior retrievers while downstream\ndeep learning (DL) models handle prediction, failing to improve the LLM's\nintrinsic reasoning capacity and inheriting the generalization limitations of\nDL models. To this end, we propose EAG-RL, a novel two-stage training framework\ndesigned to intrinsically enhance LLMs' EHR reasoning ability through expert\nattention guidance, where expert EHR models refer to task-specific DL models\ntrained on EHR data. Concretely, EAG-RL first constructs high-quality, stepwise\nreasoning trajectories using expert-guided Monte Carlo Tree Search to\neffectively initialize the LLM's policy. Then, EAG-RL further optimizes the\npolicy via reinforcement learning by aligning the LLM's attention with\nclinically salient features identified by expert EHR models. Extensive\nexperiments on two real-world EHR datasets show that EAG-RL improves the\nintrinsic EHR reasoning ability of LLMs by an average of 14.62%, while also\nenhancing robustness to feature perturbations and generalization to unseen\nclinical domains. These results demonstrate the practical potential of EAG-RL\nfor real-world deployment in clinical prediction tasks. Our code have been\navailable at https://github.com/devilran6/EAG-RL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving large language models (LLMs) for electronic health record (EHR)\nreasoning is essential for enabling accurate and generalizable clinical\npredictions. While LLMs excel at medical text understanding, they underperform\non EHR-based prediction tasks due to challenges in modeling temporally\nstructured, high-dimensional data. Existing approaches often rely on hybrid\nparadigms, where LLMs serve merely as frozen prior retrievers while downstream\ndeep learning (DL) models handle prediction, failing to improve the LLM's\nintrinsic reasoning capacity and inheriting the generalization limitations of\nDL models. To this end, we propose EAG-RL, a novel two-stage training framework\ndesigned to intrinsically enhance LLMs' EHR reasoning ability through expert\nattention guidance, where expert EHR models refer to task-specific DL models\ntrained on EHR data. Concretely, EAG-RL first constructs high-quality, stepwise\nreasoning trajectories using expert-guided Monte Carlo Tree Search to\neffectively initialize the LLM's policy. Then, EAG-RL further optimizes the\npolicy via reinforcement learning by aligning the LLM's attention with\nclinically salient features identified by expert EHR models. Extensive\nexperiments on two real-world EHR datasets show that EAG-RL improves the\nintrinsic EHR reasoning ability of LLMs by an average of 14.62%, while also\nenhancing robustness to feature perturbations and generalization to unseen\nclinical domains. These results demonstrate the practical potential of EAG-RL\nfor real-world deployment in clinical prediction tasks. Our code have been\navailable at https://github.com/devilran6/EAG-RL."
                },
                "authors": [
                    {
                        "name": "Yue Fang"
                    },
                    {
                        "name": "Yuxin Guo"
                    },
                    {
                        "name": "Jiaran Gao"
                    },
                    {
                        "name": "Hongxin Ding"
                    },
                    {
                        "name": "Xinke Jiang"
                    },
                    {
                        "name": "Weibin Liao"
                    },
                    {
                        "name": "Yongxin Xu"
                    },
                    {
                        "name": "Yinghao Zhu"
                    },
                    {
                        "name": "Zhibang Yang"
                    },
                    {
                        "name": "Liantao Ma"
                    },
                    {
                        "name": "Junfeng Zhao"
                    },
                    {
                        "name": "Yasha Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yasha Wang"
                },
                "author": "Yasha Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13579v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13579v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08989v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08989v3",
                "updated": "2025-08-19T07:02:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    7,
                    2,
                    28,
                    1,
                    231,
                    0
                ],
                "published": "2025-02-13T06:01:09Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    6,
                    1,
                    9,
                    3,
                    44,
                    0
                ],
                "title": "Setup Once, Secure Always: A Single-Setup Secure Federated Learning\n  Aggregation Protocol with Forward and Backward Secrecy for Dynamic Users",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Setup Once, Secure Always: A Single-Setup Secure Federated Learning\n  Aggregation Protocol with Forward and Backward Secrecy for Dynamic Users"
                },
                "summary": "Federated Learning (FL) enables multiple users to collaboratively train a\nmachine learning model without sharing raw data, making it suitable for\nprivacy-sensitive applications. However, local model or weight updates can\nstill leak sensitive information. Secure aggregation protocols mitigate this\nrisk by ensuring that only the aggregated updates are revealed. Among these,\nsingle-setup protocols, where key generation and exchange occur only once, are\nthe most efficient due to reduced communication and computation overhead.\nHowever, existing single-setup protocols often lack support for dynamic user\nparticipation and do not provide strong privacy guarantees such as forward and\nbackward secrecy. In this paper, we propose a new secure aggregation protocol\nthat requires only one setup operation for the entire FL training and allows\nnew users to join or leave at any round. It employs lightweight symmetric\nhomomorphic encryption with a key negation technique to efficiently mask\nupdates, without user-to-user communication -- unlike the existing protocols.\nTo defend against model inconsistency attacks, we introduce a simple\nverification mechanism using message authentication codes (MACs). Our protocol\nis the first to combine forward/backward secrecy, dropout resilience, and model\nintegrity verification in a single-setup design. We provide formal security\nproofs and implement an end-to-end prototype, which source code has been\nreleased. Our experimental results show that our protocol reduces user-side\ncomputation by approximately 99% compared to state-of-the-art protocols like\ne-SeaFL (ACSAC'24), making it highly practical for real-world FL deployments,\nespecially on resource-constrained devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) enables multiple users to collaboratively train a\nmachine learning model without sharing raw data, making it suitable for\nprivacy-sensitive applications. However, local model or weight updates can\nstill leak sensitive information. Secure aggregation protocols mitigate this\nrisk by ensuring that only the aggregated updates are revealed. Among these,\nsingle-setup protocols, where key generation and exchange occur only once, are\nthe most efficient due to reduced communication and computation overhead.\nHowever, existing single-setup protocols often lack support for dynamic user\nparticipation and do not provide strong privacy guarantees such as forward and\nbackward secrecy. In this paper, we propose a new secure aggregation protocol\nthat requires only one setup operation for the entire FL training and allows\nnew users to join or leave at any round. It employs lightweight symmetric\nhomomorphic encryption with a key negation technique to efficiently mask\nupdates, without user-to-user communication -- unlike the existing protocols.\nTo defend against model inconsistency attacks, we introduce a simple\nverification mechanism using message authentication codes (MACs). Our protocol\nis the first to combine forward/backward secrecy, dropout resilience, and model\nintegrity verification in a single-setup design. We provide formal security\nproofs and implement an end-to-end prototype, which source code has been\nreleased. Our experimental results show that our protocol reduces user-side\ncomputation by approximately 99% compared to state-of-the-art protocols like\ne-SeaFL (ACSAC'24), making it highly practical for real-world FL deployments,\nespecially on resource-constrained devices."
                },
                "authors": [
                    {
                        "name": "Nazatul Haque Sultan"
                    },
                    {
                        "name": "Yan Bo"
                    },
                    {
                        "name": "Yansong Gao"
                    },
                    {
                        "name": "Seyit Camtepe"
                    },
                    {
                        "name": "Arash Mahboubi"
                    },
                    {
                        "name": "Hang Thanh Bui"
                    },
                    {
                        "name": "Aufeef Chauhan"
                    },
                    {
                        "name": "Hamed Aboutorab"
                    },
                    {
                        "name": "Michael Bewong"
                    },
                    {
                        "name": "Dineshkumar Singh"
                    },
                    {
                        "name": "Praveen Gauravaram"
                    },
                    {
                        "name": "Rafiqul Islam"
                    },
                    {
                        "name": "Sharif Abuadbba"
                    }
                ],
                "author_detail": {
                    "name": "Sharif Abuadbba"
                },
                "author": "Sharif Abuadbba",
                "arxiv_comment": "17 pages, 12 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08989v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08989v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68P27",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "E.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13564v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13564v1",
                "updated": "2025-08-19T06:55:06Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    6,
                    55,
                    6,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T06:55:06Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    6,
                    55,
                    6,
                    1,
                    231,
                    0
                ],
                "title": "The 9th AI City Challenge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The 9th AI City Challenge"
                },
                "summary": "The ninth AI City Challenge continues to advance real-world applications of\ncomputer vision and AI in transportation, industrial automation, and public\nsafety. The 2025 edition featured four tracks and saw a 17% increase in\nparticipation, with 245 teams from 15 countries registered on the evaluation\nserver. Public release of challenge datasets led to over 30,000 downloads to\ndate. Track 1 focused on multi-class 3D multi-camera tracking, involving\npeople, humanoids, autonomous mobile robots, and forklifts, using detailed\ncalibration and 3D bounding box annotations. Track 2 tackled video question\nanswering in traffic safety, with multi-camera incident understanding enriched\nby 3D gaze labels. Track 3 addressed fine-grained spatial reasoning in dynamic\nwarehouse environments, requiring AI systems to interpret RGB-D inputs and\nanswer spatial questions that combine perception, geometry, and language. Both\nTrack 1 and Track 3 datasets were generated in NVIDIA Omniverse. Track 4\nemphasized efficient road object detection from fisheye cameras, supporting\nlightweight, real-time deployment on edge devices. The evaluation framework\nenforced submission limits and used a partially held-out test set to ensure\nfair benchmarking. Final rankings were revealed after the competition\nconcluded, fostering reproducibility and mitigating overfitting. Several teams\nachieved top-tier results, setting new benchmarks in multiple tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ninth AI City Challenge continues to advance real-world applications of\ncomputer vision and AI in transportation, industrial automation, and public\nsafety. The 2025 edition featured four tracks and saw a 17% increase in\nparticipation, with 245 teams from 15 countries registered on the evaluation\nserver. Public release of challenge datasets led to over 30,000 downloads to\ndate. Track 1 focused on multi-class 3D multi-camera tracking, involving\npeople, humanoids, autonomous mobile robots, and forklifts, using detailed\ncalibration and 3D bounding box annotations. Track 2 tackled video question\nanswering in traffic safety, with multi-camera incident understanding enriched\nby 3D gaze labels. Track 3 addressed fine-grained spatial reasoning in dynamic\nwarehouse environments, requiring AI systems to interpret RGB-D inputs and\nanswer spatial questions that combine perception, geometry, and language. Both\nTrack 1 and Track 3 datasets were generated in NVIDIA Omniverse. Track 4\nemphasized efficient road object detection from fisheye cameras, supporting\nlightweight, real-time deployment on edge devices. The evaluation framework\nenforced submission limits and used a partially held-out test set to ensure\nfair benchmarking. Final rankings were revealed after the competition\nconcluded, fostering reproducibility and mitigating overfitting. Several teams\nachieved top-tier results, setting new benchmarks in multiple tasks."
                },
                "authors": [
                    {
                        "name": "Zheng Tang"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "David C. Anastasiu"
                    },
                    {
                        "name": "Ming-Ching Chang"
                    },
                    {
                        "name": "Anuj Sharma"
                    },
                    {
                        "name": "Quan Kong"
                    },
                    {
                        "name": "Norimasa Kobori"
                    },
                    {
                        "name": "Munkhjargal Gochoo"
                    },
                    {
                        "name": "Ganzorig Batnasan"
                    },
                    {
                        "name": "Munkh-Erdene Otgonbold"
                    },
                    {
                        "name": "Fady Alnajjar"
                    },
                    {
                        "name": "Jun-Wei Hsieh"
                    },
                    {
                        "name": "Tomasz Kornuta"
                    },
                    {
                        "name": "Xiaolong Li"
                    },
                    {
                        "name": "Yilin Zhao"
                    },
                    {
                        "name": "Han Zhang"
                    },
                    {
                        "name": "Subhashree Radhakrishnan"
                    },
                    {
                        "name": "Arihant Jain"
                    },
                    {
                        "name": "Ratnesh Kumar"
                    },
                    {
                        "name": "Vidya N. Murali"
                    },
                    {
                        "name": "Yuxing Wang"
                    },
                    {
                        "name": "Sameer Satish Pusegaonkar"
                    },
                    {
                        "name": "Yizhou Wang"
                    },
                    {
                        "name": "Sujit Biswas"
                    },
                    {
                        "name": "Xunlei Wu"
                    },
                    {
                        "name": "Zhedong Zheng"
                    },
                    {
                        "name": "Pranamesh Chakraborty"
                    },
                    {
                        "name": "Rama Chellappa"
                    }
                ],
                "author_detail": {
                    "name": "Rama Chellappa"
                },
                "author": "Rama Chellappa",
                "arxiv_comment": "Summary of the 9th AI City Challenge Workshop in conjunction with\n  ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13564v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13564v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13559v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13559v1",
                "updated": "2025-08-19T06:38:49Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    6,
                    38,
                    49,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T06:38:49Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    6,
                    38,
                    49,
                    1,
                    231,
                    0
                ],
                "title": "Physics-Informed Neural Networks for Programmable Origami Metamaterials\n  with Controlled Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physics-Informed Neural Networks for Programmable Origami Metamaterials\n  with Controlled Deployment"
                },
                "summary": "Origami-inspired structures provide unprecedented opportunities for creating\nlightweight, deployable systems with programmable mechanical responses.\nHowever, their design remains challenging due to complex nonlinear mechanics,\nmultistability, and the need for precise control of deployment forces. Here, we\npresent a physics-informed neural network (PINN) framework for both forward\nprediction and inverse design of conical Kresling origami (CKO) without\nrequiring pre-collected training data. By embedding mechanical equilibrium\nequations directly into the learning process, the model predicts complete\nenergy landscapes with high accuracy while minimizing non-physical artifacts.\nThe inverse design routine specifies both target stable-state heights and\nseparating energy barriers, enabling freeform programming of the entire energy\ncurve. This capability is extended to hierarchical CKO assemblies, where\nsequential layer-by-layer deployment is achieved through programmed barrier\nmagnitudes. Finite element simulations and experiments on physical prototypes\nvalidate the designed deployment sequences and barrier ratios, confirming the\nrobustness of the approach. This work establishes a versatile, data-free route\nfor programming complex mechanical energy landscapes in origami-inspired\nmetamaterials, offering broad potential for deployable aerospace systems,\nmorphing structures, and soft robotic actuators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Origami-inspired structures provide unprecedented opportunities for creating\nlightweight, deployable systems with programmable mechanical responses.\nHowever, their design remains challenging due to complex nonlinear mechanics,\nmultistability, and the need for precise control of deployment forces. Here, we\npresent a physics-informed neural network (PINN) framework for both forward\nprediction and inverse design of conical Kresling origami (CKO) without\nrequiring pre-collected training data. By embedding mechanical equilibrium\nequations directly into the learning process, the model predicts complete\nenergy landscapes with high accuracy while minimizing non-physical artifacts.\nThe inverse design routine specifies both target stable-state heights and\nseparating energy barriers, enabling freeform programming of the entire energy\ncurve. This capability is extended to hierarchical CKO assemblies, where\nsequential layer-by-layer deployment is achieved through programmed barrier\nmagnitudes. Finite element simulations and experiments on physical prototypes\nvalidate the designed deployment sequences and barrier ratios, confirming the\nrobustness of the approach. This work establishes a versatile, data-free route\nfor programming complex mechanical energy landscapes in origami-inspired\nmetamaterials, offering broad potential for deployable aerospace systems,\nmorphing structures, and soft robotic actuators."
                },
                "authors": [
                    {
                        "name": "Sukheon Kang"
                    },
                    {
                        "name": "Youngkwon Kim"
                    },
                    {
                        "name": "Jinkyu Yang"
                    },
                    {
                        "name": "Seunghwa Ryu"
                    }
                ],
                "author_detail": {
                    "name": "Seunghwa Ryu"
                },
                "author": "Seunghwa Ryu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13559v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13559v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.soft",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02850v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02850v2",
                "updated": "2025-08-19T06:31:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    6,
                    31,
                    37,
                    1,
                    231,
                    0
                ],
                "published": "2025-05-02T06:36:06Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    6,
                    36,
                    6,
                    4,
                    122,
                    0
                ],
                "title": "Harnessing Structured Knowledge: A Concept Map-Based Approach for\n  High-Quality Multiple Choice Question Generation with Effective Distractors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing Structured Knowledge: A Concept Map-Based Approach for\n  High-Quality Multiple Choice Question Generation with Effective Distractors"
                },
                "summary": "Generating high-quality MCQs, especially those targeting diverse cognitive\nlevels and incorporating common misconceptions into distractor design, is\ntime-consuming and expertise-intensive, making manual creation impractical at\nscale. Current automated approaches typically generate questions at lower\ncognitive levels and fail to incorporate domain-specific misconceptions. This\npaper presents a hierarchical concept map-based framework that provides\nstructured knowledge to guide LLMs in generating MCQs with distractors. We\nchose high-school physics as our test domain and began by developing a\nhierarchical concept map covering major Physics topics and their\ninterconnections with an efficient database design. Next, through an automated\npipeline, topic-relevant sections of these concept maps are retrieved to serve\nas a structured context for the LLM to generate questions and distractors that\nspecifically target common misconceptions. Lastly, an automated validation is\ncompleted to ensure that the generated MCQs meet the requirements provided. We\nevaluate our framework against two baseline approaches: a base LLM and a\nRAG-based generation. We conducted expert evaluations and student assessments\nof the generated MCQs. Expert evaluation shows that our method significantly\noutperforms the baseline approaches, achieving a success rate of 75.20% in\nmeeting all quality criteria compared to approximately 37% for both baseline\nmethods. Student assessment data reveal that our concept map-driven approach\nachieved a significantly lower guess success rate of 28.05% compared to 37.10%\nfor the baselines, indicating a more effective assessment of conceptual\nunderstanding. The results demonstrate that our concept map-based approach\nenables robust assessment across cognitive levels and instant identification of\nconceptual gaps, facilitating faster feedback loops and targeted interventions\nat scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating high-quality MCQs, especially those targeting diverse cognitive\nlevels and incorporating common misconceptions into distractor design, is\ntime-consuming and expertise-intensive, making manual creation impractical at\nscale. Current automated approaches typically generate questions at lower\ncognitive levels and fail to incorporate domain-specific misconceptions. This\npaper presents a hierarchical concept map-based framework that provides\nstructured knowledge to guide LLMs in generating MCQs with distractors. We\nchose high-school physics as our test domain and began by developing a\nhierarchical concept map covering major Physics topics and their\ninterconnections with an efficient database design. Next, through an automated\npipeline, topic-relevant sections of these concept maps are retrieved to serve\nas a structured context for the LLM to generate questions and distractors that\nspecifically target common misconceptions. Lastly, an automated validation is\ncompleted to ensure that the generated MCQs meet the requirements provided. We\nevaluate our framework against two baseline approaches: a base LLM and a\nRAG-based generation. We conducted expert evaluations and student assessments\nof the generated MCQs. Expert evaluation shows that our method significantly\noutperforms the baseline approaches, achieving a success rate of 75.20% in\nmeeting all quality criteria compared to approximately 37% for both baseline\nmethods. Student assessment data reveal that our concept map-driven approach\nachieved a significantly lower guess success rate of 28.05% compared to 37.10%\nfor the baselines, indicating a more effective assessment of conceptual\nunderstanding. The results demonstrate that our concept map-based approach\nenables robust assessment across cognitive levels and instant identification of\nconceptual gaps, facilitating faster feedback loops and targeted interventions\nat scale."
                },
                "authors": [
                    {
                        "name": "Nicy Scaria"
                    },
                    {
                        "name": "Silvester John Joseph Kennedy"
                    },
                    {
                        "name": "Diksha Seth"
                    },
                    {
                        "name": "Ananya Thakur"
                    },
                    {
                        "name": "Deepak Subramani"
                    }
                ],
                "author_detail": {
                    "name": "Deepak Subramani"
                },
                "author": "Deepak Subramani",
                "arxiv_comment": "Accepted to ECAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02850v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02850v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02262v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02262v2",
                "updated": "2025-08-19T06:25:29Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    6,
                    25,
                    29,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-04T10:10:59Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    10,
                    10,
                    59,
                    0,
                    216,
                    0
                ],
                "title": "Long-distance device-independent quantum key distribution with standard\n  optics tools",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-distance device-independent quantum key distribution with standard\n  optics tools"
                },
                "summary": "Device-independent quantum key distribution (DI-QKD) enables\ninformation-theoretically secure key exchange between remote parties without\nany assumptions on the internal workings of the devices used for its\nimplementation. However, its practical deployment remains severely constrained\nby the need for loophole-free Bell inequality violations, which are highly\nsusceptible to losses and detection efficiencies. In this paper, we propose two\nlong-distance DI-QKD protocols based on a heralding scheme using single-photon\ninterference. Our protocols consist of only standard quantum optics tools such\nas two-mode squeezed states, displacement operations and on-off detectors,\nmaking them experimentally accessible. To further enhance robustness against\nrealistic imperfections, we integrate a classical noisy preprocessing technique\nduring post-processing. We calculate key rates of the protocols by numerical\noptimization and show the supremacy of this implementation over existing\nprotocols in terms of communication distances.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Device-independent quantum key distribution (DI-QKD) enables\ninformation-theoretically secure key exchange between remote parties without\nany assumptions on the internal workings of the devices used for its\nimplementation. However, its practical deployment remains severely constrained\nby the need for loophole-free Bell inequality violations, which are highly\nsusceptible to losses and detection efficiencies. In this paper, we propose two\nlong-distance DI-QKD protocols based on a heralding scheme using single-photon\ninterference. Our protocols consist of only standard quantum optics tools such\nas two-mode squeezed states, displacement operations and on-off detectors,\nmaking them experimentally accessible. To further enhance robustness against\nrealistic imperfections, we integrate a classical noisy preprocessing technique\nduring post-processing. We calculate key rates of the protocols by numerical\noptimization and show the supremacy of this implementation over existing\nprotocols in terms of communication distances."
                },
                "authors": [
                    {
                        "name": "Makoto Ishihara"
                    },
                    {
                        "name": "Anthony Brendan"
                    },
                    {
                        "name": "Wojciech Roga"
                    },
                    {
                        "name": "Ulrik L. Andersen"
                    },
                    {
                        "name": "Masahiro Takeoka"
                    }
                ],
                "author_detail": {
                    "name": "Masahiro Takeoka"
                },
                "author": "Masahiro Takeoka",
                "arxiv_comment": "11 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02262v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02262v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.14913v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.14913v3",
                "updated": "2025-08-19T06:24:38Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    6,
                    24,
                    38,
                    1,
                    231,
                    0
                ],
                "published": "2025-07-20T10:55:29Z",
                "published_parsed": [
                    2025,
                    7,
                    20,
                    10,
                    55,
                    29,
                    6,
                    201,
                    0
                ],
                "title": "PromptSuite: A Task-Agnostic Framework for Multi-Prompt Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PromptSuite: A Task-Agnostic Framework for Multi-Prompt Generation"
                },
                "summary": "Evaluating LLMs with a single prompt has proven unreliable, with small\nchanges leading to significant performance differences. However, generating the\nprompt variations needed for a more robust multi-prompt evaluation is\nchallenging, limiting its adoption in practice. To address this, we introduce\nPromptSuite, a framework that enables the automatic generation of various\nprompts. PromptSuite is flexible - working out of the box on a wide range of\ntasks and benchmarks. It follows a modular prompt design, allowing controlled\nperturbations to each component, and is extensible, supporting the addition of\nnew components and perturbation types. Through a series of case studies, we\nshow that PromptSuite provides meaningful variations to support strong\nevaluation practices. All resources, including the Python API, source code,\nuser-friendly web interface, and demonstration video, are available at:\nhttps://eliyahabba.github.io/PromptSuite/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating LLMs with a single prompt has proven unreliable, with small\nchanges leading to significant performance differences. However, generating the\nprompt variations needed for a more robust multi-prompt evaluation is\nchallenging, limiting its adoption in practice. To address this, we introduce\nPromptSuite, a framework that enables the automatic generation of various\nprompts. PromptSuite is flexible - working out of the box on a wide range of\ntasks and benchmarks. It follows a modular prompt design, allowing controlled\nperturbations to each component, and is extensible, supporting the addition of\nnew components and perturbation types. Through a series of case studies, we\nshow that PromptSuite provides meaningful variations to support strong\nevaluation practices. All resources, including the Python API, source code,\nuser-friendly web interface, and demonstration video, are available at:\nhttps://eliyahabba.github.io/PromptSuite/."
                },
                "authors": [
                    {
                        "name": "Eliya Habba"
                    },
                    {
                        "name": "Noam Dahan"
                    },
                    {
                        "name": "Gili Lior"
                    },
                    {
                        "name": "Gabriel Stanovsky"
                    }
                ],
                "author_detail": {
                    "name": "Gabriel Stanovsky"
                },
                "author": "Gabriel Stanovsky",
                "arxiv_comment": "Eliya Habba and Noam Dahan contributed equally to this work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.14913v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.14913v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11664v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11664v3",
                "updated": "2025-08-19T06:20:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    6,
                    20,
                    53,
                    1,
                    231,
                    0
                ],
                "published": "2025-02-17T10:53:57Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    10,
                    53,
                    57,
                    0,
                    48,
                    0
                ],
                "title": "VRoPE: Rotary Position Embedding for Video Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VRoPE: Rotary Position Embedding for Video Large Language Models"
                },
                "summary": "Rotary Position Embedding (RoPE) has shown strong performance in text-based\nLarge Language Models (LLMs), but extending it to video remains a challenge due\nto the intricate spatiotemporal structure of video frames. Existing\nadaptations, such as RoPE-3D, attempt to encode spatial and temporal dimensions\nseparately but suffer from two major limitations: positional bias in attention\ndistribution and disruptions in video-text transitions. To overcome these\nissues, we propose Video Rotary Position Embedding (VRoPE), a novel positional\nencoding method tailored for Video-LLMs. Specifically, we introduce a more\nbalanced encoding strategy that mitigates attention biases, ensuring a more\nuniform distribution of spatial focus. Additionally, our approach restructures\npositional indices to ensure a smooth transition between video and text tokens.\nExtensive experiments on different models demonstrate that VRoPE consistently\noutperforms previous RoPE variants, achieving significant improvements in video\nunderstanding, temporal reasoning, and retrieval tasks. Code will be available\nat https://github.com/johncaged/VRoPE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rotary Position Embedding (RoPE) has shown strong performance in text-based\nLarge Language Models (LLMs), but extending it to video remains a challenge due\nto the intricate spatiotemporal structure of video frames. Existing\nadaptations, such as RoPE-3D, attempt to encode spatial and temporal dimensions\nseparately but suffer from two major limitations: positional bias in attention\ndistribution and disruptions in video-text transitions. To overcome these\nissues, we propose Video Rotary Position Embedding (VRoPE), a novel positional\nencoding method tailored for Video-LLMs. Specifically, we introduce a more\nbalanced encoding strategy that mitigates attention biases, ensuring a more\nuniform distribution of spatial focus. Additionally, our approach restructures\npositional indices to ensure a smooth transition between video and text tokens.\nExtensive experiments on different models demonstrate that VRoPE consistently\noutperforms previous RoPE variants, achieving significant improvements in video\nunderstanding, temporal reasoning, and retrieval tasks. Code will be available\nat https://github.com/johncaged/VRoPE."
                },
                "authors": [
                    {
                        "name": "Zikang Liu"
                    },
                    {
                        "name": "Longteng Guo"
                    },
                    {
                        "name": "Yepeng Tang"
                    },
                    {
                        "name": "Tongtian Yue"
                    },
                    {
                        "name": "Junxian Cai"
                    },
                    {
                        "name": "Kai Ma"
                    },
                    {
                        "name": "Qingbin Liu"
                    },
                    {
                        "name": "Xi Chen"
                    },
                    {
                        "name": "Jing Liu"
                    }
                ],
                "author_detail": {
                    "name": "Jing Liu"
                },
                "author": "Jing Liu",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11664v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11664v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13547v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13547v1",
                "updated": "2025-08-19T06:09:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    6,
                    9,
                    28,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T06:09:28Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    6,
                    9,
                    28,
                    1,
                    231,
                    0
                ],
                "title": "A Lightweight Dual-Mode Optimization for Generative Face Video Coding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Lightweight Dual-Mode Optimization for Generative Face Video Coding"
                },
                "summary": "Generative Face Video Coding (GFVC) achieves superior rate-distortion\nperformance by leveraging the strong inference capabilities of deep generative\nmodels. However, its practical deployment is hindered by large model parameters\nand high computational costs. To address this, we propose a lightweight GFVC\nframework that introduces dual-mode optimization -- combining architectural\nredesign and operational refinement -- to reduce complexity whilst preserving\nreconstruction quality. Architecturally, we replace traditional 3 x 3\nconvolutions with slimmer and more efficient layers, reducing complexity\nwithout compromising feature expressiveness. Operationally, we develop a\ntwo-stage adaptive channel pruning strategy: (1) soft pruning during training\nidentifies redundant channels via learnable thresholds, and (2) hard pruning\npermanently eliminates these channels post-training using a derived mask. This\ndual-phase approach ensures both training stability and inference efficiency.\nExperimental results demonstrate that the proposed lightweight dual-mode\noptimization for GFVC can achieve 90.4% parameter reduction and 88.9%\ncomputation saving compared to the baseline, whilst achieving superior\nperformance compared to state-of-the-art video coding standard Versatile Video\nCoding (VVC) in terms of perceptual-level quality metrics. As such, the\nproposed method is expected to enable efficient GFVC deployment in\nresource-constrained environments such as mobile edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Face Video Coding (GFVC) achieves superior rate-distortion\nperformance by leveraging the strong inference capabilities of deep generative\nmodels. However, its practical deployment is hindered by large model parameters\nand high computational costs. To address this, we propose a lightweight GFVC\nframework that introduces dual-mode optimization -- combining architectural\nredesign and operational refinement -- to reduce complexity whilst preserving\nreconstruction quality. Architecturally, we replace traditional 3 x 3\nconvolutions with slimmer and more efficient layers, reducing complexity\nwithout compromising feature expressiveness. Operationally, we develop a\ntwo-stage adaptive channel pruning strategy: (1) soft pruning during training\nidentifies redundant channels via learnable thresholds, and (2) hard pruning\npermanently eliminates these channels post-training using a derived mask. This\ndual-phase approach ensures both training stability and inference efficiency.\nExperimental results demonstrate that the proposed lightweight dual-mode\noptimization for GFVC can achieve 90.4% parameter reduction and 88.9%\ncomputation saving compared to the baseline, whilst achieving superior\nperformance compared to state-of-the-art video coding standard Versatile Video\nCoding (VVC) in terms of perceptual-level quality metrics. As such, the\nproposed method is expected to enable efficient GFVC deployment in\nresource-constrained environments such as mobile edge devices."
                },
                "authors": [
                    {
                        "name": "Zihan Zhang"
                    },
                    {
                        "name": "Shanzhi Yin"
                    },
                    {
                        "name": "Bolin Chen"
                    },
                    {
                        "name": "Ru-Ling Liao"
                    },
                    {
                        "name": "Shiqi Wang"
                    },
                    {
                        "name": "Yan Ye"
                    }
                ],
                "author_detail": {
                    "name": "Yan Ye"
                },
                "author": "Yan Ye",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13547v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13547v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13546v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13546v1",
                "updated": "2025-08-19T06:09:23Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    6,
                    9,
                    23,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T06:09:23Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    6,
                    9,
                    23,
                    1,
                    231,
                    0
                ],
                "title": "GazeProphet: Software-Only Gaze Prediction for VR Foveated Rendering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GazeProphet: Software-Only Gaze Prediction for VR Foveated Rendering"
                },
                "summary": "Foveated rendering significantly reduces computational demands in virtual\nreality applications by concentrating rendering quality where users focus their\ngaze. Current approaches require expensive hardware-based eye tracking systems,\nlimiting widespread adoption due to cost, calibration complexity, and hardware\ncompatibility constraints. This paper presents GazeProphet, a software-only\napproach for predicting gaze locations in VR environments without requiring\ndedicated eye tracking hardware. The approach combines a Spherical Vision\nTransformer for processing 360-degree VR scenes with an LSTM-based temporal\nencoder that captures gaze sequence patterns. A multi-modal fusion network\nintegrates spatial scene features with temporal gaze dynamics to predict future\ngaze locations with associated confidence estimates. Experimental evaluation on\na comprehensive VR dataset demonstrates that GazeProphet achieves a median\nangular error of 3.83 degrees, outperforming traditional saliency-based\nbaselines by 24% while providing reliable confidence calibration. The approach\nmaintains consistent performance across different spatial regions and scene\ntypes, enabling practical deployment in VR systems without additional hardware\nrequirements. Statistical analysis confirms the significance of improvements\nacross all evaluation metrics. These results show that software-only gaze\nprediction can work for VR foveated rendering, making this performance boost\nmore accessible to different VR platforms and apps.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foveated rendering significantly reduces computational demands in virtual\nreality applications by concentrating rendering quality where users focus their\ngaze. Current approaches require expensive hardware-based eye tracking systems,\nlimiting widespread adoption due to cost, calibration complexity, and hardware\ncompatibility constraints. This paper presents GazeProphet, a software-only\napproach for predicting gaze locations in VR environments without requiring\ndedicated eye tracking hardware. The approach combines a Spherical Vision\nTransformer for processing 360-degree VR scenes with an LSTM-based temporal\nencoder that captures gaze sequence patterns. A multi-modal fusion network\nintegrates spatial scene features with temporal gaze dynamics to predict future\ngaze locations with associated confidence estimates. Experimental evaluation on\na comprehensive VR dataset demonstrates that GazeProphet achieves a median\nangular error of 3.83 degrees, outperforming traditional saliency-based\nbaselines by 24% while providing reliable confidence calibration. The approach\nmaintains consistent performance across different spatial regions and scene\ntypes, enabling practical deployment in VR systems without additional hardware\nrequirements. Statistical analysis confirms the significance of improvements\nacross all evaluation metrics. These results show that software-only gaze\nprediction can work for VR foveated rendering, making this performance boost\nmore accessible to different VR platforms and apps."
                },
                "authors": [
                    {
                        "name": "Farhaan Ebadulla"
                    },
                    {
                        "name": "Chiraag Mudlapur"
                    },
                    {
                        "name": "Gaurav BV"
                    }
                ],
                "author_detail": {
                    "name": "Gaurav BV"
                },
                "author": "Gaurav BV",
                "arxiv_comment": "8 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13546v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13546v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07252v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07252v2",
                "updated": "2025-08-19T06:07:57Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    6,
                    7,
                    57,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-10T09:09:34Z",
                "published_parsed": [
                    2025,
                    8,
                    10,
                    9,
                    9,
                    34,
                    6,
                    222,
                    0
                ],
                "title": "Tasa: Thermal-aware 3D-Stacked Architecture Design with Bandwidth\n  Sharing for LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tasa: Thermal-aware 3D-Stacked Architecture Design with Bandwidth\n  Sharing for LLM Inference"
                },
                "summary": "The autoregressive decoding in LLMs is the major inference bottleneck due to\nthe memory-intensive operations and limited hardware bandwidth. 3D-stacked\narchitecture is a promising solution with significantly improved memory\nbandwidth, which vertically stacked multi DRAM dies on top of logic die.\nHowever, our experiments also show the 3D-stacked architecture faces severer\nthermal issues compared to 2D architecture, in terms of thermal temperature,\ngradient and scalability. To better exploit the potential of 3D-stacked\narchitecture, we present Tasa, a heterogeneous architecture with cross-stack\nthermal optimizations to balance the temperature distribution and maximize the\nperformance under the thermal constraints. High-performance core is designed\nfor compute-intensive operations, while high-efficiency core is used for\nmemory-intensive operators, e.g. attention layers. Furthermore, we propose a\nbandwidth sharing scheduling to improve the bandwidth utilization in such\nheterogeneous architecture. Extensive thermal experiments show that our Tasa\narchitecture demonstrates greater scalability compared with the homogeneous\n3D-stacked architecture, i.e. up to 5.55 $\\tccentigrade$, 9.37 $\\tccentigrade$,\nand 7.91 $\\tccentigrade$ peak temperature reduction for 48, 60, and 72 core\nconfigurations. Our experimental for Llama-65B and GPT-3 66B inferences also\ndemonstrate 2.85x and 2.21x speedup are obtained over the GPU baselines and\nstate-of-the-art heterogeneous PIM-based LLM accelerator",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The autoregressive decoding in LLMs is the major inference bottleneck due to\nthe memory-intensive operations and limited hardware bandwidth. 3D-stacked\narchitecture is a promising solution with significantly improved memory\nbandwidth, which vertically stacked multi DRAM dies on top of logic die.\nHowever, our experiments also show the 3D-stacked architecture faces severer\nthermal issues compared to 2D architecture, in terms of thermal temperature,\ngradient and scalability. To better exploit the potential of 3D-stacked\narchitecture, we present Tasa, a heterogeneous architecture with cross-stack\nthermal optimizations to balance the temperature distribution and maximize the\nperformance under the thermal constraints. High-performance core is designed\nfor compute-intensive operations, while high-efficiency core is used for\nmemory-intensive operators, e.g. attention layers. Furthermore, we propose a\nbandwidth sharing scheduling to improve the bandwidth utilization in such\nheterogeneous architecture. Extensive thermal experiments show that our Tasa\narchitecture demonstrates greater scalability compared with the homogeneous\n3D-stacked architecture, i.e. up to 5.55 $\\tccentigrade$, 9.37 $\\tccentigrade$,\nand 7.91 $\\tccentigrade$ peak temperature reduction for 48, 60, and 72 core\nconfigurations. Our experimental for Llama-65B and GPT-3 66B inferences also\ndemonstrate 2.85x and 2.21x speedup are obtained over the GPU baselines and\nstate-of-the-art heterogeneous PIM-based LLM accelerator"
                },
                "authors": [
                    {
                        "name": "Siyuan He"
                    },
                    {
                        "name": "Peiran Yan"
                    },
                    {
                        "name": "Yandong He"
                    },
                    {
                        "name": "Youwei Zhuo"
                    },
                    {
                        "name": "Tianyu Jia"
                    }
                ],
                "author_detail": {
                    "name": "Tianyu Jia"
                },
                "author": "Tianyu Jia",
                "arxiv_comment": "there are some data inaccuracies in section V",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07252v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07252v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07227v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07227v2",
                "updated": "2025-08-19T06:05:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    6,
                    5,
                    42,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-10T08:11:08Z",
                "published_parsed": [
                    2025,
                    8,
                    10,
                    8,
                    11,
                    8,
                    6,
                    222,
                    0
                ],
                "title": "LP-Spec: Leveraging LPDDR PIM for Efficient LLM Mobile Speculative\n  Inference with Architecture-Dataflow Co-Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LP-Spec: Leveraging LPDDR PIM for Efficient LLM Mobile Speculative\n  Inference with Architecture-Dataflow Co-Optimization"
                },
                "summary": "LLM inference on mobile devices faces extraneous challenges due to limited\nmemory bandwidth and computational resources. To address these issues,\nspeculative inference and processing-in-memory (PIM) techniques have been\nexplored at the algorithmic and hardware levels. However, speculative inference\nresults in more compute-intensive GEMM operations, creating new design\ntrade-offs for existing GEMV-accelerated PIM architectures. Furthermore, there\nexists a significant amount of redundant draft tokens in tree-based speculative\ninference, necessitating efficient token management schemes to minimize energy\nconsumption. In this work, we present LP-Spec, an architecture-dataflow\nco-design leveraging hybrid LPDDR5 performance-enhanced PIM architecture with\ndraft token pruning and dynamic workload scheduling to accelerate LLM\nspeculative inference. A near-data memory controller is proposed to enable data\nreallocation between DRAM and PIM banks. Furthermore, a data allocation unit\nbased on the hardware-aware draft token pruner is developed to minimize energy\nconsumption and fully exploit parallel execution opportunities. Compared to\nend-to-end LLM inference on other mobile solutions such as mobile NPUs or\nGEMV-accelerated PIMs, our LP-Spec achieves 13.21x, 7.56x, and 99.87x\nimprovements in performance, energy efficiency, and energy-delay-product (EDP).\nCompared with prior AttAcc PIM and RTX 3090 GPU, LP-Spec can obtain 12.83x and\n415.31x EDP reduction benefits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM inference on mobile devices faces extraneous challenges due to limited\nmemory bandwidth and computational resources. To address these issues,\nspeculative inference and processing-in-memory (PIM) techniques have been\nexplored at the algorithmic and hardware levels. However, speculative inference\nresults in more compute-intensive GEMM operations, creating new design\ntrade-offs for existing GEMV-accelerated PIM architectures. Furthermore, there\nexists a significant amount of redundant draft tokens in tree-based speculative\ninference, necessitating efficient token management schemes to minimize energy\nconsumption. In this work, we present LP-Spec, an architecture-dataflow\nco-design leveraging hybrid LPDDR5 performance-enhanced PIM architecture with\ndraft token pruning and dynamic workload scheduling to accelerate LLM\nspeculative inference. A near-data memory controller is proposed to enable data\nreallocation between DRAM and PIM banks. Furthermore, a data allocation unit\nbased on the hardware-aware draft token pruner is developed to minimize energy\nconsumption and fully exploit parallel execution opportunities. Compared to\nend-to-end LLM inference on other mobile solutions such as mobile NPUs or\nGEMV-accelerated PIMs, our LP-Spec achieves 13.21x, 7.56x, and 99.87x\nimprovements in performance, energy efficiency, and energy-delay-product (EDP).\nCompared with prior AttAcc PIM and RTX 3090 GPU, LP-Spec can obtain 12.83x and\n415.31x EDP reduction benefits."
                },
                "authors": [
                    {
                        "name": "Siyuan He"
                    },
                    {
                        "name": "Zhantong Zhu"
                    },
                    {
                        "name": "Yandong He"
                    },
                    {
                        "name": "Tianyu Jia"
                    }
                ],
                "author_detail": {
                    "name": "Tianyu Jia"
                },
                "author": "Tianyu Jia",
                "arxiv_comment": "there are some data inaccuracies in section III",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07227v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07227v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13543v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13543v1",
                "updated": "2025-08-19T06:05:23Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    6,
                    5,
                    23,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T06:05:23Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    6,
                    5,
                    23,
                    1,
                    231,
                    0
                ],
                "title": "\"Can You See Me Think?\" Grounding LLM Feedback in Keystrokes and\n  Revision Patterns",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"Can You See Me Think?\" Grounding LLM Feedback in Keystrokes and\n  Revision Patterns"
                },
                "summary": "As large language models (LLMs) increasingly assist in evaluating student\nwriting, researchers have begun questioning whether these models can be\ncognitively grounded, that is, whether they can attend not just to the final\nproduct, but to the process by which it was written. In this study, we explore\nhow incorporating writing process data, specifically keylogs and time-stamped\nsnapshots, affects the quality of LLM-generated feedback. We conduct an\nablation study on 52 student essays comparing feedback generated with access to\nonly the final essay (C1) and feedback that also incorporates keylogs and\ntime-stamped snapshots (C2). While rubric scores changed minimally, C2 feedback\ndemonstrated significantly improved structural evaluation and greater\nprocess-sensitive justification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) increasingly assist in evaluating student\nwriting, researchers have begun questioning whether these models can be\ncognitively grounded, that is, whether they can attend not just to the final\nproduct, but to the process by which it was written. In this study, we explore\nhow incorporating writing process data, specifically keylogs and time-stamped\nsnapshots, affects the quality of LLM-generated feedback. We conduct an\nablation study on 52 student essays comparing feedback generated with access to\nonly the final essay (C1) and feedback that also incorporates keylogs and\ntime-stamped snapshots (C2). While rubric scores changed minimally, C2 feedback\ndemonstrated significantly improved structural evaluation and greater\nprocess-sensitive justification."
                },
                "authors": [
                    {
                        "name": "Samra Zafar"
                    },
                    {
                        "name": "Shifa Yousaf"
                    },
                    {
                        "name": "Muhammad Shaheer Minhas"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Shaheer Minhas"
                },
                "author": "Muhammad Shaheer Minhas",
                "arxiv_comment": "15 pages, 4 figures, 6 tables, Submitted to IJCNLP-AACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13543v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13543v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13526v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13526v1",
                "updated": "2025-08-19T05:33:57Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    5,
                    33,
                    57,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T05:33:57Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    5,
                    33,
                    57,
                    1,
                    231,
                    0
                ],
                "title": "MATA (māta): Mindful Assessment of the Telugu Abilities of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MATA (māta): Mindful Assessment of the Telugu Abilities of Large\n  Language Models"
                },
                "summary": "In this paper, we introduce MATA, a novel evaluation dataset to assess the\nability of Large Language Models (LLMs) in Telugu language, comprising 729\ncarefully curated multiple-choice and open-ended questions that span diverse\nlinguistic dimensions. We evaluate 11 open-weight and closed-source LLMs on our\ndataset and present a fine-grained analysis of their performance. Further, we\nempirically show how LLMs rely on superficial heuristics such as answer\nposition and distractor patterns for multiple-choice questions. Finally, we\nalso compare LLM-as-a-judge evaluation with human evaluation for open-ended\nquestions and draw some conclusions on its reliability in a low-resource\nlanguage. We argue that such fine-grained evaluation is essential for\nunderstanding model limitations and can inform the development of more\nlinguistically capable LLMs, while also serving as a foundation for future\nresearch in Telugu NLP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce MATA, a novel evaluation dataset to assess the\nability of Large Language Models (LLMs) in Telugu language, comprising 729\ncarefully curated multiple-choice and open-ended questions that span diverse\nlinguistic dimensions. We evaluate 11 open-weight and closed-source LLMs on our\ndataset and present a fine-grained analysis of their performance. Further, we\nempirically show how LLMs rely on superficial heuristics such as answer\nposition and distractor patterns for multiple-choice questions. Finally, we\nalso compare LLM-as-a-judge evaluation with human evaluation for open-ended\nquestions and draw some conclusions on its reliability in a low-resource\nlanguage. We argue that such fine-grained evaluation is essential for\nunderstanding model limitations and can inform the development of more\nlinguistically capable LLMs, while also serving as a foundation for future\nresearch in Telugu NLP."
                },
                "authors": [
                    {
                        "name": "Chalamalasetti Kranti"
                    },
                    {
                        "name": "Sowmya Vajjala"
                    }
                ],
                "author_detail": {
                    "name": "Sowmya Vajjala"
                },
                "author": "Sowmya Vajjala",
                "arxiv_comment": "Pre-print",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13526v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13526v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13525v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13525v1",
                "updated": "2025-08-19T05:33:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    5,
                    33,
                    48,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T05:33:48Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    5,
                    33,
                    48,
                    1,
                    231,
                    0
                ],
                "title": "Saudi-Dialect-ALLaM: LoRA Fine-Tuning for Dialectal Arabic Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Saudi-Dialect-ALLaM: LoRA Fine-Tuning for Dialectal Arabic Generation"
                },
                "summary": "Large language models (LLMs) for Arabic are still dominated by Modern\nStandard Arabic (MSA), with limited support for Saudi dialects such as Najdi\nand Hijazi. This underrepresentation hinders their ability to capture authentic\ndialectal variation. Using a privately curated Saudi Dialect Instruction\ndataset (Hijazi and Najdi; 5,466 synthetic instruction-response pairs; 50/50\nsplit), we LoRA-tune ALLaM-7B-Instruct-preview, the first foundation model\ndeveloped in Saudi Arabia, for Saudi dialect generation. We investigate two\nvariants: (i) Dialect-Token training, which prepends an explicit dialect tag to\nthe instruction, and (ii) No-Token training, which omits the tag at formatting\ntime. Evaluation on a held-out test set combines an external dialect classifier\nwith text fidelity metrics (chrF++ and BERTScore) and diversity measures. The\nDialect-Token model achieves the best control, raising the Saudi rate from\n47.97% to 84.21% and reducing MSA leakage from 32.63% to 6.21%; fidelity also\nimproves (chrF++ +3.53, BERTScore +0.059). Both LoRA variants outperform strong\ngeneric instruction models (Falcon-7B-Instruct, Llama-3.1-8B-Instruct,\nQwen-2.5-7B-Instruct, AceGPT-v2-8B-Chat, JAIS-13B-Chat) in dialect control and\nfidelity, while avoiding metadata-tag echoing that these baselines frequently\nexhibit. We do not release the dataset or any model weights/adapters; instead,\nwe release training/evaluation/inference code and a detailed datasheet (schema\nand aggregate statistics) to support independent verification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) for Arabic are still dominated by Modern\nStandard Arabic (MSA), with limited support for Saudi dialects such as Najdi\nand Hijazi. This underrepresentation hinders their ability to capture authentic\ndialectal variation. Using a privately curated Saudi Dialect Instruction\ndataset (Hijazi and Najdi; 5,466 synthetic instruction-response pairs; 50/50\nsplit), we LoRA-tune ALLaM-7B-Instruct-preview, the first foundation model\ndeveloped in Saudi Arabia, for Saudi dialect generation. We investigate two\nvariants: (i) Dialect-Token training, which prepends an explicit dialect tag to\nthe instruction, and (ii) No-Token training, which omits the tag at formatting\ntime. Evaluation on a held-out test set combines an external dialect classifier\nwith text fidelity metrics (chrF++ and BERTScore) and diversity measures. The\nDialect-Token model achieves the best control, raising the Saudi rate from\n47.97% to 84.21% and reducing MSA leakage from 32.63% to 6.21%; fidelity also\nimproves (chrF++ +3.53, BERTScore +0.059). Both LoRA variants outperform strong\ngeneric instruction models (Falcon-7B-Instruct, Llama-3.1-8B-Instruct,\nQwen-2.5-7B-Instruct, AceGPT-v2-8B-Chat, JAIS-13B-Chat) in dialect control and\nfidelity, while avoiding metadata-tag echoing that these baselines frequently\nexhibit. We do not release the dataset or any model weights/adapters; instead,\nwe release training/evaluation/inference code and a detailed datasheet (schema\nand aggregate statistics) to support independent verification."
                },
                "authors": [
                    {
                        "name": "Hassan Barmandah"
                    }
                ],
                "author_detail": {
                    "name": "Hassan Barmandah"
                },
                "author": "Hassan Barmandah",
                "arxiv_comment": "7 pages, 6 figures, 2 tables. Code:\n  https://github.com/HasanBGIt/Saudi-Dialect-ALLaM . Dataset and trained\n  weights/adapters are not released. Primary category: cs.CL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13525v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13525v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13524v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13524v1",
                "updated": "2025-08-19T05:33:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    5,
                    33,
                    10,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T05:33:10Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    5,
                    33,
                    10,
                    1,
                    231,
                    0
                ],
                "title": "Evaluating Open-Source Vision Language Models for Facial Emotion\n  Recognition against Traditional Deep Learning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Open-Source Vision Language Models for Facial Emotion\n  Recognition against Traditional Deep Learning Models"
                },
                "summary": "Facial Emotion Recognition (FER) is crucial for applications such as\nhuman-computer interaction and mental health diagnostics. This study presents\nthe first empirical comparison of open-source Vision-Language Models (VLMs),\nincluding Phi-3.5 Vision and CLIP, against traditional deep learning models\nVGG19, ResNet-50, and EfficientNet-B0 on the challenging FER-2013 dataset,\nwhich contains 35,887 low-resolution grayscale images across seven emotion\nclasses. To address the mismatch between VLM training assumptions and the noisy\nnature of FER data, we introduce a novel pipeline that integrates GFPGAN-based\nimage restoration with FER evaluation. Results show that traditional models,\nparticularly EfficientNet-B0 (86.44%) and ResNet-50 (85.72%), significantly\noutperform VLMs like CLIP (64.07%) and Phi-3.5 Vision (51.66%), highlighting\nthe limitations of VLMs in low-quality visual tasks. In addition to performance\nevaluation using precision, recall, F1-score, and accuracy, we provide a\ndetailed computational cost analysis covering preprocessing, training,\ninference, and evaluation phases, offering practical insights for deployment.\nThis work underscores the need for adapting VLMs to noisy environments and\nprovides a reproducible benchmark for future research in emotion recognition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Facial Emotion Recognition (FER) is crucial for applications such as\nhuman-computer interaction and mental health diagnostics. This study presents\nthe first empirical comparison of open-source Vision-Language Models (VLMs),\nincluding Phi-3.5 Vision and CLIP, against traditional deep learning models\nVGG19, ResNet-50, and EfficientNet-B0 on the challenging FER-2013 dataset,\nwhich contains 35,887 low-resolution grayscale images across seven emotion\nclasses. To address the mismatch between VLM training assumptions and the noisy\nnature of FER data, we introduce a novel pipeline that integrates GFPGAN-based\nimage restoration with FER evaluation. Results show that traditional models,\nparticularly EfficientNet-B0 (86.44%) and ResNet-50 (85.72%), significantly\noutperform VLMs like CLIP (64.07%) and Phi-3.5 Vision (51.66%), highlighting\nthe limitations of VLMs in low-quality visual tasks. In addition to performance\nevaluation using precision, recall, F1-score, and accuracy, we provide a\ndetailed computational cost analysis covering preprocessing, training,\ninference, and evaluation phases, offering practical insights for deployment.\nThis work underscores the need for adapting VLMs to noisy environments and\nprovides a reproducible benchmark for future research in emotion recognition."
                },
                "authors": [
                    {
                        "name": "Vamsi Krishna Mulukutla"
                    },
                    {
                        "name": "Sai Supriya Pavarala"
                    },
                    {
                        "name": "Srinivasa Raju Rudraraju"
                    },
                    {
                        "name": "Sridevi Bonthu"
                    }
                ],
                "author_detail": {
                    "name": "Sridevi Bonthu"
                },
                "author": "Sridevi Bonthu",
                "arxiv_doi": "10.4108/airo.8870",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.4108/airo.8870",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.13524v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13524v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "EAI Endorsed Transactions on AI and Robotics, 4, August 2025",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.12393v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.12393v2",
                "updated": "2025-08-19T05:18:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    5,
                    18,
                    31,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-17T15:14:03Z",
                "published_parsed": [
                    2025,
                    8,
                    17,
                    15,
                    14,
                    3,
                    6,
                    229,
                    0
                ],
                "title": "MedKGent: A Large Language Model Agent Framework for Constructing\n  Temporally Evolving Medical Knowledge Graph",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MedKGent: A Large Language Model Agent Framework for Constructing\n  Temporally Evolving Medical Knowledge Graph"
                },
                "summary": "The rapid expansion of medical literature presents growing challenges for\nstructuring and integrating domain knowledge at scale. Knowledge Graphs (KGs)\noffer a promising solution by enabling efficient retrieval, automated\nreasoning, and knowledge discovery. However, current KG construction methods\noften rely on supervised pipelines with limited generalizability or naively\naggregate outputs from Large Language Models (LLMs), treating biomedical\ncorpora as static and ignoring the temporal dynamics and contextual uncertainty\nof evolving knowledge. To address these limitations, we introduce MedKGent, a\nLLM agent framework for constructing temporally evolving medical KGs.\nLeveraging over 10 million PubMed abstracts published between 1975 and 2023, we\nsimulate the emergence of biomedical knowledge via a fine-grained daily time\nseries. MedKGent incrementally builds the KG in a day-by-day manner using two\nspecialized agents powered by the Qwen2.5-32B-Instruct model. The Extractor\nAgent identifies knowledge triples and assigns confidence scores via\nsampling-based estimation, which are used to filter low-confidence extractions\nand inform downstream processing. The Constructor Agent incrementally\nintegrates the retained triples into a temporally evolving graph, guided by\nconfidence scores and timestamps to reinforce recurring knowledge and resolve\nconflicts. The resulting KG contains 156,275 entities and 2,971,384 relational\ntriples. Quality assessments by two SOTA LLMs and three domain experts\ndemonstrate an accuracy approaching 90%, with strong inter-rater agreement. To\nevaluate downstream utility, we conduct RAG across seven medical question\nanswering benchmarks using five leading LLMs, consistently observing\nsignificant improvements over non-augmented baselines. Case studies further\ndemonstrate the KG's value in literature-based drug repurposing via\nconfidence-aware causal inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid expansion of medical literature presents growing challenges for\nstructuring and integrating domain knowledge at scale. Knowledge Graphs (KGs)\noffer a promising solution by enabling efficient retrieval, automated\nreasoning, and knowledge discovery. However, current KG construction methods\noften rely on supervised pipelines with limited generalizability or naively\naggregate outputs from Large Language Models (LLMs), treating biomedical\ncorpora as static and ignoring the temporal dynamics and contextual uncertainty\nof evolving knowledge. To address these limitations, we introduce MedKGent, a\nLLM agent framework for constructing temporally evolving medical KGs.\nLeveraging over 10 million PubMed abstracts published between 1975 and 2023, we\nsimulate the emergence of biomedical knowledge via a fine-grained daily time\nseries. MedKGent incrementally builds the KG in a day-by-day manner using two\nspecialized agents powered by the Qwen2.5-32B-Instruct model. The Extractor\nAgent identifies knowledge triples and assigns confidence scores via\nsampling-based estimation, which are used to filter low-confidence extractions\nand inform downstream processing. The Constructor Agent incrementally\nintegrates the retained triples into a temporally evolving graph, guided by\nconfidence scores and timestamps to reinforce recurring knowledge and resolve\nconflicts. The resulting KG contains 156,275 entities and 2,971,384 relational\ntriples. Quality assessments by two SOTA LLMs and three domain experts\ndemonstrate an accuracy approaching 90%, with strong inter-rater agreement. To\nevaluate downstream utility, we conduct RAG across seven medical question\nanswering benchmarks using five leading LLMs, consistently observing\nsignificant improvements over non-augmented baselines. Case studies further\ndemonstrate the KG's value in literature-based drug repurposing via\nconfidence-aware causal inference."
                },
                "authors": [
                    {
                        "name": "Duzhen Zhang"
                    },
                    {
                        "name": "Zixiao Wang"
                    },
                    {
                        "name": "Zhong-Zhi Li"
                    },
                    {
                        "name": "Yahan Yu"
                    },
                    {
                        "name": "Shuncheng Jia"
                    },
                    {
                        "name": "Jiahua Dong"
                    },
                    {
                        "name": "Haotian Xu"
                    },
                    {
                        "name": "Xing Wu"
                    },
                    {
                        "name": "Yingying Zhang"
                    },
                    {
                        "name": "Tielin Zhang"
                    },
                    {
                        "name": "Jie Yang"
                    },
                    {
                        "name": "Xiuying Chen"
                    },
                    {
                        "name": "Le Song"
                    }
                ],
                "author_detail": {
                    "name": "Le Song"
                },
                "author": "Le Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.12393v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.12393v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05668v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05668v3",
                "updated": "2025-08-19T05:15:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    5,
                    15,
                    19,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-03T08:02:51Z",
                "published_parsed": [
                    2025,
                    8,
                    3,
                    8,
                    2,
                    51,
                    6,
                    215,
                    0
                ],
                "title": "A Survey of LLM-based Deep Search Agents: Paradigm, Optimization,\n  Evaluation, and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of LLM-based Deep Search Agents: Paradigm, Optimization,\n  Evaluation, and Challenges"
                },
                "summary": "The advent of Large Language Models (LLMs) has significantly revolutionized\nweb search. The emergence of LLM-based Search Agents marks a pivotal shift\ntowards deeper, dynamic, autonomous information seeking. These agents can\ncomprehend user intentions and environmental context and execute multi-turn\nretrieval with dynamic planning, extending search capabilities far beyond the\nweb. Leading examples like OpenAI's Deep Research highlight their potential for\ndeep information mining and real-world applications. This survey provides the\nfirst systematic analysis of search agents. We comprehensively analyze and\ncategorize existing works from the perspectives of architecture, optimization,\napplication, and evaluation, ultimately identifying critical open challenges\nand outlining promising future research directions in this rapidly evolving\nfield. Our repository is available on\nhttps://github.com/YunjiaXi/Awesome-Search-Agent-Papers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of Large Language Models (LLMs) has significantly revolutionized\nweb search. The emergence of LLM-based Search Agents marks a pivotal shift\ntowards deeper, dynamic, autonomous information seeking. These agents can\ncomprehend user intentions and environmental context and execute multi-turn\nretrieval with dynamic planning, extending search capabilities far beyond the\nweb. Leading examples like OpenAI's Deep Research highlight their potential for\ndeep information mining and real-world applications. This survey provides the\nfirst systematic analysis of search agents. We comprehensively analyze and\ncategorize existing works from the perspectives of architecture, optimization,\napplication, and evaluation, ultimately identifying critical open challenges\nand outlining promising future research directions in this rapidly evolving\nfield. Our repository is available on\nhttps://github.com/YunjiaXi/Awesome-Search-Agent-Papers."
                },
                "authors": [
                    {
                        "name": "Yunjia Xi"
                    },
                    {
                        "name": "Jianghao Lin"
                    },
                    {
                        "name": "Yongzhao Xiao"
                    },
                    {
                        "name": "Zheli Zhou"
                    },
                    {
                        "name": "Rong Shan"
                    },
                    {
                        "name": "Te Gao"
                    },
                    {
                        "name": "Jiachen Zhu"
                    },
                    {
                        "name": "Weiwen Liu"
                    },
                    {
                        "name": "Yong Yu"
                    },
                    {
                        "name": "Weinan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Weinan Zhang"
                },
                "author": "Weinan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05668v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05668v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13514v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13514v1",
                "updated": "2025-08-19T05:01:40Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    5,
                    1,
                    40,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T05:01:40Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    5,
                    1,
                    40,
                    1,
                    231,
                    0
                ],
                "title": "ProMed: Shapley Information Gain Guided Reinforcement Learning for\n  Proactive Medical LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProMed: Shapley Information Gain Guided Reinforcement Learning for\n  Proactive Medical LLMs"
                },
                "summary": "Interactive medical questioning is essential in real-world clinical\nconsultations, where physicians must actively gather information from patients.\nWhile medical Large Language Models (LLMs) have shown impressive capabilities\nin static medical question answering, they predominantly operate under a\nreactive paradigm: generating answers directly without seeking additional\ninformation, which risks incorrect diagnoses in such interactive settings. To\naddress this limitation, we propose ProMed, a reinforcement learning (RL)\nframework that transitions medical LLMs toward a proactive paradigm, equipping\nthem with the ability to ask clinically valuable questions before\ndecision-making. At the core of ProMed is the Shapley Information Gain (SIG)\nreward, which quantifies the clinical utility of each question by combining the\namount of newly acquired information with its contextual importance, estimated\nvia Shapley values. We integrate SIG into a two-stage training pipeline: (1)\nSIG-Guided Model Initialization uses Monte Carlo Tree Search (MCTS) to\nconstruct high-reward interaction trajectories to supervise the model, and (2)\nSIG-Augmented Policy Optimization, which integrates SIG and enhances RL with a\nnovel SIG-guided Reward Distribution Mechanism that assigns higher rewards to\ninformative questions for targeted optimization. Extensive experiments on two\nnewly curated partial-information medical benchmarks demonstrate that ProMed\nsignificantly outperforms state-of-the-art methods by an average of 6.29% and\ndelivers a 54.45% gain over the reactive paradigm, while also generalizing\nrobustly to out-of-domain cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interactive medical questioning is essential in real-world clinical\nconsultations, where physicians must actively gather information from patients.\nWhile medical Large Language Models (LLMs) have shown impressive capabilities\nin static medical question answering, they predominantly operate under a\nreactive paradigm: generating answers directly without seeking additional\ninformation, which risks incorrect diagnoses in such interactive settings. To\naddress this limitation, we propose ProMed, a reinforcement learning (RL)\nframework that transitions medical LLMs toward a proactive paradigm, equipping\nthem with the ability to ask clinically valuable questions before\ndecision-making. At the core of ProMed is the Shapley Information Gain (SIG)\nreward, which quantifies the clinical utility of each question by combining the\namount of newly acquired information with its contextual importance, estimated\nvia Shapley values. We integrate SIG into a two-stage training pipeline: (1)\nSIG-Guided Model Initialization uses Monte Carlo Tree Search (MCTS) to\nconstruct high-reward interaction trajectories to supervise the model, and (2)\nSIG-Augmented Policy Optimization, which integrates SIG and enhances RL with a\nnovel SIG-guided Reward Distribution Mechanism that assigns higher rewards to\ninformative questions for targeted optimization. Extensive experiments on two\nnewly curated partial-information medical benchmarks demonstrate that ProMed\nsignificantly outperforms state-of-the-art methods by an average of 6.29% and\ndelivers a 54.45% gain over the reactive paradigm, while also generalizing\nrobustly to out-of-domain cases."
                },
                "authors": [
                    {
                        "name": "Hongxin Ding"
                    },
                    {
                        "name": "Baixiang Huang"
                    },
                    {
                        "name": "Yue Fang"
                    },
                    {
                        "name": "Weibin Liao"
                    },
                    {
                        "name": "Xinke Jiang"
                    },
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Junfeng Zhao"
                    },
                    {
                        "name": "Yasha Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yasha Wang"
                },
                "author": "Yasha Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13514v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13514v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06723v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06723v2",
                "updated": "2025-08-19T04:43:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    4,
                    43,
                    39,
                    1,
                    231,
                    0
                ],
                "published": "2024-11-11T05:14:14Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    5,
                    14,
                    14,
                    0,
                    316,
                    0
                ],
                "title": "Script-Strategy Aligned Generation: Aligning LLMs with Expert-Crafted\n  Dialogue Scripts and Therapeutic Strategies for Psychotherapy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Script-Strategy Aligned Generation: Aligning LLMs with Expert-Crafted\n  Dialogue Scripts and Therapeutic Strategies for Psychotherapy"
                },
                "summary": "Chatbots or conversational agents (CAs) are increasingly used to improve\naccess to digital psychotherapy. Many current systems rely on rigid, rule-based\ndesigns, heavily dependent on expert-crafted dialogue scripts for guiding\ntherapeutic conversations. Although advances in large language models (LLMs)\noffer potential for more flexible interactions, their lack of controllability\nand explanability poses challenges in high-stakes contexts like psychotherapy.\nTo address this, we conducted two studies in this work to explore how aligning\nLLMs with expert-crafted scripts can enhance psychotherapeutic chatbot\nperformance. In Study 1 (N=43), an online experiment with a within-subjects\ndesign, we compared rule-based, pure LLM, and LLMs aligned with expert-crafted\nscripts via fine-tuning and prompting. Results showed that aligned LLMs\nsignificantly outperformed the other types of chatbots in empathy, dialogue\nrelevance, and adherence to therapeutic principles. Building on findings, we\nproposed ``Script-Strategy Aligned Generation (SSAG)'', a more flexible\nalignment approach that reduces reliance on fully scripted content while\nmaintaining LLMs' therapeutic adherence and controllability. In a 10-day field\nStudy 2 (N=21), SSAG achieved comparable therapeutic effectiveness to\nfull-scripted LLMs while requiring less than 40\\% of expert-crafted dialogue\ncontent. Beyond these results, this work advances LLM applications in\npsychotherapy by providing a controllable and scalable solution, reducing\nreliance on expert effort. By enabling domain experts to align LLMs through\nhigh-level strategies rather than full scripts, SSAG supports more efficient\nco-development and expands access to a broader context of psychotherapy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chatbots or conversational agents (CAs) are increasingly used to improve\naccess to digital psychotherapy. Many current systems rely on rigid, rule-based\ndesigns, heavily dependent on expert-crafted dialogue scripts for guiding\ntherapeutic conversations. Although advances in large language models (LLMs)\noffer potential for more flexible interactions, their lack of controllability\nand explanability poses challenges in high-stakes contexts like psychotherapy.\nTo address this, we conducted two studies in this work to explore how aligning\nLLMs with expert-crafted scripts can enhance psychotherapeutic chatbot\nperformance. In Study 1 (N=43), an online experiment with a within-subjects\ndesign, we compared rule-based, pure LLM, and LLMs aligned with expert-crafted\nscripts via fine-tuning and prompting. Results showed that aligned LLMs\nsignificantly outperformed the other types of chatbots in empathy, dialogue\nrelevance, and adherence to therapeutic principles. Building on findings, we\nproposed ``Script-Strategy Aligned Generation (SSAG)'', a more flexible\nalignment approach that reduces reliance on fully scripted content while\nmaintaining LLMs' therapeutic adherence and controllability. In a 10-day field\nStudy 2 (N=21), SSAG achieved comparable therapeutic effectiveness to\nfull-scripted LLMs while requiring less than 40\\% of expert-crafted dialogue\ncontent. Beyond these results, this work advances LLM applications in\npsychotherapy by providing a controllable and scalable solution, reducing\nreliance on expert effort. By enabling domain experts to align LLMs through\nhigh-level strategies rather than full scripts, SSAG supports more efficient\nco-development and expands access to a broader context of psychotherapy."
                },
                "authors": [
                    {
                        "name": "Xin Sun"
                    },
                    {
                        "name": "Jan de Wit"
                    },
                    {
                        "name": "Zhuying Li"
                    },
                    {
                        "name": "Jiahuan Pei"
                    },
                    {
                        "name": "Abdallah El Ali"
                    },
                    {
                        "name": "Jos A. Bosch"
                    }
                ],
                "author_detail": {
                    "name": "Jos A. Bosch"
                },
                "author": "Jos A. Bosch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06723v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06723v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]