[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2501.11745v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11745v2",
                "updated": "2025-01-22T16:25:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    16,
                    25,
                    47,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-20T21:07:44Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    21,
                    7,
                    44,
                    0,
                    20,
                    0
                ],
                "title": "Personalized Federated Learning for Cellular VR: Online Learning and\n  Dynamic Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized Federated Learning for Cellular VR: Online Learning and\n  Dynamic Caching"
                },
                "summary": "Delivering an immersive experience to virtual reality (VR) users through\nwireless connectivity offers the freedom to engage from anywhere at any time.\nNevertheless, it is challenging to ensure seamless wireless connectivity that\ndelivers real-time and high-quality videos to the VR users. This paper proposes\na field of view (FoV) aware caching for mobile edge computing (MEC)-enabled\nwireless VR network. In particular, the FoV of each VR user is\ncached/prefetched at the base stations (BSs) based on the caching strategies\ntailored to each BS. Specifically, decentralized and personalized federated\nlearning (DP-FL) based caching strategies with guarantees are presented.\nConsidering VR systems composed of multiple VR devices and BSs, a DP-FL caching\nalgorithm is implemented at each BS to personalize content delivery for VR\nusers. The utilized DP-FL algorithm guarantees a probably approximately correct\n(PAC) bound on the conditional average cache hit. Further, to reduce the cost\nof communicating gradients, one-bit quantization of the stochastic gradient\ndescent (OBSGD) is proposed, and a convergence guarantee of\n$\\mathcal{O}(1/\\sqrt{T})$ is obtained for the proposed algorithm, where $T$ is\nthe number of iterations. Additionally, to better account for the wireless\nchannel dynamics, the FoVs are grouped into multicast or unicast groups based\non the number of requesting VR users. The performance of the proposed DP-FL\nalgorithm is validated through realistic VR head-tracking dataset, and the\nproposed algorithm is shown to have better performance in terms of average\ndelay and cache hit as compared to baseline algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Delivering an immersive experience to virtual reality (VR) users through\nwireless connectivity offers the freedom to engage from anywhere at any time.\nNevertheless, it is challenging to ensure seamless wireless connectivity that\ndelivers real-time and high-quality videos to the VR users. This paper proposes\na field of view (FoV) aware caching for mobile edge computing (MEC)-enabled\nwireless VR network. In particular, the FoV of each VR user is\ncached/prefetched at the base stations (BSs) based on the caching strategies\ntailored to each BS. Specifically, decentralized and personalized federated\nlearning (DP-FL) based caching strategies with guarantees are presented.\nConsidering VR systems composed of multiple VR devices and BSs, a DP-FL caching\nalgorithm is implemented at each BS to personalize content delivery for VR\nusers. The utilized DP-FL algorithm guarantees a probably approximately correct\n(PAC) bound on the conditional average cache hit. Further, to reduce the cost\nof communicating gradients, one-bit quantization of the stochastic gradient\ndescent (OBSGD) is proposed, and a convergence guarantee of\n$\\mathcal{O}(1/\\sqrt{T})$ is obtained for the proposed algorithm, where $T$ is\nthe number of iterations. Additionally, to better account for the wireless\nchannel dynamics, the FoVs are grouped into multicast or unicast groups based\non the number of requesting VR users. The performance of the proposed DP-FL\nalgorithm is validated through realistic VR head-tracking dataset, and the\nproposed algorithm is shown to have better performance in terms of average\ndelay and cache hit as compared to baseline algorithms."
                },
                "authors": [
                    {
                        "name": "Krishnendu S. Tharakan"
                    },
                    {
                        "name": "Hayssam Dahrouj"
                    },
                    {
                        "name": "Nour Kouzayha"
                    },
                    {
                        "name": "Hesham ElSawy"
                    },
                    {
                        "name": "Tareq Y. Al-Naffouri"
                    }
                ],
                "author_detail": {
                    "name": "Tareq Y. Al-Naffouri"
                },
                "author": "Tareq Y. Al-Naffouri",
                "arxiv_comment": "accepted for publication in IEEE Transactions on Communications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11745v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11745v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12959v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12959v1",
                "updated": "2025-01-22T15:33:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    15,
                    33,
                    17,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T15:33:17Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    15,
                    33,
                    17,
                    2,
                    22,
                    0
                ],
                "title": "Efficient Prompt Compression with Evaluator Heads for Long-Context\n  Transformer Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Prompt Compression with Evaluator Heads for Long-Context\n  Transformer Inference"
                },
                "summary": "Although applications involving long-context inputs are crucial for the\neffective utilization of large language models (LLMs), they also result in\nincreased computational costs and reduced performance. To address this\nchallenge, we propose an efficient, training-free prompt compression method\nthat retains key information within compressed prompts. We identify specific\nattention heads in transformer-based LLMs, which we designate as evaluator\nheads, that are capable of selecting tokens in long inputs that are most\nsignificant for inference. Building on this discovery, we develop EHPC, an\nEvaluator Head-based Prompt Compression method, which enables LLMs to rapidly\n\"skim through\" input prompts by leveraging only the first few layers with\nevaluator heads during the pre-filling stage, subsequently passing only the\nimportant tokens to the model for inference. EHPC achieves state-of-the-art\nresults across two mainstream benchmarks: prompt compression and long-context\ninference acceleration. Consequently, it effectively reduces the complexity and\ncosts associated with commercial API calls. We further demonstrate that EHPC\nattains competitive results compared to key-value cache-based acceleration\nmethods, thereby highlighting its potential to enhance the efficiency of LLMs\nfor long-context tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although applications involving long-context inputs are crucial for the\neffective utilization of large language models (LLMs), they also result in\nincreased computational costs and reduced performance. To address this\nchallenge, we propose an efficient, training-free prompt compression method\nthat retains key information within compressed prompts. We identify specific\nattention heads in transformer-based LLMs, which we designate as evaluator\nheads, that are capable of selecting tokens in long inputs that are most\nsignificant for inference. Building on this discovery, we develop EHPC, an\nEvaluator Head-based Prompt Compression method, which enables LLMs to rapidly\n\"skim through\" input prompts by leveraging only the first few layers with\nevaluator heads during the pre-filling stage, subsequently passing only the\nimportant tokens to the model for inference. EHPC achieves state-of-the-art\nresults across two mainstream benchmarks: prompt compression and long-context\ninference acceleration. Consequently, it effectively reduces the complexity and\ncosts associated with commercial API calls. We further demonstrate that EHPC\nattains competitive results compared to key-value cache-based acceleration\nmethods, thereby highlighting its potential to enhance the efficiency of LLMs\nfor long-context tasks."
                },
                "authors": [
                    {
                        "name": "Weizhi Fei"
                    },
                    {
                        "name": "Xueyan Niu"
                    },
                    {
                        "name": "Guoqing Xie"
                    },
                    {
                        "name": "Yingqing Liu"
                    },
                    {
                        "name": "Bo Bai"
                    },
                    {
                        "name": "Wei Han"
                    }
                ],
                "author_detail": {
                    "name": "Wei Han"
                },
                "author": "Wei Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12959v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12959v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01253v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01253v5",
                "updated": "2025-01-22T15:09:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    15,
                    9,
                    58,
                    2,
                    22,
                    0
                ],
                "published": "2024-12-02T08:22:56Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    8,
                    22,
                    56,
                    0,
                    337,
                    0
                ],
                "title": "Yi-Lightning Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yi-Lightning Technical Report"
                },
                "summary": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com."
                },
                "authors": [
                    {
                        "name": "Alan Wake"
                    },
                    {
                        "name": "Bei Chen"
                    },
                    {
                        "name": "C. X. Lv"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Chengen Huang"
                    },
                    {
                        "name": "Chenglin Cai"
                    },
                    {
                        "name": "Chujie Zheng"
                    },
                    {
                        "name": "Daniel Cooper"
                    },
                    {
                        "name": "Fan Zhou"
                    },
                    {
                        "name": "Feng Hu"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Guoyin Wang"
                    },
                    {
                        "name": "Heng Ji"
                    },
                    {
                        "name": "Howard Qiu"
                    },
                    {
                        "name": "Jiangcheng Zhu"
                    },
                    {
                        "name": "Jun Tian"
                    },
                    {
                        "name": "Katherine Su"
                    },
                    {
                        "name": "Lihuan Zhang"
                    },
                    {
                        "name": "Liying Li"
                    },
                    {
                        "name": "Ming Song"
                    },
                    {
                        "name": "Mou Li"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Qicheng Hu"
                    },
                    {
                        "name": "Shawn Wang"
                    },
                    {
                        "name": "Shijun Zhou"
                    },
                    {
                        "name": "Shiming Yang"
                    },
                    {
                        "name": "Shiyong Li"
                    },
                    {
                        "name": "Tianhang Zhu"
                    },
                    {
                        "name": "Wen Xie"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Xiang He"
                    },
                    {
                        "name": "Xiaobo Chen"
                    },
                    {
                        "name": "Xiaohui Hu"
                    },
                    {
                        "name": "Xiaoyi Ren"
                    },
                    {
                        "name": "Xinyao Niu"
                    },
                    {
                        "name": "Yanpeng Li"
                    },
                    {
                        "name": "Yongke Zhao"
                    },
                    {
                        "name": "Yongzhen Luo"
                    },
                    {
                        "name": "Yuchi Xu"
                    },
                    {
                        "name": "Yuxuan Sha"
                    },
                    {
                        "name": "Zhaodong Yan"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Zirui Zhang"
                    },
                    {
                        "name": "Zonghong Dai"
                    }
                ],
                "author_detail": {
                    "name": "Zonghong Dai"
                },
                "author": "Zonghong Dai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01253v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01253v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.08894v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.08894v3",
                "updated": "2025-01-22T15:05:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    15,
                    5,
                    8,
                    2,
                    22,
                    0
                ],
                "published": "2023-10-13T06:58:07Z",
                "published_parsed": [
                    2023,
                    10,
                    13,
                    6,
                    58,
                    7,
                    4,
                    286,
                    0
                ],
                "title": "Multi-Antenna Coded Caching for Multi-Access Networks with Cyclic\n  Wrap-Around",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Antenna Coded Caching for Multi-Access Networks with Cyclic\n  Wrap-Around"
                },
                "summary": "This work explores a multiple transmit antenna setting in a multi-access\ncoded caching (MACC) network where each user accesses more than one cache. A\nMACC network has $K$ users and $K$ caches, and each user has access to $r < K$\nconsecutive caches in a cyclic wrap-around manner. There are $L$ antennas at\nthe server, and each cache has a normalized size of $M/N \\leq 1$. The cyclic\nwrap-around MACC network with a single antenna at the server has been a\nwell-investigated topic, and several coded caching schemes and improved lower\nbounds on the performance are known for the same. However, this MACC network\nhas not yet been studied under multi-antenna settings in the coded caching\nliterature. We study the multi-antenna MACC problem and propose a solution for\nthe same by constructing a pair of arrays called caching and delivery arrays.\nWe present three constructions of caching and delivery arrays for different\nscenarios and obtain corresponding multi-antenna MACC schemes for the same. Two\nschemes resulting from the above constructions achieve optimal performance\nunder uncoded placement and one-shot delivery. The optimality is shown by\nmatching the performance of the multi-antenna MACC scheme to that of an optimal\nmulti-antenna scheme for a dedicated cache network having an identical number\nof users, and each user has a normalized cache size of $rM/N$. Further, as a\nspecial case, one of the proposed schemes subsumes an existing optimal MACC\nscheme for the single-antenna setting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work explores a multiple transmit antenna setting in a multi-access\ncoded caching (MACC) network where each user accesses more than one cache. A\nMACC network has $K$ users and $K$ caches, and each user has access to $r < K$\nconsecutive caches in a cyclic wrap-around manner. There are $L$ antennas at\nthe server, and each cache has a normalized size of $M/N \\leq 1$. The cyclic\nwrap-around MACC network with a single antenna at the server has been a\nwell-investigated topic, and several coded caching schemes and improved lower\nbounds on the performance are known for the same. However, this MACC network\nhas not yet been studied under multi-antenna settings in the coded caching\nliterature. We study the multi-antenna MACC problem and propose a solution for\nthe same by constructing a pair of arrays called caching and delivery arrays.\nWe present three constructions of caching and delivery arrays for different\nscenarios and obtain corresponding multi-antenna MACC schemes for the same. Two\nschemes resulting from the above constructions achieve optimal performance\nunder uncoded placement and one-shot delivery. The optimality is shown by\nmatching the performance of the multi-antenna MACC scheme to that of an optimal\nmulti-antenna scheme for a dedicated cache network having an identical number\nof users, and each user has a normalized cache size of $rM/N$. Further, as a\nspecial case, one of the proposed schemes subsumes an existing optimal MACC\nscheme for the single-antenna setting."
                },
                "authors": [
                    {
                        "name": "Elizabath Peter"
                    },
                    {
                        "name": "K. K. Krishnan Namboodiri"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "To appear in IEEE Transactions on Communications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.08894v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.08894v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03940v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03940v2",
                "updated": "2025-01-22T10:39:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    10,
                    39,
                    50,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-07T17:00:49Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    0,
                    49,
                    1,
                    7,
                    0
                ],
                "title": "Not all tokens are created equal: Perplexity Attention Weighted Networks\n  for AI generated text detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not all tokens are created equal: Perplexity Attention Weighted Networks\n  for AI generated text detection"
                },
                "summary": "The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages."
                },
                "authors": [
                    {
                        "name": "Pablo Miralles-González"
                    },
                    {
                        "name": "Javier Huertas-Tato"
                    },
                    {
                        "name": "Alejandro Martín"
                    },
                    {
                        "name": "David Camacho"
                    }
                ],
                "author_detail": {
                    "name": "David Camacho"
                },
                "author": "David Camacho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03940v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03940v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12744v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12744v1",
                "updated": "2025-01-22T09:25:29Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    9,
                    25,
                    29,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T09:25:29Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    9,
                    25,
                    29,
                    2,
                    22,
                    0
                ],
                "title": "Bright single-photon source in a silicon chip by nanoscale positioning\n  of a color center in a microcavity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bright single-photon source in a silicon chip by nanoscale positioning\n  of a color center in a microcavity"
                },
                "summary": "We present an all-silicon source of near-infrared linearly-polarized single\nphotons, fabricated by nanoscale positioning of a color center in a\nsilicon-on-insulator microcavity. The color center consists of a single W\ncenter, created at a well-defined position by Si$^{+}$ ion implantation through\na 150 nm-diameter nanohole in a mask. A circular Bragg grating cavity resonant\nwith the W's zero-phonon line at 1217 nm is fabricated at the same location as\nthe nanohole. Under above-gap continuous-wave excitation, a very clean photon\nantibunching behavior ($g{^2} \\leq 0.06$) is observed over the entire power\nrange, which highlights the absence of parasitic emitters. Purcell-enhancement\nof W's zero-phonon emission provides both a record-high photoluminescence count\nrate among Si color centers (ca $1.2 \\times 10^{6}$ counts/s) and apparent\nDebye-Waller factor around 99%. We also demonstrate the triggered emission of\nsingle photons with 93% purity under weak pulsed laser excitation. At high\npulsed laser power, we reveal a detrimental effect of repumping processes, that\ncould be mitigated using selective pumping schemes in the future. These results\nrepresent a major step towards on-demand sources of indistinguishable\nnear-infrared single photons within silicon photonics chips.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an all-silicon source of near-infrared linearly-polarized single\nphotons, fabricated by nanoscale positioning of a color center in a\nsilicon-on-insulator microcavity. The color center consists of a single W\ncenter, created at a well-defined position by Si$^{+}$ ion implantation through\na 150 nm-diameter nanohole in a mask. A circular Bragg grating cavity resonant\nwith the W's zero-phonon line at 1217 nm is fabricated at the same location as\nthe nanohole. Under above-gap continuous-wave excitation, a very clean photon\nantibunching behavior ($g{^2} \\leq 0.06$) is observed over the entire power\nrange, which highlights the absence of parasitic emitters. Purcell-enhancement\nof W's zero-phonon emission provides both a record-high photoluminescence count\nrate among Si color centers (ca $1.2 \\times 10^{6}$ counts/s) and apparent\nDebye-Waller factor around 99%. We also demonstrate the triggered emission of\nsingle photons with 93% purity under weak pulsed laser excitation. At high\npulsed laser power, we reveal a detrimental effect of repumping processes, that\ncould be mitigated using selective pumping schemes in the future. These results\nrepresent a major step towards on-demand sources of indistinguishable\nnear-infrared single photons within silicon photonics chips."
                },
                "authors": [
                    {
                        "name": "Baptiste Lefaucher"
                    },
                    {
                        "name": "Yoann Baron"
                    },
                    {
                        "name": "Jean-Baptiste Jager"
                    },
                    {
                        "name": "Vincent Calvo"
                    },
                    {
                        "name": "Christian Elsässer"
                    },
                    {
                        "name": "Giuliano Coppola"
                    },
                    {
                        "name": "Frédéric Mazen"
                    },
                    {
                        "name": "Sébastien Kerdilès"
                    },
                    {
                        "name": "Félix Cache"
                    },
                    {
                        "name": "Anaïs Dréau"
                    },
                    {
                        "name": "Jean-Michel Gérard"
                    }
                ],
                "author_detail": {
                    "name": "Jean-Michel Gérard"
                },
                "arxiv_affiliation": "Univ. Grenoble Alpes, CEA, Grenoble INP, IRIG, PHELIQS",
                "author": "Jean-Michel Gérard",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12744v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12744v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12689v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12689v1",
                "updated": "2025-01-22T07:52:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    7,
                    52,
                    38,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T07:52:38Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    7,
                    52,
                    38,
                    2,
                    22,
                    0
                ],
                "title": "EchoLM: Accelerating LLM Serving with Real-time Knowledge Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EchoLM: Accelerating LLM Serving with Real-time Knowledge Distillation"
                },
                "summary": "Large language models (LLMs) have excelled in various applications, yet\nserving them at scale is challenging due to their substantial resource demands\nand high latency. Our real-world studies reveal that over 60% of user requests\nto LLMs have semantically similar counterparts, suggesting the potential for\nknowledge sharing among requests. However, naively caching and reusing past\nresponses leads to large quality degradation. In this paper, we introduce\nEchoLM, an in-context caching system that leverages historical requests as\nexamples to guide response generation, enabling selective offloading of\nrequests to more efficient LLMs. However, enabling this real-time knowledge\ntransfer leads to intricate tradeoffs between response quality, latency, and\nsystem throughput at scale. For a new request, EchoLM identifies similar,\nhigh-utility examples and efficiently prepends them to the input for better\nresponse. At scale, EchoLM adaptively routes requests to LLMs of varying\ncapabilities, accounting for response quality and serving loads. EchoLM employs\na cost-aware cache replay mechanism to improve example quality and coverage\noffline, maximizing cache utility and runtime efficiency. Evaluations on\nmillions of open-source requests demonstrate that EchoLM has a throughput\nimprovement of 1.4-5.9x while reducing latency by 28-71% without hurting\nresponse quality on average.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have excelled in various applications, yet\nserving them at scale is challenging due to their substantial resource demands\nand high latency. Our real-world studies reveal that over 60% of user requests\nto LLMs have semantically similar counterparts, suggesting the potential for\nknowledge sharing among requests. However, naively caching and reusing past\nresponses leads to large quality degradation. In this paper, we introduce\nEchoLM, an in-context caching system that leverages historical requests as\nexamples to guide response generation, enabling selective offloading of\nrequests to more efficient LLMs. However, enabling this real-time knowledge\ntransfer leads to intricate tradeoffs between response quality, latency, and\nsystem throughput at scale. For a new request, EchoLM identifies similar,\nhigh-utility examples and efficiently prepends them to the input for better\nresponse. At scale, EchoLM adaptively routes requests to LLMs of varying\ncapabilities, accounting for response quality and serving loads. EchoLM employs\na cost-aware cache replay mechanism to improve example quality and coverage\noffline, maximizing cache utility and runtime efficiency. Evaluations on\nmillions of open-source requests demonstrate that EchoLM has a throughput\nimprovement of 1.4-5.9x while reducing latency by 28-71% without hurting\nresponse quality on average."
                },
                "authors": [
                    {
                        "name": "Yifan Yu"
                    },
                    {
                        "name": "Yu Gan"
                    },
                    {
                        "name": "Lily Tasi"
                    },
                    {
                        "name": "Nikhil Sarda"
                    },
                    {
                        "name": "Jiaming Shen"
                    },
                    {
                        "name": "Yanqi Zhou"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "Fan Lai"
                    },
                    {
                        "name": "Henry M. Levy"
                    },
                    {
                        "name": "David Culler"
                    }
                ],
                "author_detail": {
                    "name": "David Culler"
                },
                "author": "David Culler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12689v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12689v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12528v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12528v1",
                "updated": "2025-01-21T22:33:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    22,
                    33,
                    15,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T22:33:15Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    22,
                    33,
                    15,
                    1,
                    21,
                    0
                ],
                "title": "Improved Coded Caching Scheme for Multi-User Information Retrieval\n  System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improved Coded Caching Scheme for Multi-User Information Retrieval\n  System"
                },
                "summary": "In this paper, we study the coded caching scheme for the $(L, K, M, N)$\nmulti-user information retrieval (MIR) system, which consists of a content\nlibrary containing $N$ files, a base station (BS) with $L$ antennas that cannot\naccess the library, and $K$ single-antenna users, each of which can cache at\nmost $M$ files from the library. The users communicate with the others assisted\nby the BS to decode their required files. In this paper, we focus on designing\na coded caching scheme with low communication latency measured by normalized\ndelivery time (NDT), computational complexity, and subpacketizations. When\n$\\frac{KM}{N}\\geq L$ we first simply the precoding matrix in the downlink step\nto an identity matrix and use the multiple-antenna placement delivery array\n(MAPDA), which was originally proposed for the multiple-input single-output\nnetworks, to generate several new schemes for MIR system. Compared to the\nexisting schemes, both the theoretical and numerical analyses show that our new\nschemes achieve much lower computational complexity and smaller\nsubpacketizations with the same NDT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we study the coded caching scheme for the $(L, K, M, N)$\nmulti-user information retrieval (MIR) system, which consists of a content\nlibrary containing $N$ files, a base station (BS) with $L$ antennas that cannot\naccess the library, and $K$ single-antenna users, each of which can cache at\nmost $M$ files from the library. The users communicate with the others assisted\nby the BS to decode their required files. In this paper, we focus on designing\na coded caching scheme with low communication latency measured by normalized\ndelivery time (NDT), computational complexity, and subpacketizations. When\n$\\frac{KM}{N}\\geq L$ we first simply the precoding matrix in the downlink step\nto an identity matrix and use the multiple-antenna placement delivery array\n(MAPDA), which was originally proposed for the multiple-input single-output\nnetworks, to generate several new schemes for MIR system. Compared to the\nexisting schemes, both the theoretical and numerical analyses show that our new\nschemes achieve much lower computational complexity and smaller\nsubpacketizations with the same NDT."
                },
                "authors": [
                    {
                        "name": "Junyi Wang"
                    },
                    {
                        "name": "Quan Zang"
                    },
                    {
                        "name": "Jinyu Wang"
                    },
                    {
                        "name": "Minquan Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Minquan Cheng"
                },
                "author": "Minquan Cheng",
                "arxiv_comment": "14",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12528v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12528v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12084v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12084v1",
                "updated": "2025-01-21T12:19:02Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    12,
                    19,
                    2,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T12:19:02Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    12,
                    19,
                    2,
                    1,
                    21,
                    0
                ],
                "title": "Dissecting the NVIDIA Hopper Architecture through Microbenchmarking and\n  Multiple Level Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dissecting the NVIDIA Hopper Architecture through Microbenchmarking and\n  Multiple Level Analysis"
                },
                "summary": "Modern GPUs, with their specialized hardware like tensor cores, are essential\nfor demanding AI and deep learning applications. This study presents a\ncomprehensive, multi-level microbenchmarking analysis of the NVIDIA Hopper GPU\narchitecture, delving into its performance characteristics and novel features.\nWe benchmark Hopper's memory subsystem latency and throughput, comparing its L2\npartitioned cache behavior and global memory access patterns against recent GPU\ngenerations, Ampere and Ada Lovelace. Our analysis reveals significant\nperformance differences and architectural improvements in Hopper. A core\ncontribution of this work is a detailed evaluation of Hopper's\nfourth-generation tensor cores, including their FP8 precision support and the\nnovel asynchronous wgmma instructions, assessing their impact on matrix\nmultiply-accumulate operations. We further investigate the performance\nimplications of other key Hopper innovations: DPX instructions for accelerating\ndynamic programming algorithms, distributed shared memory (DSM) for inter-SM\ncommunication, and the Tensor Memory Accelerator (TMA) for asynchronous data\nmovement. This multi-level approach encompasses instruction-level\nmicrobenchmarks, library-level analysis of the Transformer Engine, and\napplication-level benchmarks of tensor core performance within large language\nmodels. Our findings provide valuable, in-depth insights for software\ndevelopers seeking to optimize performance and develop accurate performance\nmodels for the Hopper architecture, ultimately contributing to a deeper\nunderstanding of its potential for accelerating AI and other computationally\nintensive workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern GPUs, with their specialized hardware like tensor cores, are essential\nfor demanding AI and deep learning applications. This study presents a\ncomprehensive, multi-level microbenchmarking analysis of the NVIDIA Hopper GPU\narchitecture, delving into its performance characteristics and novel features.\nWe benchmark Hopper's memory subsystem latency and throughput, comparing its L2\npartitioned cache behavior and global memory access patterns against recent GPU\ngenerations, Ampere and Ada Lovelace. Our analysis reveals significant\nperformance differences and architectural improvements in Hopper. A core\ncontribution of this work is a detailed evaluation of Hopper's\nfourth-generation tensor cores, including their FP8 precision support and the\nnovel asynchronous wgmma instructions, assessing their impact on matrix\nmultiply-accumulate operations. We further investigate the performance\nimplications of other key Hopper innovations: DPX instructions for accelerating\ndynamic programming algorithms, distributed shared memory (DSM) for inter-SM\ncommunication, and the Tensor Memory Accelerator (TMA) for asynchronous data\nmovement. This multi-level approach encompasses instruction-level\nmicrobenchmarks, library-level analysis of the Transformer Engine, and\napplication-level benchmarks of tensor core performance within large language\nmodels. Our findings provide valuable, in-depth insights for software\ndevelopers seeking to optimize performance and develop accurate performance\nmodels for the Hopper architecture, ultimately contributing to a deeper\nunderstanding of its potential for accelerating AI and other computationally\nintensive workloads."
                },
                "authors": [
                    {
                        "name": "Weile Luo"
                    },
                    {
                        "name": "Ruibo Fan"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Dayou Du"
                    },
                    {
                        "name": "Hongyuan Liu"
                    },
                    {
                        "name": "Qiang Wang"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2402.13499",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12084v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12084v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11940v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11940v1",
                "updated": "2025-01-21T07:32:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    7,
                    32,
                    6,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T07:32:06Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    7,
                    32,
                    6,
                    1,
                    21,
                    0
                ],
                "title": "Build Optimization: A Systematic Literature Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Build Optimization: A Systematic Literature Review"
                },
                "summary": "Continuous Integration (CI) consists of an automated build process involving\ncontinuous compilation, testing, and packaging of the software system. While CI\ncomes up with several advantages related to quality and time to delivery, CI\nalso presents several challenges addressed by a large body of research. To\nbetter understand the literature so as to help practitioners find solutions for\ntheir problems and guide future research, we conduct a systematic review of 97\nstudies on build optimization published between 2006 and 2024, which we\nsummarized according to their goals, methodologies, used datasets, and\nleveraged metrics. The identified build optimization studies focus on two main\nchallenges: (1) long build durations, and (2) build failures. To meet the first\nchallenge, existing studies have developed a range of techniques, including\npredicting build outcome and duration, selective build execution, and build\nacceleration using caching or repairing performance smells. The causes of build\nfailures have been the subject of several studies, leading to the development\nof techniques for predicting build script maintenance and automating repair.\nRecent studies have also focused on predicting flaky build failures caused by\nenvironmental issues. The majority of these techniques use machine learning\nalgorithms and leverage build metrics, which we classify into five categories.\nAdditionally, we identify eight publicly available build datasets for build\noptimization research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continuous Integration (CI) consists of an automated build process involving\ncontinuous compilation, testing, and packaging of the software system. While CI\ncomes up with several advantages related to quality and time to delivery, CI\nalso presents several challenges addressed by a large body of research. To\nbetter understand the literature so as to help practitioners find solutions for\ntheir problems and guide future research, we conduct a systematic review of 97\nstudies on build optimization published between 2006 and 2024, which we\nsummarized according to their goals, methodologies, used datasets, and\nleveraged metrics. The identified build optimization studies focus on two main\nchallenges: (1) long build durations, and (2) build failures. To meet the first\nchallenge, existing studies have developed a range of techniques, including\npredicting build outcome and duration, selective build execution, and build\nacceleration using caching or repairing performance smells. The causes of build\nfailures have been the subject of several studies, leading to the development\nof techniques for predicting build script maintenance and automating repair.\nRecent studies have also focused on predicting flaky build failures caused by\nenvironmental issues. The majority of these techniques use machine learning\nalgorithms and leverage build metrics, which we classify into five categories.\nAdditionally, we identify eight publicly available build datasets for build\noptimization research."
                },
                "authors": [
                    {
                        "name": "Henri Aïdasso"
                    },
                    {
                        "name": "Mohammed Sayagh"
                    },
                    {
                        "name": "Francis Bordeleau"
                    }
                ],
                "author_detail": {
                    "name": "Francis Bordeleau"
                },
                "author": "Francis Bordeleau",
                "arxiv_comment": "An earlier version of this work was submitted to ACM CSUR in November\n  2023",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11940v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11940v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11855v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11855v1",
                "updated": "2025-01-21T03:13:21Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    3,
                    13,
                    21,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T03:13:21Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    3,
                    13,
                    21,
                    1,
                    21,
                    0
                ],
                "title": "A New Construction Structure on Coded Caching with Linear\n  Subpacketization: Non-Half-Sum Disjoint Packing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A New Construction Structure on Coded Caching with Linear\n  Subpacketization: Non-Half-Sum Disjoint Packing"
                },
                "summary": "Coded caching is a promising technique to effectively reduce peak traffic by\nusing local caches and the multicast gains generated by these local caches. We\nprefer to design a coded caching scheme with the subpacketization $F$ and\ntransmission load $R$ as small as possible since these are the key metrics for\nevaluating the implementation complexity and transmission efficiency of the\nscheme, respectively. However, most of the existing coded caching schemes have\nlarge subpacketizations which grow exponentially with the number of users $K$,\nand there are a few schemes with linear subpacketizations which have large\ntransmission loads. In this paper, we focus on studying the linear\nsubpacketization, i.e., $K=F$, coded caching scheme with low transmission load.\nSpecifically, we first introduce a new combinatorial structure called\nnon-half-sum disjoint packing (NHSDP) which can be used to generate a coded\ncaching scheme with $K=F$. Then a class of new schemes is obtained by\nconstructing NHSDP. Theoretical and numerical comparisons show that (i)\ncompared to the existing schemes with linear subpacketization (to the number of\nusers), the proposed scheme achieves a lower load; (ii) compared to some\nexisting schemes with polynomial subpacketization, the proposed scheme can also\nachieve a lower load in some cases; (iii) compared to some existing schemes\nwith exponential subpacketization, the proposed scheme has loads close to those\nof these schemes in some cases. Moreover, the new concept of NHSDP is closely\nrelated to the classical combinatorial structures such as cyclic difference\npacking (CDP), non-three-term arithmetic progressions (NTAP), and perfect hash\nfamily (PHF). These connections indicate that NHSDP is an important\ncombinatorial structure in the field of combinatorial design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching is a promising technique to effectively reduce peak traffic by\nusing local caches and the multicast gains generated by these local caches. We\nprefer to design a coded caching scheme with the subpacketization $F$ and\ntransmission load $R$ as small as possible since these are the key metrics for\nevaluating the implementation complexity and transmission efficiency of the\nscheme, respectively. However, most of the existing coded caching schemes have\nlarge subpacketizations which grow exponentially with the number of users $K$,\nand there are a few schemes with linear subpacketizations which have large\ntransmission loads. In this paper, we focus on studying the linear\nsubpacketization, i.e., $K=F$, coded caching scheme with low transmission load.\nSpecifically, we first introduce a new combinatorial structure called\nnon-half-sum disjoint packing (NHSDP) which can be used to generate a coded\ncaching scheme with $K=F$. Then a class of new schemes is obtained by\nconstructing NHSDP. Theoretical and numerical comparisons show that (i)\ncompared to the existing schemes with linear subpacketization (to the number of\nusers), the proposed scheme achieves a lower load; (ii) compared to some\nexisting schemes with polynomial subpacketization, the proposed scheme can also\nachieve a lower load in some cases; (iii) compared to some existing schemes\nwith exponential subpacketization, the proposed scheme has loads close to those\nof these schemes in some cases. Moreover, the new concept of NHSDP is closely\nrelated to the classical combinatorial structures such as cyclic difference\npacking (CDP), non-three-term arithmetic progressions (NTAP), and perfect hash\nfamily (PHF). These connections indicate that NHSDP is an important\ncombinatorial structure in the field of combinatorial design."
                },
                "authors": [
                    {
                        "name": "Minquan Cheng"
                    },
                    {
                        "name": "Huimei Wei"
                    },
                    {
                        "name": "Kai Wan"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11855v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11855v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11834v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11834v1",
                "updated": "2025-01-21T02:35:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    2,
                    35,
                    31,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T02:35:31Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    2,
                    35,
                    31,
                    1,
                    21,
                    0
                ],
                "title": "PDA Construction via Union of Cartesian Product Cache Configurations for\n  Coded Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PDA Construction via Union of Cartesian Product Cache Configurations for\n  Coded Caching"
                },
                "summary": "Caching is an efficient technique to reduce peak traffic by storing popular\ncontent in local caches. Placement delivery array (PDA) proposed by Yan et al.\nis a combinatorial structure to design coded caching schemes with uncoded\nplacement and one-shot linear delivery. By taking the $m$-fold Cartesian\nproduct of a small base PDA, Wang et al. constructed a big PDA while\nmaintaining the memory ratio and transmission load unchanged, which achieves\nlinear growth in both the number of users and coded caching gain. In order to\nachieve exponential growth in both the number of users and coded caching gain,\nin this paper we propose a PDA construction by taking the union operation of\nthe cache configurations from the $m$-fold Cartesian product of a base PDA. The\nresulting PDA leads to a coded caching scheme with subpacketization increasing\nsub-exponentially with the number of users while keeping the load constant for\nfixed memory ratio. By applying the proposed construction to existing base\nPDAs, three new coded caching schemes are obtained, which cover some existing\nschemes as special cases and can achieve lower load with simultaneously lower\nsubpacketization for some memory ratios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching is an efficient technique to reduce peak traffic by storing popular\ncontent in local caches. Placement delivery array (PDA) proposed by Yan et al.\nis a combinatorial structure to design coded caching schemes with uncoded\nplacement and one-shot linear delivery. By taking the $m$-fold Cartesian\nproduct of a small base PDA, Wang et al. constructed a big PDA while\nmaintaining the memory ratio and transmission load unchanged, which achieves\nlinear growth in both the number of users and coded caching gain. In order to\nachieve exponential growth in both the number of users and coded caching gain,\nin this paper we propose a PDA construction by taking the union operation of\nthe cache configurations from the $m$-fold Cartesian product of a base PDA. The\nresulting PDA leads to a coded caching scheme with subpacketization increasing\nsub-exponentially with the number of users while keeping the load constant for\nfixed memory ratio. By applying the proposed construction to existing base\nPDAs, three new coded caching schemes are obtained, which cover some existing\nschemes as special cases and can achieve lower load with simultaneously lower\nsubpacketization for some memory ratios."
                },
                "authors": [
                    {
                        "name": "Jinyu Wang"
                    },
                    {
                        "name": "Minquan Cheng"
                    },
                    {
                        "name": "Kai Wan"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "arxiv_comment": "35 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11834v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11834v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11779v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11779v1",
                "updated": "2025-01-20T23:10:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    20,
                    23,
                    10,
                    13,
                    0,
                    20,
                    0
                ],
                "published": "2025-01-20T23:10:13Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    23,
                    10,
                    13,
                    0,
                    20,
                    0
                ],
                "title": "Glinthawk: A Two-Tiered Architecture for High-Throughput LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Glinthawk: A Two-Tiered Architecture for High-Throughput LLM Inference"
                },
                "summary": "Large Language Models (LLM) have revolutionized natural language processing,\nbut their inference demands substantial resources, while under-utilizing\nhigh-end accelerators like GPUs. A major bottleneck arises from the attention\nmechanism, which requires storing large key-value caches, limiting the maximum\nachievable throughput way below the available computing resources. Current\napproaches attempt to mitigate this issue through memory-efficient attention\nand paging mechanisms, but remained constrained by the assumption that all\noperations must be performed on high-end accelerators.\n  In this work, we propose Glinthawk, a two-tiered architecture that decouples\nthe attention mechanism from the rest of the Transformer model. This approach\nallows the memory requirements for attention to scale independently, enabling\nlarger batch sizes and more efficient use of the high-end accelerators. We\nprototype Glinthawk with NVIDIA T4 GPUs as one tier and standard CPU VMs as the\nother. Compared to a traditional single-tier setup, it improves throughput by\n$5.9\\times$ and reduces cost of generation by $2.8\\times$. For longer sequence\nlengths, it achieves $16.3\\times$ throughput improvement at $2.4\\times$ less\ncost. Our evaluation shows that this architecture can tolerate moderate network\nlatency with minimal performance degradation, making it highly effective for\nlatency-tolerant, throughput-oriented applications such as batch processing. We\nshared our prototype publicly at \\url{https://github.com/microsoft/glinthawk}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLM) have revolutionized natural language processing,\nbut their inference demands substantial resources, while under-utilizing\nhigh-end accelerators like GPUs. A major bottleneck arises from the attention\nmechanism, which requires storing large key-value caches, limiting the maximum\nachievable throughput way below the available computing resources. Current\napproaches attempt to mitigate this issue through memory-efficient attention\nand paging mechanisms, but remained constrained by the assumption that all\noperations must be performed on high-end accelerators.\n  In this work, we propose Glinthawk, a two-tiered architecture that decouples\nthe attention mechanism from the rest of the Transformer model. This approach\nallows the memory requirements for attention to scale independently, enabling\nlarger batch sizes and more efficient use of the high-end accelerators. We\nprototype Glinthawk with NVIDIA T4 GPUs as one tier and standard CPU VMs as the\nother. Compared to a traditional single-tier setup, it improves throughput by\n$5.9\\times$ and reduces cost of generation by $2.8\\times$. For longer sequence\nlengths, it achieves $16.3\\times$ throughput improvement at $2.4\\times$ less\ncost. Our evaluation shows that this architecture can tolerate moderate network\nlatency with minimal performance degradation, making it highly effective for\nlatency-tolerant, throughput-oriented applications such as batch processing. We\nshared our prototype publicly at \\url{https://github.com/microsoft/glinthawk}."
                },
                "authors": [
                    {
                        "name": "Pouya Hamadanian"
                    },
                    {
                        "name": "Sadjad Fouladi"
                    }
                ],
                "author_detail": {
                    "name": "Sadjad Fouladi"
                },
                "author": "Sadjad Fouladi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11779v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11779v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11502v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11502v1",
                "updated": "2025-01-20T14:19:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    20,
                    14,
                    19,
                    48,
                    0,
                    20,
                    0
                ],
                "published": "2025-01-20T14:19:48Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    14,
                    19,
                    48,
                    0,
                    20,
                    0
                ],
                "title": "Hierarchical Coded Caching in High Memory Regime with Coded Placement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Coded Caching in High Memory Regime with Coded Placement"
                },
                "summary": "We consider a two-layer hierarchical coded caching network where a server\nwith a library of $N$ files is connected to $K_1$ mirrors, each having a cache\nmemory of size $M_1$. Each mirror is further connected to $K_2$ users, each\nequipped with a dedicated cache of size $M_2$. In this paper, we propose two\ndistinct coded caching schemes based on coded placement, corresponding to two\ndistinct memory pairs, \\( (M_1, M_2) \\). We show that the proposed schemes\noutperform the existing schemes at these memory points given by the proposed\nschemes for smaller values of $K_2$. In setups where mirrors are positioned\nnear each other, avoiding signal interference is crucial. This can be ensured\nby having all mirrors transmit using orthogonal carrier frequencies. To compare\nour schemes with existing ones, we used the composite rate metric, which\naccurately represents the total bandwidth utilized in such setups. The\ncomposite rate is given by $\\overline{R} = R_1 + K_1 R_2$, where $R_1$ is the\nrate from the server to the mirrors, and $R_2$ is the rate from the mirrors to\nthe users, with respect to $M_1$ and $M_2$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a two-layer hierarchical coded caching network where a server\nwith a library of $N$ files is connected to $K_1$ mirrors, each having a cache\nmemory of size $M_1$. Each mirror is further connected to $K_2$ users, each\nequipped with a dedicated cache of size $M_2$. In this paper, we propose two\ndistinct coded caching schemes based on coded placement, corresponding to two\ndistinct memory pairs, \\( (M_1, M_2) \\). We show that the proposed schemes\noutperform the existing schemes at these memory points given by the proposed\nschemes for smaller values of $K_2$. In setups where mirrors are positioned\nnear each other, avoiding signal interference is crucial. This can be ensured\nby having all mirrors transmit using orthogonal carrier frequencies. To compare\nour schemes with existing ones, we used the composite rate metric, which\naccurately represents the total bandwidth utilized in such setups. The\ncomposite rate is given by $\\overline{R} = R_1 + K_1 R_2$, where $R_1$ is the\nrate from the server to the mirrors, and $R_2$ is the rate from the mirrors to\nthe users, with respect to $M_1$ and $M_2$."
                },
                "authors": [
                    {
                        "name": "Rajlaxmi Pandey"
                    },
                    {
                        "name": "Charul Rajput"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "7 pages, 3 figures and 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11502v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11502v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10659v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10659v3",
                "updated": "2025-01-20T08:44:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    20,
                    8,
                    44,
                    1,
                    0,
                    20,
                    0
                ],
                "published": "2024-11-16T01:39:44Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "title": "Spineless Traversal for Layout Invalidation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spineless Traversal for Layout Invalidation"
                },
                "summary": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations."
                },
                "authors": [
                    {
                        "name": "Marisa Kirisame"
                    },
                    {
                        "name": "Tiezhi Wang"
                    },
                    {
                        "name": "Pavel Panchekha"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Panchekha"
                },
                "author": "Pavel Panchekha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10659v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10659v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11175v1",
                "updated": "2025-01-19T21:25:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    19,
                    21,
                    25,
                    53,
                    6,
                    19,
                    0
                ],
                "published": "2025-01-19T21:25:53Z",
                "published_parsed": [
                    2025,
                    1,
                    19,
                    21,
                    25,
                    53,
                    6,
                    19,
                    0
                ],
                "title": "ProKeR: A Kernel Perspective on Few-Shot Adaptation of Large\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProKeR: A Kernel Perspective on Few-Shot Adaptation of Large\n  Vision-Language Models"
                },
                "summary": "The growing popularity of Contrastive Language-Image Pretraining (CLIP) has\nled to its widespread application in various visual downstream tasks. To\nenhance CLIP's effectiveness and versatility, efficient few-shot adaptation\ntechniques have been widely adopted. Among these approaches, training-free\nmethods, particularly caching methods exemplified by Tip-Adapter, have gained\nattention for their lightweight adaptation without the need for additional\nfine-tuning. In this paper, we revisit Tip-Adapter from a kernel perspective,\nshowing that caching methods function as local adapters and are connected to a\nwell-established kernel literature. Drawing on this insight, we offer a\ntheoretical understanding of how these methods operate and suggest multiple\navenues for enhancing the Tip-Adapter baseline. Notably, our analysis shows the\nimportance of incorporating global information in local adapters. Therefore, we\nsubsequently propose a global method that learns a proximal regularizer in a\nreproducing kernel Hilbert space (RKHS) using CLIP as a base learner. Our\nmethod, which we call ProKeR (Proximal Kernel ridge Regression), has a closed\nform solution and achieves state-of-the-art performances across 11 datasets in\nthe standard few-shot adaptation benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing popularity of Contrastive Language-Image Pretraining (CLIP) has\nled to its widespread application in various visual downstream tasks. To\nenhance CLIP's effectiveness and versatility, efficient few-shot adaptation\ntechniques have been widely adopted. Among these approaches, training-free\nmethods, particularly caching methods exemplified by Tip-Adapter, have gained\nattention for their lightweight adaptation without the need for additional\nfine-tuning. In this paper, we revisit Tip-Adapter from a kernel perspective,\nshowing that caching methods function as local adapters and are connected to a\nwell-established kernel literature. Drawing on this insight, we offer a\ntheoretical understanding of how these methods operate and suggest multiple\navenues for enhancing the Tip-Adapter baseline. Notably, our analysis shows the\nimportance of incorporating global information in local adapters. Therefore, we\nsubsequently propose a global method that learns a proximal regularizer in a\nreproducing kernel Hilbert space (RKHS) using CLIP as a base learner. Our\nmethod, which we call ProKeR (Proximal Kernel ridge Regression), has a closed\nform solution and achieves state-of-the-art performances across 11 datasets in\nthe standard few-shot adaptation benchmark."
                },
                "authors": [
                    {
                        "name": "Yassir Bendou"
                    },
                    {
                        "name": "Amine Ouasfi"
                    },
                    {
                        "name": "Vincent Gripon"
                    },
                    {
                        "name": "Adnane Boukhayma"
                    }
                ],
                "author_detail": {
                    "name": "Adnane Boukhayma"
                },
                "author": "Adnane Boukhayma",
                "arxiv_comment": "Code available at https://ybendou.github.io/ProKeR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02088v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02088v3",
                "updated": "2025-01-19T19:46:21Z",
                "updated_parsed": [
                    2025,
                    1,
                    19,
                    19,
                    46,
                    21,
                    6,
                    19,
                    0
                ],
                "published": "2024-09-03T17:40:24Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    17,
                    40,
                    24,
                    1,
                    247,
                    0
                ],
                "title": "Cache Coherence Over Disaggregated Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Coherence Over Disaggregated Memory"
                },
                "summary": "Disaggregating memory from compute offers the opportunity to better utilize\nstranded memory in cloud data centers. It is important to cache data in the\ncompute nodes and maintain cache coherence across multiple compute nodes.\nHowever, the limited computing power on disaggregated memory servers makes\ntraditional cache coherence protocols suboptimal, particularly in the case of\nstranded memory. This paper introduces SELCC; a Shared-Exclusive Latch Cache\nCoherence protocol that maintains cache coherence without imposing any\ncomputational burden on the remote memory side. It aligns the state machine of\nthe shared-exclusive latch protocol with the MSI protocol by introducing lazy\nlatch-release and invalidation messages, thereby ensuring both atomicity of\ndata access and cache coherence. SELCC embeds cache-ownership metadata directly\ninto the RDMA latch word, enabling efficient cache ownership management via\nRDMA atomic operations. SELCC can serve as an abstraction layer over\ndisaggregated memory with APIs that resemble main-memory accesses. A concurrent\nB-tree and three transaction concurrency control algorithms are realized using\nSELCC's abstraction layer. Experimental results show that SELCC significantly\noutperforms Remote-Procedure-Call-based protocols for cache coherence under\nlimited remote computing power. Applications on SELCC achieve comparable or\nsuperior performance over disaggregated memory compared to competitors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregating memory from compute offers the opportunity to better utilize\nstranded memory in cloud data centers. It is important to cache data in the\ncompute nodes and maintain cache coherence across multiple compute nodes.\nHowever, the limited computing power on disaggregated memory servers makes\ntraditional cache coherence protocols suboptimal, particularly in the case of\nstranded memory. This paper introduces SELCC; a Shared-Exclusive Latch Cache\nCoherence protocol that maintains cache coherence without imposing any\ncomputational burden on the remote memory side. It aligns the state machine of\nthe shared-exclusive latch protocol with the MSI protocol by introducing lazy\nlatch-release and invalidation messages, thereby ensuring both atomicity of\ndata access and cache coherence. SELCC embeds cache-ownership metadata directly\ninto the RDMA latch word, enabling efficient cache ownership management via\nRDMA atomic operations. SELCC can serve as an abstraction layer over\ndisaggregated memory with APIs that resemble main-memory accesses. A concurrent\nB-tree and three transaction concurrency control algorithms are realized using\nSELCC's abstraction layer. Experimental results show that SELCC significantly\noutperforms Remote-Procedure-Call-based protocols for cache coherence under\nlimited remote computing power. Applications on SELCC achieve comparable or\nsuperior performance over disaggregated memory compared to competitors."
                },
                "authors": [
                    {
                        "name": "Ruihong Wang"
                    },
                    {
                        "name": "Jianguo Wang"
                    },
                    {
                        "name": "Walid G. Aref"
                    }
                ],
                "author_detail": {
                    "name": "Walid G. Aref"
                },
                "author": "Walid G. Aref",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02088v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02088v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11126v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11126v1",
                "updated": "2025-01-19T17:33:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    19,
                    17,
                    33,
                    28,
                    6,
                    19,
                    0
                ],
                "published": "2025-01-19T17:33:28Z",
                "published_parsed": [
                    2025,
                    1,
                    19,
                    17,
                    33,
                    28,
                    6,
                    19,
                    0
                ],
                "title": "SIC-free Multicast Scheduling for Multi-antenna Coded Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SIC-free Multicast Scheduling for Multi-antenna Coded Caching"
                },
                "summary": "Multi-antenna coded caching (CC) with multicast beamforming often relies on\ncomplex successive interference cancellation (SIC) structures to decode a\nsuperposition of multiple streams received by each user. Traditional\nsignal-level schemes require the regeneration of interfering signals from the\ncache, adding significant computational complexity. To address this, we propose\na bit-level multicast scheduling scheme enabling linear, SIC-free decoding of\nparallel streams by repeatedly transmitting data terms with linearly\nindependent coefficients. Two reference strategies for constructing the\ncoefficients matrix are considered: a random strategy, which lacks control over\nmatrix construction, and an equal-distant strategy, which balances users'\ninterference and data terms equally. In contrast, the proposed sparse strategy\nminimizes the number of multicast streams transmitted in parallel during each\ninterval, simplifying the system while optimizing resource usage. To further\nenhance the symmetric rate, a successive projection algorithm is applied to\nexploit channel properties and optimize user ordering. With the coefficients\nmatrix and optimized user ordering in place, multicast beamformers are refined\nto aggregate desired data from relevant multicast streams. Numerical\nsimulations validate the effectiveness of the sparse strategy, demonstrating\nsignificant gains in symmetric rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-antenna coded caching (CC) with multicast beamforming often relies on\ncomplex successive interference cancellation (SIC) structures to decode a\nsuperposition of multiple streams received by each user. Traditional\nsignal-level schemes require the regeneration of interfering signals from the\ncache, adding significant computational complexity. To address this, we propose\na bit-level multicast scheduling scheme enabling linear, SIC-free decoding of\nparallel streams by repeatedly transmitting data terms with linearly\nindependent coefficients. Two reference strategies for constructing the\ncoefficients matrix are considered: a random strategy, which lacks control over\nmatrix construction, and an equal-distant strategy, which balances users'\ninterference and data terms equally. In contrast, the proposed sparse strategy\nminimizes the number of multicast streams transmitted in parallel during each\ninterval, simplifying the system while optimizing resource usage. To further\nenhance the symmetric rate, a successive projection algorithm is applied to\nexploit channel properties and optimize user ordering. With the coefficients\nmatrix and optimized user ordering in place, multicast beamformers are refined\nto aggregate desired data from relevant multicast streams. Numerical\nsimulations validate the effectiveness of the sparse strategy, demonstrating\nsignificant gains in symmetric rate."
                },
                "authors": [
                    {
                        "name": "MohammadJavad Sojdeh"
                    },
                    {
                        "name": "MohammadJavad Salehi"
                    },
                    {
                        "name": "Antti Tölli"
                    }
                ],
                "author_detail": {
                    "name": "Antti Tölli"
                },
                "author": "Antti Tölli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11126v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11126v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.15024v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.15024v2",
                "updated": "2025-01-19T15:47:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    19,
                    15,
                    47,
                    14,
                    6,
                    19,
                    0
                ],
                "published": "2023-12-22T19:15:23Z",
                "published_parsed": [
                    2023,
                    12,
                    22,
                    19,
                    15,
                    23,
                    4,
                    356,
                    0
                ],
                "title": "Coded Caching for Hierarchical Two-Layer Networks with Coded Placement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded Caching for Hierarchical Two-Layer Networks with Coded Placement"
                },
                "summary": "We examine a two-layered hierarchical coded caching problem, a configuration\naddressed in existing research. This involves a server connected to $K_1$\nmirrors, each of which serves $K_2$ users. The mirrors and the users are\nequipped with caches of size $M_1$ and $M_2$, respectively. We propose a\nhierarchical coded caching scheme with coded placements that outperforms\nexisting schemes. To ensure a fair comparison, we introduce the notion of\ncomposite rate, defined as $\\overline{R} = R_1 + K_1 R_2$, where $R_1$ is the\nrate from the server to mirrors and $R_2$ is the rate from mirrors to users.\nThe composite rate has not been discussed before in the literature and is\npertinent when mirrors transmit with different carrier frequencies. For the\nproposed scheme, we show a trade-off between the global memory\n$\\overline{M}=K_1M_1+K_1K_2M_2$ of the system and the composite rate and\ncompare with the existing schemes. Additionally, we conduct this comparative\nanalysis by plotting $R_1$ + $R_2$ against global memory, which is particularly\nbeneficial for systems wherein each mirror can utilize the same carrier\nfrequency, given their significant spatial separation. Additionally, we propose\nan optimized scheme for the specific case of a single mirror, showing improved\nperformance in this scenario.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We examine a two-layered hierarchical coded caching problem, a configuration\naddressed in existing research. This involves a server connected to $K_1$\nmirrors, each of which serves $K_2$ users. The mirrors and the users are\nequipped with caches of size $M_1$ and $M_2$, respectively. We propose a\nhierarchical coded caching scheme with coded placements that outperforms\nexisting schemes. To ensure a fair comparison, we introduce the notion of\ncomposite rate, defined as $\\overline{R} = R_1 + K_1 R_2$, where $R_1$ is the\nrate from the server to mirrors and $R_2$ is the rate from mirrors to users.\nThe composite rate has not been discussed before in the literature and is\npertinent when mirrors transmit with different carrier frequencies. For the\nproposed scheme, we show a trade-off between the global memory\n$\\overline{M}=K_1M_1+K_1K_2M_2$ of the system and the composite rate and\ncompare with the existing schemes. Additionally, we conduct this comparative\nanalysis by plotting $R_1$ + $R_2$ against global memory, which is particularly\nbeneficial for systems wherein each mirror can utilize the same carrier\nfrequency, given their significant spatial separation. Additionally, we propose\nan optimized scheme for the specific case of a single mirror, showing improved\nperformance in this scenario."
                },
                "authors": [
                    {
                        "name": "Rajlaxmi Pandey"
                    },
                    {
                        "name": "Charul Rajput"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "47 pages, 16 figures and 2 tables. More figures, explanations and\n  comparisons included",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.15024v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.15024v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10854v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10854v1",
                "updated": "2025-01-18T19:10:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    18,
                    19,
                    10,
                    23,
                    5,
                    18,
                    0
                ],
                "published": "2025-01-18T19:10:23Z",
                "published_parsed": [
                    2025,
                    1,
                    18,
                    19,
                    10,
                    23,
                    5,
                    18,
                    0
                ],
                "title": "Achievable DoF Bounds for Cache-Aided Asymmetric MIMO Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Achievable DoF Bounds for Cache-Aided Asymmetric MIMO Communications"
                },
                "summary": "Integrating coded caching (CC) into multiple-input multiple-output (MIMO)\ncommunications can significantly enhance the achievable degrees of freedom\n(DoF) in wireless networks. This paper investigates a practical cache-aided\nasymmetric MIMO configuration with cache ratio $\\gamma$, where a server\nequipped with $L$ transmit antennas communicates with $K$ users, each having\n$G_k$ receive antennas. We propose three content-aware MIMO-CC strategies: the\n\\emph{min-G} scheme, which treats the system as symmetric by assuming all users\nhave the same number of antennas, equal to the smallest among them; the\n\\emph{Grouping} scheme, which maximizes spatial multiplexing gain separately\nwithin each user subset at the cost of some global caching gain; and the\n\\emph{Phantom} scheme, which dynamically redistributes spatial resources using\nvirtual or \"phantom\" antenna users, bridging the performance gains of the min-G\nand Grouping schemes. These strategies jointly optimize the number of users,\n$\\Omega$, and the parallel streams decoded by each user, $\\beta_k$, ensuring\nlinear decodability for all target users. Analytical and numerical results\nconfirm that the proposed schemes achieve significant DoF improvements across\nvarious system configurations, demonstrating the potential of content-aware\nMIMO-CC strategies for enhancing wireless network performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating coded caching (CC) into multiple-input multiple-output (MIMO)\ncommunications can significantly enhance the achievable degrees of freedom\n(DoF) in wireless networks. This paper investigates a practical cache-aided\nasymmetric MIMO configuration with cache ratio $\\gamma$, where a server\nequipped with $L$ transmit antennas communicates with $K$ users, each having\n$G_k$ receive antennas. We propose three content-aware MIMO-CC strategies: the\n\\emph{min-G} scheme, which treats the system as symmetric by assuming all users\nhave the same number of antennas, equal to the smallest among them; the\n\\emph{Grouping} scheme, which maximizes spatial multiplexing gain separately\nwithin each user subset at the cost of some global caching gain; and the\n\\emph{Phantom} scheme, which dynamically redistributes spatial resources using\nvirtual or \"phantom\" antenna users, bridging the performance gains of the min-G\nand Grouping schemes. These strategies jointly optimize the number of users,\n$\\Omega$, and the parallel streams decoded by each user, $\\beta_k$, ensuring\nlinear decodability for all target users. Analytical and numerical results\nconfirm that the proposed schemes achieve significant DoF improvements across\nvarious system configurations, demonstrating the potential of content-aware\nMIMO-CC strategies for enhancing wireless network performance."
                },
                "authors": [
                    {
                        "name": "Mohammad NaseriTehrani"
                    },
                    {
                        "name": "MohammadJavad Salehi"
                    },
                    {
                        "name": "Antti Tölli"
                    }
                ],
                "author_detail": {
                    "name": "Antti Tölli"
                },
                "author": "Antti Tölli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10854v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10854v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10756v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10756v1",
                "updated": "2025-01-18T13:04:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    18,
                    13,
                    4,
                    23,
                    5,
                    18,
                    0
                ],
                "published": "2025-01-18T13:04:23Z",
                "published_parsed": [
                    2025,
                    1,
                    18,
                    13,
                    4,
                    23,
                    5,
                    18,
                    0
                ],
                "title": "D2D Coded Caching Schemes for Multiaccess Networks with Combinatorial\n  Access Topology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "D2D Coded Caching Schemes for Multiaccess Networks with Combinatorial\n  Access Topology"
                },
                "summary": "This paper considers wireless device-to-device (D2D) coded caching in a\nmultiaccess network, where the users communicate with each other and each user\ncan access multiple cache nodes. Access topologies derived from two\ncombinatorial designs known as the $t$-design and $t$-group divisible design\n($t$-GDD), referred to as the $t$-design and $t$-GDD topologies respectively,\nwhich subsume a few other known topologies, have been studied for the\nmultiaccess coded caching (MACC) network by Cheng \\textit{et al.} in\n\\cite{MACC_des}. These access topologies are extended to a multiaccess D2D\ncoded caching (MADCC) network and novel MADCC schemes are proposed. MADCC\nnetwork has been studied so far only for the cyclic wrap-around topology. Apart\nfrom the proposed novel MADCC schemes, MADCC schemes are also derived from the\nexisting MACC schemes in \\cite{MACC_des}. To compare the performance of\ndifferent MADCC schemes, the metrics of load per user and subpacketization\nlevel are used while keeping the number of caches and cache memory size same.\nThe proposed MADCC scheme with $t$-design topology performs better in terms of\nsubpacketization level while achieving the same load per user compared to the\nMADCC scheme derived from the MACC scheme with $t$-design topology in\n\\cite{MACC_des}. The proposed MADCC scheme with $t$-GDD topology performs\nbetter in terms of load per user while achieving the same subpacketization\nlevel compared to the MADCC scheme derived from the MACC scheme with $t$-GDD\ntopology in \\cite{MACC_des} in some cases. Compared to the existing MADCC\nscheme with cyclic wrap-around topology, the proposed MADCC scheme with\n$t$-design topology performs better in terms of load per user, and the proposed\nMADCC scheme with $t$-GDD topology performs better in terms of subpacketization\nlevel at the expense of an increase in load per user.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper considers wireless device-to-device (D2D) coded caching in a\nmultiaccess network, where the users communicate with each other and each user\ncan access multiple cache nodes. Access topologies derived from two\ncombinatorial designs known as the $t$-design and $t$-group divisible design\n($t$-GDD), referred to as the $t$-design and $t$-GDD topologies respectively,\nwhich subsume a few other known topologies, have been studied for the\nmultiaccess coded caching (MACC) network by Cheng \\textit{et al.} in\n\\cite{MACC_des}. These access topologies are extended to a multiaccess D2D\ncoded caching (MADCC) network and novel MADCC schemes are proposed. MADCC\nnetwork has been studied so far only for the cyclic wrap-around topology. Apart\nfrom the proposed novel MADCC schemes, MADCC schemes are also derived from the\nexisting MACC schemes in \\cite{MACC_des}. To compare the performance of\ndifferent MADCC schemes, the metrics of load per user and subpacketization\nlevel are used while keeping the number of caches and cache memory size same.\nThe proposed MADCC scheme with $t$-design topology performs better in terms of\nsubpacketization level while achieving the same load per user compared to the\nMADCC scheme derived from the MACC scheme with $t$-design topology in\n\\cite{MACC_des}. The proposed MADCC scheme with $t$-GDD topology performs\nbetter in terms of load per user while achieving the same subpacketization\nlevel compared to the MADCC scheme derived from the MACC scheme with $t$-GDD\ntopology in \\cite{MACC_des} in some cases. Compared to the existing MADCC\nscheme with cyclic wrap-around topology, the proposed MADCC scheme with\n$t$-design topology performs better in terms of load per user, and the proposed\nMADCC scheme with $t$-GDD topology performs better in terms of subpacketization\nlevel at the expense of an increase in load per user."
                },
                "authors": [
                    {
                        "name": "Rashid Ummer N. T."
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "21 pages, 12 figures and 4 tables. Some overlap with 2409.14350v1\n  [cs.IT] 22 Sept. 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10756v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10756v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10682v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10682v1",
                "updated": "2025-01-18T07:29:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    18,
                    7,
                    29,
                    20,
                    5,
                    18,
                    0
                ],
                "published": "2025-01-18T07:29:20Z",
                "published_parsed": [
                    2025,
                    1,
                    18,
                    7,
                    29,
                    20,
                    5,
                    18,
                    0
                ],
                "title": "SkyByte: Architecting an Efficient Memory-Semantic CXL-based SSD with OS\n  and Hardware Co-design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SkyByte: Architecting an Efficient Memory-Semantic CXL-based SSD with OS\n  and Hardware Co-design"
                },
                "summary": "The CXL-based solid-state drive (CXL-SSD) provides a promising approach\ntowards scaling the main memory capacity at low cost. However, the CXL-SSD\nfaces performance challenges due to the long flash access latency and\nunpredictable events such as garbage collection in the SSD device, stalling the\nhost processor and wasting compute cycles. Although the CXL interface enables\nthe byte-granular data access to the SSD, accessing flash chips is still at\npage granularity due to physical limitations. The mismatch of access\ngranularity causes significant unnecessary I/O traffic to flash chips,\nworsening the suboptimal end-to-end data access performance. In this paper, we\npresent SkyByte, an efficient CXL-based SSD that employs a holistic approach to\naddress the aforementioned challenges by co-designing the host operating system\n(OS) and SSD controller. To alleviate the long memory stall when accessing the\nCXL-SSD, SkyByte revisits the OS context switch mechanism and enables\nopportunistic context switches upon the detection of long access delays. To\naccommodate byte-granular data accesses, SkyByte architects the internal DRAM\nof the SSD controller into a cacheline-level write log and a page-level data\ncache, and enables data coalescing upon log cleaning to reduce the I/O traffic\nto flash chips. SkyByte also employs optimization techniques that include\nadaptive page migration for exploring the performance benefits of fast host\nmemory by promoting hot pages in CXL-SSD to the host. We implement SkyByte with\na CXL-SSD simulator and evaluate its efficiency with various data-intensive\napplications. Our experiments show that SkyByte outperforms current CXL-based\nSSD by 6.11X, and reduces the I/O traffic to flash chips by 23.08X on average.\nSkyByte also reaches 75% of the performance of the ideal case that assumes\nunlimited DRAM capacity in the host, which offers an attractive cost-effective\nsolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The CXL-based solid-state drive (CXL-SSD) provides a promising approach\ntowards scaling the main memory capacity at low cost. However, the CXL-SSD\nfaces performance challenges due to the long flash access latency and\nunpredictable events such as garbage collection in the SSD device, stalling the\nhost processor and wasting compute cycles. Although the CXL interface enables\nthe byte-granular data access to the SSD, accessing flash chips is still at\npage granularity due to physical limitations. The mismatch of access\ngranularity causes significant unnecessary I/O traffic to flash chips,\nworsening the suboptimal end-to-end data access performance. In this paper, we\npresent SkyByte, an efficient CXL-based SSD that employs a holistic approach to\naddress the aforementioned challenges by co-designing the host operating system\n(OS) and SSD controller. To alleviate the long memory stall when accessing the\nCXL-SSD, SkyByte revisits the OS context switch mechanism and enables\nopportunistic context switches upon the detection of long access delays. To\naccommodate byte-granular data accesses, SkyByte architects the internal DRAM\nof the SSD controller into a cacheline-level write log and a page-level data\ncache, and enables data coalescing upon log cleaning to reduce the I/O traffic\nto flash chips. SkyByte also employs optimization techniques that include\nadaptive page migration for exploring the performance benefits of fast host\nmemory by promoting hot pages in CXL-SSD to the host. We implement SkyByte with\na CXL-SSD simulator and evaluate its efficiency with various data-intensive\napplications. Our experiments show that SkyByte outperforms current CXL-based\nSSD by 6.11X, and reduces the I/O traffic to flash chips by 23.08X on average.\nSkyByte also reaches 75% of the performance of the ideal case that assumes\nunlimited DRAM capacity in the host, which offers an attractive cost-effective\nsolution."
                },
                "authors": [
                    {
                        "name": "Haoyang Zhang"
                    },
                    {
                        "name": "Yuqi Xue"
                    },
                    {
                        "name": "Yirui Eric Zhou"
                    },
                    {
                        "name": "Shaobo Li"
                    },
                    {
                        "name": "Jian Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Huang"
                },
                "author": "Jian Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10682v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10682v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05221v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05221v3",
                "updated": "2025-01-17T16:16:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    16,
                    16,
                    54,
                    4,
                    17,
                    0
                ],
                "published": "2024-09-08T20:47:44Z",
                "published_parsed": [
                    2024,
                    9,
                    8,
                    20,
                    47,
                    44,
                    6,
                    252,
                    0
                ],
                "title": "Geometric rigidity of simple modules for algebraic groups",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geometric rigidity of simple modules for algebraic groups"
                },
                "summary": "Let k be a field, let G be an affine algebraic k-group and V a\nfinite-dimensional G-module. We say V is rigid if the socle series and radical\nseries coincide for the action of G on each indecomposable summand of V; say V\nis geometrically rigid (resp. absolutely rigid) if V is rigid after base change\nof G and V to k (resp. any field extension of k). We show that all simple\nG-modules are geometrically rigid, though not in general absolutely rigid. More\nprecisely, we show that if V is a simple G-module, then there is a finite\npurely inseparable extension kV /k naturally attached to V such that V is\nabsolutely rigid as a G-module after base change to kV. The proof turns on an\ninvestigation of algebras of the form K otimes E where K and E are field\nextensions of k; we give an example of such an algebra which is not rigid as a\nmodule over itself. We establish the existence of the purely inseparable field\nextension kV /k through an analogous version for artinian algebras.\n  In the second half of the paper we apply recent results on the structure and\nrepresentation theory of pseudo-reductive groups to give a concrete description\nof kV when G is smooth and connected. Namely, we combine the main structure\ntheorem of the Conrad-Prasad classification of pseudo-reductive G together with\nour previous high weight theory. For V a simple G-module, we calculate the\nminimal field of definition of the geometric Jacobson radical of EndG(V) in\nterms of the high weight of V and the Conrad-Prasad classification data; this\ngives a concrete construction of the field kV as a subextension of the minimal\nfield of definition of the geometric unipotent radical of G. We also observe\nthat the Conrad-Prasad classification can be used to hone the dimension formula\nfor V we had previously established; we also use it to give a description of\nEndG(V) which includes a dimension formula.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Let k be a field, let G be an affine algebraic k-group and V a\nfinite-dimensional G-module. We say V is rigid if the socle series and radical\nseries coincide for the action of G on each indecomposable summand of V; say V\nis geometrically rigid (resp. absolutely rigid) if V is rigid after base change\nof G and V to k (resp. any field extension of k). We show that all simple\nG-modules are geometrically rigid, though not in general absolutely rigid. More\nprecisely, we show that if V is a simple G-module, then there is a finite\npurely inseparable extension kV /k naturally attached to V such that V is\nabsolutely rigid as a G-module after base change to kV. The proof turns on an\ninvestigation of algebras of the form K otimes E where K and E are field\nextensions of k; we give an example of such an algebra which is not rigid as a\nmodule over itself. We establish the existence of the purely inseparable field\nextension kV /k through an analogous version for artinian algebras.\n  In the second half of the paper we apply recent results on the structure and\nrepresentation theory of pseudo-reductive groups to give a concrete description\nof kV when G is smooth and connected. Namely, we combine the main structure\ntheorem of the Conrad-Prasad classification of pseudo-reductive G together with\nour previous high weight theory. For V a simple G-module, we calculate the\nminimal field of definition of the geometric Jacobson radical of EndG(V) in\nterms of the high weight of V and the Conrad-Prasad classification data; this\ngives a concrete construction of the field kV as a subextension of the minimal\nfield of definition of the geometric unipotent radical of G. We also observe\nthat the Conrad-Prasad classification can be used to hone the dimension formula\nfor V we had previously established; we also use it to give a description of\nEndG(V) which includes a dimension formula."
                },
                "authors": [
                    {
                        "name": "Michael Bate"
                    },
                    {
                        "name": "David I. Stewart"
                    }
                ],
                "author_detail": {
                    "name": "David I. Stewart"
                },
                "author": "David I. Stewart",
                "arxiv_comment": "v3; 30 pages; Theorem 1 now holds for arbitrary affine algebraic\n  groups over fields",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05221v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05221v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.RT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.RT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.RA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "20G05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10138v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10138v1",
                "updated": "2025-01-17T12:01:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    12,
                    1,
                    28,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T12:01:28Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    12,
                    1,
                    28,
                    4,
                    17,
                    0
                ],
                "title": "The NIC should be part of the OS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The NIC should be part of the OS"
                },
                "summary": "The network interface adapter (NIC) is a critical component of a modern cloud\nserver which occupies a unique position. Not only is network performance vital\nto the efficient operation of the machine, but unlike application-oriented\ncompute accelerators like GPUs, the network subsystem must react to\nunpredictable events like the arrival of a network packet and communicate with\nthe appropriate application end point with minimal latency. Current approaches\nto server stacks navigate a trade-off between flexibility, efficiency, and\nperformance: the fastest kernel-bypass approaches dedicate cores to\napplications, busy-wait on receive queues, etc. while more flexible approaches\nappropriate to more dynamic workload mixes incur much greater software overhead\non the data path. However, we reject this trade-off, which we ascribe to an\narbitrary (and sub-optimal) split in system state between the OS and the NIC.\nInstead, by exploiting the properties of cache-coherent interconnects and\nintegrating the NIC closely with the OS kernel, we can achieve something\nsurprising: performance for RPC workloads better than the fastest kernel-bypass\napproaches without sacrificing the robustness and dynamic adaptation of\nkernel-based network subsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The network interface adapter (NIC) is a critical component of a modern cloud\nserver which occupies a unique position. Not only is network performance vital\nto the efficient operation of the machine, but unlike application-oriented\ncompute accelerators like GPUs, the network subsystem must react to\nunpredictable events like the arrival of a network packet and communicate with\nthe appropriate application end point with minimal latency. Current approaches\nto server stacks navigate a trade-off between flexibility, efficiency, and\nperformance: the fastest kernel-bypass approaches dedicate cores to\napplications, busy-wait on receive queues, etc. while more flexible approaches\nappropriate to more dynamic workload mixes incur much greater software overhead\non the data path. However, we reject this trade-off, which we ascribe to an\narbitrary (and sub-optimal) split in system state between the OS and the NIC.\nInstead, by exploiting the properties of cache-coherent interconnects and\nintegrating the NIC closely with the OS kernel, we can achieve something\nsurprising: performance for RPC workloads better than the fastest kernel-bypass\napproaches without sacrificing the robustness and dynamic adaptation of\nkernel-based network subsystems."
                },
                "authors": [
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10138v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10138v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03594v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03594v2",
                "updated": "2025-01-17T09:37:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    9,
                    37,
                    36,
                    4,
                    17,
                    0
                ],
                "published": "2024-11-29T05:57:37Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    5,
                    57,
                    37,
                    4,
                    334,
                    0
                ],
                "title": "BatchLLM: Optimizing Large Batched LLM Inference with Global Prefix\n  Sharing and Throughput-oriented Token Batching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BatchLLM: Optimizing Large Batched LLM Inference with Global Prefix\n  Sharing and Throughput-oriented Token Batching"
                },
                "summary": "Large language models (LLMs) increasingly play an important role in a wide\nrange of information processing and management tasks. Many of these tasks are\nperformed in large batches or even offline, and the performance indictor for\nwhich is throughput. These tasks usually show the characteristic of prefix\nsharing, where different prompt input can partially show the common prefix.\nHowever, the existing LLM inference engines tend to optimize the streaming\nrequests and show limitations of supporting the large batched tasks with the\nprefix sharing characteristic. The existing solutions use the LRU-based cache\nto reuse the KV context of common prefix between requests. The KV context that\nare about to be reused may prematurely evicted with the implicit cache\nmanagement. Besides, the streaming oriented systems do not leverage the\nrequest-batch information and can not mix the decoding tokens with the prefill\nchunks to the best for the batched scenarios, and thus fails to saturate the\nGPU. We propose BatchLLM to address the above problems. BatchLLM explicitly\nidentifies the common prefixes globally. The requests sharing the same prefix\nwill be scheduled together to reuse the KV context the best. BatchLLM reorders\nthe requests and schedules the requests with larger ratio of decoding first to\nbetter mix the decoding tokens with the latter prefill chunks, and applies\nmemory-centric token batching to enlarge the token-batch sizes, which helps to\nincrease the GPU utilization. Finally, BatchLLM optimizes the prefix-shared\nAttention kernel with horizontal fusion to reduce tail effect and kernel launch\noverhead. Extensive evaluation shows that BatchLLM outperforms vLLM and SGLang\nby 1.3$\\times$ to 10.8$\\times$ on a set of microbenchmarks and a typical\nindustry workload under different hardware environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) increasingly play an important role in a wide\nrange of information processing and management tasks. Many of these tasks are\nperformed in large batches or even offline, and the performance indictor for\nwhich is throughput. These tasks usually show the characteristic of prefix\nsharing, where different prompt input can partially show the common prefix.\nHowever, the existing LLM inference engines tend to optimize the streaming\nrequests and show limitations of supporting the large batched tasks with the\nprefix sharing characteristic. The existing solutions use the LRU-based cache\nto reuse the KV context of common prefix between requests. The KV context that\nare about to be reused may prematurely evicted with the implicit cache\nmanagement. Besides, the streaming oriented systems do not leverage the\nrequest-batch information and can not mix the decoding tokens with the prefill\nchunks to the best for the batched scenarios, and thus fails to saturate the\nGPU. We propose BatchLLM to address the above problems. BatchLLM explicitly\nidentifies the common prefixes globally. The requests sharing the same prefix\nwill be scheduled together to reuse the KV context the best. BatchLLM reorders\nthe requests and schedules the requests with larger ratio of decoding first to\nbetter mix the decoding tokens with the latter prefill chunks, and applies\nmemory-centric token batching to enlarge the token-batch sizes, which helps to\nincrease the GPU utilization. Finally, BatchLLM optimizes the prefix-shared\nAttention kernel with horizontal fusion to reduce tail effect and kernel launch\noverhead. Extensive evaluation shows that BatchLLM outperforms vLLM and SGLang\nby 1.3$\\times$ to 10.8$\\times$ on a set of microbenchmarks and a typical\nindustry workload under different hardware environments."
                },
                "authors": [
                    {
                        "name": "Zhen Zheng"
                    },
                    {
                        "name": "Xin Ji"
                    },
                    {
                        "name": "Taosong Fang"
                    },
                    {
                        "name": "Fanghao Zhou"
                    },
                    {
                        "name": "Chuanjie Liu"
                    },
                    {
                        "name": "Gang Peng"
                    }
                ],
                "author_detail": {
                    "name": "Gang Peng"
                },
                "author": "Gang Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03594v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03594v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09902v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09902v1",
                "updated": "2025-01-17T01:24:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    1,
                    24,
                    12,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T01:24:12Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    1,
                    24,
                    12,
                    4,
                    17,
                    0
                ],
                "title": "Multi-Dimensional Vector ISA Extension for Mobile In-Cache Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Dimensional Vector ISA Extension for Mobile In-Cache Computing"
                },
                "summary": "In-cache computing technology transforms existing caches into long-vector\ncompute units and offers low-cost alternatives to building expensive vector\nengines for mobile CPUs. Unfortunately, existing long-vector Instruction Set\nArchitecture (ISA) extensions, such as RISC-V Vector Extension (RVV) and Arm\nScalable Vector Extension (SVE), provide only one-dimensional strided and\nrandom memory accesses. While this is sufficient for typical vector engines, it\nfails to effectively utilize the large Single Instruction, Multiple Data (SIMD)\nwidths of in-cache vector engines. This is because mobile data-parallel kernels\nexpose limited parallelism across a single dimension.\n  Based on our analysis of mobile vector kernels, we introduce a long-vector\nMulti-dimensional Vector ISA Extension (MVE) for mobile in-cache computing. MVE\nachieves high SIMD resource utilization and enables flexible programming by\nabstracting cache geometry and data layout. The proposed ISA features\nmulti-dimensional strided and random memory accesses and efficient\ndimension-level masked execution to encode parallelism across multiple\ndimensions. Using a wide range of data-parallel mobile workloads, we\ndemonstrate that MVE offers significant performance and energy reduction\nbenefits of 2.9x and 8.8x, on average, compared to the SIMD units of a\ncommercial mobile processor, at an area overhead of 3.6%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-cache computing technology transforms existing caches into long-vector\ncompute units and offers low-cost alternatives to building expensive vector\nengines for mobile CPUs. Unfortunately, existing long-vector Instruction Set\nArchitecture (ISA) extensions, such as RISC-V Vector Extension (RVV) and Arm\nScalable Vector Extension (SVE), provide only one-dimensional strided and\nrandom memory accesses. While this is sufficient for typical vector engines, it\nfails to effectively utilize the large Single Instruction, Multiple Data (SIMD)\nwidths of in-cache vector engines. This is because mobile data-parallel kernels\nexpose limited parallelism across a single dimension.\n  Based on our analysis of mobile vector kernels, we introduce a long-vector\nMulti-dimensional Vector ISA Extension (MVE) for mobile in-cache computing. MVE\nachieves high SIMD resource utilization and enables flexible programming by\nabstracting cache geometry and data layout. The proposed ISA features\nmulti-dimensional strided and random memory accesses and efficient\ndimension-level masked execution to encode parallelism across multiple\ndimensions. Using a wide range of data-parallel mobile workloads, we\ndemonstrate that MVE offers significant performance and energy reduction\nbenefits of 2.9x and 8.8x, on average, compared to the SIMD units of a\ncommercial mobile processor, at an area overhead of 3.6%."
                },
                "authors": [
                    {
                        "name": "Alireza Khadem"
                    },
                    {
                        "name": "Daichi Fujiki"
                    },
                    {
                        "name": "Hilbert Chen"
                    },
                    {
                        "name": "Yufeng Gu"
                    },
                    {
                        "name": "Nishil Talati"
                    },
                    {
                        "name": "Scott Mahlke"
                    },
                    {
                        "name": "Reetuparna Das"
                    }
                ],
                "author_detail": {
                    "name": "Reetuparna Das"
                },
                "author": "Reetuparna Das",
                "arxiv_comment": "2025 IEEE International Symposium on High-Performance Computer\n  Architecture (HPCA)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09902v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09902v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.04501v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.04501v2",
                "updated": "2025-01-16T15:11:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    15,
                    11,
                    42,
                    3,
                    16,
                    0
                ],
                "published": "2024-07-05T13:42:30Z",
                "published_parsed": [
                    2024,
                    7,
                    5,
                    13,
                    42,
                    30,
                    4,
                    187,
                    0
                ],
                "title": "Cryogenic Behavior of High-Permittivity Gate Dielectrics: The Impact of\n  the Atomic Layer Deposition Temperature and the Lithographic Patterning\n  Method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cryogenic Behavior of High-Permittivity Gate Dielectrics: The Impact of\n  the Atomic Layer Deposition Temperature and the Lithographic Patterning\n  Method"
                },
                "summary": "Dielectrics featuring a high relative permittivity, i.e., high-k dielectrics,\nhave become the standard insulators in gate architectures, enhancing the\nelectrical performance of both room temperature and cryogenic electronics. This\nstudy delves into the cryogenic (3 K) performance of high-k dielectrics\ncommonly used as gate insulators. We fabricated Al2O3 and HfO2 layers via\nAtomic Layer Deposition (ALD) and we extrapolated relative permittivity (k) and\ndielectric strength (E_BD) from AC (100 Hz to 100 kHz) and DC measurements on\nmetal-insulator-metal capacitors. Our findings reveal a strong dependence of\nHfO2 cryogenic performance on the ALD growth temperature, while the latter\nshows a negligible impact on Al2O3. We estimated a ~9 % and ~14 % reduction of\nthe relative permittivity of HfO2 and Al2O3, respectively, from 300 K to 3 K.\nAdditionally, we designed and fabricated Al2O3/HfO2 bilayers and we checked\ntheir properties at cryogenic temperatures. The study also investigates the\nimpact of the patterning method, namely, UV or electron-beam lithography\n(acceleration voltage of 10, 20, or 30 kV), on the high-k dielectric\nproperties.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dielectrics featuring a high relative permittivity, i.e., high-k dielectrics,\nhave become the standard insulators in gate architectures, enhancing the\nelectrical performance of both room temperature and cryogenic electronics. This\nstudy delves into the cryogenic (3 K) performance of high-k dielectrics\ncommonly used as gate insulators. We fabricated Al2O3 and HfO2 layers via\nAtomic Layer Deposition (ALD) and we extrapolated relative permittivity (k) and\ndielectric strength (E_BD) from AC (100 Hz to 100 kHz) and DC measurements on\nmetal-insulator-metal capacitors. Our findings reveal a strong dependence of\nHfO2 cryogenic performance on the ALD growth temperature, while the latter\nshows a negligible impact on Al2O3. We estimated a ~9 % and ~14 % reduction of\nthe relative permittivity of HfO2 and Al2O3, respectively, from 300 K to 3 K.\nAdditionally, we designed and fabricated Al2O3/HfO2 bilayers and we checked\ntheir properties at cryogenic temperatures. The study also investigates the\nimpact of the patterning method, namely, UV or electron-beam lithography\n(acceleration voltage of 10, 20, or 30 kV), on the high-k dielectric\nproperties."
                },
                "authors": [
                    {
                        "name": "Alessandro Paghi"
                    },
                    {
                        "name": "Sebastiano Battisti"
                    },
                    {
                        "name": "Simone Tortorella"
                    },
                    {
                        "name": "Giorgio De Simoni"
                    },
                    {
                        "name": "Francesco Giazotto"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Giazotto"
                },
                "author": "Francesco Giazotto",
                "arxiv_comment": "17 pages, 4 figures, supporting information at the end of the paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.04501v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.04501v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.11501v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.11501v2",
                "updated": "2025-01-16T10:35:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    10,
                    35,
                    59,
                    3,
                    16,
                    0
                ],
                "published": "2023-12-08T15:11:26Z",
                "published_parsed": [
                    2023,
                    12,
                    8,
                    15,
                    11,
                    26,
                    4,
                    342,
                    0
                ],
                "title": "Write+Sync: Software Cache Write Covert Channels Exploiting Memory-disk\n  Synchronization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Write+Sync: Software Cache Write Covert Channels Exploiting Memory-disk\n  Synchronization"
                },
                "summary": "Memory-disk synchronization is a critical technology for ensuring data\ncorrectness, integrity, and security, especially in systems that handle\nsensitive information like financial transactions and medical records. We\npropose SYNC+SYNC, a group of attacks that exploit the memory-disk\nsynchronization primitives. SYNC+SYNC works by subtly varying the timing of\nsynchronization on the write buffer, offering several advantages: 1)\nimplemented purely in software, enabling deployment on any hardware devices; 2)\nresilient against existing cache partitioning and randomization techniques; 3)\nunaffected by prefetching techniques and cache replacement strategies. We\npresent the principles of SYNC+SYNC through the implementation of two write\ncovert channel protocols, using either a single file or page, and introduce\nthree enhanced strategies that utilize multiple files and pages. The\nfeasibility of these channels is demonstrated in both cross-process and\ncross-sandbox scenarios across diverse operating systems (OSes). Experimental\nresults show that, the average rate can reach 2.036 Kb/s (with a peak rate of\n14.762 Kb/s) and the error rate is 0% on Linux; when running on macOS, the\naverage rate achieves 10.211 Kb/s (with a peak rate of 253.022 Kb/s) and the\nerror rate is 0.004%. To the best of our knowledge, SYNC+SYNC is the first\nhigh-speed write covert channel for software cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-disk synchronization is a critical technology for ensuring data\ncorrectness, integrity, and security, especially in systems that handle\nsensitive information like financial transactions and medical records. We\npropose SYNC+SYNC, a group of attacks that exploit the memory-disk\nsynchronization primitives. SYNC+SYNC works by subtly varying the timing of\nsynchronization on the write buffer, offering several advantages: 1)\nimplemented purely in software, enabling deployment on any hardware devices; 2)\nresilient against existing cache partitioning and randomization techniques; 3)\nunaffected by prefetching techniques and cache replacement strategies. We\npresent the principles of SYNC+SYNC through the implementation of two write\ncovert channel protocols, using either a single file or page, and introduce\nthree enhanced strategies that utilize multiple files and pages. The\nfeasibility of these channels is demonstrated in both cross-process and\ncross-sandbox scenarios across diverse operating systems (OSes). Experimental\nresults show that, the average rate can reach 2.036 Kb/s (with a peak rate of\n14.762 Kb/s) and the error rate is 0% on Linux; when running on macOS, the\naverage rate achieves 10.211 Kb/s (with a peak rate of 253.022 Kb/s) and the\nerror rate is 0.004%. To the best of our knowledge, SYNC+SYNC is the first\nhigh-speed write covert channel for software cache."
                },
                "authors": [
                    {
                        "name": "Congcong Chen"
                    },
                    {
                        "name": "Jinhua Cui"
                    },
                    {
                        "name": "Gang Qu"
                    },
                    {
                        "name": "Jiliang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiliang Zhang"
                },
                "author": "Jiliang Zhang",
                "arxiv_comment": "This manuscript was published in IEEE Transactions on Information\n  Forensics and Security, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.11501v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.11501v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09383v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09383v1",
                "updated": "2025-01-16T08:52:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    8,
                    52,
                    38,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T08:52:38Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    8,
                    52,
                    38,
                    3,
                    16,
                    0
                ],
                "title": "Adaptive Contextual Caching for Mobile Edge Large Language Model Service",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Contextual Caching for Mobile Edge Large Language Model Service"
                },
                "summary": "Mobile edge Large Language Model (LLM) deployments face inherent constraints,\nsuch as limited computational resources and network bandwidth. Although\nRetrieval-Augmented Generation (RAG) mitigates some challenges by integrating\nexternal knowledge bases, inefficient cache management can still result in high\nretrieval latency and frequent cache updates. To address these issues, we\npropose an Adaptive Contextual Caching (ACC) framework that anticipates user\nneeds by proactively caching semantically relevant data for mobile-edge LLMs.\nACC utilizes a deep reinforcement learning (DRL) module to refine cache\nreplacement policies, balancing user context, document similarity, and the\noverhead associated with cache misses. Experimental results demonstrate that\nACC increases cache hit rates to over 80\\% after only 11 training episodes,\noutperforming FIFO, LRU, and semantic-only caching while reducing retrieval\nlatency by up to 40\\%. In particular, ACC also reduces local caching overhead\n(i.e., the cost of updating the cache when a miss occurs) by as much as 55\\%,\nenabling scalable, low-latency LLM services in resource-constrained edge\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile edge Large Language Model (LLM) deployments face inherent constraints,\nsuch as limited computational resources and network bandwidth. Although\nRetrieval-Augmented Generation (RAG) mitigates some challenges by integrating\nexternal knowledge bases, inefficient cache management can still result in high\nretrieval latency and frequent cache updates. To address these issues, we\npropose an Adaptive Contextual Caching (ACC) framework that anticipates user\nneeds by proactively caching semantically relevant data for mobile-edge LLMs.\nACC utilizes a deep reinforcement learning (DRL) module to refine cache\nreplacement policies, balancing user context, document similarity, and the\noverhead associated with cache misses. Experimental results demonstrate that\nACC increases cache hit rates to over 80\\% after only 11 training episodes,\noutperforming FIFO, LRU, and semantic-only caching while reducing retrieval\nlatency by up to 40\\%. In particular, ACC also reduces local caching overhead\n(i.e., the cost of updating the cache when a miss occurs) by as much as 55\\%,\nenabling scalable, low-latency LLM services in resource-constrained edge\nenvironments."
                },
                "authors": [
                    {
                        "name": "Guangyuan Liu"
                    },
                    {
                        "name": "Yinqiu Liu"
                    },
                    {
                        "name": "Jiacheng Wang"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Jiawen Kang"
                    },
                    {
                        "name": "Zehui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Zehui Xiong"
                },
                "author": "Zehui Xiong",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09383v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09383v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09290v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09290v1",
                "updated": "2025-01-16T04:50:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    4,
                    50,
                    15,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T04:50:15Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    4,
                    50,
                    15,
                    3,
                    16,
                    0
                ],
                "title": "Interoceptive Robots for Convergent Shared Control in Collaborative\n  Construction Work",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interoceptive Robots for Convergent Shared Control in Collaborative\n  Construction Work"
                },
                "summary": "Building autonomous mobile robots (AMRs) with optimized efficiency and\nadaptive capabilities-able to respond to changing task demands and dynamic\nenvironments-is a strongly desired goal for advancing construction robotics.\nSuch robots can play a critical role in enabling automation, reducing\noperational carbon footprints, and supporting modular construction processes.\nInspired by the adaptive autonomy of living organisms, we introduce\ninteroception, which centers on the robot's internal state representation, as a\nfoundation for developing self-reflection and conscious learning to enable\ncontinual learning and adaptability in robotic agents. In this paper, we\nfactorize internal state variables and mathematical properties as \"cognitive\ndissonance\" in shared control paradigms, where human interventions occasionally\noccur. We offer a new perspective on how interoception can help build adaptive\nmotion planning in AMRs by integrating the legacy of heuristic costs from\ngrid/graph-based algorithms with recent advances in neuroscience and\nreinforcement learning. Declarative and procedural knowledge extracted from\nhuman semantic inputs is encoded into a hypergraph model that overlaps with the\nspatial configuration of onsite layout for path planning. In addition, we\ndesign a velocity-replay module using an encoder-decoder architecture with\nfew-shot learning to enable robots to replicate velocity profiles in\ncontextualized scenarios for multi-robot synchronization and handover\ncollaboration. These \"cached\" knowledge representations are demonstrated in\nsimulated environments for multi-robot motion planning and stacking tasks. The\ninsights from this study pave the way toward artificial general intelligence in\nAMRs, fostering their progression from complexity to competence in construction\nautomation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building autonomous mobile robots (AMRs) with optimized efficiency and\nadaptive capabilities-able to respond to changing task demands and dynamic\nenvironments-is a strongly desired goal for advancing construction robotics.\nSuch robots can play a critical role in enabling automation, reducing\noperational carbon footprints, and supporting modular construction processes.\nInspired by the adaptive autonomy of living organisms, we introduce\ninteroception, which centers on the robot's internal state representation, as a\nfoundation for developing self-reflection and conscious learning to enable\ncontinual learning and adaptability in robotic agents. In this paper, we\nfactorize internal state variables and mathematical properties as \"cognitive\ndissonance\" in shared control paradigms, where human interventions occasionally\noccur. We offer a new perspective on how interoception can help build adaptive\nmotion planning in AMRs by integrating the legacy of heuristic costs from\ngrid/graph-based algorithms with recent advances in neuroscience and\nreinforcement learning. Declarative and procedural knowledge extracted from\nhuman semantic inputs is encoded into a hypergraph model that overlaps with the\nspatial configuration of onsite layout for path planning. In addition, we\ndesign a velocity-replay module using an encoder-decoder architecture with\nfew-shot learning to enable robots to replicate velocity profiles in\ncontextualized scenarios for multi-robot synchronization and handover\ncollaboration. These \"cached\" knowledge representations are demonstrated in\nsimulated environments for multi-robot motion planning and stacking tasks. The\ninsights from this study pave the way toward artificial general intelligence in\nAMRs, fostering their progression from complexity to competence in construction\nautomation."
                },
                "authors": [
                    {
                        "name": "Xiaoshan Zhou"
                    },
                    {
                        "name": "Carol C. Menassa"
                    },
                    {
                        "name": "Vineet R. Kamat"
                    }
                ],
                "author_detail": {
                    "name": "Vineet R. Kamat"
                },
                "author": "Vineet R. Kamat",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09290v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09290v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09253v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09253v1",
                "updated": "2025-01-16T02:40:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    2,
                    40,
                    7,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T02:40:07Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    2,
                    40,
                    7,
                    3,
                    16,
                    0
                ],
                "title": "PATCHEDSERVE: A Patch Management Framework for SLO-Optimized Hybrid\n  Resolution Diffusion Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PATCHEDSERVE: A Patch Management Framework for SLO-Optimized Hybrid\n  Resolution Diffusion Serving"
                },
                "summary": "The Text-to-Image (T2I) diffusion model is one of the most popular models in\nthe world. However, serving diffusion models at the entire image level faces\nseveral problems, especially when there are multiple candidate resolutions.\nFirst, image based serving system prevents requests with different resolutions\nfrom batching together. On the other hand, requests with hybrid resolutions\nalso indicate diverse locality features, which makes it hard to apply the same\ncache policy to all of them. To this end, we propose PATCHEDSERVE, A Patch\nManagement Framework for SLO-Optimized Hybrid Resolution Diffusion Serving that\nprovides a patch-level management strategy to gather hybrid resolution requests\ninto batches. Specifically, PATCHEDSERVE incorporates a novel patch-based\nprocessing workflow, significantly enhancing throughput for hybrid resolution\ninputs. Furthermore, PATCHEDSERVE designs a patch-level cache reuse policy to\nfully exploit the redundancy in diffusion. In addition, PATCHEDSERVE features\nan SLO-aware scheduling algorithm with lightweight online latency prediction,\nachieving higher SLO satisfaction rates. We show that PATCHEDSERVE can achieve\n30.1 % higher SLO satisfaction compared to SOTA diffusion serving system while\nnot hurt the image quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Text-to-Image (T2I) diffusion model is one of the most popular models in\nthe world. However, serving diffusion models at the entire image level faces\nseveral problems, especially when there are multiple candidate resolutions.\nFirst, image based serving system prevents requests with different resolutions\nfrom batching together. On the other hand, requests with hybrid resolutions\nalso indicate diverse locality features, which makes it hard to apply the same\ncache policy to all of them. To this end, we propose PATCHEDSERVE, A Patch\nManagement Framework for SLO-Optimized Hybrid Resolution Diffusion Serving that\nprovides a patch-level management strategy to gather hybrid resolution requests\ninto batches. Specifically, PATCHEDSERVE incorporates a novel patch-based\nprocessing workflow, significantly enhancing throughput for hybrid resolution\ninputs. Furthermore, PATCHEDSERVE designs a patch-level cache reuse policy to\nfully exploit the redundancy in diffusion. In addition, PATCHEDSERVE features\nan SLO-aware scheduling algorithm with lightweight online latency prediction,\nachieving higher SLO satisfaction rates. We show that PATCHEDSERVE can achieve\n30.1 % higher SLO satisfaction compared to SOTA diffusion serving system while\nnot hurt the image quality."
                },
                "authors": [
                    {
                        "name": "Desen Sun"
                    },
                    {
                        "name": "Zepeng Zhao"
                    },
                    {
                        "name": "Yuke Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yuke Wang"
                },
                "author": "Yuke Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09253v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09253v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.10845v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.10845v2",
                "updated": "2025-01-15T21:09:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    21,
                    9,
                    22,
                    2,
                    15,
                    0
                ],
                "published": "2024-04-16T18:47:07Z",
                "published_parsed": [
                    2024,
                    4,
                    16,
                    18,
                    47,
                    7,
                    1,
                    107,
                    0
                ],
                "title": "Top-k Multi-Armed Bandit Learning for Content Dissemination in Swarms of\n  Micro-UAVs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Top-k Multi-Armed Bandit Learning for Content Dissemination in Swarms of\n  Micro-UAVs"
                },
                "summary": "This paper presents a Micro-Unmanned Aerial Vehicle (UAV)-enhanced content\nmanagement system for disaster scenarios where communication infrastructure is\ngenerally compromised. Utilizing a hybrid network of stationary and mobile\nMicro-UAVs, this system aims to provide crucial content access to isolated\ncommunities. In the developed architecture, stationary anchor UAVs, equipped\nwith vertical and lateral links, serve users in individual disaster-affected\ncommunities. and mobile micro-ferrying UAVs, with enhanced mobility, extend\ncoverage across multiple such communities. The primary goal is to devise a\ncontent dissemination system that dynamically learns caching policies to\nmaximize content accessibility to users left without communication\ninfrastructure. The core contribution is an adaptive content dissemination\nframework that employs a decentralized Top-k Multi-Armed Bandit learning\napproach for efficient UAV caching decisions. This approach accounts for\ngeo-temporal variations in content popularity and diverse user demands.\nAdditionally, a Selective Caching Algorithm is proposed to minimize redundant\ncontent copies by leveraging inter-UAV information sharing. Through functional\nverification and performance evaluation, the proposed framework demonstrates\nimproved system performance and adaptability across varying network sizes,\nmicro-UAV swarms, and content popularity distributions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a Micro-Unmanned Aerial Vehicle (UAV)-enhanced content\nmanagement system for disaster scenarios where communication infrastructure is\ngenerally compromised. Utilizing a hybrid network of stationary and mobile\nMicro-UAVs, this system aims to provide crucial content access to isolated\ncommunities. In the developed architecture, stationary anchor UAVs, equipped\nwith vertical and lateral links, serve users in individual disaster-affected\ncommunities. and mobile micro-ferrying UAVs, with enhanced mobility, extend\ncoverage across multiple such communities. The primary goal is to devise a\ncontent dissemination system that dynamically learns caching policies to\nmaximize content accessibility to users left without communication\ninfrastructure. The core contribution is an adaptive content dissemination\nframework that employs a decentralized Top-k Multi-Armed Bandit learning\napproach for efficient UAV caching decisions. This approach accounts for\ngeo-temporal variations in content popularity and diverse user demands.\nAdditionally, a Selective Caching Algorithm is proposed to minimize redundant\ncontent copies by leveraging inter-UAV information sharing. Through functional\nverification and performance evaluation, the proposed framework demonstrates\nimproved system performance and adaptability across varying network sizes,\nmicro-UAV swarms, and content popularity distributions."
                },
                "authors": [
                    {
                        "name": "Amit Kumar Bhuyan"
                    },
                    {
                        "name": "Hrishikesh Dutta"
                    },
                    {
                        "name": "Subir Biswas"
                    }
                ],
                "author_detail": {
                    "name": "Subir Biswas"
                },
                "author": "Subir Biswas",
                "arxiv_comment": "16 pages, 8 figures, 2 algorithms, 2 tables, journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.10845v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.10845v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09146v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09146v1",
                "updated": "2025-01-15T20:55:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    20,
                    55,
                    13,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T20:55:13Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    20,
                    55,
                    13,
                    2,
                    15,
                    0
                ],
                "title": "Towards Federated Multi-Armed Bandit Learning for Content Dissemination\n  using Swarm of UAVs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Federated Multi-Armed Bandit Learning for Content Dissemination\n  using Swarm of UAVs"
                },
                "summary": "This paper introduces an Unmanned Aerial Vehicle - enabled content management\narchitecture that is suitable for critical content access in communities of\nusers that are communication-isolated during diverse types of disaster\nscenarios. The proposed architecture leverages a hybrid network of stationary\nanchor UAVs and mobile Micro-UAVs for ubiquitous content dissemination. The\nanchor UAVs are equipped with both vertical and lateral communication links,\nand they serve local users, while the mobile micro-ferrying UAVs extend\ncoverage across communities with increased mobility. The focus is on developing\na content dissemination system that dynamically learns optimal caching policies\nto maximize content availability. The core innovation is an adaptive content\ndissemination framework based on distributed Federated Multi-Armed Bandit\nlearning. The goal is to optimize UAV content caching decisions based on\ngeo-temporal content popularity and user demand variations. A Selective Caching\nAlgorithm is also introduced to reduce redundant content replication by\nincorporating inter-UAV information sharing. This method strategically\npreserves the uniqueness in user preferences while amalgamating the\nintelligence across a distributed learning system. This approach improves the\nlearning algorithm's ability to adapt to diverse user preferences. Functional\nverification and performance evaluation confirm the proposed architecture's\nutility across different network sizes, UAV swarms, and content popularity\npatterns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces an Unmanned Aerial Vehicle - enabled content management\narchitecture that is suitable for critical content access in communities of\nusers that are communication-isolated during diverse types of disaster\nscenarios. The proposed architecture leverages a hybrid network of stationary\nanchor UAVs and mobile Micro-UAVs for ubiquitous content dissemination. The\nanchor UAVs are equipped with both vertical and lateral communication links,\nand they serve local users, while the mobile micro-ferrying UAVs extend\ncoverage across communities with increased mobility. The focus is on developing\na content dissemination system that dynamically learns optimal caching policies\nto maximize content availability. The core innovation is an adaptive content\ndissemination framework based on distributed Federated Multi-Armed Bandit\nlearning. The goal is to optimize UAV content caching decisions based on\ngeo-temporal content popularity and user demand variations. A Selective Caching\nAlgorithm is also introduced to reduce redundant content replication by\nincorporating inter-UAV information sharing. This method strategically\npreserves the uniqueness in user preferences while amalgamating the\nintelligence across a distributed learning system. This approach improves the\nlearning algorithm's ability to adapt to diverse user preferences. Functional\nverification and performance evaluation confirm the proposed architecture's\nutility across different network sizes, UAV swarms, and content popularity\npatterns."
                },
                "authors": [
                    {
                        "name": "Amit Kumar Bhuyan"
                    },
                    {
                        "name": "Hrishikesh Dutta"
                    },
                    {
                        "name": "Subir Biswas"
                    }
                ],
                "author_detail": {
                    "name": "Subir Biswas"
                },
                "author": "Subir Biswas",
                "arxiv_comment": "25 pages, 11 figures, 1 table, 4 algorithms, journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09146v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09146v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20166v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20166v2",
                "updated": "2025-01-15T01:34:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    1,
                    34,
                    46,
                    2,
                    15,
                    0
                ],
                "published": "2024-12-28T14:38:16Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    14,
                    38,
                    16,
                    5,
                    363,
                    0
                ],
                "title": "LoL-PIM: Long-Context LLM Decoding with Scalable DRAM-PIM System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoL-PIM: Long-Context LLM Decoding with Scalable DRAM-PIM System"
                },
                "summary": "The expansion of large language models (LLMs) with hundreds of billions of\nparameters presents significant challenges to computational resources,\nparticularly data movement and memory bandwidth. Long-context LLMs, which\nprocess sequences of tens of thousands of tokens, further increase the demand\non the memory system as the complexity in attention layers and key-value cache\nsizes is proportional to the context length. Processing-in-Memory (PIM)\nmaximizes memory bandwidth by moving compute to the data and can address the\nmemory bandwidth challenges; however, PIM is not necessarily scalable to\naccelerate long-context LLM because of limited per-module memory capacity and\nthe inflexibility of fixed-functional unit PIM architecture and static memory\nmanagement. In this work, we propose LoL-PIM which is a multi-node PIM\narchitecture that accelerates long context LLM through hardware-software\nco-design. In particular, we propose how pipeline parallelism can be exploited\nacross a multi-PIM module while a direct PIM access (DPA) controller (or DMA\nfor PIM) is proposed that enables dynamic PIM memory management and results in\nefficient PIM utilization across a diverse range of context length. We\ndeveloped an MLIR-based compiler for LoL-PIM extending a commercial PIM-based\ncompiler where the software modifications were implemented and evaluated, while\nthe hardware changes were modeled in the simulator. Our evaluations demonstrate\nthat LoL-PIM significantly improves throughput and reduces latency for\nlong-context LLM inference, outperforming both multi-GPU and GPU-PIM systems\n(up to 8.54x and 16.0x speedup, respectively), thereby enabling more efficient\ndeployment of LLMs in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The expansion of large language models (LLMs) with hundreds of billions of\nparameters presents significant challenges to computational resources,\nparticularly data movement and memory bandwidth. Long-context LLMs, which\nprocess sequences of tens of thousands of tokens, further increase the demand\non the memory system as the complexity in attention layers and key-value cache\nsizes is proportional to the context length. Processing-in-Memory (PIM)\nmaximizes memory bandwidth by moving compute to the data and can address the\nmemory bandwidth challenges; however, PIM is not necessarily scalable to\naccelerate long-context LLM because of limited per-module memory capacity and\nthe inflexibility of fixed-functional unit PIM architecture and static memory\nmanagement. In this work, we propose LoL-PIM which is a multi-node PIM\narchitecture that accelerates long context LLM through hardware-software\nco-design. In particular, we propose how pipeline parallelism can be exploited\nacross a multi-PIM module while a direct PIM access (DPA) controller (or DMA\nfor PIM) is proposed that enables dynamic PIM memory management and results in\nefficient PIM utilization across a diverse range of context length. We\ndeveloped an MLIR-based compiler for LoL-PIM extending a commercial PIM-based\ncompiler where the software modifications were implemented and evaluated, while\nthe hardware changes were modeled in the simulator. Our evaluations demonstrate\nthat LoL-PIM significantly improves throughput and reduces latency for\nlong-context LLM inference, outperforming both multi-GPU and GPU-PIM systems\n(up to 8.54x and 16.0x speedup, respectively), thereby enabling more efficient\ndeployment of LLMs in real-world applications."
                },
                "authors": [
                    {
                        "name": "Hyucksung Kwon"
                    },
                    {
                        "name": "Kyungmo Koo"
                    },
                    {
                        "name": "Janghyeon Kim"
                    },
                    {
                        "name": "Woongkyu Lee"
                    },
                    {
                        "name": "Minjae Lee"
                    },
                    {
                        "name": "Hyungdeok Lee"
                    },
                    {
                        "name": "Yousub Jung"
                    },
                    {
                        "name": "Jaehan Park"
                    },
                    {
                        "name": "Yosub Song"
                    },
                    {
                        "name": "Byeongsu Yang"
                    },
                    {
                        "name": "Haerang Choi"
                    },
                    {
                        "name": "Guhyun Kim"
                    },
                    {
                        "name": "Jongsoon Won"
                    },
                    {
                        "name": "Woojae Shin"
                    },
                    {
                        "name": "Changhyun Kim"
                    },
                    {
                        "name": "Gyeongcheol Shin"
                    },
                    {
                        "name": "Yongkee Kwon"
                    },
                    {
                        "name": "Ilkon Kim"
                    },
                    {
                        "name": "Euicheol Lim"
                    },
                    {
                        "name": "John Kim"
                    },
                    {
                        "name": "Jungwook Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jungwook Choi"
                },
                "author": "Jungwook Choi",
                "arxiv_comment": "15 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20166v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20166v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08484v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08484v1",
                "updated": "2025-01-14T23:13:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    23,
                    13,
                    14,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T23:13:14Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    23,
                    13,
                    14,
                    1,
                    14,
                    0
                ],
                "title": "CORD: Co-design of Resource Allocation and Deadline Decomposition with\n  Generative Profiling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CORD: Co-design of Resource Allocation and Deadline Decomposition with\n  Generative Profiling"
                },
                "summary": "As multicore hardware is becoming increasingly common in real-time systems,\ntraditional scheduling techniques that assume a single worst-case execution\ntime for a task are no longer adequate, since they ignore the impact of shared\nresources on execution time. When tasks execute concurrently on different\ncores, their execution times often vary substantially with their allocated\nbudgets of shared resources, such as cache and memory bandwidth. Even under a\nspecific resource allocation, the resource use pattern of a task also changes\nwith time during a job execution. It is therefore important to consider the\nrelationship between multicore resources and execution time in task modeling\nand scheduling algorithm design.\n  In this paper, we propose a much more precise execution model for DAG-based\nreal-time tasks that captures the time-varying resource use characteristics of\na task under different budgets of shared resources. We present a generative\nresource profiling algorithm that efficiently predicts, from limited\nmeasurement data, the resource profile of a task at any time during its\nexecution under a given resource budget. The generative profiles can then be\nused to construct the execution models for tasks, using which one can make\ninformed resource allocation decisions. We further introduce a multicore\nresource allocation and deadline decomposition co-design technique for\nDAG-based tasks that leverages the generated execution models to jointly\nallocate resources and deadlines to subtasks, to maximize resource efficiency\nand schedulability. Our evaluation results show that our generative profiling\nalgorithm achieves high accuracy while being efficient, and that our\nco-allocation technique substantially improves schedulability compared to a\nstate-of-the-art deadline decomposition method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As multicore hardware is becoming increasingly common in real-time systems,\ntraditional scheduling techniques that assume a single worst-case execution\ntime for a task are no longer adequate, since they ignore the impact of shared\nresources on execution time. When tasks execute concurrently on different\ncores, their execution times often vary substantially with their allocated\nbudgets of shared resources, such as cache and memory bandwidth. Even under a\nspecific resource allocation, the resource use pattern of a task also changes\nwith time during a job execution. It is therefore important to consider the\nrelationship between multicore resources and execution time in task modeling\nand scheduling algorithm design.\n  In this paper, we propose a much more precise execution model for DAG-based\nreal-time tasks that captures the time-varying resource use characteristics of\na task under different budgets of shared resources. We present a generative\nresource profiling algorithm that efficiently predicts, from limited\nmeasurement data, the resource profile of a task at any time during its\nexecution under a given resource budget. The generative profiles can then be\nused to construct the execution models for tasks, using which one can make\ninformed resource allocation decisions. We further introduce a multicore\nresource allocation and deadline decomposition co-design technique for\nDAG-based tasks that leverages the generated execution models to jointly\nallocate resources and deadlines to subtasks, to maximize resource efficiency\nand schedulability. Our evaluation results show that our generative profiling\nalgorithm achieves high accuracy while being efficient, and that our\nco-allocation technique substantially improves schedulability compared to a\nstate-of-the-art deadline decomposition method."
                },
                "authors": [
                    {
                        "name": "Robert Gifford"
                    },
                    {
                        "name": "Abby Eisenklam"
                    },
                    {
                        "name": "Georgiy A. Bondar"
                    },
                    {
                        "name": "Yifan Cai"
                    },
                    {
                        "name": "Tushar Sial"
                    },
                    {
                        "name": "Linh Thi Xuan Phan"
                    },
                    {
                        "name": "Abhishek Halder"
                    }
                ],
                "author_detail": {
                    "name": "Abhishek Halder"
                },
                "author": "Abhishek Halder",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08484v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08484v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02380v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02380v3",
                "updated": "2025-01-14T20:04:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    20,
                    4,
                    15,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-04T20:59:34Z",
                "published_parsed": [
                    2025,
                    1,
                    4,
                    20,
                    59,
                    34,
                    5,
                    4,
                    0
                ],
                "title": "Reciprocating Locks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reciprocating Locks"
                },
                "summary": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks."
                },
                "authors": [
                    {
                        "name": "Dave Dice"
                    },
                    {
                        "name": "Alex Kogan"
                    }
                ],
                "author_detail": {
                    "name": "Alex Kogan"
                },
                "author": "Alex Kogan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02380v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02380v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08192v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08192v1",
                "updated": "2025-01-14T15:14:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    15,
                    14,
                    10,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T15:14:10Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    15,
                    14,
                    10,
                    1,
                    14,
                    0
                ],
                "title": "PRESERVE: Prefetching Model Weights and KV-Cache in Distributed LLM\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRESERVE: Prefetching Model Weights and KV-Cache in Distributed LLM\n  Serving"
                },
                "summary": "Large language models (LLMs) are widely used across various applications, but\ntheir substantial computational requirements pose significant challenges,\nparticularly in terms of HBM bandwidth bottlenecks and inter-device\ncommunication overhead. In this paper, we present PRESERVE, a novel prefetching\nframework designed to optimize LLM inference by overlapping memory reads for\nmodel weights and KV-cache with collective communication operations. Through\nextensive experiments conducted on commercial AI accelerators, we demonstrate\nup to 1.6x end-to-end speedup on state-of-the-art, open-source LLMs.\nAdditionally, we perform a design space exploration that identifies the optimal\nhardware configuration for the proposed method, showing a further 1.25x\nimprovement in performance per cost by selecting the optimal L2 cache size. Our\nresults show that PRESERVE has the potential to mitigate the memory bottlenecks\nand communication overheads, offering a solution to improve the performance and\nscalability of the LLM inference systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are widely used across various applications, but\ntheir substantial computational requirements pose significant challenges,\nparticularly in terms of HBM bandwidth bottlenecks and inter-device\ncommunication overhead. In this paper, we present PRESERVE, a novel prefetching\nframework designed to optimize LLM inference by overlapping memory reads for\nmodel weights and KV-cache with collective communication operations. Through\nextensive experiments conducted on commercial AI accelerators, we demonstrate\nup to 1.6x end-to-end speedup on state-of-the-art, open-source LLMs.\nAdditionally, we perform a design space exploration that identifies the optimal\nhardware configuration for the proposed method, showing a further 1.25x\nimprovement in performance per cost by selecting the optimal L2 cache size. Our\nresults show that PRESERVE has the potential to mitigate the memory bottlenecks\nand communication overheads, offering a solution to improve the performance and\nscalability of the LLM inference systems."
                },
                "authors": [
                    {
                        "name": "Ahmet Caner Yüzügüler"
                    },
                    {
                        "name": "Jiawei Zhuang"
                    },
                    {
                        "name": "Lukas Cavigelli"
                    }
                ],
                "author_detail": {
                    "name": "Lukas Cavigelli"
                },
                "author": "Lukas Cavigelli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08192v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08192v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15102v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15102v2",
                "updated": "2025-01-14T14:07:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    14,
                    7,
                    55,
                    1,
                    14,
                    0
                ],
                "published": "2024-11-22T18:06:14Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    18,
                    6,
                    14,
                    4,
                    327,
                    0
                ],
                "title": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution"
                },
                "summary": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods."
                },
                "authors": [
                    {
                        "name": "Fengyuan Liu"
                    },
                    {
                        "name": "Nikhil Kandpal"
                    },
                    {
                        "name": "Colin Raffel"
                    }
                ],
                "author_detail": {
                    "name": "Colin Raffel"
                },
                "author": "Colin Raffel",
                "arxiv_comment": "29 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15102v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15102v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04987v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04987v2",
                "updated": "2025-01-14T12:06:33Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    12,
                    6,
                    33,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-09T06:00:27Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    6,
                    0,
                    27,
                    3,
                    9,
                    0
                ],
                "title": "TreeKV: Smooth Key-Value Cache Compression with Tree Structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TreeKV: Smooth Key-Value Cache Compression with Tree Structures"
                },
                "summary": "Efficient key-value (KV) cache compression is critical for scaling\ntransformer-based Large Language Models (LLMs) in long sequences and\nresource-limited settings. Existing methods evict tokens based on their\npositions or importance scores, but position-based strategies can miss crucial\ninformation outside predefined regions, while those relying on global\nimportance scores resulting in strong regional biases, limiting the KV cache's\noverall context retention and potentially impairing the performance of LLMs on\ncomplex tasks. Our wavelet analysis reveals that as tokens approach the end of\nsequence, their contributions to generation gradually increase and tends to\ndiverge more from neighboring tokens, indicating a smooth transition with\nincreasing complexity and variability from distant to nearby context. Motivated\nby this observation, we propose TreeKV, an intuitive, training-free method that\nemploys a tree structure for smooth cache compression. TreeKV maintains a fixed\ncache size, allowing LLMs to deliver high-quality output even in long text\nscenarios. Unlike most compression methods, TreeKV is applicable to both the\ngeneration and prefilling stages. TreeKV consistently surpasses all baseline\nmodels in language modeling tasks on PG19 and OpenWebText2, allowing LLMs\ntrained with short context window to generalize to longer window with a 16x\ncache reduction. On the Longbench benchmark, TreeKV achieves the best\nperformance with only 6\\% of the budget at optimal efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient key-value (KV) cache compression is critical for scaling\ntransformer-based Large Language Models (LLMs) in long sequences and\nresource-limited settings. Existing methods evict tokens based on their\npositions or importance scores, but position-based strategies can miss crucial\ninformation outside predefined regions, while those relying on global\nimportance scores resulting in strong regional biases, limiting the KV cache's\noverall context retention and potentially impairing the performance of LLMs on\ncomplex tasks. Our wavelet analysis reveals that as tokens approach the end of\nsequence, their contributions to generation gradually increase and tends to\ndiverge more from neighboring tokens, indicating a smooth transition with\nincreasing complexity and variability from distant to nearby context. Motivated\nby this observation, we propose TreeKV, an intuitive, training-free method that\nemploys a tree structure for smooth cache compression. TreeKV maintains a fixed\ncache size, allowing LLMs to deliver high-quality output even in long text\nscenarios. Unlike most compression methods, TreeKV is applicable to both the\ngeneration and prefilling stages. TreeKV consistently surpasses all baseline\nmodels in language modeling tasks on PG19 and OpenWebText2, allowing LLMs\ntrained with short context window to generalize to longer window with a 16x\ncache reduction. On the Longbench benchmark, TreeKV achieves the best\nperformance with only 6\\% of the budget at optimal efficiency."
                },
                "authors": [
                    {
                        "name": "Ziwei He"
                    },
                    {
                        "name": "Jian Yuan"
                    },
                    {
                        "name": "Haoli Bai"
                    },
                    {
                        "name": "Jingwen Leng"
                    },
                    {
                        "name": "Bo Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Jiang"
                },
                "author": "Bo Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04987v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04987v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.15896v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.15896v2",
                "updated": "2025-01-14T11:41:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    11,
                    41,
                    14,
                    1,
                    14,
                    0
                ],
                "published": "2024-03-23T17:38:57Z",
                "published_parsed": [
                    2024,
                    3,
                    23,
                    17,
                    38,
                    57,
                    5,
                    83,
                    0
                ],
                "title": "Cell-level modelling of homeostasis in confined epithelial monolayers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cell-level modelling of homeostasis in confined epithelial monolayers"
                },
                "summary": "Tissue homeostasis, the biological process of maintaining a steady state in\ntissue via control of cell proliferation, death, and metabolic function, is\nessential for the development, growth, maintenance, and proper function of\nliving organisms. Disruptions to this process can lead to serious diseases and\neven death. In this study, we use the vertex model for the cell-level\ndescription of tissue mechanics to investigate the impact of the tissue\nmicroenvironment and local mechanical properties of cells on homeostasis in\nconfined epithelial tissues. We find a dynamic steady state, where the balance\nbetween cell divisions and removals sustains homeostasis. By characterising\nhomeostasis in terms of cell count, tissue area, and the cells' neighbour count\ndistribution, we identify the factors that govern regulated and ordered tissue\ngrowth. This work, therefore, sheds light on the mechanisms underlying tissue\nhomeostasis and highlights the importance of mechanics in the control of\nbiological processes such as tissue development and disease pathology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tissue homeostasis, the biological process of maintaining a steady state in\ntissue via control of cell proliferation, death, and metabolic function, is\nessential for the development, growth, maintenance, and proper function of\nliving organisms. Disruptions to this process can lead to serious diseases and\neven death. In this study, we use the vertex model for the cell-level\ndescription of tissue mechanics to investigate the impact of the tissue\nmicroenvironment and local mechanical properties of cells on homeostasis in\nconfined epithelial tissues. We find a dynamic steady state, where the balance\nbetween cell divisions and removals sustains homeostasis. By characterising\nhomeostasis in terms of cell count, tissue area, and the cells' neighbour count\ndistribution, we identify the factors that govern regulated and ordered tissue\ngrowth. This work, therefore, sheds light on the mechanisms underlying tissue\nhomeostasis and highlights the importance of mechanics in the control of\nbiological processes such as tissue development and disease pathology."
                },
                "authors": [
                    {
                        "name": "KVS Chaithanya"
                    },
                    {
                        "name": "Jan Rozman"
                    },
                    {
                        "name": "Andrej Košmrlj"
                    },
                    {
                        "name": "Rastko Sknepnek"
                    }
                ],
                "author_detail": {
                    "name": "Rastko Sknepnek"
                },
                "author": "Rastko Sknepnek",
                "arxiv_comment": "18 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.15896v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.15896v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.bio-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19255v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19255v2",
                "updated": "2025-01-14T05:48:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    5,
                    48,
                    7,
                    1,
                    14,
                    0
                ],
                "published": "2024-12-26T15:45:45Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    15,
                    45,
                    45,
                    3,
                    361,
                    0
                ],
                "title": "Multi-matrix Factorization Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-matrix Factorization Attention"
                },
                "summary": "We propose novel attention architectures, Multi-matrix Factorization\nAttention (MFA) and MFA-Key-Reuse (MFA-KR). Existing variants for standard\nMulti-Head Attention (MHA), including SOTA methods like MLA, fail to maintain\nas strong performance under stringent Key-Value cache (KV cache) constraints.\nMFA enhances model capacity by efficiently scaling up both the number and\ndimension of attention heads through low-rank matrix factorization in the\nQuery-Key (QK) circuit. Extending MFA, MFA-KR further reduces memory\nrequirements by repurposing the key cache as value through value projection\nre-parameterization. MFA's design enables strong model capacity when working\nunder tight KV cache budget, while MFA-KR is suitable for even harsher KV cache\nlimits with minor performance trade-off. Notably, in our extensive and\nlarge-scale experiments, the proposed architecture outperforms MLA and performs\ncomparably to MHA, while reducing KV cache usage by up to 56% and 93.7%,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose novel attention architectures, Multi-matrix Factorization\nAttention (MFA) and MFA-Key-Reuse (MFA-KR). Existing variants for standard\nMulti-Head Attention (MHA), including SOTA methods like MLA, fail to maintain\nas strong performance under stringent Key-Value cache (KV cache) constraints.\nMFA enhances model capacity by efficiently scaling up both the number and\ndimension of attention heads through low-rank matrix factorization in the\nQuery-Key (QK) circuit. Extending MFA, MFA-KR further reduces memory\nrequirements by repurposing the key cache as value through value projection\nre-parameterization. MFA's design enables strong model capacity when working\nunder tight KV cache budget, while MFA-KR is suitable for even harsher KV cache\nlimits with minor performance trade-off. Notably, in our extensive and\nlarge-scale experiments, the proposed architecture outperforms MLA and performs\ncomparably to MHA, while reducing KV cache usage by up to 56% and 93.7%,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Jingcheng Hu"
                    },
                    {
                        "name": "Houyi Li"
                    },
                    {
                        "name": "Yinmin Zhang"
                    },
                    {
                        "name": "Zili Wang"
                    },
                    {
                        "name": "Shuigeng Zhou"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Heung-Yeung Shum"
                    },
                    {
                        "name": "Daxin Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Daxin Jiang"
                },
                "author": "Daxin Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19255v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19255v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.10480v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.10480v2",
                "updated": "2025-01-14T05:00:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    5,
                    0,
                    34,
                    1,
                    14,
                    0
                ],
                "published": "2024-05-17T00:52:39Z",
                "published_parsed": [
                    2024,
                    5,
                    17,
                    0,
                    52,
                    39,
                    4,
                    138,
                    0
                ],
                "title": "Lean Attention: Hardware-Aware Scalable Attention Mechanism for the\n  Decode-Phase of Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lean Attention: Hardware-Aware Scalable Attention Mechanism for the\n  Decode-Phase of Transformers"
                },
                "summary": "Transformer-based models have emerged as one of the most widely used\narchitectures for natural language processing, natural language generation, and\nimage generation. The size of the state-of-the-art models has increased\nsteadily reaching billions of parameters. These huge models are memory hungry\nand incur significant inference latency even on cutting edge AI-accelerators,\nsuch as GPUs. Specifically, the time and memory complexity of the attention\noperation is quadratic in terms of the total context length, i.e., prompt and\noutput tokens. Thus, several optimizations such as key-value tensor caching and\nFlashAttention computation have been proposed to deliver the low latency\ndemands of applications relying on such large models. However, these techniques\ndo not cater to the computationally distinct nature of different phases during\ninference.\n  To that end, we propose LeanAttention, a scalable technique of computing\nself-attention for the token-generation phase (decode-phase) of decoder-only\ntransformer models. LeanAttention enables scaling the attention mechanism\nimplementation for the challenging case of long context lengths by re-designing\nthe execution flow for the decode-phase. We identify that the associative\nproperty of online softmax can be treated as a reduction operation thus\nallowing us to parallelize the attention computation over these large context\nlengths. We extend the \"stream-K\" style reduction of tiled calculation to\nself-attention to enable parallel computation resulting in an average of 2.6x\nattention execution speedup over FlashAttention-2 and up to 8.33x speedup for\n512k context lengths.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based models have emerged as one of the most widely used\narchitectures for natural language processing, natural language generation, and\nimage generation. The size of the state-of-the-art models has increased\nsteadily reaching billions of parameters. These huge models are memory hungry\nand incur significant inference latency even on cutting edge AI-accelerators,\nsuch as GPUs. Specifically, the time and memory complexity of the attention\noperation is quadratic in terms of the total context length, i.e., prompt and\noutput tokens. Thus, several optimizations such as key-value tensor caching and\nFlashAttention computation have been proposed to deliver the low latency\ndemands of applications relying on such large models. However, these techniques\ndo not cater to the computationally distinct nature of different phases during\ninference.\n  To that end, we propose LeanAttention, a scalable technique of computing\nself-attention for the token-generation phase (decode-phase) of decoder-only\ntransformer models. LeanAttention enables scaling the attention mechanism\nimplementation for the challenging case of long context lengths by re-designing\nthe execution flow for the decode-phase. We identify that the associative\nproperty of online softmax can be treated as a reduction operation thus\nallowing us to parallelize the attention computation over these large context\nlengths. We extend the \"stream-K\" style reduction of tiled calculation to\nself-attention to enable parallel computation resulting in an average of 2.6x\nattention execution speedup over FlashAttention-2 and up to 8.33x speedup for\n512k context lengths."
                },
                "authors": [
                    {
                        "name": "Rya Sanovar"
                    },
                    {
                        "name": "Srikant Bharadwaj"
                    },
                    {
                        "name": "Renee St. Amant"
                    },
                    {
                        "name": "Victor Rühle"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    }
                ],
                "author_detail": {
                    "name": "Saravan Rajmohan"
                },
                "author": "Saravan Rajmohan",
                "arxiv_comment": "13 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.10480v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.10480v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; C.1.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05262v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05262v2",
                "updated": "2025-01-14T02:02:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    2,
                    2,
                    1,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-09T14:16:43Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    14,
                    16,
                    43,
                    3,
                    9,
                    0
                ],
                "title": "QMDB: Quick Merkle Database",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QMDB: Quick Merkle Database"
                },
                "summary": "Quick Merkle Database (QMDB) addresses longstanding bottlenecks in blockchain\nstate management by integrating key-value (KV) and Merkle tree storage into a\nsingle unified architecture. QMDB delivers a significant throughput improvement\nover existing architectures, achieving up to 6X over the widely used RocksDB\nand 8X over NOMT, a leading verifiable database. Its novel append-only\ntwig-based design enables one SSD read per state access, O(1) IOs for updates,\nand in-memory Merkleization on a memory footprint as small as 2.3 bytes per\nentry, enabling it to run on even modest consumer-grade PCs. QMDB scales\nseamlessly across both commodity and enterprise hardware, achieving up to 2.28\nmillion state updates per second. This performance enables support for 1\nmillion token transfers per second (TPS), marking QMDB as the first solution\nachieving such a milestone. QMDB has been benchmarked with workloads exceeding\n15 billion entries (10X Ethereum's 2024 state) and has proven the capacity to\nscale to 280 billion entries on a single server. Furthermore, QMDB introduces\nhistorical proofs, unlocking the ability to query its blockchain's historical\nstate at the latest block. QMDB not only meets the demands of current\nblockchains but also provides a robust foundation for building scalable,\nefficient, and verifiable decentralized applications across diverse use cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quick Merkle Database (QMDB) addresses longstanding bottlenecks in blockchain\nstate management by integrating key-value (KV) and Merkle tree storage into a\nsingle unified architecture. QMDB delivers a significant throughput improvement\nover existing architectures, achieving up to 6X over the widely used RocksDB\nand 8X over NOMT, a leading verifiable database. Its novel append-only\ntwig-based design enables one SSD read per state access, O(1) IOs for updates,\nand in-memory Merkleization on a memory footprint as small as 2.3 bytes per\nentry, enabling it to run on even modest consumer-grade PCs. QMDB scales\nseamlessly across both commodity and enterprise hardware, achieving up to 2.28\nmillion state updates per second. This performance enables support for 1\nmillion token transfers per second (TPS), marking QMDB as the first solution\nachieving such a milestone. QMDB has been benchmarked with workloads exceeding\n15 billion entries (10X Ethereum's 2024 state) and has proven the capacity to\nscale to 280 billion entries on a single server. Furthermore, QMDB introduces\nhistorical proofs, unlocking the ability to query its blockchain's historical\nstate at the latest block. QMDB not only meets the demands of current\nblockchains but also provides a robust foundation for building scalable,\nefficient, and verifiable decentralized applications across diverse use cases."
                },
                "authors": [
                    {
                        "name": "Isaac Zhang"
                    },
                    {
                        "name": "Ryan Zarick"
                    },
                    {
                        "name": "Daniel Wong"
                    },
                    {
                        "name": "Thomas Kim"
                    },
                    {
                        "name": "Bryan Pellegrino"
                    },
                    {
                        "name": "Mignon Li"
                    },
                    {
                        "name": "Kelvin Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kelvin Wong"
                },
                "author": "Kelvin Wong",
                "arxiv_comment": "11 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05262v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05262v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07523v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07523v1",
                "updated": "2025-01-13T17:50:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    17,
                    50,
                    30,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T17:50:30Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    17,
                    50,
                    30,
                    0,
                    13,
                    0
                ],
                "title": "Parallel Key-Value Cache Fusion for Position Invariant RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel Key-Value Cache Fusion for Position Invariant RAG"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) underscore the necessity\nof Retrieval Augmented Generation (RAG) to leverage external information.\nHowever, LLMs are sensitive to the position of relevant information within\ncontexts and tend to generate incorrect responses when such information is\nplaced in the middle, known as `Lost in the Middle' phenomenon. In this paper,\nwe introduce a framework that generates consistent outputs for decoder-only\nmodels, irrespective of the input context order. Experimental results for three\nopen domain question answering tasks demonstrate position invariance, where the\nmodel is not sensitive to input context order, and superior robustness to\nirrelevent passages compared to prevailing approaches for RAG pipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) underscore the necessity\nof Retrieval Augmented Generation (RAG) to leverage external information.\nHowever, LLMs are sensitive to the position of relevant information within\ncontexts and tend to generate incorrect responses when such information is\nplaced in the middle, known as `Lost in the Middle' phenomenon. In this paper,\nwe introduce a framework that generates consistent outputs for decoder-only\nmodels, irrespective of the input context order. Experimental results for three\nopen domain question answering tasks demonstrate position invariance, where the\nmodel is not sensitive to input context order, and superior robustness to\nirrelevent passages compared to prevailing approaches for RAG pipelines."
                },
                "authors": [
                    {
                        "name": "Philhoon Oh"
                    },
                    {
                        "name": "Jinwoo Shin"
                    },
                    {
                        "name": "James Thorne"
                    }
                ],
                "author_detail": {
                    "name": "James Thorne"
                },
                "author": "James Thorne",
                "arxiv_comment": "5 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07523v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07752v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07752v2",
                "updated": "2025-01-13T17:34:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    17,
                    34,
                    22,
                    0,
                    13,
                    0
                ],
                "published": "2024-12-10T18:50:37Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    50,
                    37,
                    1,
                    345,
                    0
                ],
                "title": "FlashRNN: Optimizing Traditional RNNs on Modern Hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashRNN: Optimizing Traditional RNNs on Modern Hardware"
                },
                "summary": "While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: \\url{https://github.com/NX-AI/flashrnn}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: \\url{https://github.com/NX-AI/flashrnn}"
                },
                "authors": [
                    {
                        "name": "Korbinian Pöppel"
                    },
                    {
                        "name": "Maximilian Beck"
                    },
                    {
                        "name": "Sepp Hochreiter"
                    }
                ],
                "author_detail": {
                    "name": "Sepp Hochreiter"
                },
                "author": "Sepp Hochreiter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07752v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07752v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.07533v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.07533v4",
                "updated": "2025-01-13T09:33:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    9,
                    33,
                    25,
                    0,
                    13,
                    0
                ],
                "published": "2024-05-13T08:03:32Z",
                "published_parsed": [
                    2024,
                    5,
                    13,
                    8,
                    3,
                    32,
                    0,
                    134,
                    0
                ],
                "title": "DID Link: Authentication in TLS with Decentralized Identifiers and\n  Verifiable Credentials",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DID Link: Authentication in TLS with Decentralized Identifiers and\n  Verifiable Credentials"
                },
                "summary": "Authentication in TLS is predominately carried out with X.509 digital\ncertificates issued by certificate authorities (CA). The centralized nature of\ncurrent public key infrastructures, however, comes along with severe risks,\nsuch as single points of failure and susceptibility to cyber-attacks,\npotentially undermining the security and trustworthiness of the entire system.\nWith Decentralized Identifiers (DID) alongside distributed ledger technology,\nit becomes technically feasible to prove ownership of a unique identifier\nwithout requiring an attestation of the proof's public key by a centralized and\ntherefore vulnerable CA. This article presents DID Link, a novel authentication\nscheme for TLS 1.3 that empowers entities to authenticate in a TLS-compliant\nway with self-issued X.509 certificates that are equipped with ledger-anchored\nDIDs instead of CA-issued identifiers. It facilitates the exchange of\ntamper-proof and 3rd-party attested claims in the form of DID-bound Verifiable\nCredentials after the TLS handshake to complete the authentication with a full\nidentification of the communication partner. A prototypical implementation\nshows comparable TLS handshake durations of DID Link if verification material\nis cached and reasonable prolongations if it is obtained from a ledger. The\nsignificant speed improvement of the resulting TLS channel over a widely used,\nDID-based alternative transport protocol on the application layer demonstrates\nthe potential of DID Link to become a viable solution for the establishment of\nsecure and trustful end-to-end communication links with decentrally managed\ndigital identities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Authentication in TLS is predominately carried out with X.509 digital\ncertificates issued by certificate authorities (CA). The centralized nature of\ncurrent public key infrastructures, however, comes along with severe risks,\nsuch as single points of failure and susceptibility to cyber-attacks,\npotentially undermining the security and trustworthiness of the entire system.\nWith Decentralized Identifiers (DID) alongside distributed ledger technology,\nit becomes technically feasible to prove ownership of a unique identifier\nwithout requiring an attestation of the proof's public key by a centralized and\ntherefore vulnerable CA. This article presents DID Link, a novel authentication\nscheme for TLS 1.3 that empowers entities to authenticate in a TLS-compliant\nway with self-issued X.509 certificates that are equipped with ledger-anchored\nDIDs instead of CA-issued identifiers. It facilitates the exchange of\ntamper-proof and 3rd-party attested claims in the form of DID-bound Verifiable\nCredentials after the TLS handshake to complete the authentication with a full\nidentification of the communication partner. A prototypical implementation\nshows comparable TLS handshake durations of DID Link if verification material\nis cached and reasonable prolongations if it is obtained from a ledger. The\nsignificant speed improvement of the resulting TLS channel over a widely used,\nDID-based alternative transport protocol on the application layer demonstrates\nthe potential of DID Link to become a viable solution for the establishment of\nsecure and trustful end-to-end communication links with decentrally managed\ndigital identities."
                },
                "authors": [
                    {
                        "name": "Sandro Rodriguez Garzon"
                    },
                    {
                        "name": "Dennis Natusch"
                    },
                    {
                        "name": "Artur Philipp"
                    },
                    {
                        "name": "Axel Küpper"
                    },
                    {
                        "name": "Hans Joachim Einsiedler"
                    },
                    {
                        "name": "Daniela Schneider"
                    }
                ],
                "author_detail": {
                    "name": "Daniela Schneider"
                },
                "author": "Daniela Schneider",
                "arxiv_doi": "10.1109/PST62714.2024.10788053",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/PST62714.2024.10788053",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.07533v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.07533v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by and presented at 21st Annual International Conference on\n  Privacy, Security, and Trust (PST2024)",
                "arxiv_journal_ref": "2024 21st Annual International Conference on Privacy, Security and\n  Trust (PST), 2024, pp. 1-11",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07056v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07056v1",
                "updated": "2025-01-13T04:31:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    4,
                    31,
                    4,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T04:31:04Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    4,
                    31,
                    4,
                    0,
                    13,
                    0
                ],
                "title": "Generating Data Locality to Accelerate Sparse Matrix-Matrix\n  Multiplication on CPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating Data Locality to Accelerate Sparse Matrix-Matrix\n  Multiplication on CPUs"
                },
                "summary": "Sparse GEneral Matrix-matrix Multiplication (SpGEMM) is a critical operation\nin many applications. Current multithreaded implementations are based on\nGustavson's algorithm and often perform poorly on large matrices due to limited\ncache reuse by the accumulators. We present MAGNUS (Matrix Algebra for Gigantic\nNUmerical Systems), a novel algorithm to maximize data locality in SpGEMM. To\ngenerate locality, MAGNUS reorders the intermediate product into discrete\ncache-friendly chunks using a two-level hierarchical approach. The accumulator\nis applied to each chunk, where the chunk size is chosen such that the\naccumulator is cache-efficient. MAGNUS is input- and system-aware: based on the\nmatrix characteristics and target system specifications, the optimal number of\nchunks is computed by minimizing the storage cost of the necessary data\nstructures. MAGNUS allows for a hybrid accumulation strategy in which each\nchunk uses a different accumulator based on an input threshold. We consider two\naccumulators: an AVX-512 vectorized bitonic sorting algorithm and classical\ndense accumulation. An OpenMP implementation of MAGNUS is compared with several\nbaselines for a variety of different matrices on three Intel x86 architectures.\nFor matrices from the SuiteSparse collection, MAGNUS is faster than all the\nbaselines in most cases and is orders of magnitude faster than Intel MKL for\nseveral matrices. For massive random matrices that model social network graphs,\nMAGNUS scales to the largest matrix sizes, while the baselines fail to do so.\nFurthermore, MAGNUS is close to the optimal bound for these matrices,\nregardless of the matrix size, structure, and density.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse GEneral Matrix-matrix Multiplication (SpGEMM) is a critical operation\nin many applications. Current multithreaded implementations are based on\nGustavson's algorithm and often perform poorly on large matrices due to limited\ncache reuse by the accumulators. We present MAGNUS (Matrix Algebra for Gigantic\nNUmerical Systems), a novel algorithm to maximize data locality in SpGEMM. To\ngenerate locality, MAGNUS reorders the intermediate product into discrete\ncache-friendly chunks using a two-level hierarchical approach. The accumulator\nis applied to each chunk, where the chunk size is chosen such that the\naccumulator is cache-efficient. MAGNUS is input- and system-aware: based on the\nmatrix characteristics and target system specifications, the optimal number of\nchunks is computed by minimizing the storage cost of the necessary data\nstructures. MAGNUS allows for a hybrid accumulation strategy in which each\nchunk uses a different accumulator based on an input threshold. We consider two\naccumulators: an AVX-512 vectorized bitonic sorting algorithm and classical\ndense accumulation. An OpenMP implementation of MAGNUS is compared with several\nbaselines for a variety of different matrices on three Intel x86 architectures.\nFor matrices from the SuiteSparse collection, MAGNUS is faster than all the\nbaselines in most cases and is orders of magnitude faster than Intel MKL for\nseveral matrices. For massive random matrices that model social network graphs,\nMAGNUS scales to the largest matrix sizes, while the baselines fail to do so.\nFurthermore, MAGNUS is close to the optimal bound for these matrices,\nregardless of the matrix size, structure, and density."
                },
                "authors": [
                    {
                        "name": "Jordi Wolfson-Pou"
                    },
                    {
                        "name": "Jan Laukemann"
                    },
                    {
                        "name": "Fabrizio Petrini"
                    }
                ],
                "author_detail": {
                    "name": "Fabrizio Petrini"
                },
                "author": "Fabrizio Petrini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07056v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07056v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.03058v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.03058v5",
                "updated": "2025-01-13T03:11:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    3,
                    11,
                    28,
                    0,
                    13,
                    0
                ],
                "published": "2024-05-05T21:41:43Z",
                "published_parsed": [
                    2024,
                    5,
                    5,
                    21,
                    41,
                    43,
                    6,
                    126,
                    0
                ],
                "title": "A Unified Framework for Automated Code Transformation and Pragma\n  Insertion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Framework for Automated Code Transformation and Pragma\n  Insertion"
                },
                "summary": "High-level synthesis, source-to-source compilers, and various Design Space\nExploration techniques for pragma insertion have significantly improved the\nQuality of Results of generated designs. These tools offer benefits such as\nreduced development time and enhanced performance. However, achieving\nhigh-quality results often requires additional manual code transformations and\ntiling selections, which are typically performed separately or as\npre-processing steps. Although DSE techniques enable code transformation\nupfront, the vastness of the search space often limits the exploration of all\npossible code transformations, making it challenging to determine which\ntransformations are necessary. Additionally, ensuring correctness remains\nchallenging, especially for complex transformations and optimizations.\n  To tackle this obstacle, we first propose a comprehensive framework\nleveraging HLS compilers. Our system streamlines code transformation, pragma\ninsertion, and tiles size selection for on-chip data caching through a unified\noptimization problem, aiming to enhance parallelization, particularly\nbeneficial for computation-bound kernels. Them employing a novel Non-Linear\nProgramming (NLP) approach, we simultaneously ascertain transformations,\npragmas, and tile sizes, focusing on regular loop-based kernels. Our evaluation\ndemonstrates that our framework adeptly identifies the appropriate\ntransformations, including scenarios where no transformation is necessary, and\ninserts pragmas to achieve a favorable Quality of Results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-level synthesis, source-to-source compilers, and various Design Space\nExploration techniques for pragma insertion have significantly improved the\nQuality of Results of generated designs. These tools offer benefits such as\nreduced development time and enhanced performance. However, achieving\nhigh-quality results often requires additional manual code transformations and\ntiling selections, which are typically performed separately or as\npre-processing steps. Although DSE techniques enable code transformation\nupfront, the vastness of the search space often limits the exploration of all\npossible code transformations, making it challenging to determine which\ntransformations are necessary. Additionally, ensuring correctness remains\nchallenging, especially for complex transformations and optimizations.\n  To tackle this obstacle, we first propose a comprehensive framework\nleveraging HLS compilers. Our system streamlines code transformation, pragma\ninsertion, and tiles size selection for on-chip data caching through a unified\noptimization problem, aiming to enhance parallelization, particularly\nbeneficial for computation-bound kernels. Them employing a novel Non-Linear\nProgramming (NLP) approach, we simultaneously ascertain transformations,\npragmas, and tile sizes, focusing on regular loop-based kernels. Our evaluation\ndemonstrates that our framework adeptly identifies the appropriate\ntransformations, including scenarios where no transformation is necessary, and\ninserts pragmas to achieve a favorable Quality of Results."
                },
                "authors": [
                    {
                        "name": "Stéphane Pouget"
                    },
                    {
                        "name": "Louis-Noël Pouchet"
                    },
                    {
                        "name": "Jason Cong"
                    }
                ],
                "author_detail": {
                    "name": "Jason Cong"
                },
                "author": "Jason Cong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.03058v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.03058v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06872v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06872v1",
                "updated": "2025-01-12T17:01:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    17,
                    1,
                    40,
                    6,
                    12,
                    0
                ],
                "published": "2025-01-12T17:01:40Z",
                "published_parsed": [
                    2025,
                    1,
                    12,
                    17,
                    1,
                    40,
                    6,
                    12,
                    0
                ],
                "title": "On Optimizing Locality of Graph Transposition on Modern Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Optimizing Locality of Graph Transposition on Modern Architectures"
                },
                "summary": "This paper investigates the shared-memory Graph Transposition (GT) problem, a\nfundamental graph algorithm that is widely used in graph analytics and\nscientific computing.\n  Previous GT algorithms have significant memory requirements that are\nproportional to the number of vertices and threads which obstructs their use on\nlarge graphs. Moreover, atomic memory operations have become comparably fast on\nrecent CPU architectures, which creates new opportunities for improving the\nperformance of concurrent atomic accesses in GT.\n  We design PoTra, a GT algorithm which leverages graph structure and processor\nand memory architecture to optimize locality and performance. PoTra limits the\nsize of additional data structures close to CPU cache sizes and utilizes the\nskewed degree distribution of graph datasets to optimize locality and\nperformance. We present the performance model of PoTra to explain the\nconnection between cache and memory response times and graph locality.\n  Our evaluation of PoTra on three CPU architectures and 20 real-world and\nsynthetic graph datasets with up to 128 billion edges demonstrates that PoTra\nachieves up to 8.7 times speedup compared to previous works and if there is a\nperformance loss it remains limited to 15.7%, on average.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the shared-memory Graph Transposition (GT) problem, a\nfundamental graph algorithm that is widely used in graph analytics and\nscientific computing.\n  Previous GT algorithms have significant memory requirements that are\nproportional to the number of vertices and threads which obstructs their use on\nlarge graphs. Moreover, atomic memory operations have become comparably fast on\nrecent CPU architectures, which creates new opportunities for improving the\nperformance of concurrent atomic accesses in GT.\n  We design PoTra, a GT algorithm which leverages graph structure and processor\nand memory architecture to optimize locality and performance. PoTra limits the\nsize of additional data structures close to CPU cache sizes and utilizes the\nskewed degree distribution of graph datasets to optimize locality and\nperformance. We present the performance model of PoTra to explain the\nconnection between cache and memory response times and graph locality.\n  Our evaluation of PoTra on three CPU architectures and 20 real-world and\nsynthetic graph datasets with up to 128 billion edges demonstrates that PoTra\nachieves up to 8.7 times speedup compared to previous works and if there is a\nperformance loss it remains limited to 15.7%, on average."
                },
                "authors": [
                    {
                        "name": "Mohsen Koohi Esfahani"
                    },
                    {
                        "name": "Hans Vandierendonck"
                    }
                ],
                "author_detail": {
                    "name": "Hans Vandierendonck"
                },
                "author": "Hans Vandierendonck",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06872v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06872v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06807v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06807v1",
                "updated": "2025-01-12T13:18:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    13,
                    18,
                    4,
                    6,
                    12,
                    0
                ],
                "published": "2025-01-12T13:18:04Z",
                "published_parsed": [
                    2025,
                    1,
                    12,
                    13,
                    18,
                    4,
                    6,
                    12,
                    0
                ],
                "title": "MPCache: MPC-Friendly KV Cache Eviction for Efficient Private Large\n  Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MPCache: MPC-Friendly KV Cache Eviction for Efficient Private Large\n  Language Model Inference"
                },
                "summary": "Private large language model (LLM) inference based on secure multi-party\ncomputation (MPC) offers cryptographically-secure protection for both user\nprompt and proprietary model weights. However, it suffers from large latency\noverhead especially for long input sequences. While key-value (KV) cache\neviction algorithms have been proposed to reduce the computation and memory\ncost for plaintext inference, they are not designed for MPC and cannot benefit\nprivate inference easily. In this paper, we propose an accurate and\nMPC-friendly KV cache eviction framework, dubbed MPCache. MPCache is built on\nthe observation that historical tokens in a long sequence may have different\neffects on the downstream decoding. Hence, MPCache combines a look-once static\neviction algorithm to discard unimportant tokens and a query-aware dynamic\nselection algorithm to further select a small subset of tokens for attention\ncomputation. As existing dynamic selection algorithms incur too much latency,\nwe propose a series of optimizations to drastically reduce the KV cache\nselection overhead, including MPC-friendly similarity approximation,\nhierarchical KV cache clustering, and cross-layer index sharing strategy. With\nextensive experiments, we demonstrate that MPCache consistently outperforms\nprior-art KV cache eviction baselines across different LLM generation tasks and\nachieves 1.8~2.01x and 3.39~8.37x decoding latency and communication reduction\non different sequence lengths, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Private large language model (LLM) inference based on secure multi-party\ncomputation (MPC) offers cryptographically-secure protection for both user\nprompt and proprietary model weights. However, it suffers from large latency\noverhead especially for long input sequences. While key-value (KV) cache\neviction algorithms have been proposed to reduce the computation and memory\ncost for plaintext inference, they are not designed for MPC and cannot benefit\nprivate inference easily. In this paper, we propose an accurate and\nMPC-friendly KV cache eviction framework, dubbed MPCache. MPCache is built on\nthe observation that historical tokens in a long sequence may have different\neffects on the downstream decoding. Hence, MPCache combines a look-once static\neviction algorithm to discard unimportant tokens and a query-aware dynamic\nselection algorithm to further select a small subset of tokens for attention\ncomputation. As existing dynamic selection algorithms incur too much latency,\nwe propose a series of optimizations to drastically reduce the KV cache\nselection overhead, including MPC-friendly similarity approximation,\nhierarchical KV cache clustering, and cross-layer index sharing strategy. With\nextensive experiments, we demonstrate that MPCache consistently outperforms\nprior-art KV cache eviction baselines across different LLM generation tasks and\nachieves 1.8~2.01x and 3.39~8.37x decoding latency and communication reduction\non different sequence lengths, respectively."
                },
                "authors": [
                    {
                        "name": "Wenxuan Zeng"
                    },
                    {
                        "name": "Ye Dong"
                    },
                    {
                        "name": "Jinjin Zhou"
                    },
                    {
                        "name": "Junming Ma"
                    },
                    {
                        "name": "Jin Tan"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06807v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06807v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.02882v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02882v2",
                "updated": "2025-01-12T12:01:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    12,
                    1,
                    47,
                    6,
                    12,
                    0
                ],
                "published": "2024-04-03T17:33:21Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    17,
                    33,
                    21,
                    2,
                    94,
                    0
                ],
                "title": "Linear Attention Sequence Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linear Attention Sequence Parallelism"
                },
                "summary": "Sequence parallelism (SP) serves as a prevalent strategy to handle long\nsequences that exceed the memory limit of a single device. However, for linear\nsequence modeling methods like linear attention, existing SP approaches do not\ntake advantage of their right-product-first feature, resulting in sub-optimal\ncommunication efficiency and usability. In this paper, we introduce Linear\nAttention Sequence Parallelism (LASP), an efficient SP approach designed for\nlinear attention-based transformer models. Specifically, we design an efficient\npoint-to-point ring-style communication mechanism to leverage the right-product\nkernel trick of linear attention, which sharply decreases the communication\noverhead, comparing with existing SP methods. We enhance the computation\nefficiency of LASP by performing kernel fusion and intermediate state caching,\nmaking the implementation of LASP hardware-friendly on GPUs. Furthermore, we\nmeticulously ensure the compatibility of sequence-level LASP with all types of\nbatch-level data parallel methods, which is vital for distributed training on\nlarge clusters with very-long sequences. We also discuss the generalization of\nLASP on other linear sequence modeling methods. Extensive experiments on linear\nattention-based models are conducted with varying sequence lengths from 2K to\n4096K. LASP scales sequence length up to 4096K on 128 GPUs, which is 8$\\times$\nlonger than existing SP methods. The code is available at\nhttps://github.com/OpenNLPLab/LASP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequence parallelism (SP) serves as a prevalent strategy to handle long\nsequences that exceed the memory limit of a single device. However, for linear\nsequence modeling methods like linear attention, existing SP approaches do not\ntake advantage of their right-product-first feature, resulting in sub-optimal\ncommunication efficiency and usability. In this paper, we introduce Linear\nAttention Sequence Parallelism (LASP), an efficient SP approach designed for\nlinear attention-based transformer models. Specifically, we design an efficient\npoint-to-point ring-style communication mechanism to leverage the right-product\nkernel trick of linear attention, which sharply decreases the communication\noverhead, comparing with existing SP methods. We enhance the computation\nefficiency of LASP by performing kernel fusion and intermediate state caching,\nmaking the implementation of LASP hardware-friendly on GPUs. Furthermore, we\nmeticulously ensure the compatibility of sequence-level LASP with all types of\nbatch-level data parallel methods, which is vital for distributed training on\nlarge clusters with very-long sequences. We also discuss the generalization of\nLASP on other linear sequence modeling methods. Extensive experiments on linear\nattention-based models are conducted with varying sequence lengths from 2K to\n4096K. LASP scales sequence length up to 4096K on 128 GPUs, which is 8$\\times$\nlonger than existing SP methods. The code is available at\nhttps://github.com/OpenNLPLab/LASP."
                },
                "authors": [
                    {
                        "name": "Weigao Sun"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Xuyang Shen"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Yiran Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Yiran Zhong"
                },
                "author": "Yiran Zhong",
                "arxiv_comment": "Technical report, 20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02882v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02882v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07196v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07196v3",
                "updated": "2025-01-12T11:15:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    11,
                    15,
                    41,
                    6,
                    12,
                    0
                ],
                "published": "2024-09-11T11:40:23Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    11,
                    40,
                    23,
                    2,
                    255,
                    0
                ],
                "title": "Sub-cycle Nanotip Field Emission of Electrons Driven by Air Plasma\n  Generated THz Pulses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sub-cycle Nanotip Field Emission of Electrons Driven by Air Plasma\n  Generated THz Pulses"
                },
                "summary": "Terahertz pulses generated by two-color laser plasmas have reported peak\nfield strengths exceeding MV/cm, and when illuminating metal nanotips the\nnear-field enhancement at the tip apex should result in extremely high bunch\ncharges and electron energies via sub-cycle cold field emission. Here, electron\nemission from tungsten nanotips driven by THz pulses generated by a long\nfilament air-plasma are reported. Electron energies up to 1.1 keV and bunch\ncharges up to 2x$10^5$ electrons per pulse were detected, well below values\nexpected for peak field calculated via the time averaged Poynting vector.\nInvestigations revealed a failure in the use of the time-averaged Poynting\nvector when applied to long filament THz pulses, due to spatio-temporal\nrestructuring of the THz pulse in the focus. Accounting for this restructuring\nsignificantly reduces the field strength to approximately 160 ~kV/cm,\nconsistent with the observed electron bunch charges, peak energies and their\ndependence on the tip position in the THz focus. Despite these findings, our\nresults surpass previous THz plasma-driven electron generation by an order of\nmagnitude in both electron energy and bunch charge and a path to increasing\nthese by an additional order of magnitude by modification of the THz optics is\nproposed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Terahertz pulses generated by two-color laser plasmas have reported peak\nfield strengths exceeding MV/cm, and when illuminating metal nanotips the\nnear-field enhancement at the tip apex should result in extremely high bunch\ncharges and electron energies via sub-cycle cold field emission. Here, electron\nemission from tungsten nanotips driven by THz pulses generated by a long\nfilament air-plasma are reported. Electron energies up to 1.1 keV and bunch\ncharges up to 2x$10^5$ electrons per pulse were detected, well below values\nexpected for peak field calculated via the time averaged Poynting vector.\nInvestigations revealed a failure in the use of the time-averaged Poynting\nvector when applied to long filament THz pulses, due to spatio-temporal\nrestructuring of the THz pulse in the focus. Accounting for this restructuring\nsignificantly reduces the field strength to approximately 160 ~kV/cm,\nconsistent with the observed electron bunch charges, peak energies and their\ndependence on the tip position in the THz focus. Despite these findings, our\nresults surpass previous THz plasma-driven electron generation by an order of\nmagnitude in both electron energy and bunch charge and a path to increasing\nthese by an additional order of magnitude by modification of the THz optics is\nproposed."
                },
                "authors": [
                    {
                        "name": "Benjamin Colmey"
                    },
                    {
                        "name": "Rodrigo T. Paulino"
                    },
                    {
                        "name": "Gaspard Beaufort"
                    },
                    {
                        "name": "David G. Cooke"
                    }
                ],
                "author_detail": {
                    "name": "David G. Cooke"
                },
                "author": "David G. Cooke",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07196v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07196v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00857v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00857v2",
                "updated": "2025-01-12T05:25:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    5,
                    25,
                    6,
                    6,
                    12,
                    0
                ],
                "published": "2024-12-01T15:45:26Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    15,
                    45,
                    26,
                    6,
                    336,
                    0
                ],
                "title": "Advanced Video Inpainting Using Optical Flow-Guided Efficient Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced Video Inpainting Using Optical Flow-Guided Efficient Diffusion"
                },
                "summary": "Recently, diffusion-based methods have achieved great improvements in the\nvideo inpainting task. However, these methods still face many challenges, such\nas maintaining temporal consistency and the time-consuming issue. This paper\nproposes an advanced video inpainting framework using optical Flow-guided\nEfficient Diffusion, called FloED. Specifically, FloED employs a dual-branch\narchitecture, where a flow branch first restores corrupted flow and a\nmulti-scale flow adapter provides motion guidance to the main inpainting\nbranch. Additionally, a training-free latent interpolation method is proposed\nto accelerate the multi-step denoising process using flow warping. Further\nintroducing a flow attention cache mechanism, FLoED efficiently reduces the\ncomputational cost brought by incorporating optical flow. Comprehensive\nexperiments in both background restoration and object removal tasks demonstrate\nthat FloED outperforms state-of-the-art methods from the perspective of both\nperformance and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, diffusion-based methods have achieved great improvements in the\nvideo inpainting task. However, these methods still face many challenges, such\nas maintaining temporal consistency and the time-consuming issue. This paper\nproposes an advanced video inpainting framework using optical Flow-guided\nEfficient Diffusion, called FloED. Specifically, FloED employs a dual-branch\narchitecture, where a flow branch first restores corrupted flow and a\nmulti-scale flow adapter provides motion guidance to the main inpainting\nbranch. Additionally, a training-free latent interpolation method is proposed\nto accelerate the multi-step denoising process using flow warping. Further\nintroducing a flow attention cache mechanism, FLoED efficiently reduces the\ncomputational cost brought by incorporating optical flow. Comprehensive\nexperiments in both background restoration and object removal tasks demonstrate\nthat FloED outperforms state-of-the-art methods from the perspective of both\nperformance and efficiency."
                },
                "authors": [
                    {
                        "name": "Bohai Gu"
                    },
                    {
                        "name": "Hao Luo"
                    },
                    {
                        "name": "Song Guo"
                    },
                    {
                        "name": "Peiran Dong"
                    }
                ],
                "author_detail": {
                    "name": "Peiran Dong"
                },
                "author": "Peiran Dong",
                "arxiv_comment": "Project page: https://nevsnev.github.io/FloED/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00857v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00857v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06709v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06709v1",
                "updated": "2025-01-12T04:29:39Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    4,
                    29,
                    39,
                    6,
                    12,
                    0
                ],
                "published": "2025-01-12T04:29:39Z",
                "published_parsed": [
                    2025,
                    1,
                    12,
                    4,
                    29,
                    39,
                    6,
                    12,
                    0
                ],
                "title": "Mell: Memory-Efficient Large Language Model Serving via Multi-GPU KV\n  Cache Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mell: Memory-Efficient Large Language Model Serving via Multi-GPU KV\n  Cache Management"
                },
                "summary": "Serving large language models (LLMs) for massive users is challenged by the\nsignificant memory footprint of the transient state, known as the key-value\n(KV) cache, which scales with sequence length and number of requests. Instead\nof renting or buying more expensive GPUs, the load imbalance of the KV cache\nacross GPUs, coupled with recent advances in inter-GPU communication, provides\nan opportunity to serve more requests via request migration. However, high\nmigration overhead and unpredictable request patterns make it challenging.\nTherefore, this paper proposes MELL, a memory-efficient LLM serving system via\nmulti-GPU KV cache management. It saves the number of GPUs needed in the system\nby considering the dynamic KV cache load and the costly request migration.\nSpecifically, we first develop an adaptive request migration mechanism to\nbalance the computational and communication overheads and adapt to diverse\nresource conditions. Then, we design an online algorithm tailored to a\nmulti-LLM request and multi-GPU scheduling problem with migration enabled. It\naims to minimise the required GPUs while limiting the number of migrations.\nFinally, we implement a prototype of MELL and demonstrate that it reduces the\nnumber of GPUs by 31% and increases the GPU utilization by 43% at most compared\nto existing LLM serving systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving large language models (LLMs) for massive users is challenged by the\nsignificant memory footprint of the transient state, known as the key-value\n(KV) cache, which scales with sequence length and number of requests. Instead\nof renting or buying more expensive GPUs, the load imbalance of the KV cache\nacross GPUs, coupled with recent advances in inter-GPU communication, provides\nan opportunity to serve more requests via request migration. However, high\nmigration overhead and unpredictable request patterns make it challenging.\nTherefore, this paper proposes MELL, a memory-efficient LLM serving system via\nmulti-GPU KV cache management. It saves the number of GPUs needed in the system\nby considering the dynamic KV cache load and the costly request migration.\nSpecifically, we first develop an adaptive request migration mechanism to\nbalance the computational and communication overheads and adapt to diverse\nresource conditions. Then, we design an online algorithm tailored to a\nmulti-LLM request and multi-GPU scheduling problem with migration enabled. It\naims to minimise the required GPUs while limiting the number of migrations.\nFinally, we implement a prototype of MELL and demonstrate that it reduces the\nnumber of GPUs by 31% and increases the GPU utilization by 43% at most compared\nto existing LLM serving systems."
                },
                "authors": [
                    {
                        "name": "Liu Qianli"
                    },
                    {
                        "name": "Hong Zicong"
                    },
                    {
                        "name": "Chen Fahao"
                    },
                    {
                        "name": "Li Peng"
                    },
                    {
                        "name": "Guo Song"
                    }
                ],
                "author_detail": {
                    "name": "Guo Song"
                },
                "author": "Guo Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06709v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06709v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17918v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17918v4",
                "updated": "2025-01-11T15:26:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    11,
                    15,
                    26,
                    48,
                    5,
                    11,
                    0
                ],
                "published": "2024-06-25T20:00:32Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    20,
                    0,
                    32,
                    1,
                    177,
                    0
                ],
                "title": "GraphSnapShot: Caching Local Structure for Fast Graph Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphSnapShot: Caching Local Structure for Fast Graph Learning"
                },
                "summary": "In our recent research, we have developed a framework called GraphSnapShot,\nwhich has been proven an useful tool for graph learning acceleration.\nGraphSnapShot is a framework for fast cache, storage, retrieval and computation\nfor graph learning. It can quickly store and update the local topology of graph\nstructure and allows us to track patterns in the structure of graph networks,\njust like take snapshots of the graphs. In experiments, GraphSnapShot shows\nefficiency, it can achieve up to 30% training acceleration and 73% memory\nreduction for lossless graph ML training compared to current baselines such as\ndgl.This technique is particular useful for large dynamic graph learning tasks\nsuch as social media analysis and recommendation systems to process complex\nrelationships between entities.\n  The code for GraphSnapShot is publicly available at\nhttps://github.com/NoakLiu/GraphSnapShot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In our recent research, we have developed a framework called GraphSnapShot,\nwhich has been proven an useful tool for graph learning acceleration.\nGraphSnapShot is a framework for fast cache, storage, retrieval and computation\nfor graph learning. It can quickly store and update the local topology of graph\nstructure and allows us to track patterns in the structure of graph networks,\njust like take snapshots of the graphs. In experiments, GraphSnapShot shows\nefficiency, it can achieve up to 30% training acceleration and 73% memory\nreduction for lossless graph ML training compared to current baselines such as\ndgl.This technique is particular useful for large dynamic graph learning tasks\nsuch as social media analysis and recommendation systems to process complex\nrelationships between entities.\n  The code for GraphSnapShot is publicly available at\nhttps://github.com/NoakLiu/GraphSnapShot."
                },
                "authors": [
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Roger Waleffe"
                    },
                    {
                        "name": "Meng Jiang"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17918v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17918v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06428v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06428v1",
                "updated": "2025-01-11T03:47:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    47,
                    4,
                    5,
                    11,
                    0
                ],
                "published": "2025-01-11T03:47:04Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    47,
                    4,
                    5,
                    11,
                    0
                ],
                "title": "Optimizing digital experiences with content delivery networks:\n  Architectures, performance strategies, and future trends",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing digital experiences with content delivery networks:\n  Architectures, performance strategies, and future trends"
                },
                "summary": "This research investigates how CDNs (Content Delivery Networks) can improve\nthe digital experience, as consumers increasingly expect fast, efficient, and\neffortless access to online resources. CDNs play a crucial role in reducing\nlatency, enhancing scalability, and optimizing delivery mechanisms, which is\nevident across various platforms and regions. The study focuses on key CDN\nconcerns, such as foundational and modern CDN architectures, edge computing,\nhybrid CDNs, and multi-CDN strategies. It also explores performance-enhancing\ntopics, including caching, load balancing, and the novel features of HTTP/3 and\nQUIC.\n  Current trends, such as integrating CDNs with 5G networks, serverless\narchitectures, and AI-driven traffic management, are examined to demonstrate\nhow CDN technology is likely to evolve. The study also addresses challenges\nrelated to security, cost, and global regulations. Practical examples from the\ne-commerce, streaming, and gaming industries highlight how enhanced CDNs are\ntransforming these sectors.\n  The conclusions emphasize the need to evolve CDN strategies to meet growing\nuser expectations and adapt to the rapidly changing digital landscape.\nAdditionally, the research identifies future research opportunities,\nparticularly in exploring the impact of QC, the enhancement of AI services, and\nthe sustainability of CDN solutions. Overall, the study situates architectural\ndesign, performance strategies, and emerging trends to address gaps and create\na more efficient and secure approach for improving digital experiences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This research investigates how CDNs (Content Delivery Networks) can improve\nthe digital experience, as consumers increasingly expect fast, efficient, and\neffortless access to online resources. CDNs play a crucial role in reducing\nlatency, enhancing scalability, and optimizing delivery mechanisms, which is\nevident across various platforms and regions. The study focuses on key CDN\nconcerns, such as foundational and modern CDN architectures, edge computing,\nhybrid CDNs, and multi-CDN strategies. It also explores performance-enhancing\ntopics, including caching, load balancing, and the novel features of HTTP/3 and\nQUIC.\n  Current trends, such as integrating CDNs with 5G networks, serverless\narchitectures, and AI-driven traffic management, are examined to demonstrate\nhow CDN technology is likely to evolve. The study also addresses challenges\nrelated to security, cost, and global regulations. Practical examples from the\ne-commerce, streaming, and gaming industries highlight how enhanced CDNs are\ntransforming these sectors.\n  The conclusions emphasize the need to evolve CDN strategies to meet growing\nuser expectations and adapt to the rapidly changing digital landscape.\nAdditionally, the research identifies future research opportunities,\nparticularly in exploring the impact of QC, the enhancement of AI services, and\nthe sustainability of CDN solutions. Overall, the study situates architectural\ndesign, performance strategies, and emerging trends to address gaps and create\na more efficient and secure approach for improving digital experiences."
                },
                "authors": [
                    {
                        "name": "Anuj Tyagi"
                    }
                ],
                "author_detail": {
                    "name": "Anuj Tyagi"
                },
                "author": "Anuj Tyagi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06428v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06428v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06425v1",
                "updated": "2025-01-11T03:37:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    37,
                    10,
                    5,
                    11,
                    0
                ],
                "published": "2025-01-11T03:37:10Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    37,
                    10,
                    5,
                    11,
                    0
                ],
                "title": "Tensor Product Attention Is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tensor Product Attention Is All You Need"
                },
                "summary": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, significantly shrinking KV cache size at\ninference time. By factorizing these representations into contextual low-rank\ncomponents (contextual factorization) and seamlessly integrating with RoPE, TPA\nachieves improved model quality alongside memory efficiency. Based on TPA, we\nintroduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation of\nlanguage modeling tasks, we demonstrate that T6 exceeds the performance of\nstandard Transformer baselines including MHA, MQA, GQA, and MLA across various\nmetrics, including perplexity and a range of renowned evaluation benchmarks.\nNotably, TPAs memory efficiency enables the processing of significantly longer\nsequences under fixed resource constraints, addressing a critical scalability\nchallenge in modern language models. The code is available at\nhttps://github.com/tensorgi/T6.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, significantly shrinking KV cache size at\ninference time. By factorizing these representations into contextual low-rank\ncomponents (contextual factorization) and seamlessly integrating with RoPE, TPA\nachieves improved model quality alongside memory efficiency. Based on TPA, we\nintroduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation of\nlanguage modeling tasks, we demonstrate that T6 exceeds the performance of\nstandard Transformer baselines including MHA, MQA, GQA, and MLA across various\nmetrics, including perplexity and a range of renowned evaluation benchmarks.\nNotably, TPAs memory efficiency enables the processing of significantly longer\nsequences under fixed resource constraints, addressing a critical scalability\nchallenge in modern language models. The code is available at\nhttps://github.com/tensorgi/T6."
                },
                "authors": [
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Yifeng Liu"
                    },
                    {
                        "name": "Huizhuo Yuan"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Yang Yuan"
                    },
                    {
                        "name": "Quanquan Gu"
                    },
                    {
                        "name": "Andrew Chi-Chih Yao"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Chi-Chih Yao"
                },
                "author": "Andrew Chi-Chih Yao",
                "arxiv_comment": "23 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06394v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06394v1",
                "updated": "2025-01-11T00:47:29Z",
                "updated_parsed": [
                    2025,
                    1,
                    11,
                    0,
                    47,
                    29,
                    5,
                    11,
                    0
                ],
                "published": "2025-01-11T00:47:29Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    0,
                    47,
                    29,
                    5,
                    11,
                    0
                ],
                "title": "Unispeaker: A Unified Approach for Multimodality-driven Speaker\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unispeaker: A Unified Approach for Multimodality-driven Speaker\n  Generation"
                },
                "summary": "Recent advancements in personalized speech generation have brought synthetic\nspeech increasingly close to the realism of target speakers' recordings, yet\nmultimodal speaker generation remains on the rise. This paper introduces\nUniSpeaker, a unified approach for multimodality-driven speaker generation.\nSpecifically, we propose a unified voice aggregator based on KV-Former,\napplying soft contrastive loss to map diverse voice description modalities into\na shared voice space, ensuring that the generated voice aligns more closely\nwith the input descriptions. To evaluate multimodality-driven voice control, we\nbuild the first multimodality-based voice control (MVC) benchmark, focusing on\nvoice suitability, voice diversity, and speech quality. UniSpeaker is evaluated\nacross five tasks using the MVC benchmark, and the experimental results\ndemonstrate that UniSpeaker outperforms previous modality-specific models.\nSpeech samples are available at \\url{https://UniSpeaker.github.io}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in personalized speech generation have brought synthetic\nspeech increasingly close to the realism of target speakers' recordings, yet\nmultimodal speaker generation remains on the rise. This paper introduces\nUniSpeaker, a unified approach for multimodality-driven speaker generation.\nSpecifically, we propose a unified voice aggregator based on KV-Former,\napplying soft contrastive loss to map diverse voice description modalities into\na shared voice space, ensuring that the generated voice aligns more closely\nwith the input descriptions. To evaluate multimodality-driven voice control, we\nbuild the first multimodality-based voice control (MVC) benchmark, focusing on\nvoice suitability, voice diversity, and speech quality. UniSpeaker is evaluated\nacross five tasks using the MVC benchmark, and the experimental results\ndemonstrate that UniSpeaker outperforms previous modality-specific models.\nSpeech samples are available at \\url{https://UniSpeaker.github.io}."
                },
                "authors": [
                    {
                        "name": "Zhengyan Sheng"
                    },
                    {
                        "name": "Zhihao Du"
                    },
                    {
                        "name": "Heng Lu"
                    },
                    {
                        "name": "Shiliang Zhang"
                    },
                    {
                        "name": "Zhen-Hua Ling"
                    }
                ],
                "author_detail": {
                    "name": "Zhen-Hua Ling"
                },
                "author": "Zhen-Hua Ling",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06394v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.01030v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.01030v2",
                "updated": "2025-01-10T10:11:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    10,
                    11,
                    45,
                    4,
                    10,
                    0
                ],
                "published": "2024-07-01T07:25:08Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    7,
                    25,
                    8,
                    0,
                    183,
                    0
                ],
                "title": "Tame fields, Graded Rings and Finite Complete Sequences of Key\n  Polynomials",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tame fields, Graded Rings and Finite Complete Sequences of Key\n  Polynomials"
                },
                "summary": "In this paper, we present a criterion for $(K,v)$ to be henselian and\ndefectless in terms of finite complete sequences of key polynomials. For this,\nwe use the theory of Mac Lane-Vaqui\\'e chains and abstract key polynomials. We\nthen prove that a valued field $(K,v)$ is tame if and only if $vK$ is\n$p$-divisible, $Kv$ is perfect and every simple algebraic extension of $K$\nadmits a finite complete sequence of key polynomials. The properties $vK$\n$p$-divisible and $Kv$ perfect are described by the Frobenius endomorphism on\nthe associated graded ring. We also make considerations on simply defectless\nand algebraically maximal valued fields and purely inertial and purely ramified\nextensions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present a criterion for $(K,v)$ to be henselian and\ndefectless in terms of finite complete sequences of key polynomials. For this,\nwe use the theory of Mac Lane-Vaqui\\'e chains and abstract key polynomials. We\nthen prove that a valued field $(K,v)$ is tame if and only if $vK$ is\n$p$-divisible, $Kv$ is perfect and every simple algebraic extension of $K$\nadmits a finite complete sequence of key polynomials. The properties $vK$\n$p$-divisible and $Kv$ perfect are described by the Frobenius endomorphism on\nthe associated graded ring. We also make considerations on simply defectless\nand algebraically maximal valued fields and purely inertial and purely ramified\nextensions."
                },
                "authors": [
                    {
                        "name": "Caio Henrique Silva de Souza"
                    }
                ],
                "author_detail": {
                    "name": "Caio Henrique Silva de Souza"
                },
                "author": "Caio Henrique Silva de Souza",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.01030v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.01030v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.AC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.AC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "13A18",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20433v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20433v2",
                "updated": "2025-01-09T15:14:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    15,
                    14,
                    5,
                    3,
                    9,
                    0
                ],
                "published": "2024-09-30T15:53:36Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    15,
                    53,
                    36,
                    0,
                    274,
                    0
                ],
                "title": "Handover_Management_in_UAV_Networks_with_Blockages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Handover_Management_in_UAV_Networks_with_Blockages"
                },
                "summary": "We investigate the performance of unmanned aerial vehicle (UAV)-based\nnetworks in urban environments characterized by blockages, focusing on their\ncapability to support the service demands of mobile users. The UAV-base\nstations (UAV-BSs) are modeled using a two-dimensional (2-D) marked- Poisson\npoint process (MPPP), where the marks represent the altitude of each UAV-BS.\nLeveraging stochastic geometry, we analyze the impact of blockages on network\nreliability by studying the meta distribution (MD) of the\nsignal-to-interference noise ratio (SINR) for a specific reliability threshold\nand the association probabilities for both line-of-sight (LoS) and non\nline-of-sight (NLoS) UAV-BSs. Furthermore, to enhance the performance of mobile\nusers, we propose a novel cache-based handover management strategy that\ndynamically selects the cell search time and delays the received signal\nstrength (RSS)-based base station (BS) associations. This strategy aims to\nminimize unnecessary handovers (HOs) experienced by users by leveraging caching\ncapabilities at user equipment (UE), thus reducing latency, ensuring seamless\nconnectivity, and maintaining the quality of service (QoS). This study provides\nvaluable insights into optimizing UAV network deployments to support the\nstringent requirements in the network, ensuring reliable, low-latency, and\nhigh-throughput communication for next-generation smart cities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the performance of unmanned aerial vehicle (UAV)-based\nnetworks in urban environments characterized by blockages, focusing on their\ncapability to support the service demands of mobile users. The UAV-base\nstations (UAV-BSs) are modeled using a two-dimensional (2-D) marked- Poisson\npoint process (MPPP), where the marks represent the altitude of each UAV-BS.\nLeveraging stochastic geometry, we analyze the impact of blockages on network\nreliability by studying the meta distribution (MD) of the\nsignal-to-interference noise ratio (SINR) for a specific reliability threshold\nand the association probabilities for both line-of-sight (LoS) and non\nline-of-sight (NLoS) UAV-BSs. Furthermore, to enhance the performance of mobile\nusers, we propose a novel cache-based handover management strategy that\ndynamically selects the cell search time and delays the received signal\nstrength (RSS)-based base station (BS) associations. This strategy aims to\nminimize unnecessary handovers (HOs) experienced by users by leveraging caching\ncapabilities at user equipment (UE), thus reducing latency, ensuring seamless\nconnectivity, and maintaining the quality of service (QoS). This study provides\nvaluable insights into optimizing UAV network deployments to support the\nstringent requirements in the network, ensuring reliable, low-latency, and\nhigh-throughput communication for next-generation smart cities."
                },
                "authors": [
                    {
                        "name": "Neetu R R"
                    },
                    {
                        "name": "Gourab Ghatak"
                    },
                    {
                        "name": "Vivek Ashok Bohara"
                    }
                ],
                "author_detail": {
                    "name": "Vivek Ashok Bohara"
                },
                "author": "Vivek Ashok Bohara",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20433v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20433v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04993v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04993v1",
                "updated": "2025-01-09T06:18:39Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    6,
                    18,
                    39,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T06:18:39Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    6,
                    18,
                    39,
                    3,
                    9,
                    0
                ],
                "title": "ByteFS: System Support for (CXL-based) Memory-Semantic Solid-State\n  Drives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ByteFS: System Support for (CXL-based) Memory-Semantic Solid-State\n  Drives"
                },
                "summary": "Unlike non-volatile memory that resides on the processor memory bus,\nmemory-semantic solid-state drives (SSDs) support both byte and block access\ngranularity via PCIe or CXL interconnects. They provide scalable memory\ncapacity using NAND flash at a much lower cost. In addition, they have\ndifferent performance characteristics for their dual byte/block interface\nrespectively, while offering essential memory semantics for upper-level\nsoftware. Such a byte-accessible storage device provides new implications on\nthe software system design.\n  In this paper, we develop a new file system, named ByteFS, by rethinking the\ndesign primitives of file systems and SSD firmware to exploit the advantages of\nboth byte and block-granular data accesses. ByteFS supports byte-granular data\npersistence to retain the persistence nature of SSDs. It extends the core data\nstructure of file systems by enabling dual byte/block-granular data accesses.\nTo facilitate the support for byte-granular writes, \\pname{} manages the\ninternal DRAM of SSD firmware in a log-structured manner and enables data\ncoalescing to reduce the unnecessary I/O traffic to flash chips. ByteFS also\nenables coordinated data caching between the host page cache and SSD cache for\nbest utilizing the precious memory resource. We implement ByteFS on both a real\nprogrammable SSD and an emulated memory-semantic SSD for sensitivity study.\nCompared to state-of-the-art file systems for non-volatile memory and\nconventional SSDs, ByteFS outperforms them by up to 2.7$\\times$, while\npreserving the essential properties of a file system. ByteFS also reduces the\nwrite traffic to SSDs by up to 5.1$\\times$ by alleviating unnecessary writes\ncaused by both metadata and data updates in file systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlike non-volatile memory that resides on the processor memory bus,\nmemory-semantic solid-state drives (SSDs) support both byte and block access\ngranularity via PCIe or CXL interconnects. They provide scalable memory\ncapacity using NAND flash at a much lower cost. In addition, they have\ndifferent performance characteristics for their dual byte/block interface\nrespectively, while offering essential memory semantics for upper-level\nsoftware. Such a byte-accessible storage device provides new implications on\nthe software system design.\n  In this paper, we develop a new file system, named ByteFS, by rethinking the\ndesign primitives of file systems and SSD firmware to exploit the advantages of\nboth byte and block-granular data accesses. ByteFS supports byte-granular data\npersistence to retain the persistence nature of SSDs. It extends the core data\nstructure of file systems by enabling dual byte/block-granular data accesses.\nTo facilitate the support for byte-granular writes, \\pname{} manages the\ninternal DRAM of SSD firmware in a log-structured manner and enables data\ncoalescing to reduce the unnecessary I/O traffic to flash chips. ByteFS also\nenables coordinated data caching between the host page cache and SSD cache for\nbest utilizing the precious memory resource. We implement ByteFS on both a real\nprogrammable SSD and an emulated memory-semantic SSD for sensitivity study.\nCompared to state-of-the-art file systems for non-volatile memory and\nconventional SSDs, ByteFS outperforms them by up to 2.7$\\times$, while\npreserving the essential properties of a file system. ByteFS also reduces the\nwrite traffic to SSDs by up to 5.1$\\times$ by alleviating unnecessary writes\ncaused by both metadata and data updates in file systems."
                },
                "authors": [
                    {
                        "name": "Shaobo Li"
                    },
                    {
                        "name": "Yirui Eric Zhou"
                    },
                    {
                        "name": "Hao Ren"
                    },
                    {
                        "name": "Jian Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Huang"
                },
                "author": "Jian Huang",
                "arxiv_comment": "This paper is accepted at the 30th Conference on Architectural\n  Support for Programming Languages and Operating Systems (ASPLOS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04993v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04993v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04216v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04216v2",
                "updated": "2025-01-09T03:02:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    3,
                    2,
                    31,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-08T01:23:29Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    1,
                    23,
                    29,
                    2,
                    8,
                    0
                ],
                "title": "Optimal Oblivious Algorithms for Multi-way Joins",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Oblivious Algorithms for Multi-way Joins"
                },
                "summary": "In cloud databases, cloud computation over sensitive data uploaded by clients\ninevitably causes concern about data security and privacy. Even when encryption\nprimitives and trusted computing environments are integrated into query\nprocessing to safeguard the actual contents of the data, access patterns of\nalgorithms can still leak private information about the data. Oblivious Random\nAccess Memory (ORAM) and circuits are two generic approaches to address this\nissue, ensuring that access patterns of algorithms remain oblivious to the\ndata. However, deploying these methods on insecure algorithms, particularly for\nmulti-way join processing, is computationally expensive and inherently\nchallenging.\n  In this paper, we propose a novel sorting-based algorithm for multi-way join\nprocessing that operates without relying on ORAM simulations or other security\nassumptions. Our algorithm is a non-trivial, provably oblivious composition of\nbasic primitives, with time complexity matching the insecure worst-case optimal\njoin algorithm, up to a logarithmic factor. Furthermore, it is cache-agnostic,\nwith cache complexity matching the insecure lower bound, also up to a\nlogarithmic factor. This clean and straightforward approach has the potential\nto be extended to other security settings and implemented in practical database\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In cloud databases, cloud computation over sensitive data uploaded by clients\ninevitably causes concern about data security and privacy. Even when encryption\nprimitives and trusted computing environments are integrated into query\nprocessing to safeguard the actual contents of the data, access patterns of\nalgorithms can still leak private information about the data. Oblivious Random\nAccess Memory (ORAM) and circuits are two generic approaches to address this\nissue, ensuring that access patterns of algorithms remain oblivious to the\ndata. However, deploying these methods on insecure algorithms, particularly for\nmulti-way join processing, is computationally expensive and inherently\nchallenging.\n  In this paper, we propose a novel sorting-based algorithm for multi-way join\nprocessing that operates without relying on ORAM simulations or other security\nassumptions. Our algorithm is a non-trivial, provably oblivious composition of\nbasic primitives, with time complexity matching the insecure worst-case optimal\njoin algorithm, up to a logarithmic factor. Furthermore, it is cache-agnostic,\nwith cache complexity matching the insecure lower bound, also up to a\nlogarithmic factor. This clean and straightforward approach has the potential\nto be extended to other security settings and implemented in practical database\nsystems."
                },
                "authors": [
                    {
                        "name": "Xiao Hu"
                    },
                    {
                        "name": "Zhiang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiang Wu"
                },
                "author": "Zhiang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04216v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04216v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04394v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04394v1",
                "updated": "2025-01-08T10:14:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    10,
                    14,
                    19,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T10:14:19Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    10,
                    14,
                    19,
                    2,
                    8,
                    0
                ],
                "title": "Modern Hardware Security: A Review of Attacks and Countermeasures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Hardware Security: A Review of Attacks and Countermeasures"
                },
                "summary": "With the exponential rise in the use of cloud services, smart devices, and\nIoT devices, advanced cyber attacks have become increasingly sophisticated and\nubiquitous. Furthermore, the rapid evolution of computing architectures and\nmemory technologies has created an urgent need to understand and address\nhardware security vulnerabilities. In this paper, we review the current state\nof vulnerabilities and mitigation strategies in contemporary computing systems.\nWe discuss cache side-channel attacks (including Spectre and Meltdown), power\nside-channel attacks (such as Simple Power Analysis, Differential Power\nAnalysis, Correlation Power Analysis, and Template Attacks), and advanced\ntechniques like Voltage Glitching and Electromagnetic Analysis to help\nunderstand and build robust cybersecurity defense systems and guide further\nresearch. We also examine memory encryption, focusing on confidentiality,\ngranularity, key management, masking, and re-keying strategies. Additionally,\nwe cover Cryptographic Instruction Set Architectures, Secure Boot, Root of\nTrust mechanisms, Physical Unclonable Functions, and hardware fault injection\ntechniques. The paper concludes with an analysis of the RISC-V architecture's\nunique security challenges. The comprehensive analysis presented in this paper\nis essential for building resilient hardware security solutions that can\nprotect against both current and emerging threats in an increasingly\nchallenging security landscape.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the exponential rise in the use of cloud services, smart devices, and\nIoT devices, advanced cyber attacks have become increasingly sophisticated and\nubiquitous. Furthermore, the rapid evolution of computing architectures and\nmemory technologies has created an urgent need to understand and address\nhardware security vulnerabilities. In this paper, we review the current state\nof vulnerabilities and mitigation strategies in contemporary computing systems.\nWe discuss cache side-channel attacks (including Spectre and Meltdown), power\nside-channel attacks (such as Simple Power Analysis, Differential Power\nAnalysis, Correlation Power Analysis, and Template Attacks), and advanced\ntechniques like Voltage Glitching and Electromagnetic Analysis to help\nunderstand and build robust cybersecurity defense systems and guide further\nresearch. We also examine memory encryption, focusing on confidentiality,\ngranularity, key management, masking, and re-keying strategies. Additionally,\nwe cover Cryptographic Instruction Set Architectures, Secure Boot, Root of\nTrust mechanisms, Physical Unclonable Functions, and hardware fault injection\ntechniques. The paper concludes with an analysis of the RISC-V architecture's\nunique security challenges. The comprehensive analysis presented in this paper\nis essential for building resilient hardware security solutions that can\nprotect against both current and emerging threats in an increasingly\nchallenging security landscape."
                },
                "authors": [
                    {
                        "name": "Jyotiprakash Mishra"
                    },
                    {
                        "name": "Sanjay K. Sahay"
                    }
                ],
                "author_detail": {
                    "name": "Sanjay K. Sahay"
                },
                "author": "Sanjay K. Sahay",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04394v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00799v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00799v2",
                "updated": "2025-01-07T17:32:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    32,
                    19,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-01T10:50:35Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    10,
                    50,
                    35,
                    2,
                    1,
                    0
                ],
                "title": "Follow The Approximate Sparse Leader for No-Regret Online Sparse Linear\n  Approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Follow The Approximate Sparse Leader for No-Regret Online Sparse Linear\n  Approximation"
                },
                "summary": "We consider the problem of \\textit{online sparse linear approximation}, where\none predicts the best sparse approximation of a sequence of measurements in\nterms of linear combination of columns of a given measurement matrix. Such\nonline prediction problems are ubiquitous, ranging from medical trials to web\ncaching to resource allocation. The inherent difficulty of offline recovery\nalso makes the online problem challenging. In this letter, we propose\nFollow-The-Approximate-Sparse-Leader, an efficient online meta-policy to\naddress this online problem. Through a detailed theoretical analysis, we prove\nthat under certain assumptions on the measurement sequence, the proposed policy\nenjoys a data-dependent sublinear upper bound on the static regret, which can\nrange from logarithmic to square-root. Numerical simulations are performed to\ncorroborate the theoretical findings and demonstrate the efficacy of the\nproposed online policy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the problem of \\textit{online sparse linear approximation}, where\none predicts the best sparse approximation of a sequence of measurements in\nterms of linear combination of columns of a given measurement matrix. Such\nonline prediction problems are ubiquitous, ranging from medical trials to web\ncaching to resource allocation. The inherent difficulty of offline recovery\nalso makes the online problem challenging. In this letter, we propose\nFollow-The-Approximate-Sparse-Leader, an efficient online meta-policy to\naddress this online problem. Through a detailed theoretical analysis, we prove\nthat under certain assumptions on the measurement sequence, the proposed policy\nenjoys a data-dependent sublinear upper bound on the static regret, which can\nrange from logarithmic to square-root. Numerical simulations are performed to\ncorroborate the theoretical findings and demonstrate the efficacy of the\nproposed online policy."
                },
                "authors": [
                    {
                        "name": "Samrat Mukhopadhyay"
                    },
                    {
                        "name": "Debasmita Mukherjee"
                    }
                ],
                "author_detail": {
                    "name": "Debasmita Mukherjee"
                },
                "author": "Debasmita Mukherjee",
                "arxiv_comment": "12 pages, 5 figures, corrected title, added proof of a lemma in\n  appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00799v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00799v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09275v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09275v2",
                "updated": "2025-01-06T23:16:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    23,
                    16,
                    22,
                    0,
                    6,
                    0
                ],
                "published": "2024-11-14T08:25:31Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    8,
                    25,
                    31,
                    3,
                    319,
                    0
                ],
                "title": "Parallel $k$d-tree with Batch Updates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel $k$d-tree with Batch Updates"
                },
                "summary": "The $k$d-tree is one of the most widely used data structures to manage\nmulti-dimensional data. Due to the ever-growing data volume, it is imperative\nto consider parallelism in $k$d-trees. However, we observed challenges in\nexisting parallel kd-tree implementations, for both constructions and updates.\n  The goal of this paper is to develop efficient in-memory $k$d-trees by\nsupporting high parallelism and cache-efficiency. We propose the Pkd-tree\n(Parallel $k$d-tree), a parallel $k$d-tree that is efficient both in theory and\nin practice. The Pkd-tree supports parallel tree construction, batch update\n(insertion and deletion), and various queries including k-nearest neighbor\nsearch, range query, and range count. We proved that our algorithms have strong\ntheoretical bounds in work (sequential time complexity), span (parallelism),\nand cache complexity. Our key techniques include 1) an efficient construction\nalgorithm that optimizes work, span, and cache complexity simultaneously, and\n2) reconstruction-based update algorithms that guarantee the tree to be\nweight-balanced. With the new algorithmic insights and careful engineering\neffort, we achieved a highly optimized implementation of the Pkd-tree.\n  We tested Pkd-tree with various synthetic and real-world datasets, including\nboth uniform and highly skewed data. We compare the Pkd-tree with\nstate-of-the-art parallel $k$d-tree implementations. In all tests, with better\nor competitive query performance, Pkd-tree is much faster in construction and\nupdates consistently than all baselines. We released our code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The $k$d-tree is one of the most widely used data structures to manage\nmulti-dimensional data. Due to the ever-growing data volume, it is imperative\nto consider parallelism in $k$d-trees. However, we observed challenges in\nexisting parallel kd-tree implementations, for both constructions and updates.\n  The goal of this paper is to develop efficient in-memory $k$d-trees by\nsupporting high parallelism and cache-efficiency. We propose the Pkd-tree\n(Parallel $k$d-tree), a parallel $k$d-tree that is efficient both in theory and\nin practice. The Pkd-tree supports parallel tree construction, batch update\n(insertion and deletion), and various queries including k-nearest neighbor\nsearch, range query, and range count. We proved that our algorithms have strong\ntheoretical bounds in work (sequential time complexity), span (parallelism),\nand cache complexity. Our key techniques include 1) an efficient construction\nalgorithm that optimizes work, span, and cache complexity simultaneously, and\n2) reconstruction-based update algorithms that guarantee the tree to be\nweight-balanced. With the new algorithmic insights and careful engineering\neffort, we achieved a highly optimized implementation of the Pkd-tree.\n  We tested Pkd-tree with various synthetic and real-world datasets, including\nboth uniform and highly skewed data. We compare the Pkd-tree with\nstate-of-the-art parallel $k$d-tree implementations. In all tests, with better\nor competitive query performance, Pkd-tree is much faster in construction and\nupdates consistently than all baselines. We released our code."
                },
                "authors": [
                    {
                        "name": "Ziyang Men"
                    },
                    {
                        "name": "Zheqi Shen"
                    },
                    {
                        "name": "Yan Gu"
                    },
                    {
                        "name": "Yihan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yihan Sun"
                },
                "author": "Yihan Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09275v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09275v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04052v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04052v1",
                "updated": "2025-01-06T22:40:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    22,
                    40,
                    40,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T22:40:40Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    22,
                    40,
                    40,
                    0,
                    6,
                    0
                ],
                "title": "The Power of Negative Zero: Datatype Customization for Quantized Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Power of Negative Zero: Datatype Customization for Quantized Large\n  Language Models"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable performance across\nvarious machine learning tasks, quickly becoming one of the most prevalent AI\nworkloads. Yet the substantial memory requirement of LLMs significantly hinders\ntheir deployment for end users. Post-training quantization (PTQ) serves as one\nof the most hardware-efficient methods to mitigate the memory and computational\ndemands of LLMs. Although the traditional integer (INT) datatype has received\nwidespread adoption in PTQ methods, floating-point (FP) quantization has\nemerged as a viable alternative thanks to its effectiveness in fitting LLM\nnumerical distributions. However, the FP datatype in sign-magnitude binary\nrepresentation contains both positive and negative zero, which constrains its\nrepresentation capability, particularly under low precision (3 and 4 bits). In\nthis paper, we extend the basic FP datatype to perform Redundant Zero Remapping\n(RaZeR), which remaps the negative zero FP encoding to a set of pre-defined\nspecial values to maximally utilize FP quantization encodings and to better fit\nLLM numerical distributions. Through careful selection of special values, RaZeR\noutperforms conventional asymmetric INT quantization while achieving high\ncomputational efficiency. We demonstrate that RaZeR can be seamlessly\nintegrated with quantization algorithms for both weights and KV-cache,\nincluding advanced methods with clipping and transformations, and consistently\nachieve better model accuracy. Additionally, we implement a fast GEMV kernel\nwith fused dequantization that efficiently converts the 4-bit RaZeR value to\nFP16 through novel bit-level manipulation. On modern GPUs, our evaluation shows\nthat RaZeR improves the GEMV speed by up to 7.56$\\times$ compared to the FP16\nimplementation, while achieving up to 2.72$\\times$ speedup in the LLM decoding\nthroughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable performance across\nvarious machine learning tasks, quickly becoming one of the most prevalent AI\nworkloads. Yet the substantial memory requirement of LLMs significantly hinders\ntheir deployment for end users. Post-training quantization (PTQ) serves as one\nof the most hardware-efficient methods to mitigate the memory and computational\ndemands of LLMs. Although the traditional integer (INT) datatype has received\nwidespread adoption in PTQ methods, floating-point (FP) quantization has\nemerged as a viable alternative thanks to its effectiveness in fitting LLM\nnumerical distributions. However, the FP datatype in sign-magnitude binary\nrepresentation contains both positive and negative zero, which constrains its\nrepresentation capability, particularly under low precision (3 and 4 bits). In\nthis paper, we extend the basic FP datatype to perform Redundant Zero Remapping\n(RaZeR), which remaps the negative zero FP encoding to a set of pre-defined\nspecial values to maximally utilize FP quantization encodings and to better fit\nLLM numerical distributions. Through careful selection of special values, RaZeR\noutperforms conventional asymmetric INT quantization while achieving high\ncomputational efficiency. We demonstrate that RaZeR can be seamlessly\nintegrated with quantization algorithms for both weights and KV-cache,\nincluding advanced methods with clipping and transformations, and consistently\nachieve better model accuracy. Additionally, we implement a fast GEMV kernel\nwith fused dequantization that efficiently converts the 4-bit RaZeR value to\nFP16 through novel bit-level manipulation. On modern GPUs, our evaluation shows\nthat RaZeR improves the GEMV speed by up to 7.56$\\times$ compared to the FP16\nimplementation, while achieving up to 2.72$\\times$ speedup in the LLM decoding\nthroughput."
                },
                "authors": [
                    {
                        "name": "Yuzong Chen"
                    },
                    {
                        "name": "Xilai Dai"
                    },
                    {
                        "name": "Chi-chih Chang"
                    },
                    {
                        "name": "Yash Akhauri"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed S. Abdelfattah"
                },
                "author": "Mohamed S. Abdelfattah",
                "arxiv_comment": "under submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04052v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04052v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03322v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03322v1",
                "updated": "2025-01-06T19:00:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    19,
                    0,
                    3,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T19:00:03Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    19,
                    0,
                    3,
                    0,
                    6,
                    0
                ],
                "title": "Twinkle: A GPU-based binary-lens microlensing code with contour\n  integration method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Twinkle: A GPU-based binary-lens microlensing code with contour\n  integration method"
                },
                "summary": "With the rapidly increasing rate of microlensing planet detections,\nmicrolensing modeling software faces significant challenges in computation\nefficiency. Here, we develop the Twinkle code, an efficient and robust\nbinary-lens modeling software suite optimized for heterogeneous computing\ndevices, especially GPUs. Existing microlensing codes have the issue of\ncatastrophic cancellation that undermines the numerical stability and\nprecision, and Twinkle resolves them by refining the coefficients of the\nbinary-lens equation. We also devise an improved method for robustly\nidentifying ghost images, thereby enhancing computational reliability. We have\nadvanced the state of the art by optimizing Twinkle specifically for\nheterogeneous computing devices by taking into account the unique task and\ncache memory dispatching patterns of GPUs, while the compatibility with the\ntraditional computing architectures of CPUs is still maintained. Twinkle has\ndemonstrated an acceleration of approximately 2 orders of magnitude (>~100\ntimes) on contemporary GPUs. The enhancement in computational speed of Twinkle\nwill translate to the delivery of accurate and highly efficient data analysis\nfor ongoing and upcoming microlensing projects. Both GPU and CPU versions of\nTwinkle are open-source and publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapidly increasing rate of microlensing planet detections,\nmicrolensing modeling software faces significant challenges in computation\nefficiency. Here, we develop the Twinkle code, an efficient and robust\nbinary-lens modeling software suite optimized for heterogeneous computing\ndevices, especially GPUs. Existing microlensing codes have the issue of\ncatastrophic cancellation that undermines the numerical stability and\nprecision, and Twinkle resolves them by refining the coefficients of the\nbinary-lens equation. We also devise an improved method for robustly\nidentifying ghost images, thereby enhancing computational reliability. We have\nadvanced the state of the art by optimizing Twinkle specifically for\nheterogeneous computing devices by taking into account the unique task and\ncache memory dispatching patterns of GPUs, while the compatibility with the\ntraditional computing architectures of CPUs is still maintained. Twinkle has\ndemonstrated an acceleration of approximately 2 orders of magnitude (>~100\ntimes) on contemporary GPUs. The enhancement in computational speed of Twinkle\nwill translate to the delivery of accurate and highly efficient data analysis\nfor ongoing and upcoming microlensing projects. Both GPU and CPU versions of\nTwinkle are open-source and publicly available."
                },
                "authors": [
                    {
                        "name": "Suwei Wang"
                    },
                    {
                        "name": "Lile Wang"
                    },
                    {
                        "name": "Subo Dong"
                    }
                ],
                "author_detail": {
                    "name": "Subo Dong"
                },
                "author": "Subo Dong",
                "arxiv_doi": "10.3847/1538-4365/ad9b8d",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/1538-4365/ad9b8d",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.03322v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03322v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by ApJS, GitHub link:\n  https://github.com/AsterLight0626/Twinkle",
                "arxiv_journal_ref": "Astrophys. j., suppl. ser. 276 (2025) 40",
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19919v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19919v2",
                "updated": "2025-01-06T15:59:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    15,
                    59,
                    23,
                    0,
                    6,
                    0
                ],
                "published": "2024-12-27T20:47:23Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    20,
                    47,
                    23,
                    4,
                    362,
                    0
                ],
                "title": "Direct Comparison of Magnetic Penetration Depth in Kagome\n  Superconductors AV$_3$Sb$_5$ (A = Cs, K, Rb)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Comparison of Magnetic Penetration Depth in Kagome\n  Superconductors AV$_3$Sb$_5$ (A = Cs, K, Rb)"
                },
                "summary": "We report measurements of the local temperature-dependent penetration depth,\n$\\lambda(T)$, in the Kagome superconductors AV$_3$Sb$_5$ (A = Cs, K, Rb) using\nscanning superconducting quantum interference device (SQUID) microscopy. Our\nresults suggest that the superconducting order in all three compounds is fully\ngapped, in contrast to reports of nodal superconductivity in KV$_3$Sb$_5$ and\nRbV$_3$Sb$_5$. Analysis of the temperature-dependent superfluid density,\n$\\rho_s(T)$, shows deviations from the behavior expected for a single isotropic\ngap, but the data are well described by models incorporating either a single\nanisotropic gap or two isotropic gaps. Notably, the temperature dependences of\n$\\lambda(T)$ and $\\rho_s(T)$ in KV$_3$Sb$_5$ and RbV$_3$Sb$_5$ are\nqualitatively more similar to each other than to CsV$_3$Sb$_5$, consistent with\nthe superconducting phase reflecting features of the normal-state band\nstructure. Our findings provide a direct comparison of the superconducting\nproperties across the AV$_3$Sb$_5$ family.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report measurements of the local temperature-dependent penetration depth,\n$\\lambda(T)$, in the Kagome superconductors AV$_3$Sb$_5$ (A = Cs, K, Rb) using\nscanning superconducting quantum interference device (SQUID) microscopy. Our\nresults suggest that the superconducting order in all three compounds is fully\ngapped, in contrast to reports of nodal superconductivity in KV$_3$Sb$_5$ and\nRbV$_3$Sb$_5$. Analysis of the temperature-dependent superfluid density,\n$\\rho_s(T)$, shows deviations from the behavior expected for a single isotropic\ngap, but the data are well described by models incorporating either a single\nanisotropic gap or two isotropic gaps. Notably, the temperature dependences of\n$\\lambda(T)$ and $\\rho_s(T)$ in KV$_3$Sb$_5$ and RbV$_3$Sb$_5$ are\nqualitatively more similar to each other than to CsV$_3$Sb$_5$, consistent with\nthe superconducting phase reflecting features of the normal-state band\nstructure. Our findings provide a direct comparison of the superconducting\nproperties across the AV$_3$Sb$_5$ family."
                },
                "authors": [
                    {
                        "name": "Austin Kaczmarek"
                    },
                    {
                        "name": "Andrea Capa Salinas"
                    },
                    {
                        "name": "Stephen D. Wilson"
                    },
                    {
                        "name": "Katja C. Nowack"
                    }
                ],
                "author_detail": {
                    "name": "Katja C. Nowack"
                },
                "author": "Katja C. Nowack",
                "arxiv_comment": "5 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19919v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19919v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.supr-con",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02803v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02803v1",
                "updated": "2025-01-06T06:44:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    6,
                    44,
                    13,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T06:44:13Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    6,
                    44,
                    13,
                    0,
                    6,
                    0
                ],
                "title": "Enhancing Lifelong Multi-Agent Path Finding with Cache Mechanism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Lifelong Multi-Agent Path Finding with Cache Mechanism"
                },
                "summary": "Multi-Agent Path Finding (MAPF), which focuses on finding collision-free\npaths for multiple robots, is crucial in autonomous warehouse operations.\nLifelong MAPF (L-MAPF), where agents are continuously reassigned new targets\nupon completing their current tasks, offers a more realistic approximation of\nreal-world warehouse scenarios. While cache storage systems can enhance\nefficiency and reduce operational costs, existing approaches primarily rely on\nexpectations and mathematical models, often without adequately addressing the\nchallenges of multi-robot planning and execution. In this paper, we introduce a\nnovel mechanism called Lifelong MAPF with Cache Mechanism (L-MAPF-CM), which\nintegrates high-level cache storage with low-level path planning. We have\ninvolved a new type of map grid called cache for temporary item storage.\nAdditionally, we involved a task assigner (TA) with a locking mechanism to\nbridge the gap between the new cache grid and L-MAPF algorithm. The TA\ndynamically allocates target locations to agents based on their status in\nvarious scenarios. We evaluated L-MAPF-CM using different cache replacement\npolicies and task distributions. L-MAPF-CM has demonstrated performance\nimprovements particularly with high cache hit rates and smooth traffic\nconditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Agent Path Finding (MAPF), which focuses on finding collision-free\npaths for multiple robots, is crucial in autonomous warehouse operations.\nLifelong MAPF (L-MAPF), where agents are continuously reassigned new targets\nupon completing their current tasks, offers a more realistic approximation of\nreal-world warehouse scenarios. While cache storage systems can enhance\nefficiency and reduce operational costs, existing approaches primarily rely on\nexpectations and mathematical models, often without adequately addressing the\nchallenges of multi-robot planning and execution. In this paper, we introduce a\nnovel mechanism called Lifelong MAPF with Cache Mechanism (L-MAPF-CM), which\nintegrates high-level cache storage with low-level path planning. We have\ninvolved a new type of map grid called cache for temporary item storage.\nAdditionally, we involved a task assigner (TA) with a locking mechanism to\nbridge the gap between the new cache grid and L-MAPF algorithm. The TA\ndynamically allocates target locations to agents based on their status in\nvarious scenarios. We evaluated L-MAPF-CM using different cache replacement\npolicies and task distributions. L-MAPF-CM has demonstrated performance\nimprovements particularly with high cache hit rates and smooth traffic\nconditions."
                },
                "authors": [
                    {
                        "name": "Yimin Tang"
                    },
                    {
                        "name": "Zhenghong Yu"
                    },
                    {
                        "name": "Yi Zheng"
                    },
                    {
                        "name": "T. K. Satish Kumar"
                    },
                    {
                        "name": "Jiaoyang Li"
                    },
                    {
                        "name": "Sven Koenig"
                    }
                ],
                "author_detail": {
                    "name": "Sven Koenig"
                },
                "author": "Sven Koenig",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2403.13421",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02803v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02803v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07772v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07772v2",
                "updated": "2025-01-06T01:26:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    1,
                    26,
                    42,
                    0,
                    6,
                    0
                ],
                "published": "2024-12-10T18:59:50Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    59,
                    50,
                    1,
                    345,
                    0
                ],
                "title": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models"
                },
                "summary": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to an autoregressive\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nachieves a total score of 84.27 on the VBench-Long benchmark, surpassing all\nprevious video generation models. It enables fast streaming generation of\nhigh-quality videos at 9.4 FPS on a single GPU thanks to KV caching. Our\napproach also enables streaming video-to-video translation, image-to-video, and\ndynamic prompting in a zero-shot manner. We will release the code based on an\nopen-source model in the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to an autoregressive\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nachieves a total score of 84.27 on the VBench-Long benchmark, surpassing all\nprevious video generation models. It enables fast streaming generation of\nhigh-quality videos at 9.4 FPS on a single GPU thanks to KV caching. Our\napproach also enables streaming video-to-video translation, image-to-video, and\ndynamic prompting in a zero-shot manner. We will release the code based on an\nopen-source model in the future."
                },
                "authors": [
                    {
                        "name": "Tianwei Yin"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Richard Zhang"
                    },
                    {
                        "name": "William T. Freeman"
                    },
                    {
                        "name": "Fredo Durand"
                    },
                    {
                        "name": "Eli Shechtman"
                    },
                    {
                        "name": "Xun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xun Huang"
                },
                "author": "Xun Huang",
                "arxiv_comment": "Project Page: https://causvid.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07772v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07772v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20504v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20504v2",
                "updated": "2025-01-05T14:11:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    5,
                    14,
                    11,
                    48,
                    6,
                    5,
                    0
                ],
                "published": "2024-12-29T15:42:24Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    15,
                    42,
                    24,
                    6,
                    364,
                    0
                ],
                "title": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding"
                },
                "summary": "Video Large Language Models (VideoLLMs) have achieved remarkable progress in\nvideo understanding. However, existing VideoLLMs often inherit the limitations\nof their backbone LLMs in handling long sequences, leading to challenges for\nlong video understanding. Common solutions either simply uniformly sample\nvideos' frames or compress visual tokens, which focus primarily on low-level\ntemporal visual redundancy, overlooking high-level knowledge redundancy. This\nlimits the achievable compression rate with minimal loss. To this end. we\nintroduce a training-free method, $\\textbf{ReTaKe}$, containing two novel\nmodules DPSelect and PivotKV, to jointly model and reduce both temporal visual\nredundancy and knowledge redundancy for long video understanding. Specifically,\nDPSelect identifies keyframes with local maximum peak distance based on their\nvisual features, which are closely aligned with human video perception. PivotKV\nemploys the obtained keyframes as pivots and conducts KV-Cache compression for\nthe non-pivot tokens with low attention scores, which are derived from the\nlearned prior knowledge of LLMs. Experiments on benchmarks VideoMME, MLVU, and\nLVBench, show that ReTaKe can support 4x longer video sequences with minimal\nperformance loss (<1%) and outperform all similar-size VideoLLMs with 3%-5%,\neven surpassing or on par with much larger ones. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Large Language Models (VideoLLMs) have achieved remarkable progress in\nvideo understanding. However, existing VideoLLMs often inherit the limitations\nof their backbone LLMs in handling long sequences, leading to challenges for\nlong video understanding. Common solutions either simply uniformly sample\nvideos' frames or compress visual tokens, which focus primarily on low-level\ntemporal visual redundancy, overlooking high-level knowledge redundancy. This\nlimits the achievable compression rate with minimal loss. To this end. we\nintroduce a training-free method, $\\textbf{ReTaKe}$, containing two novel\nmodules DPSelect and PivotKV, to jointly model and reduce both temporal visual\nredundancy and knowledge redundancy for long video understanding. Specifically,\nDPSelect identifies keyframes with local maximum peak distance based on their\nvisual features, which are closely aligned with human video perception. PivotKV\nemploys the obtained keyframes as pivots and conducts KV-Cache compression for\nthe non-pivot tokens with low attention scores, which are derived from the\nlearned prior knowledge of LLMs. Experiments on benchmarks VideoMME, MLVU, and\nLVBench, show that ReTaKe can support 4x longer video sequences with minimal\nperformance loss (<1%) and outperform all similar-size VideoLLMs with 3%-5%,\neven surpassing or on par with much larger ones. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe"
                },
                "authors": [
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Qingyi Si"
                    },
                    {
                        "name": "Jianlong Wu"
                    },
                    {
                        "name": "Shiyu Zhu"
                    },
                    {
                        "name": "Li Cao"
                    },
                    {
                        "name": "Liqiang Nie"
                    }
                ],
                "author_detail": {
                    "name": "Liqiang Nie"
                },
                "author": "Liqiang Nie",
                "arxiv_comment": "Update performance in MLVU-dev and LVBench",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20504v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20504v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02524v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02524v1",
                "updated": "2025-01-05T12:51:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    5,
                    12,
                    51,
                    8,
                    6,
                    5,
                    0
                ],
                "published": "2025-01-05T12:51:08Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    12,
                    51,
                    8,
                    6,
                    5,
                    0
                ],
                "title": "A Full-System Simulation Framework for CXL-Based SSD Memory System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Full-System Simulation Framework for CXL-Based SSD Memory System"
                },
                "summary": "Compute eXpress Link (CXL) is a promising technology for memory\ndisaggregation and expansion. Especially, CXL makes it more effectively for\nlarge-capacity storage devices such as Solid State Drive (SSD) to be deployed\nin the memory pool. However, CXL-based SSDs are still in early stages,\nnecessitating the development of reliable simulation tools. In this paper, we\npropose CXL-SSD-Sim, the first open-source full-system simulator designed to\nsimulate CXL-based SSD memory system. Constructed on the foundation of gem5 and\nSimpleSSD, CXL-SSD-Sim extends an high fidelity SSD memory expander model along\nwith the corresponding device driver. In addition, CXL-SSD-Sim models a DRAM\nlayer as a caching mechanism for the SSD, meticulously engineered to counteract\nlatency issues inherent to CXL-based SSD memory access. Experiments are\nperformed among five different memory devices with CXL-SSD-Sim in aspect of\nlatency, bandwidth and real-world benchmark performance. These experiments\nserve to underscore the efficacy of our simulation tool in providing a\ncomprehensive analysis of CXL-based SSD memory systems. The CXL-SSD-Sim\nsimulator is available at https://github.com/WangYaohuii/CXL-SSD-Sim.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute eXpress Link (CXL) is a promising technology for memory\ndisaggregation and expansion. Especially, CXL makes it more effectively for\nlarge-capacity storage devices such as Solid State Drive (SSD) to be deployed\nin the memory pool. However, CXL-based SSDs are still in early stages,\nnecessitating the development of reliable simulation tools. In this paper, we\npropose CXL-SSD-Sim, the first open-source full-system simulator designed to\nsimulate CXL-based SSD memory system. Constructed on the foundation of gem5 and\nSimpleSSD, CXL-SSD-Sim extends an high fidelity SSD memory expander model along\nwith the corresponding device driver. In addition, CXL-SSD-Sim models a DRAM\nlayer as a caching mechanism for the SSD, meticulously engineered to counteract\nlatency issues inherent to CXL-based SSD memory access. Experiments are\nperformed among five different memory devices with CXL-SSD-Sim in aspect of\nlatency, bandwidth and real-world benchmark performance. These experiments\nserve to underscore the efficacy of our simulation tool in providing a\ncomprehensive analysis of CXL-based SSD memory systems. The CXL-SSD-Sim\nsimulator is available at https://github.com/WangYaohuii/CXL-SSD-Sim."
                },
                "authors": [
                    {
                        "name": "Yaohui Wang"
                    },
                    {
                        "name": "Zicong Wang"
                    },
                    {
                        "name": "Fanfeng Meng"
                    },
                    {
                        "name": "Yanjing Wang"
                    },
                    {
                        "name": "Yang Ou"
                    },
                    {
                        "name": "Lizhou Wu"
                    },
                    {
                        "name": "Wentao Hong"
                    },
                    {
                        "name": "Xuran Ge"
                    },
                    {
                        "name": "Jijun Cao"
                    }
                ],
                "author_detail": {
                    "name": "Jijun Cao"
                },
                "author": "Jijun Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02524v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02524v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02469v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02469v1",
                "updated": "2025-01-05T07:41:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    5,
                    7,
                    41,
                    53,
                    6,
                    5,
                    0
                ],
                "published": "2025-01-05T07:41:53Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    7,
                    41,
                    53,
                    6,
                    5,
                    0
                ],
                "title": "LoRaConnect: Unlocking HTTP Potential on LoRa Backbones for Remote Areas\n  and Ad-Hoc Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRaConnect: Unlocking HTTP Potential on LoRa Backbones for Remote Areas\n  and Ad-Hoc Networks"
                },
                "summary": "The minimal infrastructure requirements of LoRa make it suitable for\ndeployments in remote and disaster-stricken areas. Concomitantly, the modern\nera is witnessing the proliferation of web applications in all aspects of human\nlife, including IoT and other network services. Contemporary IoT and network\nsolutions heavily rely on web applications to render services. However, despite\nthe recent research and development pivoted around LoRa, there is still a lack\nof studies focusing on web application access over LoRa networks. Specifically,\ntechnical challenges like payload size limitation, low data rate, and\ncontentions in multi-user setups limit the applicability of LoRa for web\napplications. Hence, we propose LoRaWeb, which enables web access over LoRa\nnetworks. The LoRaWeb hardware tethers a WiFi hotspot to which the client\ndevices connect and access the web pages using a web browser. LoRa backbone of\nthe network handles the web page transmission from the requester and receiver\ndevices. LoRaWeb implements a synchronization procedure to address the\naforementioned challenges for effective message exchange between requesters and\nresponders. The system implements a caching mechanism to reduce latency and\ncontention. Additionally, it implements a message-slicing mechanism in the\napplication layer to overcome the hardware limitations on the message length.\nThe actual hardware-based implementation results indicate seamless deployment,\nand the results indicate an average access time of ~$0.95 S$ for a $1.5 KB$ and\n~$6 S$ for a $10 KB$ size web page.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The minimal infrastructure requirements of LoRa make it suitable for\ndeployments in remote and disaster-stricken areas. Concomitantly, the modern\nera is witnessing the proliferation of web applications in all aspects of human\nlife, including IoT and other network services. Contemporary IoT and network\nsolutions heavily rely on web applications to render services. However, despite\nthe recent research and development pivoted around LoRa, there is still a lack\nof studies focusing on web application access over LoRa networks. Specifically,\ntechnical challenges like payload size limitation, low data rate, and\ncontentions in multi-user setups limit the applicability of LoRa for web\napplications. Hence, we propose LoRaWeb, which enables web access over LoRa\nnetworks. The LoRaWeb hardware tethers a WiFi hotspot to which the client\ndevices connect and access the web pages using a web browser. LoRa backbone of\nthe network handles the web page transmission from the requester and receiver\ndevices. LoRaWeb implements a synchronization procedure to address the\naforementioned challenges for effective message exchange between requesters and\nresponders. The system implements a caching mechanism to reduce latency and\ncontention. Additionally, it implements a message-slicing mechanism in the\napplication layer to overcome the hardware limitations on the message length.\nThe actual hardware-based implementation results indicate seamless deployment,\nand the results indicate an average access time of ~$0.95 S$ for a $1.5 KB$ and\n~$6 S$ for a $10 KB$ size web page."
                },
                "authors": [
                    {
                        "name": "Atonu Ghosh"
                    },
                    {
                        "name": "Sudip Misra"
                    }
                ],
                "author_detail": {
                    "name": "Sudip Misra"
                },
                "author": "Sudip Misra",
                "arxiv_comment": "11 pages, 15 figures, and 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02469v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02469v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01805v1",
                "updated": "2025-01-03T13:32:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    13,
                    32,
                    57,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T13:32:57Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    13,
                    32,
                    57,
                    4,
                    3,
                    0
                ],
                "title": "End-to-End Long Document Summarization using Gradient Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-End Long Document Summarization using Gradient Caching"
                },
                "summary": "Training transformer-based encoder-decoder models for long document\nsummarization poses a significant challenge due to the quadratic memory\nconsumption during training. Several approaches have been proposed to extend\nthe input length at test time, but training with these approaches is still\ndifficult, requiring truncation of input documents and causing a mismatch\nbetween training and test conditions. In this work, we propose CachED (Gradient\n$\\textbf{Cach}$ing for $\\textbf{E}$ncoder-$\\textbf{D}$ecoder models), an\napproach that enables end-to-end training of existing transformer-based\nencoder-decoder models, using the entire document without truncation.\nSpecifically, we apply non-overlapping sliding windows to input documents,\nfollowed by fusion in decoder. During backpropagation, the gradients are cached\nat the decoder and are passed through the encoder in chunks by re-computing the\nhidden vectors, similar to gradient checkpointing. In the experiments on long\ndocument summarization, we extend BART to CachED BART, processing more than\n500K tokens during training and achieving superior performance without using\nany additional parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training transformer-based encoder-decoder models for long document\nsummarization poses a significant challenge due to the quadratic memory\nconsumption during training. Several approaches have been proposed to extend\nthe input length at test time, but training with these approaches is still\ndifficult, requiring truncation of input documents and causing a mismatch\nbetween training and test conditions. In this work, we propose CachED (Gradient\n$\\textbf{Cach}$ing for $\\textbf{E}$ncoder-$\\textbf{D}$ecoder models), an\napproach that enables end-to-end training of existing transformer-based\nencoder-decoder models, using the entire document without truncation.\nSpecifically, we apply non-overlapping sliding windows to input documents,\nfollowed by fusion in decoder. During backpropagation, the gradients are cached\nat the decoder and are passed through the encoder in chunks by re-computing the\nhidden vectors, similar to gradient checkpointing. In the experiments on long\ndocument summarization, we extend BART to CachED BART, processing more than\n500K tokens during training and achieving superior performance without using\nany additional parameters."
                },
                "authors": [
                    {
                        "name": "Rohit Saxena"
                    },
                    {
                        "name": "Hao Tang"
                    },
                    {
                        "name": "Frank Keller"
                    }
                ],
                "author_detail": {
                    "name": "Frank Keller"
                },
                "author": "Frank Keller",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01792v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01792v1",
                "updated": "2025-01-03T12:51:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    12,
                    51,
                    37,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T12:51:37Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    12,
                    51,
                    37,
                    4,
                    3,
                    0
                ],
                "title": "Efficient LLM Inference with Activation Checkpointing and Hybrid Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Inference with Activation Checkpointing and Hybrid Caching"
                },
                "summary": "Recent large language models (LLMs) with enormous model sizes use many GPUs\nto meet memory capacity requirements incurring substantial costs for token\ngeneration. To provide cost-effective LLM inference with relaxed latency\nconstraints, extensive research has focused on expanding GPU memory by\nleveraging the host memory. However, LLM inference engines that utilize the\nhost memory often face underutilization of GPU compute units, as a considerable\nportion of inference time is spent in loading the model onto the GPU via\nhost-GPU interconnect. To tackle these challenges of the host memory offloading\nfor LLM, we introduce HybridServe, an LLM inference system with activation\ncheckpointing based on activation caching. The activation cache stores\nactivation checkpoints generated during intermediate inference stages, allowing\nthe fast recomputation of KV cache while model parameters are transferred to\nGPU from host memory. Unlike conventional methods that recompute the KV cache\nfrom scratch using token IDs, the activation cache allows bypassing projection\nand FFN operations. To balance between the activation recomputation and\nparameter loading overhead, this study proposes a KV-activation hybrid caching\nscheme which finds the best ratio of the key-value and activation caches to\nadjust the recomputation time. Our system achieves 2.19x throughput improvement\nover the state-of-the-art prior work for offloading both model weights and KV\ncache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language models (LLMs) with enormous model sizes use many GPUs\nto meet memory capacity requirements incurring substantial costs for token\ngeneration. To provide cost-effective LLM inference with relaxed latency\nconstraints, extensive research has focused on expanding GPU memory by\nleveraging the host memory. However, LLM inference engines that utilize the\nhost memory often face underutilization of GPU compute units, as a considerable\nportion of inference time is spent in loading the model onto the GPU via\nhost-GPU interconnect. To tackle these challenges of the host memory offloading\nfor LLM, we introduce HybridServe, an LLM inference system with activation\ncheckpointing based on activation caching. The activation cache stores\nactivation checkpoints generated during intermediate inference stages, allowing\nthe fast recomputation of KV cache while model parameters are transferred to\nGPU from host memory. Unlike conventional methods that recompute the KV cache\nfrom scratch using token IDs, the activation cache allows bypassing projection\nand FFN operations. To balance between the activation recomputation and\nparameter loading overhead, this study proposes a KV-activation hybrid caching\nscheme which finds the best ratio of the key-value and activation caches to\nadjust the recomputation time. Our system achieves 2.19x throughput improvement\nover the state-of-the-art prior work for offloading both model weights and KV\ncache."
                },
                "authors": [
                    {
                        "name": "Sanghyeon Lee"
                    },
                    {
                        "name": "Hongbeen Kim"
                    },
                    {
                        "name": "Soojin Hwang"
                    },
                    {
                        "name": "Guseul Heo"
                    },
                    {
                        "name": "Minwoo Noh"
                    },
                    {
                        "name": "Jaehyuk Huh"
                    }
                ],
                "author_detail": {
                    "name": "Jaehyuk Huh"
                },
                "author": "Jaehyuk Huh",
                "arxiv_comment": "14 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01792v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01792v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01424v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01424v1",
                "updated": "2025-01-02T18:59:44Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    18,
                    59,
                    44,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T18:59:44Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    18,
                    59,
                    44,
                    3,
                    2,
                    0
                ],
                "title": "Object-level Visual Prompts for Compositional Image Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Object-level Visual Prompts for Compositional Image Generation"
                },
                "summary": "We introduce a method for composing object-level visual prompts within a\ntext-to-image diffusion model. Our approach addresses the task of generating\nsemantically coherent compositions across diverse scenes and styles, similar to\nthe versatility and expressiveness offered by text prompts. A key challenge in\nthis task is to preserve the identity of the objects depicted in the input\nvisual prompts, while also generating diverse compositions across different\nimages. To address this challenge, we introduce a new KV-mixed cross-attention\nmechanism, in which keys and values are learned from distinct visual\nrepresentations. The keys are derived from an encoder with a small bottleneck\nfor layout control, whereas the values come from a larger bottleneck encoder\nthat captures fine-grained appearance details. By mixing keys and values from\nthese complementary sources, our model preserves the identity of the visual\nprompts while supporting flexible variations in object arrangement, pose, and\ncomposition. During inference, we further propose object-level compositional\nguidance to improve the method's identity preservation and layout correctness.\nResults show that our technique produces diverse scene compositions that\npreserve the unique characteristics of each visual prompt, expanding the\ncreative potential of text-to-image generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a method for composing object-level visual prompts within a\ntext-to-image diffusion model. Our approach addresses the task of generating\nsemantically coherent compositions across diverse scenes and styles, similar to\nthe versatility and expressiveness offered by text prompts. A key challenge in\nthis task is to preserve the identity of the objects depicted in the input\nvisual prompts, while also generating diverse compositions across different\nimages. To address this challenge, we introduce a new KV-mixed cross-attention\nmechanism, in which keys and values are learned from distinct visual\nrepresentations. The keys are derived from an encoder with a small bottleneck\nfor layout control, whereas the values come from a larger bottleneck encoder\nthat captures fine-grained appearance details. By mixing keys and values from\nthese complementary sources, our model preserves the identity of the visual\nprompts while supporting flexible variations in object arrangement, pose, and\ncomposition. During inference, we further propose object-level compositional\nguidance to improve the method's identity preservation and layout correctness.\nResults show that our technique produces diverse scene compositions that\npreserve the unique characteristics of each visual prompt, expanding the\ncreative potential of text-to-image generation."
                },
                "authors": [
                    {
                        "name": "Gaurav Parmar"
                    },
                    {
                        "name": "Or Patashnik"
                    },
                    {
                        "name": "Kuan-Chieh Wang"
                    },
                    {
                        "name": "Daniil Ostashev"
                    },
                    {
                        "name": "Srinivasa Narasimhan"
                    },
                    {
                        "name": "Jun-Yan Zhu"
                    },
                    {
                        "name": "Daniel Cohen-Or"
                    },
                    {
                        "name": "Kfir Aberman"
                    }
                ],
                "author_detail": {
                    "name": "Kfir Aberman"
                },
                "author": "Kfir Aberman",
                "arxiv_comment": "Project: https://snap-research.github.io/visual-composer/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01424v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01424v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01039v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01039v1",
                "updated": "2025-01-02T03:41:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    41,
                    32,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T03:41:32Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    41,
                    32,
                    3,
                    2,
                    0
                ],
                "title": "MSWA: Refining Local Attention with Multi-ScaleWindow Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MSWA: Refining Local Attention with Multi-ScaleWindow Attention"
                },
                "summary": "Transformer-based LLMs have achieved exceptional performance across a wide\nrange of NLP tasks. However, the standard self-attention mechanism suffers from\nquadratic time complexity and linearly increased cache size. Sliding window\nattention (SWA) solves this problem by restricting the attention range to a\nfixed-size local context window. Nevertheless, SWA employs a uniform window\nsize for each head in each layer, making it inefficient in capturing context of\nvarying scales. To mitigate this limitation, we propose Multi-Scale Window\nAttention (MSWA) which applies diverse window sizes across heads and layers in\nthe Transformer. It not only allows for different window sizes among heads\nwithin the same layer but also progressively increases window size allocation\nfrom shallow to deep layers, thus enabling the model to capture contextual\ninformation with different lengths and distances. Experimental results on\nlanguage modeling and common-sense reasoning tasks substantiate that MSWA\noutperforms traditional local attention in both effectiveness and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based LLMs have achieved exceptional performance across a wide\nrange of NLP tasks. However, the standard self-attention mechanism suffers from\nquadratic time complexity and linearly increased cache size. Sliding window\nattention (SWA) solves this problem by restricting the attention range to a\nfixed-size local context window. Nevertheless, SWA employs a uniform window\nsize for each head in each layer, making it inefficient in capturing context of\nvarying scales. To mitigate this limitation, we propose Multi-Scale Window\nAttention (MSWA) which applies diverse window sizes across heads and layers in\nthe Transformer. It not only allows for different window sizes among heads\nwithin the same layer but also progressively increases window size allocation\nfrom shallow to deep layers, thus enabling the model to capture contextual\ninformation with different lengths and distances. Experimental results on\nlanguage modeling and common-sense reasoning tasks substantiate that MSWA\noutperforms traditional local attention in both effectiveness and efficiency."
                },
                "authors": [
                    {
                        "name": "Yixing Xu"
                    },
                    {
                        "name": "Shivank Nag"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Lu Tian"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01039v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01039v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19442v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19442v2",
                "updated": "2025-01-02T03:40:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    40,
                    15,
                    3,
                    2,
                    0
                ],
                "published": "2024-12-27T04:17:57Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    4,
                    17,
                    57,
                    4,
                    362,
                    0
                ],
                "title": "A Survey on Large Language Model Acceleration based on KV Cache\n  Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Large Language Model Acceleration based on KV Cache\n  Management"
                },
                "summary": "Large Language Models (LLMs) have revolutionized a wide range of domains such\nas natural language processing, computer vision, and multi-modal tasks due to\ntheir ability to comprehend context and perform logical reasoning. However, the\ncomputational and memory demands of LLMs, particularly during inference, pose\nsignificant challenges when scaling them to real-world, long-context, and\nreal-time applications. Key-Value (KV) cache management has emerged as a\ncritical optimization technique for accelerating LLM inference by reducing\nredundant computations and improving memory utilization. This survey provides a\ncomprehensive overview of KV cache management strategies for LLM acceleration,\ncategorizing them into token-level, model-level, and system-level\noptimizations. Token-level strategies include KV cache selection, budget\nallocation, merging, quantization, and low-rank decomposition, while\nmodel-level optimizations focus on architectural innovations and attention\nmechanisms to enhance KV reuse. System-level approaches address memory\nmanagement, scheduling, and hardware-aware designs to improve efficiency across\ndiverse computing environments. Additionally, the survey provides an overview\nof both text and multimodal datasets and benchmarks used to evaluate these\nstrategies. By presenting detailed taxonomies and comparative analyses, this\nwork aims to offer useful insights for researchers and practitioners to support\nthe development of efficient and scalable KV cache management techniques,\ncontributing to the practical deployment of LLMs in real-world applications.\nThe curated paper list for KV cache management is in:\n\\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized a wide range of domains such\nas natural language processing, computer vision, and multi-modal tasks due to\ntheir ability to comprehend context and perform logical reasoning. However, the\ncomputational and memory demands of LLMs, particularly during inference, pose\nsignificant challenges when scaling them to real-world, long-context, and\nreal-time applications. Key-Value (KV) cache management has emerged as a\ncritical optimization technique for accelerating LLM inference by reducing\nredundant computations and improving memory utilization. This survey provides a\ncomprehensive overview of KV cache management strategies for LLM acceleration,\ncategorizing them into token-level, model-level, and system-level\noptimizations. Token-level strategies include KV cache selection, budget\nallocation, merging, quantization, and low-rank decomposition, while\nmodel-level optimizations focus on architectural innovations and attention\nmechanisms to enhance KV reuse. System-level approaches address memory\nmanagement, scheduling, and hardware-aware designs to improve efficiency across\ndiverse computing environments. Additionally, the survey provides an overview\nof both text and multimodal datasets and benchmarks used to evaluate these\nstrategies. By presenting detailed taxonomies and comparative analyses, this\nwork aims to offer useful insights for researchers and practitioners to support\nthe development of efficient and scalable KV cache management techniques,\ncontributing to the practical deployment of LLMs in real-world applications.\nThe curated paper list for KV cache management is in:\n\\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}."
                },
                "authors": [
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Anxin Tian"
                    },
                    {
                        "name": "Tianhao Tang"
                    },
                    {
                        "name": "Zhanchao Xu"
                    },
                    {
                        "name": "Xuejia Chen"
                    },
                    {
                        "name": "Nicole Hu"
                    },
                    {
                        "name": "Wei Dong"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19442v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19442v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01005v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01005v1",
                "updated": "2025-01-02T02:02:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    2,
                    2,
                    20,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T02:02:20Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    2,
                    2,
                    20,
                    3,
                    2,
                    0
                ],
                "title": "FlashInfer: Efficient and Customizable Attention Engine for LLM\n  Inference Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashInfer: Efficient and Customizable Attention Engine for LLM\n  Inference Serving"
                },
                "summary": "Transformers, driven by attention mechanisms, form the foundation of large\nlanguage models (LLMs). As these models scale up, efficient GPU attention\nkernels become essential for high-throughput and low-latency inference. Diverse\nLLM applications demand flexible and high-performance attention solutions. We\npresent FlashInfer: a customizable and efficient attention engine for LLM\nserving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse\nformat and composable formats to optimize memory access and reduce redundancy.\nIt also offers a customizable attention template, enabling adaptation to\nvarious settings through Just-In-Time (JIT) compilation. Additionally,\nFlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user\nrequests while maintaining compatibility with CUDAGraph which requires static\nconfiguration. FlashInfer have been integrated into leading LLM serving\nframeworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and\nend-to-end evaluations demonstrate FlashInfer's ability to significantly boost\nkernel performance across diverse inference scenarios: compared to\nstate-of-the-art LLM serving solutions, FlashInfer achieve 29-69%\ninter-token-latency reduction compared to compiler backends for LLM serving\nbenchmark, 28-30% latency reduction for long-context inference, and 13-17%\nspeedup for LLM serving with parallel generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers, driven by attention mechanisms, form the foundation of large\nlanguage models (LLMs). As these models scale up, efficient GPU attention\nkernels become essential for high-throughput and low-latency inference. Diverse\nLLM applications demand flexible and high-performance attention solutions. We\npresent FlashInfer: a customizable and efficient attention engine for LLM\nserving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse\nformat and composable formats to optimize memory access and reduce redundancy.\nIt also offers a customizable attention template, enabling adaptation to\nvarious settings through Just-In-Time (JIT) compilation. Additionally,\nFlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user\nrequests while maintaining compatibility with CUDAGraph which requires static\nconfiguration. FlashInfer have been integrated into leading LLM serving\nframeworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and\nend-to-end evaluations demonstrate FlashInfer's ability to significantly boost\nkernel performance across diverse inference scenarios: compared to\nstate-of-the-art LLM serving solutions, FlashInfer achieve 29-69%\ninter-token-latency reduction compared to compiler backends for LLM serving\nbenchmark, 28-30% latency reduction for long-context inference, and 13-17%\nspeedup for LLM serving with parallel generation."
                },
                "authors": [
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Lequn Chen"
                    },
                    {
                        "name": "Ruihang Lai"
                    },
                    {
                        "name": "Wuwei Lin"
                    },
                    {
                        "name": "Yineng Zhang"
                    },
                    {
                        "name": "Stephanie Wang"
                    },
                    {
                        "name": "Tianqi Chen"
                    },
                    {
                        "name": "Baris Kasikci"
                    },
                    {
                        "name": "Vinod Grover"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "Luis Ceze"
                    }
                ],
                "author_detail": {
                    "name": "Luis Ceze"
                },
                "author": "Luis Ceze",
                "arxiv_comment": "code available at http://github.com/flashinfer-ai/flashinfer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01005v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01005v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00946v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00946v1",
                "updated": "2025-01-01T20:16:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    20,
                    16,
                    27,
                    2,
                    1,
                    0
                ],
                "published": "2025-01-01T20:16:27Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    20,
                    16,
                    27,
                    2,
                    1,
                    0
                ],
                "title": "Cached Adaptive Token Merging: Dynamic Token Reduction and Redundant\n  Computation Elimination in Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cached Adaptive Token Merging: Dynamic Token Reduction and Redundant\n  Computation Elimination in Diffusion Model"
                },
                "summary": "Diffusion models have emerged as a promising approach for generating\nhigh-quality, high-dimensional images. Nevertheless, these models are hindered\nby their high computational cost and slow inference, partly due to the\nquadratic computational complexity of the self-attention mechanisms with\nrespect to input size. Various approaches have been proposed to address this\ndrawback. One such approach focuses on reducing the number of tokens fed into\nthe self-attention, known as token merging (ToMe). In our method, which is\ncalled cached adaptive token merging(CA-ToMe), we calculate the similarity\nbetween tokens and then merge the r proportion of the most similar tokens.\nHowever, due to the repetitive patterns observed in adjacent steps and the\nvariation in the frequency of similarities, we aim to enhance this approach by\nimplementing an adaptive threshold for merging tokens and adding a caching\nmechanism that stores similar pairs across several adjacent steps. Empirical\nresults demonstrate that our method operates as a training-free acceleration\nmethod, achieving a speedup factor of 1.24 in the denoising process while\nmaintaining the same FID scores compared to existing approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have emerged as a promising approach for generating\nhigh-quality, high-dimensional images. Nevertheless, these models are hindered\nby their high computational cost and slow inference, partly due to the\nquadratic computational complexity of the self-attention mechanisms with\nrespect to input size. Various approaches have been proposed to address this\ndrawback. One such approach focuses on reducing the number of tokens fed into\nthe self-attention, known as token merging (ToMe). In our method, which is\ncalled cached adaptive token merging(CA-ToMe), we calculate the similarity\nbetween tokens and then merge the r proportion of the most similar tokens.\nHowever, due to the repetitive patterns observed in adjacent steps and the\nvariation in the frequency of similarities, we aim to enhance this approach by\nimplementing an adaptive threshold for merging tokens and adding a caching\nmechanism that stores similar pairs across several adjacent steps. Empirical\nresults demonstrate that our method operates as a training-free acceleration\nmethod, achieving a speedup factor of 1.24 in the denoising process while\nmaintaining the same FID scores compared to existing approaches."
                },
                "authors": [
                    {
                        "name": "Omid Saghatchian"
                    },
                    {
                        "name": "Atiyeh Gh. Moghadam"
                    },
                    {
                        "name": "Ahmad Nickabadi"
                    }
                ],
                "author_detail": {
                    "name": "Ahmad Nickabadi"
                },
                "author": "Ahmad Nickabadi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00946v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00946v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21023v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21023v2",
                "updated": "2024-12-31T20:40:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    20,
                    40,
                    43,
                    1,
                    366,
                    0
                ],
                "published": "2024-12-30T15:46:53Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    46,
                    53,
                    0,
                    365,
                    0
                ],
                "title": "EdgeRAG: Online-Indexed RAG for Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EdgeRAG: Online-Indexed RAG for Edge Devices"
                },
                "summary": "Deploying Retrieval Augmented Generation (RAG) on resource-constrained edge\ndevices is challenging due to limited memory and processing power. In this\nwork, we propose EdgeRAG which addresses the memory constraint by pruning\nembeddings within clusters and generating embeddings on-demand during\nretrieval. To avoid the latency of generating embeddings for large tail\nclusters, EdgeRAG pre-computes and stores embeddings for these clusters, while\nadaptively caching remaining embeddings to minimize redundant computations and\nfurther optimize latency. The result from BEIR suite shows that EdgeRAG offers\nsignificant latency reduction over the baseline IVF index, but with similar\ngeneration quality while allowing all of our evaluated datasets to fit into the\nmemory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying Retrieval Augmented Generation (RAG) on resource-constrained edge\ndevices is challenging due to limited memory and processing power. In this\nwork, we propose EdgeRAG which addresses the memory constraint by pruning\nembeddings within clusters and generating embeddings on-demand during\nretrieval. To avoid the latency of generating embeddings for large tail\nclusters, EdgeRAG pre-computes and stores embeddings for these clusters, while\nadaptively caching remaining embeddings to minimize redundant computations and\nfurther optimize latency. The result from BEIR suite shows that EdgeRAG offers\nsignificant latency reduction over the baseline IVF index, but with similar\ngeneration quality while allowing all of our evaluated datasets to fit into the\nmemory."
                },
                "authors": [
                    {
                        "name": "Korakit Seemakhupt"
                    },
                    {
                        "name": "Sihang Liu"
                    },
                    {
                        "name": "Samira Khan"
                    }
                ],
                "author_detail": {
                    "name": "Samira Khan"
                },
                "author": "Samira Khan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21023v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21023v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00375v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00375v1",
                "updated": "2024-12-31T09:56:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    9,
                    56,
                    40,
                    1,
                    366,
                    0
                ],
                "published": "2024-12-31T09:56:40Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    9,
                    56,
                    40,
                    1,
                    366,
                    0
                ],
                "title": "Token Pruning for Caching Better: 9 Times Acceleration on Stable\n  Diffusion for Free",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token Pruning for Caching Better: 9 Times Acceleration on Stable\n  Diffusion for Free"
                },
                "summary": "Stable Diffusion has achieved remarkable success in the field of\ntext-to-image generation, with its powerful generative capabilities and diverse\ngeneration results making a lasting impact. However, its iterative denoising\nintroduces high computational costs and slows generation speed, limiting\nbroader adoption. The community has made numerous efforts to reduce this\ncomputational burden, with methods like feature caching attracting attention\ndue to their effectiveness and simplicity. Nonetheless, simply reusing features\ncomputed at previous timesteps causes the features across adjacent timesteps to\nbecome similar, reducing the dynamics of features over time and ultimately\ncompromising the quality of generated images. In this paper, we introduce a\ndynamics-aware token pruning (DaTo) approach that addresses the limitations of\nfeature caching. DaTo selectively prunes tokens with lower dynamics, allowing\nonly high-dynamic tokens to participate in self-attention layers, thereby\nextending feature dynamics across timesteps. DaTo combines feature caching with\ntoken pruning in a training-free manner, achieving both temporal and token-wise\ninformation reuse. Applied to Stable Diffusion on the ImageNet, our approach\ndelivered a 9$\\times$ speedup while reducing FID by 0.33, indicating enhanced\nimage quality. On the COCO-30k, we observed a 7$\\times$ acceleration coupled\nwith a notable FID reduction of 2.17.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stable Diffusion has achieved remarkable success in the field of\ntext-to-image generation, with its powerful generative capabilities and diverse\ngeneration results making a lasting impact. However, its iterative denoising\nintroduces high computational costs and slows generation speed, limiting\nbroader adoption. The community has made numerous efforts to reduce this\ncomputational burden, with methods like feature caching attracting attention\ndue to their effectiveness and simplicity. Nonetheless, simply reusing features\ncomputed at previous timesteps causes the features across adjacent timesteps to\nbecome similar, reducing the dynamics of features over time and ultimately\ncompromising the quality of generated images. In this paper, we introduce a\ndynamics-aware token pruning (DaTo) approach that addresses the limitations of\nfeature caching. DaTo selectively prunes tokens with lower dynamics, allowing\nonly high-dynamic tokens to participate in self-attention layers, thereby\nextending feature dynamics across timesteps. DaTo combines feature caching with\ntoken pruning in a training-free manner, achieving both temporal and token-wise\ninformation reuse. Applied to Stable Diffusion on the ImageNet, our approach\ndelivered a 9$\\times$ speedup while reducing FID by 0.33, indicating enhanced\nimage quality. On the COCO-30k, we observed a 7$\\times$ acceleration coupled\nwith a notable FID reduction of 2.17."
                },
                "authors": [
                    {
                        "name": "Evelyn Zhang"
                    },
                    {
                        "name": "Bang Xiao"
                    },
                    {
                        "name": "Jiayi Tang"
                    },
                    {
                        "name": "Qianli Ma"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Xuefei Ning"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00375v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00375v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10516v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10516v3",
                "updated": "2024-12-31T07:11:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    7,
                    11,
                    0,
                    1,
                    366,
                    0
                ],
                "published": "2024-09-16T17:59:52Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    59,
                    52,
                    0,
                    260,
                    0
                ],
                "title": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector\n  Retrieval"
                },
                "summary": "Transformer-based Large Language Models (LLMs) have become increasingly\nimportant. However, due to the quadratic time complexity of attention\ncomputation, scaling LLMs to longer contexts incurs extremely slow inference\nspeed and high GPU memory consumption for caching key-value (KV) vectors. This\npaper proposes RetrievalAttention, a training-free approach to both accelerate\nattention computation and reduce GPU memory consumption. By leveraging the\ndynamic sparsity of attention mechanism, RetrievalAttention proposes to build\napproximate nearest neighbor search (ANNS) indexes for KV vectors in CPU memory\nand retrieve the most relevant ones through vector search during generation.\nUnfortunately, we observe that the off-the-shelf ANNS indexes are often\nineffective for such retrieval tasks due to the out-of-distribution (OOD)\nbetween query vectors and key vectors in the attention mechanism.\nRetrievalAttention addresses the OOD challenge by designing an attention-aware\nvector search algorithm that can adapt to the distribution of query vectors.\nOur evaluation demonstrates that RetrievalAttention achieves near full\nattention accuracy while only requiring access to 1--3% of the data. This leads\nto a significant reduction in the inference cost of long-context LLMs, with a\nmuch lower GPU memory footprint. In particular, RetrievalAttention only needs a\nsingle NVIDIA RTX4090 (24GB) to serve 128K tokens for LLMs with 8B parameters,\nwhich is capable of generating one token in 0.188 seconds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based Large Language Models (LLMs) have become increasingly\nimportant. However, due to the quadratic time complexity of attention\ncomputation, scaling LLMs to longer contexts incurs extremely slow inference\nspeed and high GPU memory consumption for caching key-value (KV) vectors. This\npaper proposes RetrievalAttention, a training-free approach to both accelerate\nattention computation and reduce GPU memory consumption. By leveraging the\ndynamic sparsity of attention mechanism, RetrievalAttention proposes to build\napproximate nearest neighbor search (ANNS) indexes for KV vectors in CPU memory\nand retrieve the most relevant ones through vector search during generation.\nUnfortunately, we observe that the off-the-shelf ANNS indexes are often\nineffective for such retrieval tasks due to the out-of-distribution (OOD)\nbetween query vectors and key vectors in the attention mechanism.\nRetrievalAttention addresses the OOD challenge by designing an attention-aware\nvector search algorithm that can adapt to the distribution of query vectors.\nOur evaluation demonstrates that RetrievalAttention achieves near full\nattention accuracy while only requiring access to 1--3% of the data. This leads\nto a significant reduction in the inference cost of long-context LLMs, with a\nmuch lower GPU memory footprint. In particular, RetrievalAttention only needs a\nsingle NVIDIA RTX4090 (24GB) to serve 128K tokens for LLMs with 8B parameters,\nwhich is capable of generating one token in 0.188 seconds."
                },
                "authors": [
                    {
                        "name": "Di Liu"
                    },
                    {
                        "name": "Meng Chen"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Zhenhua Han"
                    },
                    {
                        "name": "Qianxi Zhang"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Bailu Ding"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "arxiv_comment": "19 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10516v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10516v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00279v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00279v1",
                "updated": "2024-12-31T05:24:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    5,
                    24,
                    30,
                    1,
                    366,
                    0
                ],
                "published": "2024-12-31T05:24:30Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    5,
                    24,
                    30,
                    1,
                    366,
                    0
                ],
                "title": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement"
                },
                "summary": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace."
                },
                "authors": [
                    {
                        "name": "Junjie Li"
                    }
                ],
                "author_detail": {
                    "name": "Junjie Li"
                },
                "author": "Junjie Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00279v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00279v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00243v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00243v1",
                "updated": "2024-12-31T03:19:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    3,
                    19,
                    38,
                    1,
                    366,
                    0
                ],
                "published": "2024-12-31T03:19:38Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    3,
                    19,
                    38,
                    1,
                    366,
                    0
                ],
                "title": "Cross-Layer Cache Aggregation for Token Reduction in Ultra-Fine-Grained\n  Image Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Layer Cache Aggregation for Token Reduction in Ultra-Fine-Grained\n  Image Recognition"
                },
                "summary": "Ultra-fine-grained image recognition (UFGIR) is a challenging task that\ninvolves classifying images within a macro-category. While traditional FGIR\ndeals with classifying different species, UFGIR goes beyond by classifying\nsub-categories within a species such as cultivars of a plant. In recent times\nthe usage of Vision Transformer-based backbones has allowed methods to obtain\noutstanding recognition performances in this task but this comes at a\nsignificant cost in terms of computation specially since this task\nsignificantly benefits from incorporating higher resolution images. Therefore,\ntechniques such as token reduction have emerged to reduce the computational\ncost. However, dropping tokens leads to loss of essential information for\nfine-grained categories, specially as the token keep rate is reduced.\nTherefore, to counteract the loss of information brought by the usage of token\nreduction we propose a novel Cross-Layer Aggregation Classification Head and a\nCross-Layer Cache mechanism to recover and access information from previous\nlayers in later locations. Extensive experiments covering more than 2000 runs\nacross diverse settings including 5 datasets, 9 backbones, 7 token reduction\nmethods, 5 keep rates, and 2 image sizes demonstrate the effectiveness of the\nproposed plug-and-play modules and allow us to push the boundaries of accuracy\nvs cost for UFGIR by reducing the kept tokens to extremely low ratios of up to\n10\\% while maintaining a competitive accuracy to state-of-the-art models. Code\nis available at: \\url{https://github.com/arkel23/CLCA}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultra-fine-grained image recognition (UFGIR) is a challenging task that\ninvolves classifying images within a macro-category. While traditional FGIR\ndeals with classifying different species, UFGIR goes beyond by classifying\nsub-categories within a species such as cultivars of a plant. In recent times\nthe usage of Vision Transformer-based backbones has allowed methods to obtain\noutstanding recognition performances in this task but this comes at a\nsignificant cost in terms of computation specially since this task\nsignificantly benefits from incorporating higher resolution images. Therefore,\ntechniques such as token reduction have emerged to reduce the computational\ncost. However, dropping tokens leads to loss of essential information for\nfine-grained categories, specially as the token keep rate is reduced.\nTherefore, to counteract the loss of information brought by the usage of token\nreduction we propose a novel Cross-Layer Aggregation Classification Head and a\nCross-Layer Cache mechanism to recover and access information from previous\nlayers in later locations. Extensive experiments covering more than 2000 runs\nacross diverse settings including 5 datasets, 9 backbones, 7 token reduction\nmethods, 5 keep rates, and 2 image sizes demonstrate the effectiveness of the\nproposed plug-and-play modules and allow us to push the boundaries of accuracy\nvs cost for UFGIR by reducing the kept tokens to extremely low ratios of up to\n10\\% while maintaining a competitive accuracy to state-of-the-art models. Code\nis available at: \\url{https://github.com/arkel23/CLCA}"
                },
                "authors": [
                    {
                        "name": "Edwin Arkel Rios"
                    },
                    {
                        "name": "Jansen Christopher Yuanda"
                    },
                    {
                        "name": "Vincent Leon Ghanz"
                    },
                    {
                        "name": "Cheng-Wei Yu"
                    },
                    {
                        "name": "Bo-Cheng Lai"
                    },
                    {
                        "name": "Min-Chun Hu"
                    }
                ],
                "author_detail": {
                    "name": "Min-Chun Hu"
                },
                "author": "Min-Chun Hu",
                "arxiv_comment": "Accepted to ICASSP 2025. Main: 5 pages, 4 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00243v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00243v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; I.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21015v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21015v1",
                "updated": "2024-12-30T15:33:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    33,
                    19,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T15:33:19Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    33,
                    19,
                    0,
                    365,
                    0
                ],
                "title": "MapQaTor: A System for Efficient Annotation of Map Query Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MapQaTor: A System for Efficient Annotation of Map Query Datasets"
                },
                "summary": "Mapping and navigation services like Google Maps, Apple Maps, Openstreet\nMaps, are essential for accessing various location-based data, yet they often\nstruggle to handle natural language geospatial queries. Recent advancements in\nLarge Language Models (LLMs) show promise in question answering (QA), but\ncreating reliable geospatial QA datasets from map services remains challenging.\nWe introduce MapQaTor, a web application that streamlines the creation of\nreproducible, traceable map-based QA datasets. With its plug-and-play\narchitecture, MapQaTor enables seamless integration with any maps API, allowing\nusers to gather and visualize data from diverse sources with minimal setup. By\ncaching API responses, the platform ensures consistent ground truth, enhancing\nthe reliability of the data even as real-world information evolves. MapQaTor\ncentralizes data retrieval, annotation, and visualization within a single\nplatform, offering a unique opportunity to evaluate the current state of\nLLM-based geospatial reasoning while advancing their capabilities for improved\ngeospatial understanding. Evaluation metrics show that, MapQaTor speeds up the\nannotation process by at least 30 times compared to manual methods,\nunderscoring its potential for developing geospatial resources, such as complex\nmap reasoning datasets. The website is live at: https://mapqator.github.io/ and\na demo video is available at: https://youtu.be/7_aV9Wmhs6Q.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mapping and navigation services like Google Maps, Apple Maps, Openstreet\nMaps, are essential for accessing various location-based data, yet they often\nstruggle to handle natural language geospatial queries. Recent advancements in\nLarge Language Models (LLMs) show promise in question answering (QA), but\ncreating reliable geospatial QA datasets from map services remains challenging.\nWe introduce MapQaTor, a web application that streamlines the creation of\nreproducible, traceable map-based QA datasets. With its plug-and-play\narchitecture, MapQaTor enables seamless integration with any maps API, allowing\nusers to gather and visualize data from diverse sources with minimal setup. By\ncaching API responses, the platform ensures consistent ground truth, enhancing\nthe reliability of the data even as real-world information evolves. MapQaTor\ncentralizes data retrieval, annotation, and visualization within a single\nplatform, offering a unique opportunity to evaluate the current state of\nLLM-based geospatial reasoning while advancing their capabilities for improved\ngeospatial understanding. Evaluation metrics show that, MapQaTor speeds up the\nannotation process by at least 30 times compared to manual methods,\nunderscoring its potential for developing geospatial resources, such as complex\nmap reasoning datasets. The website is live at: https://mapqator.github.io/ and\na demo video is available at: https://youtu.be/7_aV9Wmhs6Q."
                },
                "authors": [
                    {
                        "name": "Mahir Labib Dihan"
                    },
                    {
                        "name": "Mohammed Eunus Ali"
                    },
                    {
                        "name": "Md Rizwan Parvez"
                    }
                ],
                "author_detail": {
                    "name": "Md Rizwan Parvez"
                },
                "author": "Md Rizwan Parvez",
                "arxiv_comment": "13 pages, 35 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21015v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21015v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12094v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12094v3",
                "updated": "2024-12-30T14:54:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    14,
                    54,
                    29,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-16T18:58:57Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    58,
                    57,
                    0,
                    351,
                    0
                ],
                "title": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator"
                },
                "summary": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities."
                },
                "authors": [
                    {
                        "name": "Guoxuan Chen"
                    },
                    {
                        "name": "Han Shi"
                    },
                    {
                        "name": "Jiawei Li"
                    },
                    {
                        "name": "Yihang Gao"
                    },
                    {
                        "name": "Xiaozhe Ren"
                    },
                    {
                        "name": "Yimeng Chen"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Weiyang Liu"
                    },
                    {
                        "name": "Chao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Huang"
                },
                "author": "Chao Huang",
                "arxiv_comment": "We have made our code publicly available at sepllm.github.io. Our\n  codebase supports efficient multi-node distributed training with accelerated\n  attention module Sep-Attention and also supports numerous existing Fusion\n  Operators to accelerate the training process, such as fused rope, etc. If you\n  find our code helpful, please kindly consider giving us a **star** on\n  GitHub^_^. Thank you very much!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12094v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12094v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20887v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20887v1",
                "updated": "2024-12-30T11:54:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    11,
                    54,
                    19,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T11:54:19Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    11,
                    54,
                    19,
                    0,
                    365,
                    0
                ],
                "title": "A Hidden Quantum Paraelectric Phase in SrTiO3 Induced by Terahertz Field",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Hidden Quantum Paraelectric Phase in SrTiO3 Induced by Terahertz Field"
                },
                "summary": "Coherent manipulation of lattice vibrations using ultrafast light pulses\nenables access to nonequilibrium 'hidden' phases with designed functionalities\nin quantum materials. However, expanding the understanding of nonlinear\nlight-phonon interaction mechanisms remains crucial for developing new\nstrategies. Here, we report re-entrant ultrafast phase transitions in SrTiO3\ndriven by intense terahertz excitation. As the terahertz field increases, the\nsystem transitions from the quantum paraelectric (QPE) ground state to an\nintermediate ferroelectric phase, and then unexpectedly reverts to a QPE state\nabove ~500 kV/cm. The latter hidden QPE phase exhibits distinct lattice\ndynamics compared to the initial phases, highlighting activated\nantiferrodistortive phonon modes. Aided by first-principles dynamical\ncalculations, we identify the mechanism for these complex behaviors as a\nsuperposition of multiple coherently excited eigenstates of the polar soft\nmode. Our results reveal a previously uncharted quantum facet of SrTiO3 and\nopen pathways for harnessing high-order excitations to engineer quantum\nmaterials in the ultrafast regime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coherent manipulation of lattice vibrations using ultrafast light pulses\nenables access to nonequilibrium 'hidden' phases with designed functionalities\nin quantum materials. However, expanding the understanding of nonlinear\nlight-phonon interaction mechanisms remains crucial for developing new\nstrategies. Here, we report re-entrant ultrafast phase transitions in SrTiO3\ndriven by intense terahertz excitation. As the terahertz field increases, the\nsystem transitions from the quantum paraelectric (QPE) ground state to an\nintermediate ferroelectric phase, and then unexpectedly reverts to a QPE state\nabove ~500 kV/cm. The latter hidden QPE phase exhibits distinct lattice\ndynamics compared to the initial phases, highlighting activated\nantiferrodistortive phonon modes. Aided by first-principles dynamical\ncalculations, we identify the mechanism for these complex behaviors as a\nsuperposition of multiple coherently excited eigenstates of the polar soft\nmode. Our results reveal a previously uncharted quantum facet of SrTiO3 and\nopen pathways for harnessing high-order excitations to engineer quantum\nmaterials in the ultrafast regime."
                },
                "authors": [
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Hanbyul Kim"
                    },
                    {
                        "name": "Xinbo Wang"
                    },
                    {
                        "name": "Jianlin Luo"
                    },
                    {
                        "name": "Simone Latini"
                    },
                    {
                        "name": "Dongbin Shin"
                    },
                    {
                        "name": "Jun-Ming Liu"
                    },
                    {
                        "name": "Jing-Feng Li"
                    },
                    {
                        "name": "Angel Rubio"
                    },
                    {
                        "name": "Ce-Wen Nan"
                    },
                    {
                        "name": "Qian Li"
                    }
                ],
                "author_detail": {
                    "name": "Qian Li"
                },
                "author": "Qian Li",
                "arxiv_comment": "18 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20887v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20887v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08545v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08545v2",
                "updated": "2024-12-30T05:01:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    5,
                    1,
                    44,
                    0,
                    365,
                    0
                ],
                "published": "2024-08-16T06:11:21Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "title": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have seen widespread adoption due to their\nremarkable performance across various applications, driving the accelerated\ndevelopment of a large number of diverse LLMs. However, these individual LLMs\nshow limitations in generalization and performance on complex tasks due to\ninherent training biases, model size constraints, and the quality or diversity\nof pre-training datasets. A promising direction is to efficiently harness the\ndiverse capabilities of LLMs to overcome these individual limitations. To\naddress these limitations, we introduce a novel LLM selection algorithm called\nSelectLLM, which efficiently directs input queries to the most suitable subset\nof LLMs from a large pool, ensuring that the selected models collectively\nprovide accurate responses. SelectLLM employs a multi-label classifier and\npolicy based on the classifier's predictions and confidence scores in selecting\nan optimal, query-aware, and lightweight subset of LLMs. Our findings indicate\nthat the proposed model outperforms existing ensemble-based baselines and\nachieves competitive performance with similarly sized top-performing LLMs while\nmaintaining efficiency. Specifically, it achieves a huge reduction in inference\nlatency on two challenging reasoning benchmarks: 13% on GSM8K and 70% on MMLU,\ncompared to the top-performing baselines. Also, we establish a theoretical\nupper bound by an oracle with LLMs and explore in-depth linguistic analysis to\nunderstand the performance gap between Oracle and SelectLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have seen widespread adoption due to their\nremarkable performance across various applications, driving the accelerated\ndevelopment of a large number of diverse LLMs. However, these individual LLMs\nshow limitations in generalization and performance on complex tasks due to\ninherent training biases, model size constraints, and the quality or diversity\nof pre-training datasets. A promising direction is to efficiently harness the\ndiverse capabilities of LLMs to overcome these individual limitations. To\naddress these limitations, we introduce a novel LLM selection algorithm called\nSelectLLM, which efficiently directs input queries to the most suitable subset\nof LLMs from a large pool, ensuring that the selected models collectively\nprovide accurate responses. SelectLLM employs a multi-label classifier and\npolicy based on the classifier's predictions and confidence scores in selecting\nan optimal, query-aware, and lightweight subset of LLMs. Our findings indicate\nthat the proposed model outperforms existing ensemble-based baselines and\nachieves competitive performance with similarly sized top-performing LLMs while\nmaintaining efficiency. Specifically, it achieves a huge reduction in inference\nlatency on two challenging reasoning benchmarks: 13% on GSM8K and 70% on MMLU,\ncompared to the top-performing baselines. Also, we establish a theoretical\nupper bound by an oracle with LLMs and explore in-depth linguistic analysis to\nunderstand the performance gap between Oracle and SelectLLM."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08545v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08545v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20677v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20677v1",
                "updated": "2024-12-30T03:05:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    3,
                    5,
                    45,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T03:05:45Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    3,
                    5,
                    45,
                    0,
                    365,
                    0
                ],
                "title": "Align Attention Heads Before Merging Them: An Effective Way for\n  Converting MHA to GQA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Align Attention Heads Before Merging Them: An Effective Way for\n  Converting MHA to GQA"
                },
                "summary": "Large language models have been shown to perform well on a variety of natural\nlanguage processing problems. However, as the model size and the input\nsequence's length increase, the rapid increase of KV Cache significantly slows\ndown inference speed. Therefore GQA model, as an alternative to MHA model, has\nbeen widely introduced into LLMs. In this work, we propose a low-cost method\nfor pruning MHA models into GQA models with any compression ratio of key-value\nheads. Our method is based on $\\mathit{L_0}$ masks to gradually remove\nredundant parameters. In addition, we apply orthogonal transformations to\nattention heads without changing the model to increase similarity between\nattention heads before pruning training, in order to further improve\nperformance of the model. Our method can be compatible with rotary position\nembedding (RoPE), which means the model after training can be fully adapted to\nthe mainstream standard GQA framework. Experiments demonstrate that our\nstrategy can compress up to 87.5% of key-value heads of the LLaMA2-7B model\nwithout too much performance degradation, just achieved through supervised\nfine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have been shown to perform well on a variety of natural\nlanguage processing problems. However, as the model size and the input\nsequence's length increase, the rapid increase of KV Cache significantly slows\ndown inference speed. Therefore GQA model, as an alternative to MHA model, has\nbeen widely introduced into LLMs. In this work, we propose a low-cost method\nfor pruning MHA models into GQA models with any compression ratio of key-value\nheads. Our method is based on $\\mathit{L_0}$ masks to gradually remove\nredundant parameters. In addition, we apply orthogonal transformations to\nattention heads without changing the model to increase similarity between\nattention heads before pruning training, in order to further improve\nperformance of the model. Our method can be compatible with rotary position\nembedding (RoPE), which means the model after training can be fully adapted to\nthe mainstream standard GQA framework. Experiments demonstrate that our\nstrategy can compress up to 87.5% of key-value heads of the LLaMA2-7B model\nwithout too much performance degradation, just achieved through supervised\nfine-tuning."
                },
                "authors": [
                    {
                        "name": "Qingyun Jin"
                    },
                    {
                        "name": "Xiaohui Song"
                    },
                    {
                        "name": "Feng Zhou"
                    },
                    {
                        "name": "Zengchang Qin"
                    }
                ],
                "author_detail": {
                    "name": "Zengchang Qin"
                },
                "author": "Zengchang Qin",
                "arxiv_comment": "12 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20677v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20677v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00068v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00068v1",
                "updated": "2024-12-29T17:41:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    41,
                    40,
                    6,
                    364,
                    0
                ],
                "published": "2024-12-29T17:41:40Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    41,
                    40,
                    6,
                    364,
                    0
                ],
                "title": "Dynamic Optimization of Storage Systems Using Reinforcement Learning\n  Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Optimization of Storage Systems Using Reinforcement Learning\n  Techniques"
                },
                "summary": "The exponential growth of data-intensive applications has placed\nunprecedented demands on modern storage systems, necessitating dynamic and\nefficient optimization strategies. Traditional heuristics employed for storage\nperformance optimization often fail to adapt to the variability and complexity\nof contemporary workloads, leading to significant performance bottlenecks and\nresource inefficiencies. To address these challenges, this paper introduces\nRL-Storage, a novel reinforcement learning (RL)-based framework designed to\ndynamically optimize storage system configurations. RL-Storage leverages deep\nQ-learning algorithms to continuously learn from real-time I/O patterns and\npredict optimal storage parameters, such as cache size, queue depths, and\nreadahead settings[1]. The proposed framework operates within the storage\nkernel, ensuring minimal latency and low computational overhead. Through an\nadaptive feedback mechanism, RL-Storage dynamically adjusts critical\nparameters, achieving efficient resource utilization across a wide range of\nworkloads. Experimental evaluations conducted on a range of benchmarks,\nincluding RocksDB and PostgreSQL, demonstrate significant improvements, with\nthroughput gains of up to 2.6x and latency reductions of 43% compared to\nbaseline heuristics. Additionally, RL-Storage achieves these performance\nenhancements with a negligible CPU overhead of 0.11% and a memory footprint of\nonly 5 KB, making it suitable for seamless deployment in production\nenvironments. This work underscores the transformative potential of\nreinforcement learning techniques in addressing the dynamic nature of modern\nstorage systems. By autonomously adapting to workload variations in real time,\nRL-Storage provides a robust and scalable solution for optimizing storage\nperformance, paving the way for next-generation intelligent storage\ninfrastructures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The exponential growth of data-intensive applications has placed\nunprecedented demands on modern storage systems, necessitating dynamic and\nefficient optimization strategies. Traditional heuristics employed for storage\nperformance optimization often fail to adapt to the variability and complexity\nof contemporary workloads, leading to significant performance bottlenecks and\nresource inefficiencies. To address these challenges, this paper introduces\nRL-Storage, a novel reinforcement learning (RL)-based framework designed to\ndynamically optimize storage system configurations. RL-Storage leverages deep\nQ-learning algorithms to continuously learn from real-time I/O patterns and\npredict optimal storage parameters, such as cache size, queue depths, and\nreadahead settings[1]. The proposed framework operates within the storage\nkernel, ensuring minimal latency and low computational overhead. Through an\nadaptive feedback mechanism, RL-Storage dynamically adjusts critical\nparameters, achieving efficient resource utilization across a wide range of\nworkloads. Experimental evaluations conducted on a range of benchmarks,\nincluding RocksDB and PostgreSQL, demonstrate significant improvements, with\nthroughput gains of up to 2.6x and latency reductions of 43% compared to\nbaseline heuristics. Additionally, RL-Storage achieves these performance\nenhancements with a negligible CPU overhead of 0.11% and a memory footprint of\nonly 5 KB, making it suitable for seamless deployment in production\nenvironments. This work underscores the transformative potential of\nreinforcement learning techniques in addressing the dynamic nature of modern\nstorage systems. By autonomously adapting to workload variations in real time,\nRL-Storage provides a robust and scalable solution for optimizing storage\nperformance, paving the way for next-generation intelligent storage\ninfrastructures."
                },
                "authors": [
                    {
                        "name": "Chiyu Cheng"
                    },
                    {
                        "name": "Chang Zhou"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Jin Cao"
                    }
                ],
                "author_detail": {
                    "name": "Jin Cao"
                },
                "author": "Jin Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00068v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00068v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20524v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20524v1",
                "updated": "2024-12-29T17:18:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    18,
                    21,
                    6,
                    364,
                    0
                ],
                "published": "2024-12-29T17:18:21Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    18,
                    21,
                    6,
                    364,
                    0
                ],
                "title": "Ns3 meets Sionna: Using Realistic Channels in Network Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ns3 meets Sionna: Using Realistic Channels in Network Simulation"
                },
                "summary": "Network simulators are indispensable tools for the advancement of wireless\nnetwork technologies, offering a cost-effective and controlled environment to\nsimulate real-world network behavior. However, traditional simulators, such as\nthe widely used ns-3, exhibit limitations in accurately modeling indoor and\noutdoor scenarios due to their reliance on simplified statistical and\nstochastic channel propagation models, which often fail to accurately capture\nphysical phenomena like multipath signal propagation and shadowing by obstacles\nin the line-of-sight path. We present Ns3Sionna, which integrates a ray\ntracing-based channel model, implemented using the Sionna RT framework, within\nthe ns-3 network simulator. It allows to simulate environment-specific and\nphysically accurate channel realizations for a given 3D scene and wireless\ndevice positions. Additionally, a mobility model based on ray tracing was\ndeveloped to accurately represent device movements within the simulated 3D\nspace. Ns3Sionna provides more realistic path and delay loss estimates for both\nindoor and outdoor environments than existing ns-3 propagation models,\nparticularly in terms of spatial and temporal correlation. Moreover,\nfine-grained channel state information is provided, which could be used for the\ndevelopment of sensing applications. Due to the significant computational\ndemands of ray tracing, Ns3Sionna takes advantage of the parallel execution\ncapabilities of modern GPUs and multi-core CPUs by incorporating intelligent\npre-caching mechanisms that leverage the channel's coherence time to optimize\nruntime performance. This enables the efficient simulation of scenarios with a\nsmall to medium number of mobile nodes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Network simulators are indispensable tools for the advancement of wireless\nnetwork technologies, offering a cost-effective and controlled environment to\nsimulate real-world network behavior. However, traditional simulators, such as\nthe widely used ns-3, exhibit limitations in accurately modeling indoor and\noutdoor scenarios due to their reliance on simplified statistical and\nstochastic channel propagation models, which often fail to accurately capture\nphysical phenomena like multipath signal propagation and shadowing by obstacles\nin the line-of-sight path. We present Ns3Sionna, which integrates a ray\ntracing-based channel model, implemented using the Sionna RT framework, within\nthe ns-3 network simulator. It allows to simulate environment-specific and\nphysically accurate channel realizations for a given 3D scene and wireless\ndevice positions. Additionally, a mobility model based on ray tracing was\ndeveloped to accurately represent device movements within the simulated 3D\nspace. Ns3Sionna provides more realistic path and delay loss estimates for both\nindoor and outdoor environments than existing ns-3 propagation models,\nparticularly in terms of spatial and temporal correlation. Moreover,\nfine-grained channel state information is provided, which could be used for the\ndevelopment of sensing applications. Due to the significant computational\ndemands of ray tracing, Ns3Sionna takes advantage of the parallel execution\ncapabilities of modern GPUs and multi-core CPUs by incorporating intelligent\npre-caching mechanisms that leverage the channel's coherence time to optimize\nruntime performance. This enables the efficient simulation of scenarios with a\nsmall to medium number of mobile nodes."
                },
                "authors": [
                    {
                        "name": "Anatolij Zubow"
                    },
                    {
                        "name": "Yannik Pilz"
                    },
                    {
                        "name": "Sascha Rösler"
                    },
                    {
                        "name": "Falko Dressler"
                    }
                ],
                "author_detail": {
                    "name": "Falko Dressler"
                },
                "author": "Falko Dressler",
                "arxiv_comment": "9 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20524v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20524v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20221v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20221v1",
                "updated": "2024-12-28T17:17:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    28,
                    17,
                    17,
                    3,
                    5,
                    363,
                    0
                ],
                "published": "2024-12-28T17:17:03Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    17,
                    17,
                    3,
                    5,
                    363,
                    0
                ],
                "title": "Revisiting Cache Freshness for Emerging Real-Time Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting Cache Freshness for Emerging Real-Time Applications"
                },
                "summary": "Caching is widely used in industry to improve application performance by\nreducing data-access latency and taking the load off the backend\ninfrastructure. TTLs have become the de-facto mechanism used to keep cached\ndata reasonably fresh (i.e., not too out of date with the backend). However,\nthe emergence of real-time applications requires tighter data freshness, which\nis impractical to achieve with TTLs. We discuss why this is the case, and\npropose a simple yet effective adaptive policy to achieve the desired\nfreshness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching is widely used in industry to improve application performance by\nreducing data-access latency and taking the load off the backend\ninfrastructure. TTLs have become the de-facto mechanism used to keep cached\ndata reasonably fresh (i.e., not too out of date with the backend). However,\nthe emergence of real-time applications requires tighter data freshness, which\nis impractical to achieve with TTLs. We discuss why this is the case, and\npropose a simple yet effective adaptive policy to achieve the desired\nfreshness."
                },
                "authors": [
                    {
                        "name": "Ziming Mao"
                    },
                    {
                        "name": "Rishabh Iyer"
                    },
                    {
                        "name": "Scott Shenker"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "arxiv_doi": "10.1145/3696348.3696858",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3696348.3696858",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.20221v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20221v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "HotNets '24",
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20105v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20105v1",
                "updated": "2024-12-28T10:17:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    28,
                    10,
                    17,
                    29,
                    5,
                    363,
                    0
                ],
                "published": "2024-12-28T10:17:29Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    10,
                    17,
                    29,
                    5,
                    363,
                    0
                ],
                "title": "ST$^3$: Accelerating Multimodal Large Language Model by Spatial-Temporal\n  Visual Token Trimming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ST$^3$: Accelerating Multimodal Large Language Model by Spatial-Temporal\n  Visual Token Trimming"
                },
                "summary": "Multimodal large language models (MLLMs) enhance their perceptual\ncapabilities by integrating visual and textual information. However, processing\nthe massive number of visual tokens incurs a significant computational cost.\nExisting analysis of the MLLM attention mechanisms remains shallow, leading to\ncoarse-grain token pruning strategies that fail to effectively balance speed\nand accuracy. In this paper, we conduct a comprehensive investigation of MLLM\nattention mechanisms with LLaVA. We find that numerous visual tokens and\npartial attention computations are redundant during the decoding process. Based\non this insight, we propose Spatial-Temporal Visual Token Trimming\n($\\textbf{ST}^{3}$), a framework designed to accelerate MLLM inference without\nretraining. $\\textbf{ST}^{3}$ consists of two primary components: 1)\nProgressive Visual Token Pruning (\\textbf{PVTP}), which eliminates inattentive\nvisual tokens across layers, and 2) Visual Token Annealing (\\textbf{VTA}),\nwhich dynamically reduces the number of visual tokens in each layer as the\ngenerated tokens grow. Together, these techniques deliver around\n$\\mathbf{2\\times}$ faster inference with only about $\\mathbf{30\\%}$ KV cache\nmemory compared to the original LLaVA, while maintaining consistent performance\nacross various datasets. Crucially, $\\textbf{ST}^{3}$ can be seamlessly\nintegrated into existing pre-trained MLLMs, providing a plug-and-play solution\nfor efficient inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) enhance their perceptual\ncapabilities by integrating visual and textual information. However, processing\nthe massive number of visual tokens incurs a significant computational cost.\nExisting analysis of the MLLM attention mechanisms remains shallow, leading to\ncoarse-grain token pruning strategies that fail to effectively balance speed\nand accuracy. In this paper, we conduct a comprehensive investigation of MLLM\nattention mechanisms with LLaVA. We find that numerous visual tokens and\npartial attention computations are redundant during the decoding process. Based\non this insight, we propose Spatial-Temporal Visual Token Trimming\n($\\textbf{ST}^{3}$), a framework designed to accelerate MLLM inference without\nretraining. $\\textbf{ST}^{3}$ consists of two primary components: 1)\nProgressive Visual Token Pruning (\\textbf{PVTP}), which eliminates inattentive\nvisual tokens across layers, and 2) Visual Token Annealing (\\textbf{VTA}),\nwhich dynamically reduces the number of visual tokens in each layer as the\ngenerated tokens grow. Together, these techniques deliver around\n$\\mathbf{2\\times}$ faster inference with only about $\\mathbf{30\\%}$ KV cache\nmemory compared to the original LLaVA, while maintaining consistent performance\nacross various datasets. Crucially, $\\textbf{ST}^{3}$ can be seamlessly\nintegrated into existing pre-trained MLLMs, providing a plug-and-play solution\nfor efficient inference."
                },
                "authors": [
                    {
                        "name": "Jiedong Zhuang"
                    },
                    {
                        "name": "Lu Lu"
                    },
                    {
                        "name": "Ming Dai"
                    },
                    {
                        "name": "Rui Hu"
                    },
                    {
                        "name": "Jian Chen"
                    },
                    {
                        "name": "Qiang Liu"
                    },
                    {
                        "name": "Haoji Hu"
                    }
                ],
                "author_detail": {
                    "name": "Haoji Hu"
                },
                "author": "Haoji Hu",
                "arxiv_comment": "Accepted to AAAI2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20105v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20105v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19991v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19991v1",
                "updated": "2024-12-28T03:28:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    28,
                    3,
                    28,
                    52,
                    5,
                    363,
                    0
                ],
                "published": "2024-12-28T03:28:52Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    3,
                    28,
                    52,
                    5,
                    363,
                    0
                ],
                "title": "A Robust Federated Learning Framework for Undependable Devices at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Robust Federated Learning Framework for Undependable Devices at Scale"
                },
                "summary": "In a federated learning (FL) system, many devices, such as smartphones, are\noften undependable (e.g., frequently disconnected from WiFi) during training.\nExisting FL frameworks always assume a dependable environment and exclude\nundependable devices from training, leading to poor model performance and\nresource wastage. In this paper, we propose FLUDE to effectively deal with\nundependable environments. First, FLUDE assesses the dependability of devices\nbased on the probability distribution of their historical behaviors (e.g., the\nlikelihood of successfully completing training). Based on this assessment,\nFLUDE adaptively selects devices with high dependability for training. To\nmitigate resource wastage during the training phase, FLUDE maintains a model\ncache on each device, aiming to preserve the latest training state for later\nuse in case local training on an undependable device is interrupted. Moreover,\nFLUDE proposes a staleness-aware strategy to judiciously distribute the global\nmodel to a subset of devices, thus significantly reducing resource wastage\nwhile maintaining model performance. We have implemented FLUDE on two physical\nplatforms with 120 smartphones and NVIDIA Jetson devices. Extensive\nexperimental results demonstrate that FLUDE can effectively improve model\nperformance and resource efficiency of FL training in undependable\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In a federated learning (FL) system, many devices, such as smartphones, are\noften undependable (e.g., frequently disconnected from WiFi) during training.\nExisting FL frameworks always assume a dependable environment and exclude\nundependable devices from training, leading to poor model performance and\nresource wastage. In this paper, we propose FLUDE to effectively deal with\nundependable environments. First, FLUDE assesses the dependability of devices\nbased on the probability distribution of their historical behaviors (e.g., the\nlikelihood of successfully completing training). Based on this assessment,\nFLUDE adaptively selects devices with high dependability for training. To\nmitigate resource wastage during the training phase, FLUDE maintains a model\ncache on each device, aiming to preserve the latest training state for later\nuse in case local training on an undependable device is interrupted. Moreover,\nFLUDE proposes a staleness-aware strategy to judiciously distribute the global\nmodel to a subset of devices, thus significantly reducing resource wastage\nwhile maintaining model performance. We have implemented FLUDE on two physical\nplatforms with 120 smartphones and NVIDIA Jetson devices. Extensive\nexperimental results demonstrate that FLUDE can effectively improve model\nperformance and resource efficiency of FL training in undependable\nenvironments."
                },
                "authors": [
                    {
                        "name": "Shilong Wang"
                    },
                    {
                        "name": "Jianchun Liu"
                    },
                    {
                        "name": "Hongli Xu"
                    },
                    {
                        "name": "Chunming Qiao"
                    },
                    {
                        "name": "Huarong Deng"
                    },
                    {
                        "name": "Qiuye Zheng"
                    },
                    {
                        "name": "Jiantao Gong"
                    }
                ],
                "author_detail": {
                    "name": "Jiantao Gong"
                },
                "author": "Jiantao Gong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19991v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19991v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19051v1",
                "updated": "2024-12-26T04:13:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    4,
                    13,
                    52,
                    3,
                    361,
                    0
                ],
                "published": "2024-12-26T04:13:52Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    4,
                    13,
                    52,
                    3,
                    361,
                    0
                ],
                "title": "Performance Characterization and Optimizations of Traditional ML\n  Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Characterization and Optimizations of Traditional ML\n  Applications"
                },
                "summary": "Even in the era of Deep Learning based methods, traditional machine learning\nmethods with large data sets continue to attract significant attention.\nHowever, we find an apparent lack of a detailed performance characterization of\nthese methods in the context of large training datasets. In this work, we study\nthe system's behavior of a number of traditional ML methods as implemented in\npopular free software libraries/modules to identify critical performance\nbottlenecks experienced by these applications. The performance characterization\nstudy reveals several interesting insights on the performance of these\napplications. Then we evaluate the performance benefits of applying some\nwell-known optimizations at the levels of caches and the main memory. More\nspecifically, we test the usefulness of optimizations such as (i) software\nprefetching to improve cache performance and (ii) data layout and computation\nreordering optimizations to improve locality in DRAM accesses. These\noptimizations are implemented as modifications to the well-known scikit-learn\nlibrary, and hence can be easily leveraged by application programmers. We\nevaluate the impact of the proposed optimizations using a combination of\nsimulation and execution on a real system. The software prefetching\noptimization results in performance benefits varying from 5.2%-27.1% on\ndifferent ML applications while the data layout and computation reordering\napproaches yield 6.16%-28.0% performance improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Even in the era of Deep Learning based methods, traditional machine learning\nmethods with large data sets continue to attract significant attention.\nHowever, we find an apparent lack of a detailed performance characterization of\nthese methods in the context of large training datasets. In this work, we study\nthe system's behavior of a number of traditional ML methods as implemented in\npopular free software libraries/modules to identify critical performance\nbottlenecks experienced by these applications. The performance characterization\nstudy reveals several interesting insights on the performance of these\napplications. Then we evaluate the performance benefits of applying some\nwell-known optimizations at the levels of caches and the main memory. More\nspecifically, we test the usefulness of optimizations such as (i) software\nprefetching to improve cache performance and (ii) data layout and computation\nreordering optimizations to improve locality in DRAM accesses. These\noptimizations are implemented as modifications to the well-known scikit-learn\nlibrary, and hence can be easily leveraged by application programmers. We\nevaluate the impact of the proposed optimizations using a combination of\nsimulation and execution on a real system. The software prefetching\noptimization results in performance benefits varying from 5.2%-27.1% on\ndifferent ML applications while the data layout and computation reordering\napproaches yield 6.16%-28.0% performance improvement."
                },
                "authors": [
                    {
                        "name": "Harsh Kumar"
                    },
                    {
                        "name": "R. Govindarajan"
                    }
                ],
                "author_detail": {
                    "name": "R. Govindarajan"
                },
                "author": "R. Govindarajan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18960v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18960v1",
                "updated": "2024-12-25T18:36:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    25,
                    18,
                    36,
                    21,
                    2,
                    360,
                    0
                ],
                "published": "2024-12-25T18:36:21Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    18,
                    36,
                    21,
                    2,
                    360,
                    0
                ],
                "title": "XRFlux: Virtual Reality Benchmark for Edge Caching Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XRFlux: Virtual Reality Benchmark for Edge Caching Systems"
                },
                "summary": "We introduce a Unity based benchmark XRFlux for evaluating Virtual Reality\n(VR) delivery systems using edge-cloud caching. As VR applications and systems\nprogress, the need to meet strict latency and Quality of Experience (QoE)\nrequirements is increasingly evident. In the context of VR, traditional cloud\narchitectures (e.g., remote AWS S3 for content delivery) often struggle to meet\nthese demands, especially for users of the same application in different\nlocations. With edge computing, resources are brought closer to users in\nefforts to reduce latency and improve QoEs. However, VR's dynamic nature, with\nchanging fields of view (FoVs) and user synchronization requirements, creates\nvarious challenges for edge caching. We address the lack of suitable benchmarks\nand propose a framework that simulates multiuser VR scenarios while logging\nusers' interaction with objects within their actual and predicted FoVs. The\nbenchmark's activity log can then be played back through an edge cache to\nassess the resulting QoEs. This tool fills a gap by supporting research in the\noptimization of edge caching (and other edge-cloud functions) for VR streaming.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a Unity based benchmark XRFlux for evaluating Virtual Reality\n(VR) delivery systems using edge-cloud caching. As VR applications and systems\nprogress, the need to meet strict latency and Quality of Experience (QoE)\nrequirements is increasingly evident. In the context of VR, traditional cloud\narchitectures (e.g., remote AWS S3 for content delivery) often struggle to meet\nthese demands, especially for users of the same application in different\nlocations. With edge computing, resources are brought closer to users in\nefforts to reduce latency and improve QoEs. However, VR's dynamic nature, with\nchanging fields of view (FoVs) and user synchronization requirements, creates\nvarious challenges for edge caching. We address the lack of suitable benchmarks\nand propose a framework that simulates multiuser VR scenarios while logging\nusers' interaction with objects within their actual and predicted FoVs. The\nbenchmark's activity log can then be played back through an edge cache to\nassess the resulting QoEs. This tool fills a gap by supporting research in the\noptimization of edge caching (and other edge-cloud functions) for VR streaming."
                },
                "authors": [
                    {
                        "name": "Nader Alfares"
                    },
                    {
                        "name": "George Kesidis"
                    }
                ],
                "author_detail": {
                    "name": "George Kesidis"
                },
                "author": "George Kesidis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18960v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18960v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18914v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18914v1",
                "updated": "2024-12-25T14:14:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    14,
                    31,
                    2,
                    360,
                    0
                ],
                "published": "2024-12-25T14:14:31Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    14,
                    31,
                    2,
                    360,
                    0
                ],
                "title": "Long-Range Tasks Using Short-Context LLMs: Incremental Reasoning With\n  Structured Memories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-Range Tasks Using Short-Context LLMs: Incremental Reasoning With\n  Structured Memories"
                },
                "summary": "Long-range tasks require reasoning over long inputs. Existing solutions\neither need large compute budgets, training data, access to model weights, or\nuse complex, task-specific approaches. We present PRISM, which alleviates these\nconcerns by processing information as a stream of chunks, maintaining a\nstructured in-context memory specified by a typed hierarchy schema. This\napproach demonstrates superior performance to baselines on diverse tasks while\nusing at least 4x smaller contexts than long-context models. Moreover, PRISM is\ntoken-efficient. By producing short outputs and efficiently leveraging\nkey-value (KV) caches, it achieves up to 54% cost reduction when compared to\nalternative short-context approaches. The method also scales down to tiny\ninformation chunks (e.g., 500 tokens) without increasing the number of tokens\nencoded or sacrificing quality. Furthermore, we show that it is possible to\ngenerate schemas to generalize our approach to new tasks with minimal effort.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-range tasks require reasoning over long inputs. Existing solutions\neither need large compute budgets, training data, access to model weights, or\nuse complex, task-specific approaches. We present PRISM, which alleviates these\nconcerns by processing information as a stream of chunks, maintaining a\nstructured in-context memory specified by a typed hierarchy schema. This\napproach demonstrates superior performance to baselines on diverse tasks while\nusing at least 4x smaller contexts than long-context models. Moreover, PRISM is\ntoken-efficient. By producing short outputs and efficiently leveraging\nkey-value (KV) caches, it achieves up to 54% cost reduction when compared to\nalternative short-context approaches. The method also scales down to tiny\ninformation chunks (e.g., 500 tokens) without increasing the number of tokens\nencoded or sacrificing quality. Furthermore, we show that it is possible to\ngenerate schemas to generalize our approach to new tasks with minimal effort."
                },
                "authors": [
                    {
                        "name": "Dulhan Jayalath"
                    },
                    {
                        "name": "James Bradley Wendt"
                    },
                    {
                        "name": "Nicholas Monath"
                    },
                    {
                        "name": "Sandeep Tata"
                    },
                    {
                        "name": "Beliz Gunel"
                    }
                ],
                "author_detail": {
                    "name": "Beliz Gunel"
                },
                "author": "Beliz Gunel",
                "arxiv_comment": "23 pages, 7 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18914v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18914v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18911v1",
                "updated": "2024-12-25T14:00:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    0,
                    14,
                    2,
                    360,
                    0
                ],
                "published": "2024-12-25T14:00:14Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    0,
                    14,
                    2,
                    360,
                    0
                ],
                "title": "Accelerating Diffusion Transformers with Dual Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformers with Dual Feature Caching"
                },
                "summary": "Diffusion Transformers (DiT) have become the dominant methods in image and\nvideo generation yet still suffer substantial computational costs. As an\neffective approach for DiT acceleration, feature caching methods are designed\nto cache the features of DiT in previous timesteps and reuse them in the next\ntimesteps, allowing us to skip the computation in the next timesteps. However,\non the one hand, aggressively reusing all the features cached in previous\ntimesteps leads to a severe drop in generation quality. On the other hand,\nconservatively caching only the features in the redundant layers or tokens but\nstill computing the important ones successfully preserves the generation\nquality but results in reductions in acceleration ratios. Observing such a\ntradeoff between generation quality and acceleration performance, this paper\nbegins by quantitatively studying the accumulated error from cached features.\nSurprisingly, we find that aggressive caching does not introduce significantly\nmore caching errors in the caching step, and the conservative feature caching\ncan fix the error introduced by aggressive caching. Thereby, we propose a dual\ncaching strategy that adopts aggressive and conservative caching iteratively,\nleading to significant acceleration and high generation quality at the same\ntime. Besides, we further introduce a V-caching strategy for token-wise\nconservative caching, which is compatible with flash attention and requires no\ntraining and calibration data.\n  Our codes have been released in Github: \\textbf{Code:\n\\href{https://github.com/Shenyi-Z/DuCa}{\\texttt{\\textcolor{cyan}{https://github.com/Shenyi-Z/DuCa}}}}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) have become the dominant methods in image and\nvideo generation yet still suffer substantial computational costs. As an\neffective approach for DiT acceleration, feature caching methods are designed\nto cache the features of DiT in previous timesteps and reuse them in the next\ntimesteps, allowing us to skip the computation in the next timesteps. However,\non the one hand, aggressively reusing all the features cached in previous\ntimesteps leads to a severe drop in generation quality. On the other hand,\nconservatively caching only the features in the redundant layers or tokens but\nstill computing the important ones successfully preserves the generation\nquality but results in reductions in acceleration ratios. Observing such a\ntradeoff between generation quality and acceleration performance, this paper\nbegins by quantitatively studying the accumulated error from cached features.\nSurprisingly, we find that aggressive caching does not introduce significantly\nmore caching errors in the caching step, and the conservative feature caching\ncan fix the error introduced by aggressive caching. Thereby, we propose a dual\ncaching strategy that adopts aggressive and conservative caching iteratively,\nleading to significant acceleration and high generation quality at the same\ntime. Besides, we further introduce a V-caching strategy for token-wise\nconservative caching, which is compatible with flash attention and requires no\ntraining and calibration data.\n  Our codes have been released in Github: \\textbf{Code:\n\\href{https://github.com/Shenyi-Z/DuCa}{\\texttt{\\textcolor{cyan}{https://github.com/Shenyi-Z/DuCa}}}}"
                },
                "authors": [
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Evelyn Zhang"
                    },
                    {
                        "name": "Runlin Guo"
                    },
                    {
                        "name": "Haohang Xu"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18885v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18885v1",
                "updated": "2024-12-25T11:59:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    25,
                    11,
                    59,
                    17,
                    2,
                    360,
                    0
                ],
                "published": "2024-12-25T11:59:17Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    11,
                    59,
                    17,
                    2,
                    360,
                    0
                ],
                "title": "Aspect-oriented Programming with Julia",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aspect-oriented Programming with Julia"
                },
                "summary": "This paper proposes integrating Aspect-oriented Programming (AOP) into Julia,\na language widely used in scientific and High-Performance Computing (HPC). AOP\nenhances software modularity by encapsulating cross-cutting concerns, such as\nlogging, caching, and parallelizing, into separate, reusable aspects.\nLeveraging Julia's powerful metaprogramming and abstract syntax tree (AST)\nmanipulation capabilities, we introduce AspectJulia, an AOP framework designed\nto operate within Julia's runtime environment as a package. AspectJulia enables\ndevelopers to define and apply aspects seamlessly, leading to more modular,\nmaintainable, and adaptable code. We detail the implementation of AspectJulia\nand present diverse use cases, ranging from HPC and scientific computing to\nbusiness applications, demonstrating its effectiveness in managing\ncross-cutting concerns. This integration simplifies application development and\nimproves the adaptability of existing Julia modules and packages, paving the\nway for more efficient and maintainable software systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes integrating Aspect-oriented Programming (AOP) into Julia,\na language widely used in scientific and High-Performance Computing (HPC). AOP\nenhances software modularity by encapsulating cross-cutting concerns, such as\nlogging, caching, and parallelizing, into separate, reusable aspects.\nLeveraging Julia's powerful metaprogramming and abstract syntax tree (AST)\nmanipulation capabilities, we introduce AspectJulia, an AOP framework designed\nto operate within Julia's runtime environment as a package. AspectJulia enables\ndevelopers to define and apply aspects seamlessly, leading to more modular,\nmaintainable, and adaptable code. We detail the implementation of AspectJulia\nand present diverse use cases, ranging from HPC and scientific computing to\nbusiness applications, demonstrating its effectiveness in managing\ncross-cutting concerns. This integration simplifies application development and\nimproves the adaptability of existing Julia modules and packages, paving the\nway for more efficient and maintainable software systems."
                },
                "authors": [
                    {
                        "name": "Osamu Ishimura"
                    },
                    {
                        "name": "Yoshihide Yoshimoto"
                    }
                ],
                "author_detail": {
                    "name": "Yoshihide Yoshimoto"
                },
                "author": "Yoshihide Yoshimoto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18885v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18885v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2501.13107v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13107v1",
                "updated": "2025-01-22T18:59:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    18,
                    59,
                    58,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T18:59:58Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    18,
                    59,
                    58,
                    2,
                    22,
                    0
                ],
                "title": "Accelerate High-Quality Diffusion Models with Inner Loop Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerate High-Quality Diffusion Models with Inner Loop Feedback"
                },
                "summary": "We propose Inner Loop Feedback (ILF), a novel approach to accelerate\ndiffusion models' inference. ILF trains a lightweight module to predict future\nfeatures in the denoising process by leveraging the outputs from a chosen\ndiffusion backbone block at a given time step. This approach exploits two key\nintuitions; (1) the outputs of a given block at adjacent time steps are\nsimilar, and (2) performing partial computations for a step imposes a lower\nburden on the model than skipping the step entirely. Our method is highly\nflexible, since we find that the feedback module itself can simply be a block\nfrom the diffusion backbone, with all settings copied. Its influence on the\ndiffusion forward can be tempered with a learnable scaling factor from zero\ninitialization. We train this module using distillation losses; however, unlike\nsome prior work where a full diffusion backbone serves as the student, our\nmodel freezes the backbone, training only the feedback module. While many\nefforts to optimize diffusion models focus on achieving acceptable image\nquality in extremely few steps (1-4 steps), our emphasis is on matching best\ncase results (typically achieved in 20 steps) while significantly reducing\nruntime. ILF achieves this balance effectively, demonstrating strong\nperformance for both class-to-image generation with diffusion transformer (DiT)\nand text-to-image generation with DiT-based PixArt-alpha and PixArt-sigma. The\nquality of ILF's 1.7x-1.8x speedups are confirmed by FID, CLIP score, CLIP\nImage Quality Assessment, ImageReward, and qualitative comparisons.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Inner Loop Feedback (ILF), a novel approach to accelerate\ndiffusion models' inference. ILF trains a lightweight module to predict future\nfeatures in the denoising process by leveraging the outputs from a chosen\ndiffusion backbone block at a given time step. This approach exploits two key\nintuitions; (1) the outputs of a given block at adjacent time steps are\nsimilar, and (2) performing partial computations for a step imposes a lower\nburden on the model than skipping the step entirely. Our method is highly\nflexible, since we find that the feedback module itself can simply be a block\nfrom the diffusion backbone, with all settings copied. Its influence on the\ndiffusion forward can be tempered with a learnable scaling factor from zero\ninitialization. We train this module using distillation losses; however, unlike\nsome prior work where a full diffusion backbone serves as the student, our\nmodel freezes the backbone, training only the feedback module. While many\nefforts to optimize diffusion models focus on achieving acceptable image\nquality in extremely few steps (1-4 steps), our emphasis is on matching best\ncase results (typically achieved in 20 steps) while significantly reducing\nruntime. ILF achieves this balance effectively, demonstrating strong\nperformance for both class-to-image generation with diffusion transformer (DiT)\nand text-to-image generation with DiT-based PixArt-alpha and PixArt-sigma. The\nquality of ILF's 1.7x-1.8x speedups are confirmed by FID, CLIP score, CLIP\nImage Quality Assessment, ImageReward, and qualitative comparisons."
                },
                "authors": [
                    {
                        "name": "Matthew Gwilliam"
                    },
                    {
                        "name": "Han Cai"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Abhinav Shrivastava"
                    },
                    {
                        "name": "Zhiyu Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyu Cheng"
                },
                "author": "Zhiyu Cheng",
                "arxiv_comment": "submission currently under review; 20 pages, 17 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13107v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13107v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13106v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13106v1",
                "updated": "2025-01-22T18:59:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    18,
                    59,
                    46,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T18:59:46Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    18,
                    59,
                    46,
                    2,
                    22,
                    0
                ],
                "title": "VideoLLaMA 3: Frontier Multimodal Foundation Models for Image and Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VideoLLaMA 3: Frontier Multimodal Foundation Models for Image and Video\n  Understanding"
                },
                "summary": "In this paper, we propose VideoLLaMA3, a more advanced multimodal foundation\nmodel for image and video understanding. The core design philosophy of\nVideoLLaMA3 is vision-centric. The meaning of \"vision-centric\" is two-fold: the\nvision-centric training paradigm and vision-centric framework design. The key\ninsight of our vision-centric training paradigm is that high-quality image-text\ndata is crucial for both image and video understanding. Instead of preparing\nmassive video-text datasets, we focus on constructing large-scale and\nhigh-quality image-text datasets. VideoLLaMA3 has four training stages: 1)\nvision-centric alignment stage, which warms up the vision encoder and\nprojector; 2) vision-language pretraining stage, which jointly tunes the vision\nencoder, projector, and LLM with large-scale image-text data covering multiple\ntypes (including scene images, documents, charts) as well as text-only data. 3)\nmulti-task fine-tuning stage, which incorporates image-text SFT data for\ndownstream tasks and video-text data to establish a foundation for video\nunderstanding. 4) video-centric fine-tuning, which further improves the model's\ncapability in video understanding. As for the framework design, to better\ncapture fine-grained details in images, the pretrained vision encoder is\nadapted to encode images of varying sizes into vision tokens with corresponding\nnumbers, rather than a fixed number of tokens. For video inputs, we reduce the\nnumber of vision tokens according to their similarity so that the\nrepresentation of videos will be more precise and compact. Benefit from\nvision-centric designs, VideoLLaMA3 achieves compelling performances in both\nimage and video understanding benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose VideoLLaMA3, a more advanced multimodal foundation\nmodel for image and video understanding. The core design philosophy of\nVideoLLaMA3 is vision-centric. The meaning of \"vision-centric\" is two-fold: the\nvision-centric training paradigm and vision-centric framework design. The key\ninsight of our vision-centric training paradigm is that high-quality image-text\ndata is crucial for both image and video understanding. Instead of preparing\nmassive video-text datasets, we focus on constructing large-scale and\nhigh-quality image-text datasets. VideoLLaMA3 has four training stages: 1)\nvision-centric alignment stage, which warms up the vision encoder and\nprojector; 2) vision-language pretraining stage, which jointly tunes the vision\nencoder, projector, and LLM with large-scale image-text data covering multiple\ntypes (including scene images, documents, charts) as well as text-only data. 3)\nmulti-task fine-tuning stage, which incorporates image-text SFT data for\ndownstream tasks and video-text data to establish a foundation for video\nunderstanding. 4) video-centric fine-tuning, which further improves the model's\ncapability in video understanding. As for the framework design, to better\ncapture fine-grained details in images, the pretrained vision encoder is\nadapted to encode images of varying sizes into vision tokens with corresponding\nnumbers, rather than a fixed number of tokens. For video inputs, we reduce the\nnumber of vision tokens according to their similarity so that the\nrepresentation of videos will be more precise and compact. Benefit from\nvision-centric designs, VideoLLaMA3 achieves compelling performances in both\nimage and video understanding benchmarks."
                },
                "authors": [
                    {
                        "name": "Boqiang Zhang"
                    },
                    {
                        "name": "Kehan Li"
                    },
                    {
                        "name": "Zesen Cheng"
                    },
                    {
                        "name": "Zhiqiang Hu"
                    },
                    {
                        "name": "Yuqian Yuan"
                    },
                    {
                        "name": "Guanzheng Chen"
                    },
                    {
                        "name": "Sicong Leng"
                    },
                    {
                        "name": "Yuming Jiang"
                    },
                    {
                        "name": "Hang Zhang"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Peng Jin"
                    },
                    {
                        "name": "Wenqi Zhang"
                    },
                    {
                        "name": "Fan Wang"
                    },
                    {
                        "name": "Lidong Bing"
                    },
                    {
                        "name": "Deli Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Deli Zhao"
                },
                "author": "Deli Zhao",
                "arxiv_comment": "BZ, KL, ZC, ZH, YY, GC, SL, YJ, HZ, and XL contributed equally to\n  this project. Code: https://github.com/DAMO-NLP-SG/VideoLLaMA3",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13106v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13106v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13094v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13094v1",
                "updated": "2025-01-22T18:52:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    18,
                    52,
                    6,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T18:52:06Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    18,
                    52,
                    6,
                    2,
                    22,
                    0
                ],
                "title": "Robust Representation Consistency Model via Contrastive Denoising",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Representation Consistency Model via Contrastive Denoising"
                },
                "summary": "Robustness is essential for deep neural networks, especially in\nsecurity-sensitive applications. To this end, randomized smoothing provides\ntheoretical guarantees for certifying robustness against adversarial\nperturbations. Recently, diffusion models have been successfully employed for\nrandomized smoothing to purify noise-perturbed samples before making\npredictions with a standard classifier. While these methods excel at small\nperturbation radii, they struggle with larger perturbations and incur a\nsignificant computational overhead during inference compared to classical\nmethods. To address this, we reformulate the generative modeling task along the\ndiffusion trajectories in pixel space as a discriminative task in the latent\nspace. Specifically, we use instance discrimination to achieve consistent\nrepresentations along the trajectories by aligning temporally adjacent points.\nAfter fine-tuning based on the learned representations, our model enables\nimplicit denoising-then-classification via a single prediction, substantially\nreducing inference costs. We conduct extensive experiments on various datasets\nand achieve state-of-the-art performance with minimal computation budget during\ninference. For example, our method outperforms the certified accuracy of\ndiffusion-based methods on ImageNet across all perturbation radii by 5.3% on\naverage, with up to 11.6% at larger radii, while reducing inference costs by\n85$\\times$ on average. Codes are available at:\nhttps://github.com/jiachenlei/rRCM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robustness is essential for deep neural networks, especially in\nsecurity-sensitive applications. To this end, randomized smoothing provides\ntheoretical guarantees for certifying robustness against adversarial\nperturbations. Recently, diffusion models have been successfully employed for\nrandomized smoothing to purify noise-perturbed samples before making\npredictions with a standard classifier. While these methods excel at small\nperturbation radii, they struggle with larger perturbations and incur a\nsignificant computational overhead during inference compared to classical\nmethods. To address this, we reformulate the generative modeling task along the\ndiffusion trajectories in pixel space as a discriminative task in the latent\nspace. Specifically, we use instance discrimination to achieve consistent\nrepresentations along the trajectories by aligning temporally adjacent points.\nAfter fine-tuning based on the learned representations, our model enables\nimplicit denoising-then-classification via a single prediction, substantially\nreducing inference costs. We conduct extensive experiments on various datasets\nand achieve state-of-the-art performance with minimal computation budget during\ninference. For example, our method outperforms the certified accuracy of\ndiffusion-based methods on ImageNet across all perturbation radii by 5.3% on\naverage, with up to 11.6% at larger radii, while reducing inference costs by\n85$\\times$ on average. Codes are available at:\nhttps://github.com/jiachenlei/rRCM."
                },
                "authors": [
                    {
                        "name": "Jiachen Lei"
                    },
                    {
                        "name": "Julius Berner"
                    },
                    {
                        "name": "Jiongxiao Wang"
                    },
                    {
                        "name": "Zhongzhu Chen"
                    },
                    {
                        "name": "Zhongjia Ba"
                    },
                    {
                        "name": "Kui Ren"
                    },
                    {
                        "name": "Jun Zhu"
                    },
                    {
                        "name": "Anima Anandkumar"
                    }
                ],
                "author_detail": {
                    "name": "Anima Anandkumar"
                },
                "author": "Anima Anandkumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13094v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13094v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13084v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13084v1",
                "updated": "2025-01-22T18:45:29Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    18,
                    45,
                    29,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T18:45:29Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    18,
                    45,
                    29,
                    2,
                    22,
                    0
                ],
                "title": "Attention-Driven Hierarchical Reinforcement Learning with Particle\n  Filtering for Source Localization in Dynamic Fields",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention-Driven Hierarchical Reinforcement Learning with Particle\n  Filtering for Source Localization in Dynamic Fields"
                },
                "summary": "In many real-world scenarios, such as gas leak detection or environmental\npollutant tracking, solving the Inverse Source Localization and\nCharacterization problem involves navigating complex, dynamic fields with\nsparse and noisy observations. Traditional methods face significant challenges,\nincluding partial observability, temporal and spatial dynamics,\nout-of-distribution generalization, and reward sparsity. To address these\nissues, we propose a hierarchical framework that integrates Bayesian inference\nand reinforcement learning. The framework leverages an attention-enhanced\nparticle filtering mechanism for efficient and accurate belief updates, and\nincorporates two complementary execution strategies: Attention Particle\nFiltering Planning and Attention Particle Filtering Reinforcement Learning.\nThese approaches optimize exploration and adaptation under uncertainty.\nTheoretical analysis proves the convergence of the attention-enhanced particle\nfilter, while extensive experiments across diverse scenarios validate the\nframework's superior accuracy, adaptability, and computational efficiency. Our\nresults highlight the framework's potential for broad applications in dynamic\nfield estimation tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In many real-world scenarios, such as gas leak detection or environmental\npollutant tracking, solving the Inverse Source Localization and\nCharacterization problem involves navigating complex, dynamic fields with\nsparse and noisy observations. Traditional methods face significant challenges,\nincluding partial observability, temporal and spatial dynamics,\nout-of-distribution generalization, and reward sparsity. To address these\nissues, we propose a hierarchical framework that integrates Bayesian inference\nand reinforcement learning. The framework leverages an attention-enhanced\nparticle filtering mechanism for efficient and accurate belief updates, and\nincorporates two complementary execution strategies: Attention Particle\nFiltering Planning and Attention Particle Filtering Reinforcement Learning.\nThese approaches optimize exploration and adaptation under uncertainty.\nTheoretical analysis proves the convergence of the attention-enhanced particle\nfilter, while extensive experiments across diverse scenarios validate the\nframework's superior accuracy, adaptability, and computational efficiency. Our\nresults highlight the framework's potential for broad applications in dynamic\nfield estimation tasks."
                },
                "authors": [
                    {
                        "name": "Yiwei Shi"
                    },
                    {
                        "name": "Mengyue Yang"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Weinan Zhang"
                    },
                    {
                        "name": "Cunjia Liu"
                    },
                    {
                        "name": "Weiru Liu"
                    }
                ],
                "author_detail": {
                    "name": "Weiru Liu"
                },
                "author": "Weiru Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13084v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13084v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13083v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13083v1",
                "updated": "2025-01-22T18:45:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    18,
                    45,
                    15,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T18:45:15Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    18,
                    45,
                    15,
                    2,
                    22,
                    0
                ],
                "title": "Boosting MCTS with Free Energy Minimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting MCTS with Free Energy Minimization"
                },
                "summary": "Active Inference, grounded in the Free Energy Principle, provides a powerful\nlens for understanding how agents balance exploration and goal-directed\nbehavior in uncertain environments. Here, we propose a new planning framework,\nthat integrates Monte Carlo Tree Search (MCTS) with active inference objectives\nto systematically reduce epistemic uncertainty while pursuing extrinsic\nrewards. Our key insight is that MCTS already renowned for its search\nefficiency can be naturally extended to incorporate free energy minimization by\nblending expected rewards with information gain. Concretely, the Cross-Entropy\nMethod (CEM) is used to optimize action proposals at the root node, while tree\nexpansions leverage reward modeling alongside intrinsic exploration bonuses.\nThis synergy allows our planner to maintain coherent estimates of value and\nuncertainty throughout planning, without sacrificing computational\ntractability. Empirically, we benchmark our planner on a diverse set of\ncontinuous control tasks, where it demonstrates performance gains over both\nstandalone CEM and MCTS with random rollouts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Active Inference, grounded in the Free Energy Principle, provides a powerful\nlens for understanding how agents balance exploration and goal-directed\nbehavior in uncertain environments. Here, we propose a new planning framework,\nthat integrates Monte Carlo Tree Search (MCTS) with active inference objectives\nto systematically reduce epistemic uncertainty while pursuing extrinsic\nrewards. Our key insight is that MCTS already renowned for its search\nefficiency can be naturally extended to incorporate free energy minimization by\nblending expected rewards with information gain. Concretely, the Cross-Entropy\nMethod (CEM) is used to optimize action proposals at the root node, while tree\nexpansions leverage reward modeling alongside intrinsic exploration bonuses.\nThis synergy allows our planner to maintain coherent estimates of value and\nuncertainty throughout planning, without sacrificing computational\ntractability. Empirically, we benchmark our planner on a diverse set of\ncontinuous control tasks, where it demonstrates performance gains over both\nstandalone CEM and MCTS with random rollouts."
                },
                "authors": [
                    {
                        "name": "Mawaba Pascal Dao"
                    },
                    {
                        "name": "Adrian Peter"
                    }
                ],
                "author_detail": {
                    "name": "Adrian Peter"
                },
                "author": "Adrian Peter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13083v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13083v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13082v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13082v1",
                "updated": "2025-01-22T18:45:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    18,
                    45,
                    12,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T18:45:12Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    18,
                    45,
                    12,
                    2,
                    22,
                    0
                ],
                "title": "BlackTHUNDER -- A non-stellar Balmer break in a black hole-dominated\n  little red dot at $z=7.04$",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BlackTHUNDER -- A non-stellar Balmer break in a black hole-dominated\n  little red dot at $z=7.04$"
                },
                "summary": "Recent observations from JWST have revealed an abundant population of active\ngalactic nuclei (AGN) and so-called ``Little Red Dots'' (LRDs) at $2\\lesssim z\n\\lesssim 11$, many of which are characterized by V-shaped UV-to-optical\ncontinua with turnovers around the Balmer limit. The physical nature of these\nLRDs is unclear, and it remains debated whether the peculiar spectral shape\noriginates from AGN, compact galaxies, or both. We present the analysis of new\nNIRSpec-IFU data from the BlackTHUNDER JWST Large Programme and archival\nNIRSpec-MSA data of a lensed LRD at $z=7.04$. The spectra confirm the presence\nof a smooth Balmer break and a broad H$\\beta$ tracing the Broad Line Region\n(BLR) of an AGN. The small velocity dispersion of the H$\\beta$ narrow component\nindicates a small dynamical mass of the host galaxy of $M_{\\rm dyn}<4 \\times\n10^8~M_{\\odot}$, which implies that the stellar population cannot contribute\nmore than 10% to the optical continuum. We show that the Balmer break can be\nwell described by an AGN continuum absorbed by very dense ($n_{\\rm H}\\sim\n10^{10}~{\\rm cm^{-3}}$) and nearly dust-free gas along our line-of-sight\n(possibly gas in the BLR or its surrounding). The same gas is expected to\nproduce H$\\beta$ absorption, at a level consistent with a tentative detection\n($3\\sigma$) in the high-resolution spectrum. Such a non-stellar origin of the\nBalmer break may apply to other LRDs, and would alleviate the issue of\nextremely high stellar mass surface densities inferred in the case of a stellar\ninterpretation of the Balmer break. We note that this is a rare case of a black\nhole that is overmassive relative to both the host galaxy stellar and dynamical\nmasses. We finally report indications of variability and the first attempt of\nAGN reverberation mapping at such an early epoch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent observations from JWST have revealed an abundant population of active\ngalactic nuclei (AGN) and so-called ``Little Red Dots'' (LRDs) at $2\\lesssim z\n\\lesssim 11$, many of which are characterized by V-shaped UV-to-optical\ncontinua with turnovers around the Balmer limit. The physical nature of these\nLRDs is unclear, and it remains debated whether the peculiar spectral shape\noriginates from AGN, compact galaxies, or both. We present the analysis of new\nNIRSpec-IFU data from the BlackTHUNDER JWST Large Programme and archival\nNIRSpec-MSA data of a lensed LRD at $z=7.04$. The spectra confirm the presence\nof a smooth Balmer break and a broad H$\\beta$ tracing the Broad Line Region\n(BLR) of an AGN. The small velocity dispersion of the H$\\beta$ narrow component\nindicates a small dynamical mass of the host galaxy of $M_{\\rm dyn}<4 \\times\n10^8~M_{\\odot}$, which implies that the stellar population cannot contribute\nmore than 10% to the optical continuum. We show that the Balmer break can be\nwell described by an AGN continuum absorbed by very dense ($n_{\\rm H}\\sim\n10^{10}~{\\rm cm^{-3}}$) and nearly dust-free gas along our line-of-sight\n(possibly gas in the BLR or its surrounding). The same gas is expected to\nproduce H$\\beta$ absorption, at a level consistent with a tentative detection\n($3\\sigma$) in the high-resolution spectrum. Such a non-stellar origin of the\nBalmer break may apply to other LRDs, and would alleviate the issue of\nextremely high stellar mass surface densities inferred in the case of a stellar\ninterpretation of the Balmer break. We note that this is a rare case of a black\nhole that is overmassive relative to both the host galaxy stellar and dynamical\nmasses. We finally report indications of variability and the first attempt of\nAGN reverberation mapping at such an early epoch."
                },
                "authors": [
                    {
                        "name": "Xihan Ji"
                    },
                    {
                        "name": "Roberto Maiolino"
                    },
                    {
                        "name": "Hannah Übler"
                    },
                    {
                        "name": "Jan Scholtz"
                    },
                    {
                        "name": "Francesco D'Eugenio"
                    },
                    {
                        "name": "Fengwu Sun"
                    },
                    {
                        "name": "Michele Perna"
                    },
                    {
                        "name": "Hannah Turner"
                    },
                    {
                        "name": "Santiago Arribas"
                    },
                    {
                        "name": "Jake S. Bennett"
                    },
                    {
                        "name": "Andrew Bunker"
                    },
                    {
                        "name": "Stefano Carniani"
                    },
                    {
                        "name": "Stéphane Charlot"
                    },
                    {
                        "name": "Giovanni Cresci"
                    },
                    {
                        "name": "Mirko Curti"
                    },
                    {
                        "name": "Eiichi Egami"
                    },
                    {
                        "name": "Andy Fabian"
                    },
                    {
                        "name": "Kohei Inayoshi"
                    },
                    {
                        "name": "Yuki Isobe"
                    },
                    {
                        "name": "Gareth Jones"
                    },
                    {
                        "name": "Ignas Juodžbalis"
                    },
                    {
                        "name": "Nimisha Kumari"
                    },
                    {
                        "name": "Jianwei Lyu"
                    },
                    {
                        "name": "Giovanni Mazzolari"
                    },
                    {
                        "name": "Eleonora Parlanti"
                    },
                    {
                        "name": "Brant Robertson"
                    },
                    {
                        "name": "Bruno Rodríguez Del Pino"
                    },
                    {
                        "name": "Raffaella Schneider"
                    },
                    {
                        "name": "Debora Sijacki"
                    },
                    {
                        "name": "Sandro Tacchella"
                    },
                    {
                        "name": "Alessandro Trinca"
                    },
                    {
                        "name": "Rosa Valiante"
                    },
                    {
                        "name": "Giacomo Venturi"
                    },
                    {
                        "name": "Marta Volonteri"
                    },
                    {
                        "name": "Chris Willott"
                    },
                    {
                        "name": "Callum Witten"
                    },
                    {
                        "name": "Joris Witstok"
                    }
                ],
                "author_detail": {
                    "name": "Joris Witstok"
                },
                "author": "Joris Witstok",
                "arxiv_comment": "34 pages, 24 figures, submitted to MNRAS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13082v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13082v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13080v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13080v1",
                "updated": "2025-01-22T18:40:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    18,
                    40,
                    57,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T18:40:57Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    18,
                    40,
                    57,
                    2,
                    22,
                    0
                ],
                "title": "Refining Input Guardrails: Enhancing LLM-as-a-Judge Efficiency Through\n  Chain-of-Thought Fine-Tuning and Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Refining Input Guardrails: Enhancing LLM-as-a-Judge Efficiency Through\n  Chain-of-Thought Fine-Tuning and Alignment"
                },
                "summary": "Large Language Models (LLMs) have demonstrated powerful capabilities that\nrender them valuable in different applications, including conversational AI\nproducts. It is paramount to ensure the security and reliability of these\nproducts by mitigating their vulnerabilities towards malicious user\ninteractions, which can lead to the exposure of great risks and reputational\nrepercussions. In this work, we present a comprehensive study on the efficacy\nof fine-tuning and aligning Chain-of-Thought (CoT) responses of different LLMs\nthat serve as input moderation guardrails. We systematically explore various\ntuning methods by leveraging a small set of training data to adapt these models\nas proxy defense mechanisms to detect malicious inputs and provide a reasoning\nfor their verdicts, thereby preventing the exploitation of conversational\nagents. We rigorously evaluate the efficacy and robustness of different tuning\nstrategies to generalize across diverse adversarial and malicious query types.\nOur experimental results outline the potential of alignment processes tailored\nto a varied range of harmful input queries, even with constrained data\nresources. These techniques significantly enhance the safety of conversational\nAI systems and provide a feasible framework for deploying more secure and\ntrustworthy AI-driven interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated powerful capabilities that\nrender them valuable in different applications, including conversational AI\nproducts. It is paramount to ensure the security and reliability of these\nproducts by mitigating their vulnerabilities towards malicious user\ninteractions, which can lead to the exposure of great risks and reputational\nrepercussions. In this work, we present a comprehensive study on the efficacy\nof fine-tuning and aligning Chain-of-Thought (CoT) responses of different LLMs\nthat serve as input moderation guardrails. We systematically explore various\ntuning methods by leveraging a small set of training data to adapt these models\nas proxy defense mechanisms to detect malicious inputs and provide a reasoning\nfor their verdicts, thereby preventing the exploitation of conversational\nagents. We rigorously evaluate the efficacy and robustness of different tuning\nstrategies to generalize across diverse adversarial and malicious query types.\nOur experimental results outline the potential of alignment processes tailored\nto a varied range of harmful input queries, even with constrained data\nresources. These techniques significantly enhance the safety of conversational\nAI systems and provide a feasible framework for deploying more secure and\ntrustworthy AI-driven interactions."
                },
                "authors": [
                    {
                        "name": "Melissa Kazemi Rad"
                    },
                    {
                        "name": "Huy Nghiem"
                    },
                    {
                        "name": "Andy Luo"
                    },
                    {
                        "name": "Sahil Wadhwa"
                    },
                    {
                        "name": "Mohammad Sorower"
                    },
                    {
                        "name": "Stephen Rawls"
                    }
                ],
                "author_detail": {
                    "name": "Stephen Rawls"
                },
                "author": "Stephen Rawls",
                "arxiv_comment": "16 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13080v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13080v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03153v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03153v2",
                "updated": "2025-01-22T18:21:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    18,
                    21,
                    13,
                    2,
                    22,
                    0
                ],
                "published": "2024-06-09T17:42:09Z",
                "published_parsed": [
                    2024,
                    6,
                    9,
                    17,
                    42,
                    9,
                    6,
                    161,
                    0
                ],
                "title": "An Efficient Framework for Crediting Data Contributors of Diffusion\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Efficient Framework for Crediting Data Contributors of Diffusion\n  Models"
                },
                "summary": "As diffusion models are deployed in real-world settings, and their\nperformance is driven by training data, appraising the contribution of data\ncontributors is crucial to creating incentives for sharing quality data and to\nimplementing policies for data compensation. Depending on the use case, model\nperformance corresponds to various global properties of the distribution\nlearned by a diffusion model (e.g., overall aesthetic quality). Hence, here we\naddress the problem of attributing global properties of diffusion models to\ndata contributors. The Shapley value provides a principled approach to\nvaluation by uniquely satisfying game-theoretic axioms of fairness. However,\nestimating Shapley values for diffusion models is computationally impractical\nbecause it requires retraining on many training data subsets corresponding to\ndifferent contributors and rerunning inference. We introduce a method to\nefficiently retrain and rerun inference for Shapley value estimation, by\nleveraging model pruning and fine-tuning. We evaluate the utility of our method\nwith three use cases: (i) image quality for a DDPM trained on a CIFAR dataset,\n(ii) demographic diversity for an LDM trained on CelebA-HQ, and (iii) aesthetic\nquality for a Stable Diffusion model LoRA-finetuned on Post-Impressionist\nartworks. Our results empirically demonstrate that our framework can identify\nimportant data contributors across models' global properties, outperforming\nexisting attribution methods for diffusion models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As diffusion models are deployed in real-world settings, and their\nperformance is driven by training data, appraising the contribution of data\ncontributors is crucial to creating incentives for sharing quality data and to\nimplementing policies for data compensation. Depending on the use case, model\nperformance corresponds to various global properties of the distribution\nlearned by a diffusion model (e.g., overall aesthetic quality). Hence, here we\naddress the problem of attributing global properties of diffusion models to\ndata contributors. The Shapley value provides a principled approach to\nvaluation by uniquely satisfying game-theoretic axioms of fairness. However,\nestimating Shapley values for diffusion models is computationally impractical\nbecause it requires retraining on many training data subsets corresponding to\ndifferent contributors and rerunning inference. We introduce a method to\nefficiently retrain and rerun inference for Shapley value estimation, by\nleveraging model pruning and fine-tuning. We evaluate the utility of our method\nwith three use cases: (i) image quality for a DDPM trained on a CIFAR dataset,\n(ii) demographic diversity for an LDM trained on CelebA-HQ, and (iii) aesthetic\nquality for a Stable Diffusion model LoRA-finetuned on Post-Impressionist\nartworks. Our results empirically demonstrate that our framework can identify\nimportant data contributors across models' global properties, outperforming\nexisting attribution methods for diffusion models."
                },
                "authors": [
                    {
                        "name": "Chris Lin"
                    },
                    {
                        "name": "Mingyu Lu"
                    },
                    {
                        "name": "Chanwoo Kim"
                    },
                    {
                        "name": "Su-In Lee"
                    }
                ],
                "author_detail": {
                    "name": "Su-In Lee"
                },
                "author": "Su-In Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03153v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03153v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11223v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11223v2",
                "updated": "2025-01-22T17:49:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    17,
                    49,
                    37,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-20T02:16:19Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    2,
                    16,
                    19,
                    0,
                    20,
                    0
                ],
                "title": "Reasoning Language Models: A Blueprint",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Language Models: A Blueprint"
                },
                "summary": "Reasoning language models (RLMs), also known as Large Reasoning Models\n(LRMs), such as OpenAI's o1 and o3, DeepSeek-V3, and Alibaba's QwQ, have\nredefined AI's problem-solving capabilities by extending LLMs with advanced\nreasoning mechanisms. Yet, their high costs, proprietary nature, and complex\narchitectures - uniquely combining Reinforcement Learning (RL), search\nheuristics, and LLMs - present accessibility and scalability challenges. To\naddress these, we propose a comprehensive blueprint that organizes RLM\ncomponents into a modular framework, based on a survey and analysis of all RLM\nworks. This blueprint incorporates diverse reasoning structures (chains, trees,\ngraphs, and nested forms), reasoning strategies (e.g., Monte Carlo Tree Search,\nBeam Search), RL concepts (policy, value models and others), supervision\nschemes (Outcome-Based and Process-Based Supervision), and other related\nconcepts (e.g., Test-Time Compute, Retrieval-Augmented Generation, agent\ntools). We provide detailed mathematical formulations and algorithmic\nspecifications to simplify RLM implementation. By showing how schemes like\nLLaMA-Berry, QwQ, Journey Learning, and Graph of Thoughts fit as special cases,\nwe demonstrate the blueprint's versatility and unifying potential. To\nillustrate its utility, we introduce x1, a modular implementation for rapid RLM\nprototyping and experimentation. Using x1 and a literature review, we provide\nkey insights, such as multi-phase training for policy and value models, and the\nimportance of familiar training distributions. Finally, we discuss scalable RLM\ncloud deployments and we outline how RLMs can integrate with a broader LLM\necosystem. Our work demystifies RLM construction, democratizes advanced\nreasoning capabilities, and fosters innovation, aiming to mitigate the gap\nbetween \"rich AI\" and \"poor AI\" by lowering barriers to RLM development and\nexperimentation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning language models (RLMs), also known as Large Reasoning Models\n(LRMs), such as OpenAI's o1 and o3, DeepSeek-V3, and Alibaba's QwQ, have\nredefined AI's problem-solving capabilities by extending LLMs with advanced\nreasoning mechanisms. Yet, their high costs, proprietary nature, and complex\narchitectures - uniquely combining Reinforcement Learning (RL), search\nheuristics, and LLMs - present accessibility and scalability challenges. To\naddress these, we propose a comprehensive blueprint that organizes RLM\ncomponents into a modular framework, based on a survey and analysis of all RLM\nworks. This blueprint incorporates diverse reasoning structures (chains, trees,\ngraphs, and nested forms), reasoning strategies (e.g., Monte Carlo Tree Search,\nBeam Search), RL concepts (policy, value models and others), supervision\nschemes (Outcome-Based and Process-Based Supervision), and other related\nconcepts (e.g., Test-Time Compute, Retrieval-Augmented Generation, agent\ntools). We provide detailed mathematical formulations and algorithmic\nspecifications to simplify RLM implementation. By showing how schemes like\nLLaMA-Berry, QwQ, Journey Learning, and Graph of Thoughts fit as special cases,\nwe demonstrate the blueprint's versatility and unifying potential. To\nillustrate its utility, we introduce x1, a modular implementation for rapid RLM\nprototyping and experimentation. Using x1 and a literature review, we provide\nkey insights, such as multi-phase training for policy and value models, and the\nimportance of familiar training distributions. Finally, we discuss scalable RLM\ncloud deployments and we outline how RLMs can integrate with a broader LLM\necosystem. Our work demystifies RLM construction, democratizes advanced\nreasoning capabilities, and fosters innovation, aiming to mitigate the gap\nbetween \"rich AI\" and \"poor AI\" by lowering barriers to RLM development and\nexperimentation."
                },
                "authors": [
                    {
                        "name": "Maciej Besta"
                    },
                    {
                        "name": "Julia Barth"
                    },
                    {
                        "name": "Eric Schreiber"
                    },
                    {
                        "name": "Ales Kubicek"
                    },
                    {
                        "name": "Afonso Catarino"
                    },
                    {
                        "name": "Robert Gerstenberger"
                    },
                    {
                        "name": "Piotr Nyczyk"
                    },
                    {
                        "name": "Patrick Iff"
                    },
                    {
                        "name": "Yueling Li"
                    },
                    {
                        "name": "Sam Houliston"
                    },
                    {
                        "name": "Tomasz Sternal"
                    },
                    {
                        "name": "Marcin Copik"
                    },
                    {
                        "name": "Grzegorz Kwaśniewski"
                    },
                    {
                        "name": "Jürgen Müller"
                    },
                    {
                        "name": "Łukasz Flis"
                    },
                    {
                        "name": "Hannes Eberhard"
                    },
                    {
                        "name": "Hubert Niewiadomski"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11223v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11223v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13042v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13042v1",
                "updated": "2025-01-22T17:44:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    17,
                    44,
                    1,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T17:44:01Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    17,
                    44,
                    1,
                    2,
                    22,
                    0
                ],
                "title": "Does Table Source Matter? Benchmarking and Improving Multimodal\n  Scientific Table Understanding and Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does Table Source Matter? Benchmarking and Improving Multimodal\n  Scientific Table Understanding and Reasoning"
                },
                "summary": "Recent large language models (LLMs) have advanced table understanding\ncapabilities but rely on converting tables into text sequences. While\nmultimodal large language models (MLLMs) enable direct visual processing, they\nface limitations in handling scientific tables due to fixed input image\nresolutions and insufficient numerical reasoning capabilities. We present a\ncomprehensive framework for multimodal scientific table understanding and\nreasoning with dynamic input image resolutions. Our framework consists of three\nkey components: (1) MMSci-Pre, a domain-specific table structure learning\ndataset of 52K scientific table structure recognition samples, (2) MMSci-Ins,\nan instruction tuning dataset with 12K samples across three table-based tasks,\nand (3) MMSci-Eval, a benchmark with 3,114 testing samples specifically\ndesigned to evaluate numerical reasoning capabilities. Extensive experiments\ndemonstrate that our domain-specific approach with 52K scientific table images\nachieves superior performance compared to 150K general-domain tables,\nhighlighting the importance of data quality over quantity. Our proposed\ntable-based MLLMs with dynamic input resolutions show significant improvements\nin both general table understanding and numerical reasoning capabilities, with\nstrong generalisation to held-out datasets. Our code and data are publicly\navailable at https://github.com/Bernard-Yang/MMSci_Table.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language models (LLMs) have advanced table understanding\ncapabilities but rely on converting tables into text sequences. While\nmultimodal large language models (MLLMs) enable direct visual processing, they\nface limitations in handling scientific tables due to fixed input image\nresolutions and insufficient numerical reasoning capabilities. We present a\ncomprehensive framework for multimodal scientific table understanding and\nreasoning with dynamic input image resolutions. Our framework consists of three\nkey components: (1) MMSci-Pre, a domain-specific table structure learning\ndataset of 52K scientific table structure recognition samples, (2) MMSci-Ins,\nan instruction tuning dataset with 12K samples across three table-based tasks,\nand (3) MMSci-Eval, a benchmark with 3,114 testing samples specifically\ndesigned to evaluate numerical reasoning capabilities. Extensive experiments\ndemonstrate that our domain-specific approach with 52K scientific table images\nachieves superior performance compared to 150K general-domain tables,\nhighlighting the importance of data quality over quantity. Our proposed\ntable-based MLLMs with dynamic input resolutions show significant improvements\nin both general table understanding and numerical reasoning capabilities, with\nstrong generalisation to held-out datasets. Our code and data are publicly\navailable at https://github.com/Bernard-Yang/MMSci_Table."
                },
                "authors": [
                    {
                        "name": "Bohao Yang"
                    },
                    {
                        "name": "Yingji Zhang"
                    },
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "André Freitas"
                    },
                    {
                        "name": "Chenghua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Chenghua Lin"
                },
                "author": "Chenghua Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13042v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13042v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21358v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21358v3",
                "updated": "2025-01-22T17:21:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    17,
                    21,
                    5,
                    2,
                    22,
                    0
                ],
                "published": "2024-10-28T17:35:59Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    35,
                    59,
                    0,
                    302,
                    0
                ],
                "title": "\"We do use it, but not how hearing people think\": How the Deaf and Hard\n  of Hearing Community Uses Large Language Model Tools",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"We do use it, but not how hearing people think\": How the Deaf and Hard\n  of Hearing Community Uses Large Language Model Tools"
                },
                "summary": "Generative AI tools, particularly those utilizing large language models\n(LLMs), are increasingly used in everyday contexts. While these tools enhance\nproductivity and accessibility, little is known about how Deaf and Hard of\nHearing (DHH) individuals engage with them or the challenges they face when\nusing them. This paper presents a mixed-method study exploring how the DHH\ncommunity uses Text AI tools like ChatGPT to reduce communication barriers and\nenhance information access. We surveyed 80 DHH participants and conducted\ninterviews with 11 participants. Our findings reveal important benefits, such\nas eased communication and bridging Deaf and hearing cultures, alongside\nchallenges like lack of American Sign Language (ASL) support and Deaf cultural\nunderstanding. We highlight unique usage patterns, propose inclusive design\nrecommendations, and outline future research directions to improve Text AI\naccessibility for the DHH community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI tools, particularly those utilizing large language models\n(LLMs), are increasingly used in everyday contexts. While these tools enhance\nproductivity and accessibility, little is known about how Deaf and Hard of\nHearing (DHH) individuals engage with them or the challenges they face when\nusing them. This paper presents a mixed-method study exploring how the DHH\ncommunity uses Text AI tools like ChatGPT to reduce communication barriers and\nenhance information access. We surveyed 80 DHH participants and conducted\ninterviews with 11 participants. Our findings reveal important benefits, such\nas eased communication and bridging Deaf and hearing cultures, alongside\nchallenges like lack of American Sign Language (ASL) support and Deaf cultural\nunderstanding. We highlight unique usage patterns, propose inclusive design\nrecommendations, and outline future research directions to improve Text AI\naccessibility for the DHH community."
                },
                "authors": [
                    {
                        "name": "Shuxu Huffman"
                    },
                    {
                        "name": "Si Chen"
                    },
                    {
                        "name": "Kelly Avery Mack"
                    },
                    {
                        "name": "Haotian Su"
                    },
                    {
                        "name": "Qi Wang"
                    },
                    {
                        "name": "Raja Kushalnagar"
                    }
                ],
                "author_detail": {
                    "name": "Raja Kushalnagar"
                },
                "author": "Raja Kushalnagar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21358v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21358v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16086v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16086v2",
                "updated": "2025-01-22T17:18:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    17,
                    18,
                    15,
                    2,
                    22,
                    0
                ],
                "published": "2024-12-20T17:33:50Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    17,
                    33,
                    50,
                    4,
                    355,
                    0
                ],
                "title": "Towards Interpretable Radiology Report Generation via Concept\n  Bottlenecks using a Multi-Agentic RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Interpretable Radiology Report Generation via Concept\n  Bottlenecks using a Multi-Agentic RAG"
                },
                "summary": "Deep learning has advanced medical image classification, but interpretability\nchallenges hinder its clinical adoption. This study enhances interpretability\nin Chest X-ray (CXR) classification by using concept bottleneck models (CBMs)\nand a multi-agent Retrieval-Augmented Generation (RAG) system for report\ngeneration. By modeling relationships between visual features and clinical\nconcepts, we create interpretable concept vectors that guide a multi-agent RAG\nsystem to generate radiology reports, enhancing clinical relevance,\nexplainability, and transparency. Evaluation of the generated reports using an\nLLM-as-a-judge confirmed the interpretability and clinical utility of our\nmodel's outputs. On the COVID-QU dataset, our model achieved 81% classification\naccuracy and demonstrated robust report generation performance, with five key\nmetrics ranging between 84% and 90%. This interpretable multi-agent framework\nbridges the gap between high-performance AI and the explainability required for\nreliable AI-driven CXR analysis in clinical settings. Our code is available at\nhttps://github.com/tifat58/IRR-with-CBM-RAG.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning has advanced medical image classification, but interpretability\nchallenges hinder its clinical adoption. This study enhances interpretability\nin Chest X-ray (CXR) classification by using concept bottleneck models (CBMs)\nand a multi-agent Retrieval-Augmented Generation (RAG) system for report\ngeneration. By modeling relationships between visual features and clinical\nconcepts, we create interpretable concept vectors that guide a multi-agent RAG\nsystem to generate radiology reports, enhancing clinical relevance,\nexplainability, and transparency. Evaluation of the generated reports using an\nLLM-as-a-judge confirmed the interpretability and clinical utility of our\nmodel's outputs. On the COVID-QU dataset, our model achieved 81% classification\naccuracy and demonstrated robust report generation performance, with five key\nmetrics ranging between 84% and 90%. This interpretable multi-agent framework\nbridges the gap between high-performance AI and the explainability required for\nreliable AI-driven CXR analysis in clinical settings. Our code is available at\nhttps://github.com/tifat58/IRR-with-CBM-RAG.git."
                },
                "authors": [
                    {
                        "name": "Hasan Md Tusfiqur Alam"
                    },
                    {
                        "name": "Devansh Srivastav"
                    },
                    {
                        "name": "Md Abdul Kadir"
                    },
                    {
                        "name": "Daniel Sonntag"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Sonntag"
                },
                "author": "Daniel Sonntag",
                "arxiv_comment": "Accepted in the 47th European Conference for Information Retrieval\n  (ECIR) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16086v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16086v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13019v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13019v1",
                "updated": "2025-01-22T17:07:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    17,
                    7,
                    48,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T17:07:48Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    17,
                    7,
                    48,
                    2,
                    22,
                    0
                ],
                "title": "Correlation Neglect in Games",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Correlation Neglect in Games"
                },
                "summary": "This paper proposes a simple framework to study the effect of correlation\nneglect on social learning and welfare in games with social incentives. It\nexamines statistical learners (frequentists, Bayesians, etc.) who make\ndecisions based on their peers' actions but overlook the correlation between\nthe actions they observe. A novel solution concept called correlated sampling\nequilibrium with statistical inference (CoSESI) reveals that correlation\nneglect affects strategic behavior through persistent overprecision, which\nleads to polarization and information cascades. CoSESI always exists and\ndiffers from existing concepts. It captures the fact that naive beliefs are\noverly sensitive to correlations, which causes failures of social learning.\nApplications of CoSESI in matching markets, monopoly pricing, and financial\nmarkets demonstrate that correlation neglect bears significant economic\nconsequences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a simple framework to study the effect of correlation\nneglect on social learning and welfare in games with social incentives. It\nexamines statistical learners (frequentists, Bayesians, etc.) who make\ndecisions based on their peers' actions but overlook the correlation between\nthe actions they observe. A novel solution concept called correlated sampling\nequilibrium with statistical inference (CoSESI) reveals that correlation\nneglect affects strategic behavior through persistent overprecision, which\nleads to polarization and information cascades. CoSESI always exists and\ndiffers from existing concepts. It captures the fact that naive beliefs are\noverly sensitive to correlations, which causes failures of social learning.\nApplications of CoSESI in matching markets, monopoly pricing, and financial\nmarkets demonstrate that correlation neglect bears significant economic\nconsequences."
                },
                "authors": [
                    {
                        "name": "Florian Mudekereza"
                    }
                ],
                "author_detail": {
                    "name": "Florian Mudekereza"
                },
                "author": "Florian Mudekereza",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13019v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13019v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.TH",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13011v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13011v1",
                "updated": "2025-01-22T16:53:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    16,
                    53,
                    8,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T16:53:08Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    16,
                    53,
                    8,
                    2,
                    22,
                    0
                ],
                "title": "MONA: Myopic Optimization with Non-myopic Approval Can Mitigate\n  Multi-step Reward Hacking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MONA: Myopic Optimization with Non-myopic Approval Can Mitigate\n  Multi-step Reward Hacking"
                },
                "summary": "Future advanced AI systems may learn sophisticated strategies through\nreinforcement learning (RL) that humans cannot understand well enough to safely\nevaluate. We propose a training method which avoids agents learning undesired\nmulti-step plans that receive high reward (multi-step \"reward hacks\") even if\nhumans are not able to detect that the behaviour is undesired. The method,\nMyopic Optimization with Non-myopic Approval (MONA), works by combining\nshort-sighted optimization with far-sighted reward. We demonstrate that MONA\ncan prevent multi-step reward hacking that ordinary RL causes, even without\nbeing able to detect the reward hacking and without any extra information that\nordinary RL does not get access to. We study MONA empirically in three settings\nwhich model different misalignment failure modes including 2-step environments\nwith LLMs representing delegated oversight and encoded reasoning and\nlonger-horizon gridworld environments representing sensor tampering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Future advanced AI systems may learn sophisticated strategies through\nreinforcement learning (RL) that humans cannot understand well enough to safely\nevaluate. We propose a training method which avoids agents learning undesired\nmulti-step plans that receive high reward (multi-step \"reward hacks\") even if\nhumans are not able to detect that the behaviour is undesired. The method,\nMyopic Optimization with Non-myopic Approval (MONA), works by combining\nshort-sighted optimization with far-sighted reward. We demonstrate that MONA\ncan prevent multi-step reward hacking that ordinary RL causes, even without\nbeing able to detect the reward hacking and without any extra information that\nordinary RL does not get access to. We study MONA empirically in three settings\nwhich model different misalignment failure modes including 2-step environments\nwith LLMs representing delegated oversight and encoded reasoning and\nlonger-horizon gridworld environments representing sensor tampering."
                },
                "authors": [
                    {
                        "name": "Sebastian Farquhar"
                    },
                    {
                        "name": "Vikrant Varma"
                    },
                    {
                        "name": "David Lindner"
                    },
                    {
                        "name": "David Elson"
                    },
                    {
                        "name": "Caleb Biddulph"
                    },
                    {
                        "name": "Ian Goodfellow"
                    },
                    {
                        "name": "Rohin Shah"
                    }
                ],
                "author_detail": {
                    "name": "Rohin Shah"
                },
                "author": "Rohin Shah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13011v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13011v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09450v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09450v2",
                "updated": "2025-01-22T16:51:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    16,
                    51,
                    23,
                    2,
                    22,
                    0
                ],
                "published": "2024-10-12T09:06:10Z",
                "published_parsed": [
                    2024,
                    10,
                    12,
                    9,
                    6,
                    10,
                    5,
                    286,
                    0
                ],
                "title": "The effects of super-Eddington accretion and feedback on the growth of\n  early supermassive black holes and galaxies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The effects of super-Eddington accretion and feedback on the growth of\n  early supermassive black holes and galaxies"
                },
                "summary": "We present results of cosmological zoom-in simulations of a massive\nprotocluster down to redshift $z\\approx4$ (when the halo mass is\n$\\approx10^{13}$ M$_\\odot$) using the SWIFT code and the EAGLE galaxy formation\nmodel, focusing on supermassive black hole (BH) physics. The BH was seeded with\na mass of $10^4$ M$_\\odot$ at redshift $z\\approx17$. We compare the base model\nthat uses an Eddington limit on the BH accretion rate and thermal isotropic\nfeedback by the AGN, with one where super-Eddington accretion is allowed, as\nwell as two other models with BH spin and jets. In the base model, the BH grows\nat the Eddington limit from $z=9$ to $z=5.5$, when it becomes massive enough to\nhalt its own and its host galaxy's growth through feedback. We find that\nallowing super-Eddington accretion leads to drastic differences, with the BH\ngoing through an intense but short super-Eddington growth burst around\n$z\\approx7.5$, during which it increases its mass by orders of magnitude,\nbefore feedback stops further growth (of both the BH and the galaxy). By\n$z\\approx4$ the galaxy is only half as massive in the super-Eddington cases,\nand an order of magnitude more extended, with the half-mass radius reaching\nvalues of a few physical kpc instead of a few hundred pc. The BH masses in our\nsimulations are consistent with the intrinsic BH mass$-$stellar mass relation\ninferred from high-redshift observations by JWST. This shows that galaxy\nformation models using the $\\Lambda$CDM cosmology are capable of reproducing\nthe observed massive BHs at high redshift. Allowing jets, either at super- or\nsub-Eddington rates, has little impact on the host galaxy properties, but leads\nto lower BH masses as a consequence of higher feedback efficiencies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present results of cosmological zoom-in simulations of a massive\nprotocluster down to redshift $z\\approx4$ (when the halo mass is\n$\\approx10^{13}$ M$_\\odot$) using the SWIFT code and the EAGLE galaxy formation\nmodel, focusing on supermassive black hole (BH) physics. The BH was seeded with\na mass of $10^4$ M$_\\odot$ at redshift $z\\approx17$. We compare the base model\nthat uses an Eddington limit on the BH accretion rate and thermal isotropic\nfeedback by the AGN, with one where super-Eddington accretion is allowed, as\nwell as two other models with BH spin and jets. In the base model, the BH grows\nat the Eddington limit from $z=9$ to $z=5.5$, when it becomes massive enough to\nhalt its own and its host galaxy's growth through feedback. We find that\nallowing super-Eddington accretion leads to drastic differences, with the BH\ngoing through an intense but short super-Eddington growth burst around\n$z\\approx7.5$, during which it increases its mass by orders of magnitude,\nbefore feedback stops further growth (of both the BH and the galaxy). By\n$z\\approx4$ the galaxy is only half as massive in the super-Eddington cases,\nand an order of magnitude more extended, with the half-mass radius reaching\nvalues of a few physical kpc instead of a few hundred pc. The BH masses in our\nsimulations are consistent with the intrinsic BH mass$-$stellar mass relation\ninferred from high-redshift observations by JWST. This shows that galaxy\nformation models using the $\\Lambda$CDM cosmology are capable of reproducing\nthe observed massive BHs at high redshift. Allowing jets, either at super- or\nsub-Eddington rates, has little impact on the host galaxy properties, but leads\nto lower BH masses as a consequence of higher feedback efficiencies."
                },
                "authors": [
                    {
                        "name": "Filip Huško"
                    },
                    {
                        "name": "Cedric G. Lacey"
                    },
                    {
                        "name": "William J. Roper"
                    },
                    {
                        "name": "Joop Schaye"
                    },
                    {
                        "name": "Jemima Mae Briggs"
                    },
                    {
                        "name": "Matthieu Schaller"
                    }
                ],
                "author_detail": {
                    "name": "Matthieu Schaller"
                },
                "arxiv_affiliation": "Lorentz Institute for Theoretical Physics, Leiden",
                "author": "Matthieu Schaller",
                "arxiv_comment": "Accepted for publication in MNRAS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09450v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09450v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13007v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13007v1",
                "updated": "2025-01-22T16:49:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    16,
                    49,
                    37,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T16:49:37Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    16,
                    49,
                    37,
                    2,
                    22,
                    0
                ],
                "title": "Pairwise RM: Perform Best-of-N Sampling with Knockout Tournament",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pairwise RM: Perform Best-of-N Sampling with Knockout Tournament"
                },
                "summary": "Best-of-N (BoN) sampling, a common strategy for test-time scaling of Large\nLanguage Models (LLMs), relies on reward models to select the best candidate\nsolution from multiple generations. However, traditional reward models often\nassign arbitrary and inconsistent scores, limiting their effectiveness. To\naddress this, we propose a Pairwise Reward Model (Pairwise RM) combined with a\nknockout tournament for BoN sampling. Instead of assigning absolute scores,\ngiven one math problem, Pairwise RM evaluates two candidate solutions'\ncorrectness simultaneously. This approach eliminates the need for arbitrary\nscoring and enables cross-validation of solutions through parallel comparison.\nIn the knockout tournament, Pairwise RM conducts pairwise comparisons between\ncandidate solutions and eliminates the incorrect ones iteratively. We construct\n\\ourdataset, a large-scale dataset of 443K pairwise comparisons derived from\nNumiaMath and annotated using \\texttt{gemini-1.5-flash}, and train the Pairwise\nRM via supervised fine-tuning. Experiments on MATH-500 and the Olympiad Bench\ndemonstrate significant improvements over traditional discriminative reward\nmodels. And a 40\\% to 60\\% relative improvement is achieved on the top 50\\%\nchallenging problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Best-of-N (BoN) sampling, a common strategy for test-time scaling of Large\nLanguage Models (LLMs), relies on reward models to select the best candidate\nsolution from multiple generations. However, traditional reward models often\nassign arbitrary and inconsistent scores, limiting their effectiveness. To\naddress this, we propose a Pairwise Reward Model (Pairwise RM) combined with a\nknockout tournament for BoN sampling. Instead of assigning absolute scores,\ngiven one math problem, Pairwise RM evaluates two candidate solutions'\ncorrectness simultaneously. This approach eliminates the need for arbitrary\nscoring and enables cross-validation of solutions through parallel comparison.\nIn the knockout tournament, Pairwise RM conducts pairwise comparisons between\ncandidate solutions and eliminates the incorrect ones iteratively. We construct\n\\ourdataset, a large-scale dataset of 443K pairwise comparisons derived from\nNumiaMath and annotated using \\texttt{gemini-1.5-flash}, and train the Pairwise\nRM via supervised fine-tuning. Experiments on MATH-500 and the Olympiad Bench\ndemonstrate significant improvements over traditional discriminative reward\nmodels. And a 40\\% to 60\\% relative improvement is achieved on the top 50\\%\nchallenging problems."
                },
                "authors": [
                    {
                        "name": "Yantao Liu"
                    },
                    {
                        "name": "Zijun Yao"
                    },
                    {
                        "name": "Rui Min"
                    },
                    {
                        "name": "Yixin Cao"
                    },
                    {
                        "name": "Lei Hou"
                    },
                    {
                        "name": "Juanzi Li"
                    }
                ],
                "author_detail": {
                    "name": "Juanzi Li"
                },
                "author": "Juanzi Li",
                "arxiv_comment": "in progress work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13007v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13007v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.01372v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.01372v3",
                "updated": "2025-01-22T16:27:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    16,
                    27,
                    54,
                    2,
                    22,
                    0
                ],
                "published": "2024-05-02T15:14:16Z",
                "published_parsed": [
                    2024,
                    5,
                    2,
                    15,
                    14,
                    16,
                    3,
                    123,
                    0
                ],
                "title": "Statistical algorithms for low-frequency diffusion data: A PDE approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistical algorithms for low-frequency diffusion data: A PDE approach"
                },
                "summary": "We consider the problem of making nonparametric inference in a class of\nmulti-dimensional diffusions in divergence form, from low-frequency data.\nStatistical analysis in this setting is notoriously challenging due to the\nintractability of the likelihood and its gradient, and computational methods\nhave thus far largely resorted to expensive simulation-based techniques. In\nthis article, we propose a new computational approach which is motivated by PDE\ntheory and is built around the characterisation of the transition densities as\nsolutions of the associated heat (Fokker-Planck) equation. Employing optimal\nregularity results from the theory of parabolic PDEs, we prove a novel\ncharacterisation for the gradient of the likelihood. Using these developments,\nfor the nonlinear inverse problem of recovering the diffusivity, we then show\nthat the numerical evaluation of the likelihood and its gradient can be reduced\nto standard elliptic eigenvalue problems, solvable by powerful finite element\nmethods. This enables the efficient implementation of a large class of popular\nstatistical algorithms, including (i) preconditioned Crank-Nicolson and\nLangevin-type methods for posterior sampling, and (ii) gradient-based descent\noptimisation schemes to compute maximum likelihood and maximum-a-posteriori\nestimates. We showcase the effectiveness of these methods via extensive\nsimulation studies in a nonparametric Bayesian model with Gaussian process\npriors, in which both the proposed optimisation and sampling schemes provide\ngood numerical recovery. The reproducible code is available online at\nhttps://github.com/MattGiord/LF-Diffusion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the problem of making nonparametric inference in a class of\nmulti-dimensional diffusions in divergence form, from low-frequency data.\nStatistical analysis in this setting is notoriously challenging due to the\nintractability of the likelihood and its gradient, and computational methods\nhave thus far largely resorted to expensive simulation-based techniques. In\nthis article, we propose a new computational approach which is motivated by PDE\ntheory and is built around the characterisation of the transition densities as\nsolutions of the associated heat (Fokker-Planck) equation. Employing optimal\nregularity results from the theory of parabolic PDEs, we prove a novel\ncharacterisation for the gradient of the likelihood. Using these developments,\nfor the nonlinear inverse problem of recovering the diffusivity, we then show\nthat the numerical evaluation of the likelihood and its gradient can be reduced\nto standard elliptic eigenvalue problems, solvable by powerful finite element\nmethods. This enables the efficient implementation of a large class of popular\nstatistical algorithms, including (i) preconditioned Crank-Nicolson and\nLangevin-type methods for posterior sampling, and (ii) gradient-based descent\noptimisation schemes to compute maximum likelihood and maximum-a-posteriori\nestimates. We showcase the effectiveness of these methods via extensive\nsimulation studies in a nonparametric Bayesian model with Gaussian process\npriors, in which both the proposed optimisation and sampling schemes provide\ngood numerical recovery. The reproducible code is available online at\nhttps://github.com/MattGiord/LF-Diffusion."
                },
                "authors": [
                    {
                        "name": "Matteo Giordano"
                    },
                    {
                        "name": "Sven Wang"
                    }
                ],
                "author_detail": {
                    "name": "Sven Wang"
                },
                "author": "Sven Wang",
                "arxiv_comment": "50 pages, 9 figures, 5 tables, to appear in the Annals of Statistics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.01372v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.01372v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "Primary 62M15, secondary 62F15, 62G05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12988v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12988v1",
                "updated": "2025-01-22T16:20:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    16,
                    20,
                    47,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T16:20:47Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    16,
                    20,
                    47,
                    2,
                    22,
                    0
                ],
                "title": "Large Language Model-Based Semantic Communication System for Image\n  Transmission",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model-Based Semantic Communication System for Image\n  Transmission"
                },
                "summary": "The remarkable success of Large Language Models (LLMs) in understanding and\ngenerating various data types, such as images and text, has demonstrated their\nability to process and extract semantic information across diverse domains.\nThis transformative capability lays the foundation for semantic communications,\nenabling highly efficient and intelligent communication systems. In this work,\nwe present a novel OFDM-based semantic communication framework for image\ntransmission. We propose an innovative semantic encoder design that leverages\nthe ability of LLMs to extract the meaning of transmitted data rather than\nfocusing on its raw representation. On the receiver side, we design an\nLLM-based semantic decoder capable of comprehending context and generating the\nmost appropriate representation to fit the given context. We evaluate our\nproposed system under different scenarios, including Urban Macro-cell\nenvironments with varying speed ranges. The evaluation metrics demonstrate that\nour proposed system reduces the data size 4250 times, while achieving a higher\ndata rate compared to conventional communication methods. This approach offers\na robust and scalable solution to unlock the full potential of 6G connectivity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The remarkable success of Large Language Models (LLMs) in understanding and\ngenerating various data types, such as images and text, has demonstrated their\nability to process and extract semantic information across diverse domains.\nThis transformative capability lays the foundation for semantic communications,\nenabling highly efficient and intelligent communication systems. In this work,\nwe present a novel OFDM-based semantic communication framework for image\ntransmission. We propose an innovative semantic encoder design that leverages\nthe ability of LLMs to extract the meaning of transmitted data rather than\nfocusing on its raw representation. On the receiver side, we design an\nLLM-based semantic decoder capable of comprehending context and generating the\nmost appropriate representation to fit the given context. We evaluate our\nproposed system under different scenarios, including Urban Macro-cell\nenvironments with varying speed ranges. The evaluation metrics demonstrate that\nour proposed system reduces the data size 4250 times, while achieving a higher\ndata rate compared to conventional communication methods. This approach offers\na robust and scalable solution to unlock the full potential of 6G connectivity."
                },
                "authors": [
                    {
                        "name": "Soheyb Ribouh"
                    },
                    {
                        "name": "Osama Saleem"
                    }
                ],
                "author_detail": {
                    "name": "Osama Saleem"
                },
                "author": "Osama Saleem",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12988v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12988v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12983v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12983v1",
                "updated": "2025-01-22T16:12:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    16,
                    12,
                    38,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T16:12:38Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    16,
                    12,
                    38,
                    2,
                    22,
                    0
                ],
                "title": "LLM4WM: Adapting LLM for Wireless Multi-Tasking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM4WM: Adapting LLM for Wireless Multi-Tasking"
                },
                "summary": "The wireless channel is fundamental to communication, encompassing numerous\ntasks collectively referred to as channel-associated tasks. These tasks can\nleverage joint learning based on channel characteristics to share\nrepresentations and enhance system design. To capitalize on this advantage,\nLLM4WM is proposed--a large language model (LLM) multi-task fine-tuning\nframework specifically tailored for channel-associated tasks. This framework\nutilizes a Mixture of Experts with Low-Rank Adaptation (MoE-LoRA) approach for\nmulti-task fine-tuning, enabling the transfer of the pre-trained LLM's general\nknowledge to these tasks. Given the unique characteristics of wireless channel\ndata, preprocessing modules, adapter modules, and multi-task output layers are\ndesigned to align the channel data with the LLM's semantic feature space.\nExperiments on a channel-associated multi-task dataset demonstrate that LLM4WM\noutperforms existing methodologies in both full-sample and few-shot\nevaluations, owing to its robust multi-task joint modeling and transfer\nlearning capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The wireless channel is fundamental to communication, encompassing numerous\ntasks collectively referred to as channel-associated tasks. These tasks can\nleverage joint learning based on channel characteristics to share\nrepresentations and enhance system design. To capitalize on this advantage,\nLLM4WM is proposed--a large language model (LLM) multi-task fine-tuning\nframework specifically tailored for channel-associated tasks. This framework\nutilizes a Mixture of Experts with Low-Rank Adaptation (MoE-LoRA) approach for\nmulti-task fine-tuning, enabling the transfer of the pre-trained LLM's general\nknowledge to these tasks. Given the unique characteristics of wireless channel\ndata, preprocessing modules, adapter modules, and multi-task output layers are\ndesigned to align the channel data with the LLM's semantic feature space.\nExperiments on a channel-associated multi-task dataset demonstrate that LLM4WM\noutperforms existing methodologies in both full-sample and few-shot\nevaluations, owing to its robust multi-task joint modeling and transfer\nlearning capabilities."
                },
                "authors": [
                    {
                        "name": "Xuanyu Liu"
                    },
                    {
                        "name": "Shijian Gao"
                    },
                    {
                        "name": "Boxun Liu"
                    },
                    {
                        "name": "Xiang Cheng"
                    },
                    {
                        "name": "Liuqing Yang"
                    }
                ],
                "author_detail": {
                    "name": "Liuqing Yang"
                },
                "author": "Liuqing Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12983v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12983v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12980v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12980v1",
                "updated": "2025-01-22T16:07:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    16,
                    7,
                    24,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T16:07:24Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    16,
                    7,
                    24,
                    2,
                    22,
                    0
                ],
                "title": "Implicit Causality-biases in humans and LLMs as a tool for benchmarking\n  LLM discourse capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implicit Causality-biases in humans and LLMs as a tool for benchmarking\n  LLM discourse capabilities"
                },
                "summary": "In this paper, we compare data generated with mono- and multilingual LLMs\nspanning a range of model sizes with data provided by human participants in an\nexperimental setting investigating well-established discourse biases. Beyond\nthe comparison as such, we aim to develop a benchmark to assess the\ncapabilities of LLMs with discourse biases as a robust proxy for more general\ndiscourse understanding capabilities. More specifically, we investigated\nImplicit Causality verbs, for which psycholinguistic research has found\nparticipants to display biases with regard to three phenomena:\\ the\nestablishment of (i) coreference relations (Experiment 1), (ii) coherence\nrelations (Experiment 2), and (iii) the use of particular referring expressions\n(Experiments 3 and 4). With regard to coreference biases we found only the\nlargest monolingual LLM (German Bloom 6.4B) to display more human-like biases.\nFor coherence relation, no LLM displayed the explanation bias usually found for\nhumans. For referring expressions, all LLMs displayed a preference for\nreferring to subject arguments with simpler forms than to objects. However, no\nbias effect on referring expression was found, as opposed to recent studies\ninvestigating human biases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we compare data generated with mono- and multilingual LLMs\nspanning a range of model sizes with data provided by human participants in an\nexperimental setting investigating well-established discourse biases. Beyond\nthe comparison as such, we aim to develop a benchmark to assess the\ncapabilities of LLMs with discourse biases as a robust proxy for more general\ndiscourse understanding capabilities. More specifically, we investigated\nImplicit Causality verbs, for which psycholinguistic research has found\nparticipants to display biases with regard to three phenomena:\\ the\nestablishment of (i) coreference relations (Experiment 1), (ii) coherence\nrelations (Experiment 2), and (iii) the use of particular referring expressions\n(Experiments 3 and 4). With regard to coreference biases we found only the\nlargest monolingual LLM (German Bloom 6.4B) to display more human-like biases.\nFor coherence relation, no LLM displayed the explanation bias usually found for\nhumans. For referring expressions, all LLMs displayed a preference for\nreferring to subject arguments with simpler forms than to objects. However, no\nbias effect on referring expression was found, as opposed to recent studies\ninvestigating human biases."
                },
                "authors": [
                    {
                        "name": "Florian Kankowski"
                    },
                    {
                        "name": "Torgrim Solstad"
                    },
                    {
                        "name": "Sina Zarriess"
                    },
                    {
                        "name": "Oliver Bott"
                    }
                ],
                "author_detail": {
                    "name": "Oliver Bott"
                },
                "author": "Oliver Bott",
                "arxiv_comment": "38 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12980v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12980v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12975v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12975v1",
                "updated": "2025-01-22T15:59:44Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    15,
                    59,
                    44,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T15:59:44Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    15,
                    59,
                    44,
                    2,
                    22,
                    0
                ],
                "title": "OnionEval: An Unified Evaluation of Fact-conflicting Hallucination for\n  Small-Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OnionEval: An Unified Evaluation of Fact-conflicting Hallucination for\n  Small-Large Language Models"
                },
                "summary": "Large Language Models (LLMs) are highly capable but require significant\ncomputational resources for both training and inference. Within the LLM family,\nsmaller models (those with fewer than 10 billion parameters) also perform well\nacross various tasks. However, these smaller models share similar limitations\nto their larger counterparts, including the tendency to hallucinate. Despite\nthe existence of many benchmarks to evaluate hallucination in LLMs, few have\nspecifically focused on small LLMs (SLLMs). Additionally, SLLMs show widely\nvarying performance across different benchmarks. In this paper, we introduce\nOnionEval, a multi-layer structured framework with a specific metric called the\ncontext-influence score (CI), designed to effectively assess the\nfact-conflicting hallucination tendencies of small LLMs across different\ncontextual levels. Our experimental results reveal a key feature of SLLMs: they\nexcel in factual analysis but face challenges with context reasoning. Further\ninvestigation shows that a simple Chain-of-Thought strategy can significantly\nreduce these limitations, improving the practical usefulness of SLLMs in\nreal-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are highly capable but require significant\ncomputational resources for both training and inference. Within the LLM family,\nsmaller models (those with fewer than 10 billion parameters) also perform well\nacross various tasks. However, these smaller models share similar limitations\nto their larger counterparts, including the tendency to hallucinate. Despite\nthe existence of many benchmarks to evaluate hallucination in LLMs, few have\nspecifically focused on small LLMs (SLLMs). Additionally, SLLMs show widely\nvarying performance across different benchmarks. In this paper, we introduce\nOnionEval, a multi-layer structured framework with a specific metric called the\ncontext-influence score (CI), designed to effectively assess the\nfact-conflicting hallucination tendencies of small LLMs across different\ncontextual levels. Our experimental results reveal a key feature of SLLMs: they\nexcel in factual analysis but face challenges with context reasoning. Further\ninvestigation shows that a simple Chain-of-Thought strategy can significantly\nreduce these limitations, improving the practical usefulness of SLLMs in\nreal-world applications."
                },
                "authors": [
                    {
                        "name": "Chongren Sun"
                    },
                    {
                        "name": "Yuran Li"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Benoit Boulet"
                    }
                ],
                "author_detail": {
                    "name": "Benoit Boulet"
                },
                "author": "Benoit Boulet",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12975v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12975v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12972v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12972v1",
                "updated": "2025-01-22T15:57:29Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    15,
                    57,
                    29,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T15:57:29Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    15,
                    57,
                    29,
                    2,
                    22,
                    0
                ],
                "title": "Accessible Smart Contracts Verification: Synthesizing Formal Models with\n  Tamed LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accessible Smart Contracts Verification: Synthesizing Formal Models with\n  Tamed LLMs"
                },
                "summary": "When blockchain systems are said to be trustless, what this really means is\nthat all the trust is put into software. Thus, there are strong incentives to\nensure blockchain software is correct -- vulnerabilities here cost millions and\nbreak businesses. One of the most powerful ways of establishing software\ncorrectness is by using formal methods. Approaches based on formal methods,\nhowever, induce a significant overhead in terms of time and expertise required\nto successfully employ them. Our work addresses this critical disadvantage by\nautomating the creation of a formal model -- a mathematical abstraction of the\nsoftware system -- which is often a core task when employing formal methods. We\nperform model synthesis in three phases: we first transpile the code into model\nstubs; then we \"fill in the blanks\" using a large language model (LLM);\nfinally, we iteratively repair the generated model, on both syntactical and\nsemantical level. In this way, we significantly reduce the amount of time\nnecessary to create formal models and increase accessibility of valuable\nsoftware verification methods that rely on them. The practical context of our\nwork was reducing the time-to-value of using formal models for correctness\naudits of smart contracts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When blockchain systems are said to be trustless, what this really means is\nthat all the trust is put into software. Thus, there are strong incentives to\nensure blockchain software is correct -- vulnerabilities here cost millions and\nbreak businesses. One of the most powerful ways of establishing software\ncorrectness is by using formal methods. Approaches based on formal methods,\nhowever, induce a significant overhead in terms of time and expertise required\nto successfully employ them. Our work addresses this critical disadvantage by\nautomating the creation of a formal model -- a mathematical abstraction of the\nsoftware system -- which is often a core task when employing formal methods. We\nperform model synthesis in three phases: we first transpile the code into model\nstubs; then we \"fill in the blanks\" using a large language model (LLM);\nfinally, we iteratively repair the generated model, on both syntactical and\nsemantical level. In this way, we significantly reduce the amount of time\nnecessary to create formal models and increase accessibility of valuable\nsoftware verification methods that rely on them. The practical context of our\nwork was reducing the time-to-value of using formal models for correctness\naudits of smart contracts."
                },
                "authors": [
                    {
                        "name": "Jan Corazza"
                    },
                    {
                        "name": "Ivan Gavran"
                    },
                    {
                        "name": "Gabriela Moreira"
                    },
                    {
                        "name": "Daniel Neider"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Neider"
                },
                "author": "Daniel Neider",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12972v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12972v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12959v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12959v1",
                "updated": "2025-01-22T15:33:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    15,
                    33,
                    17,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T15:33:17Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    15,
                    33,
                    17,
                    2,
                    22,
                    0
                ],
                "title": "Efficient Prompt Compression with Evaluator Heads for Long-Context\n  Transformer Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Prompt Compression with Evaluator Heads for Long-Context\n  Transformer Inference"
                },
                "summary": "Although applications involving long-context inputs are crucial for the\neffective utilization of large language models (LLMs), they also result in\nincreased computational costs and reduced performance. To address this\nchallenge, we propose an efficient, training-free prompt compression method\nthat retains key information within compressed prompts. We identify specific\nattention heads in transformer-based LLMs, which we designate as evaluator\nheads, that are capable of selecting tokens in long inputs that are most\nsignificant for inference. Building on this discovery, we develop EHPC, an\nEvaluator Head-based Prompt Compression method, which enables LLMs to rapidly\n\"skim through\" input prompts by leveraging only the first few layers with\nevaluator heads during the pre-filling stage, subsequently passing only the\nimportant tokens to the model for inference. EHPC achieves state-of-the-art\nresults across two mainstream benchmarks: prompt compression and long-context\ninference acceleration. Consequently, it effectively reduces the complexity and\ncosts associated with commercial API calls. We further demonstrate that EHPC\nattains competitive results compared to key-value cache-based acceleration\nmethods, thereby highlighting its potential to enhance the efficiency of LLMs\nfor long-context tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although applications involving long-context inputs are crucial for the\neffective utilization of large language models (LLMs), they also result in\nincreased computational costs and reduced performance. To address this\nchallenge, we propose an efficient, training-free prompt compression method\nthat retains key information within compressed prompts. We identify specific\nattention heads in transformer-based LLMs, which we designate as evaluator\nheads, that are capable of selecting tokens in long inputs that are most\nsignificant for inference. Building on this discovery, we develop EHPC, an\nEvaluator Head-based Prompt Compression method, which enables LLMs to rapidly\n\"skim through\" input prompts by leveraging only the first few layers with\nevaluator heads during the pre-filling stage, subsequently passing only the\nimportant tokens to the model for inference. EHPC achieves state-of-the-art\nresults across two mainstream benchmarks: prompt compression and long-context\ninference acceleration. Consequently, it effectively reduces the complexity and\ncosts associated with commercial API calls. We further demonstrate that EHPC\nattains competitive results compared to key-value cache-based acceleration\nmethods, thereby highlighting its potential to enhance the efficiency of LLMs\nfor long-context tasks."
                },
                "authors": [
                    {
                        "name": "Weizhi Fei"
                    },
                    {
                        "name": "Xueyan Niu"
                    },
                    {
                        "name": "Guoqing Xie"
                    },
                    {
                        "name": "Yingqing Liu"
                    },
                    {
                        "name": "Bo Bai"
                    },
                    {
                        "name": "Wei Han"
                    }
                ],
                "author_detail": {
                    "name": "Wei Han"
                },
                "author": "Wei Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12959v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12959v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12956v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12956v1",
                "updated": "2025-01-22T15:29:09Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    15,
                    29,
                    9,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T15:29:09Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    15,
                    29,
                    9,
                    2,
                    22,
                    0
                ],
                "title": "GANQ: GPU-Adaptive Non-Uniform Quantization for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GANQ: GPU-Adaptive Non-Uniform Quantization for Large Language Models"
                },
                "summary": "Large Language Models (LLMs) face significant deployment challenges due to\ntheir substantial resource requirements. While low-bit quantized weights can\nreduce memory usage and improve inference efficiency, current hardware lacks\nnative support for mixed-precision General Matrix Multiplication (mpGEMM),\nresulting in inefficient dequantization-based implementations. Moreover,\nuniform quantization methods often fail to capture weight distributions\nadequately, leading to performance degradation. We propose GANQ (GPU-Adaptive\nNon-Uniform Quantization), a layer-wise post-training non-uniform quantization\nframework optimized for hardware-efficient lookup table-based mpGEMM. GANQ\nachieves superior quantization performance by utilizing a training-free,\nGPU-adaptive optimization algorithm to efficiently reduce layer-wise\nquantization errors. Extensive experiments demonstrate GANQ's ability to reduce\nthe perplexity gap from the FP16 baseline compared to state-of-the-art methods\nfor both 3-bit and 4-bit quantization. Furthermore, when deployed on a single\nNVIDIA RTX 4090 GPU, GANQ's quantized models achieve up to 2.57$\\times$ speedup\nover the baseline, advancing memory and inference efficiency in LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) face significant deployment challenges due to\ntheir substantial resource requirements. While low-bit quantized weights can\nreduce memory usage and improve inference efficiency, current hardware lacks\nnative support for mixed-precision General Matrix Multiplication (mpGEMM),\nresulting in inefficient dequantization-based implementations. Moreover,\nuniform quantization methods often fail to capture weight distributions\nadequately, leading to performance degradation. We propose GANQ (GPU-Adaptive\nNon-Uniform Quantization), a layer-wise post-training non-uniform quantization\nframework optimized for hardware-efficient lookup table-based mpGEMM. GANQ\nachieves superior quantization performance by utilizing a training-free,\nGPU-adaptive optimization algorithm to efficiently reduce layer-wise\nquantization errors. Extensive experiments demonstrate GANQ's ability to reduce\nthe perplexity gap from the FP16 baseline compared to state-of-the-art methods\nfor both 3-bit and 4-bit quantization. Furthermore, when deployed on a single\nNVIDIA RTX 4090 GPU, GANQ's quantized models achieve up to 2.57$\\times$ speedup\nover the baseline, advancing memory and inference efficiency in LLM deployment."
                },
                "authors": [
                    {
                        "name": "Pengxiang Zhao"
                    },
                    {
                        "name": "Xiaoming Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoming Yuan"
                },
                "author": "Xiaoming Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12956v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12956v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12948v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12948v1",
                "updated": "2025-01-22T15:19:35Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    15,
                    19,
                    35,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T15:19:35Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    15,
                    19,
                    35,
                    2,
                    22,
                    0
                ],
                "title": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via\n  Reinforcement Learning"
                },
                "summary": "We introduce our first-generation reasoning models, DeepSeek-R1-Zero and\nDeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement\nlearning (RL) without supervised fine-tuning (SFT) as a preliminary step,\ndemonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero\nnaturally emerges with numerous powerful and intriguing reasoning behaviors.\nHowever, it encounters challenges such as poor readability, and language\nmixing. To address these issues and further enhance reasoning performance, we\nintroduce DeepSeek-R1, which incorporates multi-stage training and cold-start\ndata before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217\non reasoning tasks. To support the research community, we open-source\nDeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B,\n70B) distilled from DeepSeek-R1 based on Qwen and Llama.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce our first-generation reasoning models, DeepSeek-R1-Zero and\nDeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement\nlearning (RL) without supervised fine-tuning (SFT) as a preliminary step,\ndemonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero\nnaturally emerges with numerous powerful and intriguing reasoning behaviors.\nHowever, it encounters challenges such as poor readability, and language\nmixing. To address these issues and further enhance reasoning performance, we\nintroduce DeepSeek-R1, which incorporates multi-stage training and cold-start\ndata before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217\non reasoning tasks. To support the research community, we open-source\nDeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B,\n70B) distilled from DeepSeek-R1 based on Qwen and Llama."
                },
                "authors": [
                    {
                        "name": "DeepSeek-AI"
                    },
                    {
                        "name": "Daya Guo"
                    },
                    {
                        "name": "Dejian Yang"
                    },
                    {
                        "name": "Haowei Zhang"
                    },
                    {
                        "name": "Junxiao Song"
                    },
                    {
                        "name": "Ruoyu Zhang"
                    },
                    {
                        "name": "Runxin Xu"
                    },
                    {
                        "name": "Qihao Zhu"
                    },
                    {
                        "name": "Shirong Ma"
                    },
                    {
                        "name": "Peiyi Wang"
                    },
                    {
                        "name": "Xiao Bi"
                    },
                    {
                        "name": "Xiaokang Zhang"
                    },
                    {
                        "name": "Xingkai Yu"
                    },
                    {
                        "name": "Yu Wu"
                    },
                    {
                        "name": "Z. F. Wu"
                    },
                    {
                        "name": "Zhibin Gou"
                    },
                    {
                        "name": "Zhihong Shao"
                    },
                    {
                        "name": "Zhuoshu Li"
                    },
                    {
                        "name": "Ziyi Gao"
                    },
                    {
                        "name": "Aixin Liu"
                    },
                    {
                        "name": "Bing Xue"
                    },
                    {
                        "name": "Bingxuan Wang"
                    },
                    {
                        "name": "Bochao Wu"
                    },
                    {
                        "name": "Bei Feng"
                    },
                    {
                        "name": "Chengda Lu"
                    },
                    {
                        "name": "Chenggang Zhao"
                    },
                    {
                        "name": "Chengqi Deng"
                    },
                    {
                        "name": "Chenyu Zhang"
                    },
                    {
                        "name": "Chong Ruan"
                    },
                    {
                        "name": "Damai Dai"
                    },
                    {
                        "name": "Deli Chen"
                    },
                    {
                        "name": "Dongjie Ji"
                    },
                    {
                        "name": "Erhang Li"
                    },
                    {
                        "name": "Fangyun Lin"
                    },
                    {
                        "name": "Fucong Dai"
                    },
                    {
                        "name": "Fuli Luo"
                    },
                    {
                        "name": "Guangbo Hao"
                    },
                    {
                        "name": "Guanting Chen"
                    },
                    {
                        "name": "Guowei Li"
                    },
                    {
                        "name": "H. Zhang"
                    },
                    {
                        "name": "Han Bao"
                    },
                    {
                        "name": "Hanwei Xu"
                    },
                    {
                        "name": "Haocheng Wang"
                    },
                    {
                        "name": "Honghui Ding"
                    },
                    {
                        "name": "Huajian Xin"
                    },
                    {
                        "name": "Huazuo Gao"
                    },
                    {
                        "name": "Hui Qu"
                    },
                    {
                        "name": "Hui Li"
                    },
                    {
                        "name": "Jianzhong Guo"
                    },
                    {
                        "name": "Jiashi Li"
                    },
                    {
                        "name": "Jiawei Wang"
                    },
                    {
                        "name": "Jingchang Chen"
                    },
                    {
                        "name": "Jingyang Yuan"
                    },
                    {
                        "name": "Junjie Qiu"
                    },
                    {
                        "name": "Junlong Li"
                    },
                    {
                        "name": "J. L. Cai"
                    },
                    {
                        "name": "Jiaqi Ni"
                    },
                    {
                        "name": "Jian Liang"
                    },
                    {
                        "name": "Jin Chen"
                    },
                    {
                        "name": "Kai Dong"
                    },
                    {
                        "name": "Kai Hu"
                    },
                    {
                        "name": "Kaige Gao"
                    },
                    {
                        "name": "Kang Guan"
                    },
                    {
                        "name": "Kexin Huang"
                    },
                    {
                        "name": "Kuai Yu"
                    },
                    {
                        "name": "Lean Wang"
                    },
                    {
                        "name": "Lecong Zhang"
                    },
                    {
                        "name": "Liang Zhao"
                    },
                    {
                        "name": "Litong Wang"
                    },
                    {
                        "name": "Liyue Zhang"
                    },
                    {
                        "name": "Lei Xu"
                    },
                    {
                        "name": "Leyi Xia"
                    },
                    {
                        "name": "Mingchuan Zhang"
                    },
                    {
                        "name": "Minghua Zhang"
                    },
                    {
                        "name": "Minghui Tang"
                    },
                    {
                        "name": "Meng Li"
                    },
                    {
                        "name": "Miaojun Wang"
                    },
                    {
                        "name": "Mingming Li"
                    },
                    {
                        "name": "Ning Tian"
                    },
                    {
                        "name": "Panpan Huang"
                    },
                    {
                        "name": "Peng Zhang"
                    },
                    {
                        "name": "Qiancheng Wang"
                    },
                    {
                        "name": "Qinyu Chen"
                    },
                    {
                        "name": "Qiushi Du"
                    },
                    {
                        "name": "Ruiqi Ge"
                    },
                    {
                        "name": "Ruisong Zhang"
                    },
                    {
                        "name": "Ruizhe Pan"
                    },
                    {
                        "name": "Runji Wang"
                    },
                    {
                        "name": "R. J. Chen"
                    },
                    {
                        "name": "R. L. Jin"
                    },
                    {
                        "name": "Ruyi Chen"
                    },
                    {
                        "name": "Shanghao Lu"
                    },
                    {
                        "name": "Shangyan Zhou"
                    },
                    {
                        "name": "Shanhuang Chen"
                    },
                    {
                        "name": "Shengfeng Ye"
                    },
                    {
                        "name": "Shiyu Wang"
                    },
                    {
                        "name": "Shuiping Yu"
                    },
                    {
                        "name": "Shunfeng Zhou"
                    },
                    {
                        "name": "Shuting Pan"
                    },
                    {
                        "name": "S. S. Li"
                    },
                    {
                        "name": "Shuang Zhou"
                    },
                    {
                        "name": "Shaoqing Wu"
                    },
                    {
                        "name": "Shengfeng Ye"
                    },
                    {
                        "name": "Tao Yun"
                    },
                    {
                        "name": "Tian Pei"
                    },
                    {
                        "name": "Tianyu Sun"
                    },
                    {
                        "name": "T. Wang"
                    },
                    {
                        "name": "Wangding Zeng"
                    },
                    {
                        "name": "Wanjia Zhao"
                    },
                    {
                        "name": "Wen Liu"
                    },
                    {
                        "name": "Wenfeng Liang"
                    },
                    {
                        "name": "Wenjun Gao"
                    },
                    {
                        "name": "Wenqin Yu"
                    },
                    {
                        "name": "Wentao Zhang"
                    },
                    {
                        "name": "W. L. Xiao"
                    },
                    {
                        "name": "Wei An"
                    },
                    {
                        "name": "Xiaodong Liu"
                    },
                    {
                        "name": "Xiaohan Wang"
                    },
                    {
                        "name": "Xiaokang Chen"
                    },
                    {
                        "name": "Xiaotao Nie"
                    },
                    {
                        "name": "Xin Cheng"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Xin Xie"
                    },
                    {
                        "name": "Xingchao Liu"
                    },
                    {
                        "name": "Xinyu Yang"
                    },
                    {
                        "name": "Xinyuan Li"
                    },
                    {
                        "name": "Xuecheng Su"
                    },
                    {
                        "name": "Xuheng Lin"
                    },
                    {
                        "name": "X. Q. Li"
                    },
                    {
                        "name": "Xiangyue Jin"
                    },
                    {
                        "name": "Xiaojin Shen"
                    },
                    {
                        "name": "Xiaosha Chen"
                    },
                    {
                        "name": "Xiaowen Sun"
                    },
                    {
                        "name": "Xiaoxiang Wang"
                    },
                    {
                        "name": "Xinnan Song"
                    },
                    {
                        "name": "Xinyi Zhou"
                    },
                    {
                        "name": "Xianzu Wang"
                    },
                    {
                        "name": "Xinxia Shan"
                    },
                    {
                        "name": "Y. K. Li"
                    },
                    {
                        "name": "Y. Q. Wang"
                    },
                    {
                        "name": "Y. X. Wei"
                    },
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Yanhong Xu"
                    },
                    {
                        "name": "Yao Li"
                    },
                    {
                        "name": "Yao Zhao"
                    },
                    {
                        "name": "Yaofeng Sun"
                    },
                    {
                        "name": "Yaohui Wang"
                    },
                    {
                        "name": "Yi Yu"
                    },
                    {
                        "name": "Yichao Zhang"
                    },
                    {
                        "name": "Yifan Shi"
                    },
                    {
                        "name": "Yiliang Xiong"
                    },
                    {
                        "name": "Ying He"
                    },
                    {
                        "name": "Yishi Piao"
                    },
                    {
                        "name": "Yisong Wang"
                    },
                    {
                        "name": "Yixuan Tan"
                    },
                    {
                        "name": "Yiyang Ma"
                    },
                    {
                        "name": "Yiyuan Liu"
                    },
                    {
                        "name": "Yongqiang Guo"
                    },
                    {
                        "name": "Yuan Ou"
                    },
                    {
                        "name": "Yuduan Wang"
                    },
                    {
                        "name": "Yue Gong"
                    },
                    {
                        "name": "Yuheng Zou"
                    },
                    {
                        "name": "Yujia He"
                    },
                    {
                        "name": "Yunfan Xiong"
                    },
                    {
                        "name": "Yuxiang Luo"
                    },
                    {
                        "name": "Yuxiang You"
                    },
                    {
                        "name": "Yuxuan Liu"
                    },
                    {
                        "name": "Yuyang Zhou"
                    },
                    {
                        "name": "Y. X. Zhu"
                    },
                    {
                        "name": "Yanhong Xu"
                    },
                    {
                        "name": "Yanping Huang"
                    },
                    {
                        "name": "Yaohui Li"
                    },
                    {
                        "name": "Yi Zheng"
                    },
                    {
                        "name": "Yuchen Zhu"
                    },
                    {
                        "name": "Yunxian Ma"
                    },
                    {
                        "name": "Ying Tang"
                    },
                    {
                        "name": "Yukun Zha"
                    },
                    {
                        "name": "Yuting Yan"
                    },
                    {
                        "name": "Z. Z. Ren"
                    },
                    {
                        "name": "Zehui Ren"
                    },
                    {
                        "name": "Zhangli Sha"
                    },
                    {
                        "name": "Zhe Fu"
                    },
                    {
                        "name": "Zhean Xu"
                    },
                    {
                        "name": "Zhenda Xie"
                    },
                    {
                        "name": "Zhengyan Zhang"
                    },
                    {
                        "name": "Zhewen Hao"
                    },
                    {
                        "name": "Zhicheng Ma"
                    },
                    {
                        "name": "Zhigang Yan"
                    },
                    {
                        "name": "Zhiyu Wu"
                    },
                    {
                        "name": "Zihui Gu"
                    },
                    {
                        "name": "Zijia Zhu"
                    },
                    {
                        "name": "Zijun Liu"
                    },
                    {
                        "name": "Zilin Li"
                    },
                    {
                        "name": "Ziwei Xie"
                    },
                    {
                        "name": "Ziyang Song"
                    },
                    {
                        "name": "Zizheng Pan"
                    },
                    {
                        "name": "Zhen Huang"
                    },
                    {
                        "name": "Zhipeng Xu"
                    },
                    {
                        "name": "Zhongyu Zhang"
                    },
                    {
                        "name": "Zhen Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zhen Zhang"
                },
                "author": "Zhen Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12948v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12948v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01253v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01253v5",
                "updated": "2025-01-22T15:09:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    15,
                    9,
                    58,
                    2,
                    22,
                    0
                ],
                "published": "2024-12-02T08:22:56Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    8,
                    22,
                    56,
                    0,
                    337,
                    0
                ],
                "title": "Yi-Lightning Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yi-Lightning Technical Report"
                },
                "summary": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com."
                },
                "authors": [
                    {
                        "name": "Alan Wake"
                    },
                    {
                        "name": "Bei Chen"
                    },
                    {
                        "name": "C. X. Lv"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Chengen Huang"
                    },
                    {
                        "name": "Chenglin Cai"
                    },
                    {
                        "name": "Chujie Zheng"
                    },
                    {
                        "name": "Daniel Cooper"
                    },
                    {
                        "name": "Fan Zhou"
                    },
                    {
                        "name": "Feng Hu"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Guoyin Wang"
                    },
                    {
                        "name": "Heng Ji"
                    },
                    {
                        "name": "Howard Qiu"
                    },
                    {
                        "name": "Jiangcheng Zhu"
                    },
                    {
                        "name": "Jun Tian"
                    },
                    {
                        "name": "Katherine Su"
                    },
                    {
                        "name": "Lihuan Zhang"
                    },
                    {
                        "name": "Liying Li"
                    },
                    {
                        "name": "Ming Song"
                    },
                    {
                        "name": "Mou Li"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Qicheng Hu"
                    },
                    {
                        "name": "Shawn Wang"
                    },
                    {
                        "name": "Shijun Zhou"
                    },
                    {
                        "name": "Shiming Yang"
                    },
                    {
                        "name": "Shiyong Li"
                    },
                    {
                        "name": "Tianhang Zhu"
                    },
                    {
                        "name": "Wen Xie"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Xiang He"
                    },
                    {
                        "name": "Xiaobo Chen"
                    },
                    {
                        "name": "Xiaohui Hu"
                    },
                    {
                        "name": "Xiaoyi Ren"
                    },
                    {
                        "name": "Xinyao Niu"
                    },
                    {
                        "name": "Yanpeng Li"
                    },
                    {
                        "name": "Yongke Zhao"
                    },
                    {
                        "name": "Yongzhen Luo"
                    },
                    {
                        "name": "Yuchi Xu"
                    },
                    {
                        "name": "Yuxuan Sha"
                    },
                    {
                        "name": "Zhaodong Yan"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Zirui Zhang"
                    },
                    {
                        "name": "Zonghong Dai"
                    }
                ],
                "author_detail": {
                    "name": "Zonghong Dai"
                },
                "author": "Zonghong Dai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01253v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01253v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12934v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12934v1",
                "updated": "2025-01-22T15:04:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    15,
                    4,
                    13,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T15:04:13Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    15,
                    4,
                    13,
                    2,
                    22,
                    0
                ],
                "title": "Correctness Assessment of Code Generated by Large Language Models Using\n  Internal Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Correctness Assessment of Code Generated by Large Language Models Using\n  Internal Representations"
                },
                "summary": "Ensuring the correctness of code generated by Large Language Models (LLMs)\npresents a significant challenge in AI-driven software development. Existing\napproaches predominantly rely on black-box (closed-box) approaches that\nevaluate correctness post-generation, failing to utilize the rich insights\nembedded in the LLMs' internal states during code generation. In this paper, we\nintroduce OPENIA, a novel white-box (open-box) framework that leverages these\ninternal representations to assess the correctness of LLM-generated code.\nOPENIA systematically analyzes the intermediate states of representative\nopen-source LLMs specialized for code, including DeepSeek-Coder, CodeLlama, and\nMagicCoder, across diverse code generation benchmarks. Our empirical analysis\nreveals that these internal representations encode latent information, which\nstrongly correlates with the correctness of the generated code. Building on\nthese insights, OPENIA uses a white-box/open-box approach to make informed\npredictions about code correctness, offering significant advantages in\nadaptability and robustness over traditional classification-based methods and\nzero-shot approaches. Experimental results demonstrate that OPENIA consistently\noutperforms baseline models, achieving higher accuracy, precision, recall, and\nF1-Scores with up to a 2X improvement in standalone code generation and a 46%\nenhancement in repository-specific scenarios. By unlocking the potential of\nin-process signals, OPENIA paves the way for more proactive and efficient\nquality assurance mechanisms in LLM-assisted code generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring the correctness of code generated by Large Language Models (LLMs)\npresents a significant challenge in AI-driven software development. Existing\napproaches predominantly rely on black-box (closed-box) approaches that\nevaluate correctness post-generation, failing to utilize the rich insights\nembedded in the LLMs' internal states during code generation. In this paper, we\nintroduce OPENIA, a novel white-box (open-box) framework that leverages these\ninternal representations to assess the correctness of LLM-generated code.\nOPENIA systematically analyzes the intermediate states of representative\nopen-source LLMs specialized for code, including DeepSeek-Coder, CodeLlama, and\nMagicCoder, across diverse code generation benchmarks. Our empirical analysis\nreveals that these internal representations encode latent information, which\nstrongly correlates with the correctness of the generated code. Building on\nthese insights, OPENIA uses a white-box/open-box approach to make informed\npredictions about code correctness, offering significant advantages in\nadaptability and robustness over traditional classification-based methods and\nzero-shot approaches. Experimental results demonstrate that OPENIA consistently\noutperforms baseline models, achieving higher accuracy, precision, recall, and\nF1-Scores with up to a 2X improvement in standalone code generation and a 46%\nenhancement in repository-specific scenarios. By unlocking the potential of\nin-process signals, OPENIA paves the way for more proactive and efficient\nquality assurance mechanisms in LLM-assisted code generation."
                },
                "authors": [
                    {
                        "name": "Tuan-Dung Bui"
                    },
                    {
                        "name": "Thanh Trong Vu"
                    },
                    {
                        "name": "Thu-Trang Nguyen"
                    },
                    {
                        "name": "Son Nguyen"
                    },
                    {
                        "name": "Hieu Dinh Vo"
                    }
                ],
                "author_detail": {
                    "name": "Hieu Dinh Vo"
                },
                "author": "Hieu Dinh Vo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12934v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12934v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12925v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12925v1",
                "updated": "2025-01-22T14:54:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    14,
                    54,
                    25,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T14:54:25Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    14,
                    54,
                    25,
                    2,
                    22,
                    0
                ],
                "title": "A Denser Hydrogen Inferred from First-Principles Simulations Challenges\n  Jupiter's Interior Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Denser Hydrogen Inferred from First-Principles Simulations Challenges\n  Jupiter's Interior Models"
                },
                "summary": "First-principle modeling of dense hydrogen is crucial in materials and\nplanetary sciences. Despite its apparent simplicity, predicting the ionic and\nelectronic structure of hydrogen is a formidable challenge, and it is connected\nwith the insulator-to-metal transition, a century-old problem in condensed\nmatter. Accurate simulations of liquid hydrogen are also essential for modeling\ngas giant planets. Here we perform an exhaustive study of the equation of state\nof hydrogen using Density Functional Theory and quantum Monte Carlo\nsimulations. We find that the pressure predicted by Density Functional Theory\nmay vary qualitatively when using different functionals. The predictive power\nof first-principle simulations is restored by validating each functional\nagainst higher-level wavefunction theories, represented by computationally\nintensive variational and diffusion Monte Carlo calculations. Our simulations\nprovide evidence that hydrogen is denser at planetary conditions, compared to\ncurrently used equations of state. For Jupiter, this implies a lower bulk\nmetallicity (i.e., a smaller mass of heavy elements). Our results further\namplify the inconsistency between Jupiter's atmospheric metallicity measured by\nthe Galileo probe and the envelope metallicity inferred from interior models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "First-principle modeling of dense hydrogen is crucial in materials and\nplanetary sciences. Despite its apparent simplicity, predicting the ionic and\nelectronic structure of hydrogen is a formidable challenge, and it is connected\nwith the insulator-to-metal transition, a century-old problem in condensed\nmatter. Accurate simulations of liquid hydrogen are also essential for modeling\ngas giant planets. Here we perform an exhaustive study of the equation of state\nof hydrogen using Density Functional Theory and quantum Monte Carlo\nsimulations. We find that the pressure predicted by Density Functional Theory\nmay vary qualitatively when using different functionals. The predictive power\nof first-principle simulations is restored by validating each functional\nagainst higher-level wavefunction theories, represented by computationally\nintensive variational and diffusion Monte Carlo calculations. Our simulations\nprovide evidence that hydrogen is denser at planetary conditions, compared to\ncurrently used equations of state. For Jupiter, this implies a lower bulk\nmetallicity (i.e., a smaller mass of heavy elements). Our results further\namplify the inconsistency between Jupiter's atmospheric metallicity measured by\nthe Galileo probe and the envelope metallicity inferred from interior models."
                },
                "authors": [
                    {
                        "name": "Cesare Cozza"
                    },
                    {
                        "name": "Kousuke Nakano"
                    },
                    {
                        "name": "Saburo Howard"
                    },
                    {
                        "name": "Hao Xie"
                    },
                    {
                        "name": "Ravit Helled"
                    },
                    {
                        "name": "Guglielmo Mazzola"
                    }
                ],
                "author_detail": {
                    "name": "Guglielmo Mazzola"
                },
                "author": "Guglielmo Mazzola",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12925v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12925v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19291v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19291v2",
                "updated": "2025-01-22T14:50:33Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    14,
                    50,
                    33,
                    2,
                    22,
                    0
                ],
                "published": "2024-12-26T17:34:26Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    17,
                    34,
                    26,
                    3,
                    361,
                    0
                ],
                "title": "RAG with Differential Privacy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAG with Differential Privacy"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has emerged as the dominant technique to\nprovide \\emph{Large Language Models} (LLM) with fresh and relevant context,\nmitigating the risk of hallucinations and improving the overall quality of\nresponses in environments with large and fast moving knowledge bases. However,\nthe integration of external documents into the generation process raises\nsignificant privacy concerns. Indeed, when added to a prompt, it is not\npossible to guarantee a response will not inadvertently expose confidential\ndata, leading to potential breaches of privacy and ethical dilemmas. This paper\nexplores a practical solution to this problem suitable to general knowledge\nextraction from personal data. It shows \\emph{differentially private token\ngeneration} is a viable approach to private RAG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has emerged as the dominant technique to\nprovide \\emph{Large Language Models} (LLM) with fresh and relevant context,\nmitigating the risk of hallucinations and improving the overall quality of\nresponses in environments with large and fast moving knowledge bases. However,\nthe integration of external documents into the generation process raises\nsignificant privacy concerns. Indeed, when added to a prompt, it is not\npossible to guarantee a response will not inadvertently expose confidential\ndata, leading to potential breaches of privacy and ethical dilemmas. This paper\nexplores a practical solution to this problem suitable to general knowledge\nextraction from personal data. It shows \\emph{differentially private token\ngeneration} is a viable approach to private RAG."
                },
                "authors": [
                    {
                        "name": "Nicolas Grislain"
                    }
                ],
                "author_detail": {
                    "name": "Nicolas Grislain"
                },
                "author": "Nicolas Grislain",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19291v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19291v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12909v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12909v1",
                "updated": "2025-01-22T14:36:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    14,
                    36,
                    30,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T14:36:30Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    14,
                    36,
                    30,
                    2,
                    22,
                    0
                ],
                "title": "FilmAgent: A Multi-Agent Framework for End-to-End Film Automation in\n  Virtual 3D Spaces",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FilmAgent: A Multi-Agent Framework for End-to-End Film Automation in\n  Virtual 3D Spaces"
                },
                "summary": "Virtual film production requires intricate decision-making processes,\nincluding scriptwriting, virtual cinematography, and precise actor positioning\nand actions. Motivated by recent advances in automated decision-making with\nlanguage agent-based societies, this paper introduces FilmAgent, a novel\nLLM-based multi-agent collaborative framework for end-to-end film automation in\nour constructed 3D virtual spaces. FilmAgent simulates various crew roles,\nincluding directors, screenwriters, actors, and cinematographers, and covers\nkey stages of a film production workflow: (1) idea development transforms\nbrainstormed ideas into structured story outlines; (2) scriptwriting elaborates\non dialogue and character actions for each scene; (3) cinematography determines\nthe camera setups for each shot. A team of agents collaborates through\niterative feedback and revisions, thereby verifying intermediate scripts and\nreducing hallucinations. We evaluate the generated videos on 15 ideas and 4 key\naspects. Human evaluation shows that FilmAgent outperforms all baselines across\nall aspects and scores 3.98 out of 5 on average, showing the feasibility of\nmulti-agent collaboration in filmmaking. Further analysis reveals that\nFilmAgent, despite using the less advanced GPT-4o model, surpasses the\nsingle-agent o1, showing the advantage of a well-coordinated multi-agent\nsystem. Lastly, we discuss the complementary strengths and weaknesses of\nOpenAI's text-to-video model Sora and our FilmAgent in filmmaking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Virtual film production requires intricate decision-making processes,\nincluding scriptwriting, virtual cinematography, and precise actor positioning\nand actions. Motivated by recent advances in automated decision-making with\nlanguage agent-based societies, this paper introduces FilmAgent, a novel\nLLM-based multi-agent collaborative framework for end-to-end film automation in\nour constructed 3D virtual spaces. FilmAgent simulates various crew roles,\nincluding directors, screenwriters, actors, and cinematographers, and covers\nkey stages of a film production workflow: (1) idea development transforms\nbrainstormed ideas into structured story outlines; (2) scriptwriting elaborates\non dialogue and character actions for each scene; (3) cinematography determines\nthe camera setups for each shot. A team of agents collaborates through\niterative feedback and revisions, thereby verifying intermediate scripts and\nreducing hallucinations. We evaluate the generated videos on 15 ideas and 4 key\naspects. Human evaluation shows that FilmAgent outperforms all baselines across\nall aspects and scores 3.98 out of 5 on average, showing the feasibility of\nmulti-agent collaboration in filmmaking. Further analysis reveals that\nFilmAgent, despite using the less advanced GPT-4o model, surpasses the\nsingle-agent o1, showing the advantage of a well-coordinated multi-agent\nsystem. Lastly, we discuss the complementary strengths and weaknesses of\nOpenAI's text-to-video model Sora and our FilmAgent in filmmaking."
                },
                "authors": [
                    {
                        "name": "Zhenran Xu"
                    },
                    {
                        "name": "Longyue Wang"
                    },
                    {
                        "name": "Jifang Wang"
                    },
                    {
                        "name": "Zhouyi Li"
                    },
                    {
                        "name": "Senbao Shi"
                    },
                    {
                        "name": "Xue Yang"
                    },
                    {
                        "name": "Yiyu Wang"
                    },
                    {
                        "name": "Baotian Hu"
                    },
                    {
                        "name": "Jun Yu"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "Work in progress. Project Page: https://filmagent.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12909v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12909v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10994v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10994v2",
                "updated": "2025-01-22T14:33:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    14,
                    33,
                    23,
                    2,
                    22,
                    0
                ],
                "published": "2024-06-24T12:09:34Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    12,
                    9,
                    34,
                    0,
                    176,
                    0
                ],
                "title": "Panza: Design and Analysis of a Fully-Local Personalized Text Writing\n  Assistant",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Panza: Design and Analysis of a Fully-Local Personalized Text Writing\n  Assistant"
                },
                "summary": "The availability of powerful open-source large language models (LLMs) opens\nexciting use cases, such as automated personal assistants that adapt to the\nuser's unique data and demands. Two key requirements for such assistants are\npersonalization - in the sense that the assistant should reflect the user's own\nwriting style - and privacy - users may prefer to always store their personal\ndata locally, on their own computing device. In this application paper, we\npresent a new design and evaluation for such an automated assistant, for the\nspecific use case of email generation, which we call Panza. Specifically, Panza\ncan be trained and deployed locally on commodity hardware, and is personalized\nto the user's writing style. Panza's personalization features are based on a\ncombination of fine-tuning using a variant of the Reverse Instructions\ntechnique together with Retrieval-Augmented Generation (RAG). We demonstrate\nthat this combination allows us to fine-tune an LLM to better reflect a user's\nwriting style using limited data, while executing on extremely limited\nresources, e.g. on a free Google Colab instance. Our key methodological\ncontribution is what we believe to be the first detailed study of evaluation\nmetrics for this personalized writing task, and of how different choices of\nsystem components - e.g. the use of RAG and of different fine-tuning approaches\n- impact the system's performance. We are releasing the full Panza code as well\nas a new \"David\" personalized email dataset licensed for research use, both\navailable on https://github.com/IST-DASLab/PanzaMail.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The availability of powerful open-source large language models (LLMs) opens\nexciting use cases, such as automated personal assistants that adapt to the\nuser's unique data and demands. Two key requirements for such assistants are\npersonalization - in the sense that the assistant should reflect the user's own\nwriting style - and privacy - users may prefer to always store their personal\ndata locally, on their own computing device. In this application paper, we\npresent a new design and evaluation for such an automated assistant, for the\nspecific use case of email generation, which we call Panza. Specifically, Panza\ncan be trained and deployed locally on commodity hardware, and is personalized\nto the user's writing style. Panza's personalization features are based on a\ncombination of fine-tuning using a variant of the Reverse Instructions\ntechnique together with Retrieval-Augmented Generation (RAG). We demonstrate\nthat this combination allows us to fine-tune an LLM to better reflect a user's\nwriting style using limited data, while executing on extremely limited\nresources, e.g. on a free Google Colab instance. Our key methodological\ncontribution is what we believe to be the first detailed study of evaluation\nmetrics for this personalized writing task, and of how different choices of\nsystem components - e.g. the use of RAG and of different fine-tuning approaches\n- impact the system's performance. We are releasing the full Panza code as well\nas a new \"David\" personalized email dataset licensed for research use, both\navailable on https://github.com/IST-DASLab/PanzaMail."
                },
                "authors": [
                    {
                        "name": "Armand Nicolicioiu"
                    },
                    {
                        "name": "Eugenia Iofinova"
                    },
                    {
                        "name": "Eldar Kurtic"
                    },
                    {
                        "name": "Mahdi Nikdan"
                    },
                    {
                        "name": "Andrei Panferov"
                    },
                    {
                        "name": "Ilia Markov"
                    },
                    {
                        "name": "Nir Shavit"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "arxiv_comment": "Panza is available at https://github.com/IST-DASLab/PanzaMail",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10994v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10994v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11935v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11935v2",
                "updated": "2025-01-22T14:31:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    14,
                    31,
                    48,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-21T07:16:18Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    7,
                    16,
                    18,
                    1,
                    21,
                    0
                ],
                "title": "Web vs. LLMs: An Empirical Study of Learning Behaviors of CS2 Students",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Web vs. LLMs: An Empirical Study of Learning Behaviors of CS2 Students"
                },
                "summary": "LLMs such as ChatGPT have been widely adopted by students in higher education\nas tools for learning programming and related concepts. However, it remains\nunclear how effective students are and what strategies students use while\nlearning with LLMs. Since the majority of students' experiences in online\nself-learning have come through using search engines such as Google, evaluating\nAI tools in this context can help us address these gaps. In this mixed methods\nresearch, we conducted an exploratory within-subjects study to understand how\nCS2 students learn programming concepts using both LLMs as well as traditional\nonline methods such as educational websites and videos to examine how students\napproach learning within and across both scenarios. We discovered that students\nfound it easier to learn a more difficult concept using traditional methods\nthan using ChatGPT. We also found that students ask fewer follow-ups and use\nmore keyword-based queries for search engines while their prompts to LLMs tend\nto explicitly ask for information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs such as ChatGPT have been widely adopted by students in higher education\nas tools for learning programming and related concepts. However, it remains\nunclear how effective students are and what strategies students use while\nlearning with LLMs. Since the majority of students' experiences in online\nself-learning have come through using search engines such as Google, evaluating\nAI tools in this context can help us address these gaps. In this mixed methods\nresearch, we conducted an exploratory within-subjects study to understand how\nCS2 students learn programming concepts using both LLMs as well as traditional\nonline methods such as educational websites and videos to examine how students\napproach learning within and across both scenarios. We discovered that students\nfound it easier to learn a more difficult concept using traditional methods\nthan using ChatGPT. We also found that students ask fewer follow-ups and use\nmore keyword-based queries for search engines while their prompts to LLMs tend\nto explicitly ask for information."
                },
                "authors": [
                    {
                        "name": "Aayush Kumar"
                    },
                    {
                        "name": "Daniel Prol"
                    },
                    {
                        "name": "Amin Alipour"
                    },
                    {
                        "name": "Sruti Srinivasa Ragavan"
                    }
                ],
                "author_detail": {
                    "name": "Sruti Srinivasa Ragavan"
                },
                "author": "Sruti Srinivasa Ragavan",
                "arxiv_comment": "7 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11935v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11935v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12904v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12904v1",
                "updated": "2025-01-22T14:30:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    14,
                    30,
                    40,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T14:30:40Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    14,
                    30,
                    40,
                    2,
                    22,
                    0
                ],
                "title": "A Functional Software Reference Architecture for LLM-Integrated Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Functional Software Reference Architecture for LLM-Integrated Systems"
                },
                "summary": "The integration of large language models into software systems is\ntransforming capabilities such as natural language understanding,\ndecision-making, and autonomous task execution. However, the absence of a\ncommonly accepted software reference architecture hinders systematic reasoning\nabout their design and quality attributes. This gap makes it challenging to\naddress critical concerns like privacy, security, modularity, and\ninteroperability, which are increasingly important as these systems grow in\ncomplexity and societal impact. In this paper, we describe our\n\\textit{emerging} results for a preliminary functional reference architecture\nas a conceptual framework to address these challenges and guide the design,\nevaluation, and evolution of large language model-integrated systems. We\nidentify key architectural concerns for these systems, informed by current\nresearch and practice. We then evaluate how the architecture addresses these\nconcerns and validate its applicability using three open-source large language\nmodel-integrated systems in computer vision, text processing, and coding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of large language models into software systems is\ntransforming capabilities such as natural language understanding,\ndecision-making, and autonomous task execution. However, the absence of a\ncommonly accepted software reference architecture hinders systematic reasoning\nabout their design and quality attributes. This gap makes it challenging to\naddress critical concerns like privacy, security, modularity, and\ninteroperability, which are increasingly important as these systems grow in\ncomplexity and societal impact. In this paper, we describe our\n\\textit{emerging} results for a preliminary functional reference architecture\nas a conceptual framework to address these challenges and guide the design,\nevaluation, and evolution of large language model-integrated systems. We\nidentify key architectural concerns for these systems, informed by current\nresearch and practice. We then evaluate how the architecture addresses these\nconcerns and validate its applicability using three open-source large language\nmodel-integrated systems in computer vision, text processing, and coding."
                },
                "authors": [
                    {
                        "name": "Alessio Bucaioni"
                    },
                    {
                        "name": "Martin Weyssow"
                    },
                    {
                        "name": "Junda He"
                    },
                    {
                        "name": "Yunbo Lyu"
                    },
                    {
                        "name": "David Lo"
                    }
                ],
                "author_detail": {
                    "name": "David Lo"
                },
                "author": "David Lo",
                "arxiv_comment": "Accepted for publication at the 22nd IEEE International Conference on\n  Software Architecture (ICSA 2025) - New and Emerging Ideas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12904v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12904v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12895v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12895v1",
                "updated": "2025-01-22T14:15:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    14,
                    15,
                    46,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T14:15:46Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    14,
                    15,
                    46,
                    2,
                    22,
                    0
                ],
                "title": "Test-Time Preference Optimization: On-the-Fly Alignment via Iterative\n  Textual Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-Time Preference Optimization: On-the-Fly Alignment via Iterative\n  Textual Feedback"
                },
                "summary": "Large language models (LLMs) demonstrate impressive performance but lack the\nflexibility to adapt to human preferences quickly without retraining. In this\nwork, we introduce Test-time Preference Optimization (TPO), a framework that\naligns LLM outputs with human preferences during inference, removing the need\nto update model parameters. Rather than relying on purely numerical rewards,\nTPO translates reward signals into textual critiques and uses them as textual\nrewards to iteratively refine its response. Evaluations on benchmarks covering\ninstruction following, preference alignment, safety, and mathematics reveal\nthat TPO progressively improves alignment with human preferences. Notably,\nafter only a few TPO steps, the initially unaligned Llama-3.1-70B-SFT model can\nsurpass the aligned counterpart, Llama-3.1-70B-Instruct. Furthermore, TPO\nscales efficiently with both the search width and depth during inference.\nThrough case studies, we illustrate how TPO exploits the innate capacity of LLM\nto interpret and act upon reward signals. Our findings establish TPO as a\npractical, lightweight alternative for test-time preference optimization,\nachieving alignment on the fly. Our code is publicly available at\nhttps://github.com/yafuly/TPO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate impressive performance but lack the\nflexibility to adapt to human preferences quickly without retraining. In this\nwork, we introduce Test-time Preference Optimization (TPO), a framework that\naligns LLM outputs with human preferences during inference, removing the need\nto update model parameters. Rather than relying on purely numerical rewards,\nTPO translates reward signals into textual critiques and uses them as textual\nrewards to iteratively refine its response. Evaluations on benchmarks covering\ninstruction following, preference alignment, safety, and mathematics reveal\nthat TPO progressively improves alignment with human preferences. Notably,\nafter only a few TPO steps, the initially unaligned Llama-3.1-70B-SFT model can\nsurpass the aligned counterpart, Llama-3.1-70B-Instruct. Furthermore, TPO\nscales efficiently with both the search width and depth during inference.\nThrough case studies, we illustrate how TPO exploits the innate capacity of LLM\nto interpret and act upon reward signals. Our findings establish TPO as a\npractical, lightweight alternative for test-time preference optimization,\nachieving alignment on the fly. Our code is publicly available at\nhttps://github.com/yafuly/TPO."
                },
                "authors": [
                    {
                        "name": "Yafu Li"
                    },
                    {
                        "name": "Xuyang Hu"
                    },
                    {
                        "name": "Xiaoye Qu"
                    },
                    {
                        "name": "Linjie Li"
                    },
                    {
                        "name": "Yu Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Yu Cheng"
                },
                "author": "Yu Cheng",
                "arxiv_comment": "43 pages; work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12895v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12895v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11036v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11036v2",
                "updated": "2025-01-22T13:53:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    13,
                    53,
                    59,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-19T13:06:51Z",
                "published_parsed": [
                    2025,
                    1,
                    19,
                    13,
                    6,
                    51,
                    6,
                    19,
                    0
                ],
                "title": "LF-Steering: Latent Feature Activation Steering for Enhancing Semantic\n  Consistency in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LF-Steering: Latent Feature Activation Steering for Enhancing Semantic\n  Consistency in Large Language Models"
                },
                "summary": "Large Language Models (LLMs) often generate inconsistent responses when\nprompted with semantically equivalent paraphrased inputs. Recently, activation\nsteering, a technique that modulates LLMs' behaviours by adjusting their latent\nrepresentations during inference time, has been explored to improve the\nsemantic consistency of LLMs. However, these methods typically operate at the\nmodel component level, such as layer hidden states or attention head outputs.\nThey face a challenge due to the ``polysemanticity issue'', where the model\ncomponents of LLMs typically encode multiple entangled features, making precise\nsteering difficult. To address this challenge, we drill down to feature-level\nrepresentations and propose LF-Steering, a novel activation steering approach\nto precisely identify latent feature representations responsible for semantic\ninconsistency. More specifically, our method maps the hidden states of the\nrelevant transformer layer into a sparsely activated, high-dimensional feature\nspace based on a sparse autoencoder (SAE), ensuring model steering based on\ndecoupled feature representations with minimal interference. Comprehensive\nexperiments on NLU and NLG datasets demonstrate the effectiveness of our method\nin enhancing semantic consistency, resulting in significant performance gains\nfor various NLU and NLG tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) often generate inconsistent responses when\nprompted with semantically equivalent paraphrased inputs. Recently, activation\nsteering, a technique that modulates LLMs' behaviours by adjusting their latent\nrepresentations during inference time, has been explored to improve the\nsemantic consistency of LLMs. However, these methods typically operate at the\nmodel component level, such as layer hidden states or attention head outputs.\nThey face a challenge due to the ``polysemanticity issue'', where the model\ncomponents of LLMs typically encode multiple entangled features, making precise\nsteering difficult. To address this challenge, we drill down to feature-level\nrepresentations and propose LF-Steering, a novel activation steering approach\nto precisely identify latent feature representations responsible for semantic\ninconsistency. More specifically, our method maps the hidden states of the\nrelevant transformer layer into a sparsely activated, high-dimensional feature\nspace based on a sparse autoencoder (SAE), ensuring model steering based on\ndecoupled feature representations with minimal interference. Comprehensive\nexperiments on NLU and NLG datasets demonstrate the effectiveness of our method\nin enhancing semantic consistency, resulting in significant performance gains\nfor various NLU and NLG tasks."
                },
                "authors": [
                    {
                        "name": "Jingyuan Yang"
                    },
                    {
                        "name": "Rongjun Li"
                    },
                    {
                        "name": "Weixuan Wang"
                    },
                    {
                        "name": "Ziyu Zhou"
                    },
                    {
                        "name": "Zhiyong Feng"
                    },
                    {
                        "name": "Wei Peng"
                    }
                ],
                "author_detail": {
                    "name": "Wei Peng"
                },
                "author": "Wei Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11036v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11036v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12883v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12883v1",
                "updated": "2025-01-22T13:44:44Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    13,
                    44,
                    44,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T13:44:44Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    13,
                    44,
                    44,
                    2,
                    22,
                    0
                ],
                "title": "Generative AI Misuse Potential in Cyber Security Education: A Case Study\n  of a UK Degree Program",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI Misuse Potential in Cyber Security Education: A Case Study\n  of a UK Degree Program"
                },
                "summary": "Recent advances in generative artificial intelligence (AI), such as ChatGPT,\nGoogle Gemini, and other large language models (LLMs), pose significant\nchallenges to upholding academic integrity in higher education. This paper\ninvestigates the susceptibility of a Master's-level cyber security degree\nprogram at a UK Russell Group university, accredited by a leading national\nbody, to LLM misuse. Through the application and extension of a quantitative\nassessment framework, we identify a high exposure to misuse, particularly in\nindependent project- and report-based assessments. Contributing factors,\nincluding block teaching and a predominantly international cohort, are\nhighlighted as potential amplifiers of these vulnerabilities. To address these\nchallenges, we discuss the adoption of LLM-resistant assessments, detection\ntools, and the importance of fostering an ethical learning environment. These\napproaches aim to uphold academic standards while preparing students for the\ncomplexities of real-world cyber security.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in generative artificial intelligence (AI), such as ChatGPT,\nGoogle Gemini, and other large language models (LLMs), pose significant\nchallenges to upholding academic integrity in higher education. This paper\ninvestigates the susceptibility of a Master's-level cyber security degree\nprogram at a UK Russell Group university, accredited by a leading national\nbody, to LLM misuse. Through the application and extension of a quantitative\nassessment framework, we identify a high exposure to misuse, particularly in\nindependent project- and report-based assessments. Contributing factors,\nincluding block teaching and a predominantly international cohort, are\nhighlighted as potential amplifiers of these vulnerabilities. To address these\nchallenges, we discuss the adoption of LLM-resistant assessments, detection\ntools, and the importance of fostering an ethical learning environment. These\napproaches aim to uphold academic standards while preparing students for the\ncomplexities of real-world cyber security."
                },
                "authors": [
                    {
                        "name": "Carlton Shepherd"
                    }
                ],
                "author_detail": {
                    "name": "Carlton Shepherd"
                },
                "author": "Carlton Shepherd",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12883v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12883v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09957v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09957v2",
                "updated": "2025-01-22T13:42:26Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    13,
                    42,
                    26,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-17T05:19:14Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    5,
                    19,
                    14,
                    4,
                    17,
                    0
                ],
                "title": "FRAG: A Flexible Modular Framework for Retrieval-Augmented Generation\n  based on Knowledge Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FRAG: A Flexible Modular Framework for Retrieval-Augmented Generation\n  based on Knowledge Graphs"
                },
                "summary": "To mitigate the hallucination and knowledge deficiency in large language\nmodels (LLMs), Knowledge Graph (KG)-based Retrieval-Augmented Generation (RAG)\nhas shown promising potential by utilizing KGs as external resource to enhance\nLLMs reasoning. However, existing KG-RAG approaches struggle with a trade-off\nbetween flexibility and retrieval quality. Modular methods prioritize\nflexibility by avoiding the use of KG-fine-tuned models during retrieval,\nleading to fixed retrieval strategies and suboptimal retrieval quality.\nConversely, coupled methods embed KG information within models to improve\nretrieval quality, but at the expense of flexibility. In this paper, we propose\na novel flexible modular KG-RAG framework, termed FRAG, which synergizes the\nadvantages of both approaches. FRAG estimates the hop range of reasoning paths\nbased solely on the query and classify it as either simple or complex. To match\nthe complexity of the query, tailored pipelines are applied to ensure efficient\nand accurate reasoning path retrieval, thus fostering the final reasoning\nprocess. By using the query text instead of the KG to infer the structural\ninformation of reasoning paths and employing adaptable retrieval strategies,\nFRAG improves retrieval quality while maintaining flexibility. Moreover, FRAG\ndoes not require extra LLMs fine-tuning or calls, significantly boosting\nefficiency and conserving resources. Extensive experiments show that FRAG\nachieves state-of-the-art performance with high efficiency and low resource\nconsumption.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To mitigate the hallucination and knowledge deficiency in large language\nmodels (LLMs), Knowledge Graph (KG)-based Retrieval-Augmented Generation (RAG)\nhas shown promising potential by utilizing KGs as external resource to enhance\nLLMs reasoning. However, existing KG-RAG approaches struggle with a trade-off\nbetween flexibility and retrieval quality. Modular methods prioritize\nflexibility by avoiding the use of KG-fine-tuned models during retrieval,\nleading to fixed retrieval strategies and suboptimal retrieval quality.\nConversely, coupled methods embed KG information within models to improve\nretrieval quality, but at the expense of flexibility. In this paper, we propose\na novel flexible modular KG-RAG framework, termed FRAG, which synergizes the\nadvantages of both approaches. FRAG estimates the hop range of reasoning paths\nbased solely on the query and classify it as either simple or complex. To match\nthe complexity of the query, tailored pipelines are applied to ensure efficient\nand accurate reasoning path retrieval, thus fostering the final reasoning\nprocess. By using the query text instead of the KG to infer the structural\ninformation of reasoning paths and employing adaptable retrieval strategies,\nFRAG improves retrieval quality while maintaining flexibility. Moreover, FRAG\ndoes not require extra LLMs fine-tuning or calls, significantly boosting\nefficiency and conserving resources. Extensive experiments show that FRAG\nachieves state-of-the-art performance with high efficiency and low resource\nconsumption."
                },
                "authors": [
                    {
                        "name": "Zengyi Gao"
                    },
                    {
                        "name": "Yukun Cao"
                    },
                    {
                        "name": "Hairu Wang"
                    },
                    {
                        "name": "Ao Ke"
                    },
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Xike Xie"
                    },
                    {
                        "name": "S Kevin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "S Kevin Zhou"
                },
                "author": "S Kevin Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09957v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09957v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12880v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12880v1",
                "updated": "2025-01-22T13:40:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    13,
                    40,
                    43,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T13:40:43Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    13,
                    40,
                    43,
                    2,
                    22,
                    0
                ],
                "title": "Advanced deep architecture pruning using single filter performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced deep architecture pruning using single filter performance"
                },
                "summary": "Pruning the parameters and structure of neural networks reduces the\ncomputational complexity, energy consumption, and latency during inference.\nRecently, a novel underlying mechanism for successful deep learning (DL) was\npresented based on a method that quantitatively measures the single filter\nperformance in each layer of a DL architecture, and a new comprehensive\nmechanism of how deep learning works was presented. Herein, we demonstrate how\nthis understanding paves the path to highly dilute the convolutional layers of\ndeep architectures without affecting their overall accuracy using applied\nfilter cluster connections (AFCC). AFCC is exemplified on VGG-11 and\nEfficientNet-B0 architectures trained on CIFAR-100, and its high pruning\noutperforms other techniques using the same pruning magnitude. Additionally,\nthis technique is broadened to single nodal performance and highly pruning of\nfully connected layers, suggesting a possible implementation to considerably\nreduce the complexity of over-parameterized AI tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pruning the parameters and structure of neural networks reduces the\ncomputational complexity, energy consumption, and latency during inference.\nRecently, a novel underlying mechanism for successful deep learning (DL) was\npresented based on a method that quantitatively measures the single filter\nperformance in each layer of a DL architecture, and a new comprehensive\nmechanism of how deep learning works was presented. Herein, we demonstrate how\nthis understanding paves the path to highly dilute the convolutional layers of\ndeep architectures without affecting their overall accuracy using applied\nfilter cluster connections (AFCC). AFCC is exemplified on VGG-11 and\nEfficientNet-B0 architectures trained on CIFAR-100, and its high pruning\noutperforms other techniques using the same pruning magnitude. Additionally,\nthis technique is broadened to single nodal performance and highly pruning of\nfully connected layers, suggesting a possible implementation to considerably\nreduce the complexity of over-parameterized AI tasks."
                },
                "authors": [
                    {
                        "name": "Yarden Tzach"
                    },
                    {
                        "name": "Yuval Meir"
                    },
                    {
                        "name": "Ronit D. Gross"
                    },
                    {
                        "name": "Ofek Tevet"
                    },
                    {
                        "name": "Ella Koresh"
                    },
                    {
                        "name": "Ido Kanter"
                    }
                ],
                "author_detail": {
                    "name": "Ido Kanter"
                },
                "author": "Ido Kanter",
                "arxiv_comment": "22 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12880v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12880v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12877v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12877v1",
                "updated": "2025-01-22T13:36:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    13,
                    36,
                    46,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T13:36:46Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    13,
                    36,
                    46,
                    2,
                    22,
                    0
                ],
                "title": "WisdomBot: Tuning Large Language Models with Artificial Intelligence\n  Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WisdomBot: Tuning Large Language Models with Artificial Intelligence\n  Knowledge"
                },
                "summary": "Large language models (LLMs) have emerged as powerful tools in natural\nlanguage processing (NLP), showing a promising future of artificial generated\nintelligence (AGI). Despite their notable performance in the general domain,\nLLMs have remained suboptimal in the field of education, owing to the unique\nchallenges presented by this domain, such as the need for more specialized\nknowledge, the requirement for personalized learning experiences, and the\nnecessity for concise explanations of complex concepts. To address these\nissues, this paper presents a novel LLM for education named WisdomBot, which\ncombines the power of LLMs with educational theories, enabling their seamless\nintegration into educational contexts. To be specific, we harness\nself-instructed knowledge concepts and instructions under the guidance of\nBloom's Taxonomy as training data. To further enhance the accuracy and\nprofessionalism of model's response on factual questions, we introduce two key\nenhancements during inference, i.e., local knowledge base retrieval\naugmentation and search engine retrieval augmentation during inference. We\nsubstantiate the effectiveness of our approach by applying it to several\nChinese LLMs, thereby showcasing that the fine-tuned models can generate more\nreliable and professional responses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have emerged as powerful tools in natural\nlanguage processing (NLP), showing a promising future of artificial generated\nintelligence (AGI). Despite their notable performance in the general domain,\nLLMs have remained suboptimal in the field of education, owing to the unique\nchallenges presented by this domain, such as the need for more specialized\nknowledge, the requirement for personalized learning experiences, and the\nnecessity for concise explanations of complex concepts. To address these\nissues, this paper presents a novel LLM for education named WisdomBot, which\ncombines the power of LLMs with educational theories, enabling their seamless\nintegration into educational contexts. To be specific, we harness\nself-instructed knowledge concepts and instructions under the guidance of\nBloom's Taxonomy as training data. To further enhance the accuracy and\nprofessionalism of model's response on factual questions, we introduce two key\nenhancements during inference, i.e., local knowledge base retrieval\naugmentation and search engine retrieval augmentation during inference. We\nsubstantiate the effectiveness of our approach by applying it to several\nChinese LLMs, thereby showcasing that the fine-tuned models can generate more\nreliable and professional responses."
                },
                "authors": [
                    {
                        "name": "Jingyuan Chen"
                    },
                    {
                        "name": "Tao Wu"
                    },
                    {
                        "name": "Wei Ji"
                    },
                    {
                        "name": "Fei Wu"
                    }
                ],
                "author_detail": {
                    "name": "Fei Wu"
                },
                "author": "Fei Wu",
                "arxiv_comment": "Frontiers of Digital Education",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12877v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12877v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03948v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03948v2",
                "updated": "2025-01-22T13:35:26Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    13,
                    35,
                    26,
                    2,
                    22,
                    0
                ],
                "published": "2024-11-06T14:29:49Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    14,
                    29,
                    49,
                    2,
                    311,
                    0
                ],
                "title": "Long-Form Text-to-Music Generation with Adaptive Prompts: A Case of\n  Study in Tabletop Role-Playing Games Soundtracks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-Form Text-to-Music Generation with Adaptive Prompts: A Case of\n  Study in Tabletop Role-Playing Games Soundtracks"
                },
                "summary": "This paper investigates the capabilities of text-to-audio music generation\nmodels in producing long-form music with prompts that change over time,\nfocusing on soundtrack generation for Tabletop Role-Playing Games (TRPGs). We\nintroduce Babel Bardo, a system that uses Large Language Models (LLMs) to\ntransform speech transcriptions into music descriptions for controlling a\ntext-to-music model. Four versions of Babel Bardo were compared in two TRPG\ncampaigns: a baseline using direct speech transcriptions, and three LLM-based\nversions with varying approaches to music description generation. Evaluations\nconsidered audio quality, story alignment, and transition smoothness. Results\nindicate that detailed music descriptions improve audio quality while\nmaintaining consistency across consecutive descriptions enhances story\nalignment and transition smoothness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the capabilities of text-to-audio music generation\nmodels in producing long-form music with prompts that change over time,\nfocusing on soundtrack generation for Tabletop Role-Playing Games (TRPGs). We\nintroduce Babel Bardo, a system that uses Large Language Models (LLMs) to\ntransform speech transcriptions into music descriptions for controlling a\ntext-to-music model. Four versions of Babel Bardo were compared in two TRPG\ncampaigns: a baseline using direct speech transcriptions, and three LLM-based\nversions with varying approaches to music description generation. Evaluations\nconsidered audio quality, story alignment, and transition smoothness. Results\nindicate that detailed music descriptions improve audio quality while\nmaintaining consistency across consecutive descriptions enhances story\nalignment and transition smoothness."
                },
                "authors": [
                    {
                        "name": "Felipe Marra"
                    },
                    {
                        "name": "Lucas N. Ferreira"
                    }
                ],
                "author_detail": {
                    "name": "Lucas N. Ferreira"
                },
                "author": "Lucas N. Ferreira",
                "arxiv_comment": "Paper accepted at the LAMIR 2024 workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03948v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03948v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11885v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11885v2",
                "updated": "2025-01-22T13:32:29Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    13,
                    32,
                    29,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-21T04:40:43Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    4,
                    40,
                    43,
                    1,
                    21,
                    0
                ],
                "title": "Med-R$^2$: Crafting Trustworthy LLM Physicians through Retrieval and\n  Reasoning of Evidence-Based Medicine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Med-R$^2$: Crafting Trustworthy LLM Physicians through Retrieval and\n  Reasoning of Evidence-Based Medicine"
                },
                "summary": "In recent years, Large Language Models (LLMs) have exhibited remarkable\ncapabilities in clinical scenarios. However, despite their potential, existing\nworks face challenges when applying LLMs to medical settings. Strategies\nrelying on training with medical datasets are highly cost-intensive and may\nsuffer from outdated training data. Leveraging external knowledge bases is a\nsuitable alternative, yet it faces obstacles such as limited retrieval\nprecision and poor effectiveness in answer extraction. These issues\ncollectively prevent LLMs from demonstrating the expected level of proficiency\nin mastering medical expertise. To address these challenges, we introduce\nMed-R^2, a novel LLM physician framework that adheres to the Evidence-Based\nMedicine (EBM) process, efficiently integrating retrieval mechanisms as well as\nthe selection and reasoning processes of evidence, thereby enhancing the\nproblem-solving capabilities of LLMs in healthcare scenarios and fostering a\ntrustworthy LLM physician. Our comprehensive experiments indicate that Med-R^2\nachieves a 14.87\\% improvement over vanilla RAG methods and even a 3.59\\%\nenhancement compared to fine-tuning strategies, without incurring additional\ntraining costs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Large Language Models (LLMs) have exhibited remarkable\ncapabilities in clinical scenarios. However, despite their potential, existing\nworks face challenges when applying LLMs to medical settings. Strategies\nrelying on training with medical datasets are highly cost-intensive and may\nsuffer from outdated training data. Leveraging external knowledge bases is a\nsuitable alternative, yet it faces obstacles such as limited retrieval\nprecision and poor effectiveness in answer extraction. These issues\ncollectively prevent LLMs from demonstrating the expected level of proficiency\nin mastering medical expertise. To address these challenges, we introduce\nMed-R^2, a novel LLM physician framework that adheres to the Evidence-Based\nMedicine (EBM) process, efficiently integrating retrieval mechanisms as well as\nthe selection and reasoning processes of evidence, thereby enhancing the\nproblem-solving capabilities of LLMs in healthcare scenarios and fostering a\ntrustworthy LLM physician. Our comprehensive experiments indicate that Med-R^2\nachieves a 14.87\\% improvement over vanilla RAG methods and even a 3.59\\%\nenhancement compared to fine-tuning strategies, without incurring additional\ntraining costs."
                },
                "authors": [
                    {
                        "name": "Keer Lu"
                    },
                    {
                        "name": "Zheng Liang"
                    },
                    {
                        "name": "Da Pan"
                    },
                    {
                        "name": "Shusen Zhang"
                    },
                    {
                        "name": "Xin Wu"
                    },
                    {
                        "name": "Weipeng Chen"
                    },
                    {
                        "name": "Zenan Zhou"
                    },
                    {
                        "name": "Guosheng Dong"
                    },
                    {
                        "name": "Bin Cui"
                    },
                    {
                        "name": "Wentao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wentao Zhang"
                },
                "author": "Wentao Zhang",
                "arxiv_comment": "We need to add some experiments",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11885v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11885v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12862v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12862v1",
                "updated": "2025-01-22T13:14:02Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    13,
                    14,
                    2,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T13:14:02Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    13,
                    14,
                    2,
                    2,
                    22,
                    0
                ],
                "title": "Mutation-Guided LLM-based Test Generation at Meta",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mutation-Guided LLM-based Test Generation at Meta"
                },
                "summary": "This paper describes Meta's ACH system for mutation-guided LLM-based test\ngeneration. ACH generates relatively few mutants (aka simulated faults),\ncompared to traditional mutation testing. Instead, it focuses on generating\ncurrently undetected faults that are specific to an issue of concern. From\nthese currently uncaught faults, ACH generates tests that can catch them,\nthereby `killing' the mutants and consequently hardening the platform against\nregressions. We use privacy concerns to illustrate our approach, but ACH can\nharden code against {\\em any} type of regression. In total, ACH was applied to\n10,795 Android Kotlin classes in 7 software platforms deployed by Meta, from\nwhich it generated 9,095 mutants and 571 privacy-hardening test cases. ACH also\ndeploys an LLM-based equivalent mutant detection agent that achieves a\nprecision of 0.79 and a recall of 0.47 (rising to 0.95 and 0.96 with simple\npre-processing). ACH was used by Messenger and WhatsApp test-a-thons where\nengineers accepted 73% of its tests, judging 36% to privacy relevant. We\nconclude that ACH hardens code against specific concerns and that, even when\nits tests do not directly tackle the specific concern, engineers find them\nuseful for their other benefits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper describes Meta's ACH system for mutation-guided LLM-based test\ngeneration. ACH generates relatively few mutants (aka simulated faults),\ncompared to traditional mutation testing. Instead, it focuses on generating\ncurrently undetected faults that are specific to an issue of concern. From\nthese currently uncaught faults, ACH generates tests that can catch them,\nthereby `killing' the mutants and consequently hardening the platform against\nregressions. We use privacy concerns to illustrate our approach, but ACH can\nharden code against {\\em any} type of regression. In total, ACH was applied to\n10,795 Android Kotlin classes in 7 software platforms deployed by Meta, from\nwhich it generated 9,095 mutants and 571 privacy-hardening test cases. ACH also\ndeploys an LLM-based equivalent mutant detection agent that achieves a\nprecision of 0.79 and a recall of 0.47 (rising to 0.95 and 0.96 with simple\npre-processing). ACH was used by Messenger and WhatsApp test-a-thons where\nengineers accepted 73% of its tests, judging 36% to privacy relevant. We\nconclude that ACH hardens code against specific concerns and that, even when\nits tests do not directly tackle the specific concern, engineers find them\nuseful for their other benefits."
                },
                "authors": [
                    {
                        "name": "Christopher Foster"
                    },
                    {
                        "name": "Abhishek Gulati"
                    },
                    {
                        "name": "Mark Harman"
                    },
                    {
                        "name": "Inna Harper"
                    },
                    {
                        "name": "Ke Mao"
                    },
                    {
                        "name": "Jillian Ritchey"
                    },
                    {
                        "name": "Hervé Robert"
                    },
                    {
                        "name": "Shubho Sengupta"
                    }
                ],
                "author_detail": {
                    "name": "Shubho Sengupta"
                },
                "author": "Shubho Sengupta",
                "arxiv_comment": "Submitted to FSE 2025 Industry Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12862v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12862v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14834v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14834v2",
                "updated": "2025-01-22T13:07:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    13,
                    7,
                    7,
                    2,
                    22,
                    0
                ],
                "published": "2024-12-19T13:24:01Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    24,
                    1,
                    3,
                    354,
                    0
                ],
                "title": "Entropy Regularized Task Representation Learning for Offline\n  Meta-Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Entropy Regularized Task Representation Learning for Offline\n  Meta-Reinforcement Learning"
                },
                "summary": "Offline meta-reinforcement learning aims to equip agents with the ability to\nrapidly adapt to new tasks by training on data from a set of different tasks.\nContext-based approaches utilize a history of state-action-reward transitions\n-- referred to as the context -- to infer representations of the current task,\nand then condition the agent, i.e., the policy and value function, on the task\nrepresentations. Intuitively, the better the task representations capture the\nunderlying tasks, the better the agent can generalize to new tasks.\nUnfortunately, context-based approaches suffer from distribution mismatch, as\nthe context in the offline data does not match the context at test time,\nlimiting their ability to generalize to the test tasks. This leads to the task\nrepresentations overfitting to the offline training data. Intuitively, the task\nrepresentations should be independent of the behavior policy used to collect\nthe offline data. To address this issue, we approximately minimize the mutual\ninformation between the distribution over the task representations and behavior\npolicy by maximizing the entropy of behavior policy conditioned on the task\nrepresentations. We validate our approach in MuJoCo environments, showing that\ncompared to baselines, our task representations more faithfully represent the\nunderlying tasks, leading to outperforming prior methods in both\nin-distribution and out-of-distribution tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Offline meta-reinforcement learning aims to equip agents with the ability to\nrapidly adapt to new tasks by training on data from a set of different tasks.\nContext-based approaches utilize a history of state-action-reward transitions\n-- referred to as the context -- to infer representations of the current task,\nand then condition the agent, i.e., the policy and value function, on the task\nrepresentations. Intuitively, the better the task representations capture the\nunderlying tasks, the better the agent can generalize to new tasks.\nUnfortunately, context-based approaches suffer from distribution mismatch, as\nthe context in the offline data does not match the context at test time,\nlimiting their ability to generalize to the test tasks. This leads to the task\nrepresentations overfitting to the offline training data. Intuitively, the task\nrepresentations should be independent of the behavior policy used to collect\nthe offline data. To address this issue, we approximately minimize the mutual\ninformation between the distribution over the task representations and behavior\npolicy by maximizing the entropy of behavior policy conditioned on the task\nrepresentations. We validate our approach in MuJoCo environments, showing that\ncompared to baselines, our task representations more faithfully represent the\nunderlying tasks, leading to outperforming prior methods in both\nin-distribution and out-of-distribution tasks."
                },
                "authors": [
                    {
                        "name": "Mohammadreza Nakhaei"
                    },
                    {
                        "name": "Aidan Scannell"
                    },
                    {
                        "name": "Joni Pajarinen"
                    }
                ],
                "author_detail": {
                    "name": "Joni Pajarinen"
                },
                "author": "Joni Pajarinen",
                "arxiv_comment": "7 Pages, Accepted at AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14834v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14834v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12853v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12853v1",
                "updated": "2025-01-22T13:03:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    13,
                    3,
                    38,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T13:03:38Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    13,
                    3,
                    38,
                    2,
                    22,
                    0
                ],
                "title": "Data-and-Semantic Dual-Driven Spectrum Map Construction for 6G Spectrum\n  Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-and-Semantic Dual-Driven Spectrum Map Construction for 6G Spectrum\n  Management"
                },
                "summary": "Spectrum maps reflect the utilization and distribution of spectrum resources\nin the electromagnetic environment, serving as an effective approach to support\nspectrum management. However, the construction of spectrum maps in urban\nenvironments is challenging because of high-density connection and complex\nterrain. Moreover, the existing spectrum map construction methods are typically\napplied to a fixed frequency, which cannot cover the entire frequency band. To\naddress the aforementioned challenges, a UNet-based data-and-semantic\ndual-driven method is proposed by introducing the semantic knowledge of binary\ncity maps and binary sampling location maps to enhance the accuracy of spectrum\nmap construction in complex urban environments with dense communications.\nMoreover, a joint frequency-space reasoning model is exploited to capture the\ncorrelation of spectrum data in terms of space and frequency, enabling the\nrealization of complete spectrum map construction without sampling all\nfrequencies of spectrum data. The simulation results demonstrate that the\nproposed method can infer the spectrum utilization status of missing\nfrequencies and improve the completeness of the spectrum map construction.\nFurthermore, the accuracy of spectrum map construction achieved by the proposed\ndata-and-semantic dual-driven method outperforms the benchmark schemes,\nespecially in scenarios with low sampling density.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spectrum maps reflect the utilization and distribution of spectrum resources\nin the electromagnetic environment, serving as an effective approach to support\nspectrum management. However, the construction of spectrum maps in urban\nenvironments is challenging because of high-density connection and complex\nterrain. Moreover, the existing spectrum map construction methods are typically\napplied to a fixed frequency, which cannot cover the entire frequency band. To\naddress the aforementioned challenges, a UNet-based data-and-semantic\ndual-driven method is proposed by introducing the semantic knowledge of binary\ncity maps and binary sampling location maps to enhance the accuracy of spectrum\nmap construction in complex urban environments with dense communications.\nMoreover, a joint frequency-space reasoning model is exploited to capture the\ncorrelation of spectrum data in terms of space and frequency, enabling the\nrealization of complete spectrum map construction without sampling all\nfrequencies of spectrum data. The simulation results demonstrate that the\nproposed method can infer the spectrum utilization status of missing\nfrequencies and improve the completeness of the spectrum map construction.\nFurthermore, the accuracy of spectrum map construction achieved by the proposed\ndata-and-semantic dual-driven method outperforms the benchmark schemes,\nespecially in scenarios with low sampling density."
                },
                "authors": [
                    {
                        "name": "Jiayu Liu"
                    },
                    {
                        "name": "Fuhui Zhou"
                    },
                    {
                        "name": "Xiaodong Liu"
                    },
                    {
                        "name": "Rui Ding"
                    },
                    {
                        "name": "Lu Yuan"
                    },
                    {
                        "name": "Qihui Wu"
                    }
                ],
                "author_detail": {
                    "name": "Qihui Wu"
                },
                "author": "Qihui Wu",
                "arxiv_comment": "This paper has been accepted for presentation at the IEEE Global\n  Communications Conference (GLOBECOM), Cape Town, South Africa, December 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12853v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12853v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10289v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10289v2",
                "updated": "2025-01-22T13:02:09Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    13,
                    2,
                    9,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-17T16:34:38Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    16,
                    34,
                    38,
                    4,
                    17,
                    0
                ],
                "title": "Cheap Subsampling bootstrap confidence intervals for fast and robust\n  inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cheap Subsampling bootstrap confidence intervals for fast and robust\n  inference"
                },
                "summary": "Bootstrapping is often applied to get confidence limits for semiparametric\ninference of a target parameter in the presence of nuisance parameters.\nBootstrapping with replacement can be computationally expensive and problematic\nwhen cross-validation is used in the estimation algorithm due to duplicate\nobservations in the bootstrap samples. We provide a valid, fast,\neasy-to-implement subsampling bootstrap method for constructing confidence\nintervals for asymptotically linear estimators and discuss its application to\nsemiparametric causal inference. Our method, inspired by the Cheap Bootstrap\n(Lam, 2022), leverages the quantiles of a t-distribution and has the desired\ncoverage with few bootstrap replications. We show that the method is\nasymptotically valid if the subsample size is chosen appropriately as a\nfunction of the sample size. We illustrate our method with data from the LEADER\ntrial (Marso et al., 2016), obtaining confidence intervals for a longitudinal\ntargeted minimum loss-based estimator (van der Laan and Gruber, 2012). Through\na series of empirical experiments, we also explore the impact of subsample\nsize, sample size, and the number of bootstrap repetitions on the performance\nof the confidence interval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bootstrapping is often applied to get confidence limits for semiparametric\ninference of a target parameter in the presence of nuisance parameters.\nBootstrapping with replacement can be computationally expensive and problematic\nwhen cross-validation is used in the estimation algorithm due to duplicate\nobservations in the bootstrap samples. We provide a valid, fast,\neasy-to-implement subsampling bootstrap method for constructing confidence\nintervals for asymptotically linear estimators and discuss its application to\nsemiparametric causal inference. Our method, inspired by the Cheap Bootstrap\n(Lam, 2022), leverages the quantiles of a t-distribution and has the desired\ncoverage with few bootstrap replications. We show that the method is\nasymptotically valid if the subsample size is chosen appropriately as a\nfunction of the sample size. We illustrate our method with data from the LEADER\ntrial (Marso et al., 2016), obtaining confidence intervals for a longitudinal\ntargeted minimum loss-based estimator (van der Laan and Gruber, 2012). Through\na series of empirical experiments, we also explore the impact of subsample\nsize, sample size, and the number of bootstrap repetitions on the performance\nof the confidence interval."
                },
                "authors": [
                    {
                        "name": "Johan Sebastian Ohlendorff"
                    },
                    {
                        "name": "Anders Munch"
                    },
                    {
                        "name": "Kathrine Kold Sørensen"
                    },
                    {
                        "name": "Thomas Alexander Gerds"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Alexander Gerds"
                },
                "author": "Thomas Alexander Gerds",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10289v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10289v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12851v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12851v1",
                "updated": "2025-01-22T12:59:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    12,
                    59,
                    8,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T12:59:08Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    12,
                    59,
                    8,
                    2,
                    22,
                    0
                ],
                "title": "ACEBench: Who Wins the Match Point in Tool Learning?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ACEBench: Who Wins the Match Point in Tool Learning?"
                },
                "summary": "Large language models (LLMs) have demonstrated significant potential in\ndecision-making and reasoning, especially when combined with various tools to\neffectively solve complex problems. However, existing evaluation systems for\nassessing LLM function calling capabilities have several limitations: (1)\nlimited evaluation scenarios, lacking assessments in real multi-turn dialogue\ncontexts; (2) narrow evaluation dimensions, lacking detailed assessments for\nfine-grained function calls; (3) relying on LLMs or real API executions for\nresult evaluation, which introduces significant overhead. To address these\nissues, we propose a comprehensive evaluation system named ACEBench. This\nsystem is meticulously designed to encompass a wide spectrum of function\ncalling scenarios. Moreover, it categorizes these scenarios into three primary\ntypes according to the evaluation methodology: Normal, Special, and Agent.\nNormal evaluates function calls in basic scenarios; Special evaluates function\ncalls in scenarios with vague or incomplete instructions; Agent introduces\nmulti-agent interactions to simulate function calling evaluation in real-world\nmulti-turn interactions. We conducted extensive experiments on ACEBench,\nanalyzing various LLMs in-depth and performing a more granular analysis of\nerror causes across different data types.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated significant potential in\ndecision-making and reasoning, especially when combined with various tools to\neffectively solve complex problems. However, existing evaluation systems for\nassessing LLM function calling capabilities have several limitations: (1)\nlimited evaluation scenarios, lacking assessments in real multi-turn dialogue\ncontexts; (2) narrow evaluation dimensions, lacking detailed assessments for\nfine-grained function calls; (3) relying on LLMs or real API executions for\nresult evaluation, which introduces significant overhead. To address these\nissues, we propose a comprehensive evaluation system named ACEBench. This\nsystem is meticulously designed to encompass a wide spectrum of function\ncalling scenarios. Moreover, it categorizes these scenarios into three primary\ntypes according to the evaluation methodology: Normal, Special, and Agent.\nNormal evaluates function calls in basic scenarios; Special evaluates function\ncalls in scenarios with vague or incomplete instructions; Agent introduces\nmulti-agent interactions to simulate function calling evaluation in real-world\nmulti-turn interactions. We conducted extensive experiments on ACEBench,\nanalyzing various LLMs in-depth and performing a more granular analysis of\nerror causes across different data types."
                },
                "authors": [
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Xinlong Hao"
                    },
                    {
                        "name": "Weiwen Liu"
                    },
                    {
                        "name": "Xu Huang"
                    },
                    {
                        "name": "Xingshan Zeng"
                    },
                    {
                        "name": "Shuai Yu"
                    },
                    {
                        "name": "Dexun Li"
                    },
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Weinan Gan"
                    },
                    {
                        "name": "Yuefeng Huang"
                    },
                    {
                        "name": "Xinzhi Wang"
                    },
                    {
                        "name": "Defu Lian"
                    },
                    {
                        "name": "Baoqun Yin"
                    },
                    {
                        "name": "Yasheng Wang"
                    },
                    {
                        "name": "Wu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Wu Liu"
                },
                "author": "Wu Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12851v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12851v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12835v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12835v1",
                "updated": "2025-01-22T12:21:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    12,
                    21,
                    17,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T12:21:17Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    12,
                    21,
                    17,
                    2,
                    22,
                    0
                ],
                "title": "Adaptive Retrieval Without Self-Knowledge? Bringing Uncertainty Back\n  Home",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Retrieval Without Self-Knowledge? Bringing Uncertainty Back\n  Home"
                },
                "summary": "Retrieval Augmented Generation (RAG) improves correctness of Question\nAnswering (QA) and addresses hallucinations in Large Language Models (LLMs),\nyet greatly increase computational costs. Besides, RAG is not always needed as\nmay introduce irrelevant information. Recent adaptive retrieval methods\nintegrate LLMs' intrinsic knowledge with external information appealing to LLM\nself-knowledge, but they often neglect efficiency evaluations and comparisons\nwith uncertainty estimation techniques. We bridge this gap by conducting a\ncomprehensive analysis of 35 adaptive retrieval methods, including 8 recent\napproaches and 27 uncertainty estimation techniques, across 6 datasets using 10\nmetrics for QA performance, self-knowledge, and efficiency. Our findings show\nthat uncertainty estimation techniques often outperform complex pipelines in\nterms of efficiency and self-knowledge, while maintaining comparable QA\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval Augmented Generation (RAG) improves correctness of Question\nAnswering (QA) and addresses hallucinations in Large Language Models (LLMs),\nyet greatly increase computational costs. Besides, RAG is not always needed as\nmay introduce irrelevant information. Recent adaptive retrieval methods\nintegrate LLMs' intrinsic knowledge with external information appealing to LLM\nself-knowledge, but they often neglect efficiency evaluations and comparisons\nwith uncertainty estimation techniques. We bridge this gap by conducting a\ncomprehensive analysis of 35 adaptive retrieval methods, including 8 recent\napproaches and 27 uncertainty estimation techniques, across 6 datasets using 10\nmetrics for QA performance, self-knowledge, and efficiency. Our findings show\nthat uncertainty estimation techniques often outperform complex pipelines in\nterms of efficiency and self-knowledge, while maintaining comparable QA\nperformance."
                },
                "authors": [
                    {
                        "name": "Viktor Moskvoretskii"
                    },
                    {
                        "name": "Maria Lysyuk"
                    },
                    {
                        "name": "Mikhail Salnikov"
                    },
                    {
                        "name": "Nikolay Ivanov"
                    },
                    {
                        "name": "Sergey Pletenev"
                    },
                    {
                        "name": "Daria Galimzianova"
                    },
                    {
                        "name": "Nikita Krayko"
                    },
                    {
                        "name": "Vasily Konovalov"
                    },
                    {
                        "name": "Irina Nikishina"
                    },
                    {
                        "name": "Alexander Panchenko"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Panchenko"
                },
                "author": "Alexander Panchenko",
                "arxiv_comment": "The code and data will be published soon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12835v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12835v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12826v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12826v1",
                "updated": "2025-01-22T12:06:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    12,
                    6,
                    16,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T12:06:16Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    12,
                    6,
                    16,
                    2,
                    22,
                    0
                ],
                "title": "Open or Closed LLM for Lesser-Resourced Languages? Lessons from Greek",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open or Closed LLM for Lesser-Resourced Languages? Lessons from Greek"
                },
                "summary": "Natural Language Processing (NLP) for lesser-resourced languages faces\npersistent challenges, including limited datasets, inherited biases from\nhigh-resource languages, and the need for domain-specific solutions. This study\naddresses these gaps for Modern Greek through three key contributions. First,\nwe evaluate the performance of open-source (Llama-70b) and closed-source\n(GPT-4o mini) large language models (LLMs) on seven core NLP tasks with dataset\navailability, revealing task-specific strengths, weaknesses, and parity in\ntheir performance. Second, we expand the scope of Greek NLP by reframing\nAuthorship Attribution as a tool to assess potential data usage by LLMs in\npre-training, with high 0-shot accuracy suggesting ethical implications for\ndata provenance. Third, we showcase a legal NLP case study, where a Summarize,\nTranslate, and Embed (STE) methodology outperforms the traditional TF-IDF\napproach for clustering \\emph{long} legal texts. Together, these contributions\nprovide a roadmap to advance NLP in lesser-resourced languages, bridging gaps\nin model evaluation, task innovation, and real-world impact.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural Language Processing (NLP) for lesser-resourced languages faces\npersistent challenges, including limited datasets, inherited biases from\nhigh-resource languages, and the need for domain-specific solutions. This study\naddresses these gaps for Modern Greek through three key contributions. First,\nwe evaluate the performance of open-source (Llama-70b) and closed-source\n(GPT-4o mini) large language models (LLMs) on seven core NLP tasks with dataset\navailability, revealing task-specific strengths, weaknesses, and parity in\ntheir performance. Second, we expand the scope of Greek NLP by reframing\nAuthorship Attribution as a tool to assess potential data usage by LLMs in\npre-training, with high 0-shot accuracy suggesting ethical implications for\ndata provenance. Third, we showcase a legal NLP case study, where a Summarize,\nTranslate, and Embed (STE) methodology outperforms the traditional TF-IDF\napproach for clustering \\emph{long} legal texts. Together, these contributions\nprovide a roadmap to advance NLP in lesser-resourced languages, bridging gaps\nin model evaluation, task innovation, and real-world impact."
                },
                "authors": [
                    {
                        "name": "John Pavlopoulos"
                    },
                    {
                        "name": "Juli Bakagianni"
                    },
                    {
                        "name": "Kanella Pouli"
                    },
                    {
                        "name": "Maria Gavriilidou"
                    }
                ],
                "author_detail": {
                    "name": "Maria Gavriilidou"
                },
                "author": "Maria Gavriilidou",
                "arxiv_comment": "NLP, Modern Greek, benchmark, machine learning, language resources",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12826v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12826v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12818v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12818v1",
                "updated": "2025-01-22T11:52:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    11,
                    52,
                    41,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T11:52:41Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    11,
                    52,
                    41,
                    2,
                    22,
                    0
                ],
                "title": "Late Breaking Result: FPGA-Based Emulation and Fault Injection for CNN\n  Inference Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Late Breaking Result: FPGA-Based Emulation and Fault Injection for CNN\n  Inference Accelerators"
                },
                "summary": "A new field programmable gate array (FPGA)-based emulation platform is\nproposed to accelerate fault tolerance analysis of inference accelerators of\nconvolutional neural networks (CNN). For a given CNN model, hardware\naccelerator architecture, and FT analysis target, an FPGA-based CNN\nimplementation is generated (with the help of the Tengine framework), and fault\ninjection logic is added. In our first case study, we report how the\nclassification accuracy drop depends on the faults injected into multipliers\nused in Multiply-and-Accumulate Units of NVDLA inference accelerator executing\nResNet-18 CNN. The FT analysis emulated on Zynq UltraScale+ SoC is an order of\nmagnitude faster than software emulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A new field programmable gate array (FPGA)-based emulation platform is\nproposed to accelerate fault tolerance analysis of inference accelerators of\nconvolutional neural networks (CNN). For a given CNN model, hardware\naccelerator architecture, and FT analysis target, an FPGA-based CNN\nimplementation is generated (with the help of the Tengine framework), and fault\ninjection logic is added. In our first case study, we report how the\nclassification accuracy drop depends on the faults injected into multipliers\nused in Multiply-and-Accumulate Units of NVDLA inference accelerator executing\nResNet-18 CNN. The FT analysis emulated on Zynq UltraScale+ SoC is an order of\nmagnitude faster than software emulation."
                },
                "authors": [
                    {
                        "name": "Filip Masar"
                    },
                    {
                        "name": "Vojtech Mrazek"
                    },
                    {
                        "name": "Lukas Sekanina"
                    }
                ],
                "author_detail": {
                    "name": "Lukas Sekanina"
                },
                "author": "Lukas Sekanina",
                "arxiv_comment": "To appear at the 2025 Design, Automation & Test in Europe Conference\n  & Exhibition (DATE)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12818v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12818v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02518v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02518v2",
                "updated": "2025-01-22T11:49:44Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    11,
                    49,
                    44,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-05T12:15:02Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    12,
                    15,
                    2,
                    6,
                    5,
                    0
                ],
                "title": "CHAIR -- Classifier of Hallucination as Improver",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CHAIR -- Classifier of Hallucination as Improver"
                },
                "summary": "In this work, we introduce CHAIR (Classifier of Hallucination As ImproveR), a\nsupervised framework for detecting hallucinations by analyzing internal logits\nfrom each layer of every token. Our method extracts a compact set of features\nsuch as maximum, minimum, mean, standard deviation, and slope-from the token\nlogits across all layers, enabling effective hallucination detection without\noverfitting. Experiments on TruthfulQA and MMLU datasets demonstrate that CHAIR\nsignificantly improves detection accuracy, particularly in zero-shot scenarios,\nshowcasing its robustness and generalizability. Beyond hallucination detection,\nCHAIR highlights the potential of using internal representations for designing\nadvanced decoding strategies. By leveraging patterns in logits, we suggest that\nmore sophisticated models and adaptive decoding methods could further reduce\nhallucinations and enhance text completion quality. CHAIR not only offers a\npractical solution for detecting hallucinations but also lays the groundwork\nfor exploring richer representations in LLMs to improve their factuality and\ncoherence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we introduce CHAIR (Classifier of Hallucination As ImproveR), a\nsupervised framework for detecting hallucinations by analyzing internal logits\nfrom each layer of every token. Our method extracts a compact set of features\nsuch as maximum, minimum, mean, standard deviation, and slope-from the token\nlogits across all layers, enabling effective hallucination detection without\noverfitting. Experiments on TruthfulQA and MMLU datasets demonstrate that CHAIR\nsignificantly improves detection accuracy, particularly in zero-shot scenarios,\nshowcasing its robustness and generalizability. Beyond hallucination detection,\nCHAIR highlights the potential of using internal representations for designing\nadvanced decoding strategies. By leveraging patterns in logits, we suggest that\nmore sophisticated models and adaptive decoding methods could further reduce\nhallucinations and enhance text completion quality. CHAIR not only offers a\npractical solution for detecting hallucinations but also lays the groundwork\nfor exploring richer representations in LLMs to improve their factuality and\ncoherence."
                },
                "authors": [
                    {
                        "name": "Ao Sun"
                    }
                ],
                "author_detail": {
                    "name": "Ao Sun"
                },
                "author": "Ao Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02518v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02518v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12375v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12375v2",
                "updated": "2025-01-22T11:33:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    11,
                    33,
                    54,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-21T18:53:30Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    18,
                    53,
                    30,
                    1,
                    21,
                    0
                ],
                "title": "Video Depth Anything: Consistent Depth Estimation for Super-Long Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Depth Anything: Consistent Depth Estimation for Super-Long Videos"
                },
                "summary": "Depth Anything has achieved remarkable success in monocular depth estimation\nwith strong generalization ability. However, it suffers from temporal\ninconsistency in videos, hindering its practical applications. Various methods\nhave been proposed to alleviate this issue by leveraging video generation\nmodels or introducing priors from optical flow and camera poses. Nonetheless,\nthese methods are only applicable to short videos (< 10 seconds) and require a\ntrade-off between quality and computational efficiency. We propose Video Depth\nAnything for high-quality, consistent depth estimation in super-long videos\n(over several minutes) without sacrificing efficiency. We base our model on\nDepth Anything V2 and replace its head with an efficient spatial-temporal head.\nWe design a straightforward yet effective temporal consistency loss by\nconstraining the temporal depth gradient, eliminating the need for additional\ngeometric priors. The model is trained on a joint dataset of video depth and\nunlabeled images, similar to Depth Anything V2. Moreover, a novel\nkey-frame-based strategy is developed for long video inference. Experiments\nshow that our model can be applied to arbitrarily long videos without\ncompromising quality, consistency, or generalization ability. Comprehensive\nevaluations on multiple video benchmarks demonstrate that our approach sets a\nnew state-of-the-art in zero-shot video depth estimation. We offer models of\ndifferent scales to support a range of scenarios, with our smallest model\ncapable of real-time performance at 30 FPS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Depth Anything has achieved remarkable success in monocular depth estimation\nwith strong generalization ability. However, it suffers from temporal\ninconsistency in videos, hindering its practical applications. Various methods\nhave been proposed to alleviate this issue by leveraging video generation\nmodels or introducing priors from optical flow and camera poses. Nonetheless,\nthese methods are only applicable to short videos (< 10 seconds) and require a\ntrade-off between quality and computational efficiency. We propose Video Depth\nAnything for high-quality, consistent depth estimation in super-long videos\n(over several minutes) without sacrificing efficiency. We base our model on\nDepth Anything V2 and replace its head with an efficient spatial-temporal head.\nWe design a straightforward yet effective temporal consistency loss by\nconstraining the temporal depth gradient, eliminating the need for additional\ngeometric priors. The model is trained on a joint dataset of video depth and\nunlabeled images, similar to Depth Anything V2. Moreover, a novel\nkey-frame-based strategy is developed for long video inference. Experiments\nshow that our model can be applied to arbitrarily long videos without\ncompromising quality, consistency, or generalization ability. Comprehensive\nevaluations on multiple video benchmarks demonstrate that our approach sets a\nnew state-of-the-art in zero-shot video depth estimation. We offer models of\ndifferent scales to support a range of scenarios, with our smallest model\ncapable of real-time performance at 30 FPS."
                },
                "authors": [
                    {
                        "name": "Sili Chen"
                    },
                    {
                        "name": "Hengkai Guo"
                    },
                    {
                        "name": "Shengnan Zhu"
                    },
                    {
                        "name": "Feihu Zhang"
                    },
                    {
                        "name": "Zilong Huang"
                    },
                    {
                        "name": "Jiashi Feng"
                    },
                    {
                        "name": "Bingyi Kang"
                    }
                ],
                "author_detail": {
                    "name": "Bingyi Kang"
                },
                "author": "Bingyi Kang",
                "arxiv_comment": "Project page: https://videodepthanything.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12375v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12375v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.08704v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.08704v2",
                "updated": "2025-01-22T11:30:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    11,
                    30,
                    49,
                    2,
                    22,
                    0
                ],
                "published": "2023-12-14T07:43:53Z",
                "published_parsed": [
                    2023,
                    12,
                    14,
                    7,
                    43,
                    53,
                    3,
                    348,
                    0
                ],
                "title": "PairingNet: A Learning-based Pair-searching and -matching Network for\n  Image Fragments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PairingNet: A Learning-based Pair-searching and -matching Network for\n  Image Fragments"
                },
                "summary": "In this paper, we propose a learning-based image fragment pair-searching and\n-matching approach to solve the challenging restoration problem. Existing works\nuse rule-based methods to match similar contour shapes or textures, which are\nalways difficult to tune hyperparameters for extensive data and computationally\ntime-consuming. Therefore, we propose a neural network that can effectively\nutilize neighbor textures with contour shape information to fundamentally\nimprove performance. First, we employ a graph-based network to extract the\nlocal contour and texture features of fragments. Then, for the pair-searching\ntask, we adopt a linear transformer-based module to integrate these local\nfeatures and use contrastive loss to encode the global features of each\nfragment. For the pair-matching task, we design a weighted fusion module to\ndynamically fuse extracted local contour and texture features, and formulate a\nsimilarity matrix for each pair of fragments to calculate the matching score\nand infer the adjacent segment of contours. To faithfully evaluate our proposed\nnetwork, we created a new image fragment dataset through an algorithm we\ndesigned that tears complete images into irregular fragments. The experimental\nresults show that our proposed network achieves excellent pair-searching\naccuracy, reduces matching errors, and significantly reduces computational\ntime. Details, sourcecode, and data are available in our supplementary\nmaterial.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose a learning-based image fragment pair-searching and\n-matching approach to solve the challenging restoration problem. Existing works\nuse rule-based methods to match similar contour shapes or textures, which are\nalways difficult to tune hyperparameters for extensive data and computationally\ntime-consuming. Therefore, we propose a neural network that can effectively\nutilize neighbor textures with contour shape information to fundamentally\nimprove performance. First, we employ a graph-based network to extract the\nlocal contour and texture features of fragments. Then, for the pair-searching\ntask, we adopt a linear transformer-based module to integrate these local\nfeatures and use contrastive loss to encode the global features of each\nfragment. For the pair-matching task, we design a weighted fusion module to\ndynamically fuse extracted local contour and texture features, and formulate a\nsimilarity matrix for each pair of fragments to calculate the matching score\nand infer the adjacent segment of contours. To faithfully evaluate our proposed\nnetwork, we created a new image fragment dataset through an algorithm we\ndesigned that tears complete images into irregular fragments. The experimental\nresults show that our proposed network achieves excellent pair-searching\naccuracy, reduces matching errors, and significantly reduces computational\ntime. Details, sourcecode, and data are available in our supplementary\nmaterial."
                },
                "authors": [
                    {
                        "name": "Rixin Zhou"
                    },
                    {
                        "name": "Ding Xia"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Honglin Pang"
                    },
                    {
                        "name": "Xi Yang"
                    },
                    {
                        "name": "Chuntao Li"
                    }
                ],
                "author_detail": {
                    "name": "Chuntao Li"
                },
                "author": "Chuntao Li",
                "arxiv_comment": "25 pages, 19 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.08704v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.08704v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.18373v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.18373v3",
                "updated": "2025-01-22T11:17:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    11,
                    17,
                    23,
                    2,
                    22,
                    0
                ],
                "published": "2024-04-29T02:23:53Z",
                "published_parsed": [
                    2024,
                    4,
                    29,
                    2,
                    23,
                    53,
                    0,
                    120,
                    0
                ],
                "title": "6G comprehensive intelligence: network operations and optimization based\n  on Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "6G comprehensive intelligence: network operations and optimization based\n  on Large Language Models"
                },
                "summary": "The sixth generation mobile communication standard (6G) can promote the\ndevelopment of Industrial Internet and Internet of Things (IoT). To achieve\ncomprehensive intelligent development of the network and provide customers with\nhigher quality personalized services. This paper proposes a network performance\noptimization and intelligent operation network architecture based on Large\nLanguage Model (LLM), aiming to build a comprehensive intelligent 6G network\nsystem. The Large Language Model, with more parameters and stronger learning\nability, can more accurately capture patterns and features in data, which can\nachieve more accurate content output and high intelligence and provide strong\nsupport for related research such as network data security, privacy protection,\nand health assessment. This paper also presents the design framework of a\nnetwork health assessment system based on LLM and focuses on its potential\napplication value, through the case of network health management system, it is\nfully demonstrated that the 6G intelligent network system based on LLM has\nimportant practical significance for the comprehensive realization of\nintelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The sixth generation mobile communication standard (6G) can promote the\ndevelopment of Industrial Internet and Internet of Things (IoT). To achieve\ncomprehensive intelligent development of the network and provide customers with\nhigher quality personalized services. This paper proposes a network performance\noptimization and intelligent operation network architecture based on Large\nLanguage Model (LLM), aiming to build a comprehensive intelligent 6G network\nsystem. The Large Language Model, with more parameters and stronger learning\nability, can more accurately capture patterns and features in data, which can\nachieve more accurate content output and high intelligence and provide strong\nsupport for related research such as network data security, privacy protection,\nand health assessment. This paper also presents the design framework of a\nnetwork health assessment system based on LLM and focuses on its potential\napplication value, through the case of network health management system, it is\nfully demonstrated that the 6G intelligent network system based on LLM has\nimportant practical significance for the comprehensive realization of\nintelligence."
                },
                "authors": [
                    {
                        "name": "Sifan Long"
                    },
                    {
                        "name": "Fengxiao Tang"
                    },
                    {
                        "name": "Yangfan Li"
                    },
                    {
                        "name": "Tiao Tan"
                    },
                    {
                        "name": "Zhengjie Jin"
                    },
                    {
                        "name": "Ming Zhao"
                    },
                    {
                        "name": "Nei Kato"
                    }
                ],
                "author_detail": {
                    "name": "Nei Kato"
                },
                "author": "Nei Kato",
                "arxiv_doi": "10.1109/MNET.2024.3470774",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/MNET.2024.3470774",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2404.18373v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.18373v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This paper has been published on the IEEE network, and the relevant\n  information is as follows: 8 pages, 5 figures, 15 preferences,",
                "arxiv_journal_ref": "IEEE Network (2024)",
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "94-03",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18001v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18001v2",
                "updated": "2025-01-22T11:14:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    11,
                    14,
                    36,
                    2,
                    22,
                    0
                ],
                "published": "2024-07-25T12:52:36Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    12,
                    52,
                    36,
                    3,
                    207,
                    0
                ],
                "title": "Measurement of $D^0-\\overline{D}^0$ mixing and search for $CP$ violation\n  with $D^0\\rightarrow K^+π^-$ decays",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measurement of $D^0-\\overline{D}^0$ mixing and search for $CP$ violation\n  with $D^0\\rightarrow K^+π^-$ decays"
                },
                "summary": "A measurement of the time-dependent ratio of the $D^0\\rightarrow K^+\\pi^-$ to\n$\\overline{D}^0\\rightarrow K^+\\pi^-$ decay rates is reported. The analysis uses\na sample of proton-proton collisions corresponding to an integrated luminosity\nof 6 fb$^{-1}$ recorded by the LHCb experiment from 2015 through 2018 at a\ncenter-of-mass energy of 13 TeV. The $D^0$ meson is required to originate from\na $D^{*+}\\rightarrow D^0\\pi^+$ decay, such that its flavor at production is\ninferred from the charge of the accompanying pion. The measurement is performed\nsimultaneously for the $K^+\\pi^-$ and $K^-\\pi^+$ final states, allowing both\nmixing and $CP$-violation parameters to be determined. The value of the ratio\nof the decay rates at production is determined to be $R_{K\\pi} = (343.1 \\pm\n2.0) \\times 10^{-5}$. The mixing parameters are measured to be $c_{K\\pi} =\n(51.4 \\pm 3.5) \\times 10^{-4}$ and $c_{K\\pi}^{\\prime} = (13 \\pm 4) \\times\n10^{-6}$, where $\\sqrt{R_{K\\pi}}c_{K\\pi}$ is the linear coefficient of the\nexpansion of the ratio as a function of decay time in units of the $D^0$\nlifetime, and $c_{K\\pi}^{\\prime}$ is the quadratic coefficient, both averaged\nbetween the $K^+\\pi^-$ and $K^-\\pi^+$ final states. The precision is improved\nrelative to the previous best measurement by approximately 60%. No evidence for\n$CP$ violation is found.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A measurement of the time-dependent ratio of the $D^0\\rightarrow K^+\\pi^-$ to\n$\\overline{D}^0\\rightarrow K^+\\pi^-$ decay rates is reported. The analysis uses\na sample of proton-proton collisions corresponding to an integrated luminosity\nof 6 fb$^{-1}$ recorded by the LHCb experiment from 2015 through 2018 at a\ncenter-of-mass energy of 13 TeV. The $D^0$ meson is required to originate from\na $D^{*+}\\rightarrow D^0\\pi^+$ decay, such that its flavor at production is\ninferred from the charge of the accompanying pion. The measurement is performed\nsimultaneously for the $K^+\\pi^-$ and $K^-\\pi^+$ final states, allowing both\nmixing and $CP$-violation parameters to be determined. The value of the ratio\nof the decay rates at production is determined to be $R_{K\\pi} = (343.1 \\pm\n2.0) \\times 10^{-5}$. The mixing parameters are measured to be $c_{K\\pi} =\n(51.4 \\pm 3.5) \\times 10^{-4}$ and $c_{K\\pi}^{\\prime} = (13 \\pm 4) \\times\n10^{-6}$, where $\\sqrt{R_{K\\pi}}c_{K\\pi}$ is the linear coefficient of the\nexpansion of the ratio as a function of decay time in units of the $D^0$\nlifetime, and $c_{K\\pi}^{\\prime}$ is the quadratic coefficient, both averaged\nbetween the $K^+\\pi^-$ and $K^-\\pi^+$ final states. The precision is improved\nrelative to the previous best measurement by approximately 60%. No evidence for\n$CP$ violation is found."
                },
                "authors": [
                    {
                        "name": "LHCb collaboration"
                    },
                    {
                        "name": "R. Aaij"
                    },
                    {
                        "name": "A. S. W. Abdelmotteleb"
                    },
                    {
                        "name": "C. Abellan Beteta"
                    },
                    {
                        "name": "F. Abudinén"
                    },
                    {
                        "name": "T. Ackernley"
                    },
                    {
                        "name": "A. A. Adefisoye"
                    },
                    {
                        "name": "B. Adeva"
                    },
                    {
                        "name": "M. Adinolfi"
                    },
                    {
                        "name": "P. Adlarson"
                    },
                    {
                        "name": "C. Agapopoulou"
                    },
                    {
                        "name": "C. A. Aidala"
                    },
                    {
                        "name": "Z. Ajaltouni"
                    },
                    {
                        "name": "S. Akar"
                    },
                    {
                        "name": "K. Akiba"
                    },
                    {
                        "name": "P. Albicocco"
                    },
                    {
                        "name": "J. Albrecht"
                    },
                    {
                        "name": "F. Alessio"
                    },
                    {
                        "name": "M. Alexander"
                    },
                    {
                        "name": "Z. Aliouche"
                    },
                    {
                        "name": "P. Alvarez Cartelle"
                    },
                    {
                        "name": "R. Amalric"
                    },
                    {
                        "name": "S. Amato"
                    },
                    {
                        "name": "J. L. Amey"
                    },
                    {
                        "name": "Y. Amhis"
                    },
                    {
                        "name": "L. An"
                    },
                    {
                        "name": "L. Anderlini"
                    },
                    {
                        "name": "M. Andersson"
                    },
                    {
                        "name": "A. Andreianov"
                    },
                    {
                        "name": "P. Andreola"
                    },
                    {
                        "name": "M. Andreotti"
                    },
                    {
                        "name": "D. Andreou"
                    },
                    {
                        "name": "A. Anelli"
                    },
                    {
                        "name": "D. Ao"
                    },
                    {
                        "name": "F. Archilli"
                    },
                    {
                        "name": "M. Argenton"
                    },
                    {
                        "name": "S. Arguedas Cuendis"
                    },
                    {
                        "name": "A. Artamonov"
                    },
                    {
                        "name": "M. Artuso"
                    },
                    {
                        "name": "E. Aslanides"
                    },
                    {
                        "name": "R. Ataíde Da Silva"
                    },
                    {
                        "name": "M. Atzeni"
                    },
                    {
                        "name": "B. Audurier"
                    },
                    {
                        "name": "D. Bacher"
                    },
                    {
                        "name": "I. Bachiller Perea"
                    },
                    {
                        "name": "S. Bachmann"
                    },
                    {
                        "name": "M. Bachmayer"
                    },
                    {
                        "name": "J. J. Back"
                    },
                    {
                        "name": "P. Baladron Rodriguez"
                    },
                    {
                        "name": "V. Balagura"
                    },
                    {
                        "name": "W. Baldini"
                    },
                    {
                        "name": "H. Bao"
                    },
                    {
                        "name": "J. Baptista de Souza Leite"
                    },
                    {
                        "name": "M. Barbetti"
                    },
                    {
                        "name": "I. R. Barbosa"
                    },
                    {
                        "name": "R. J. Barlow"
                    },
                    {
                        "name": "M. Barnyakov"
                    },
                    {
                        "name": "S. Barsuk"
                    },
                    {
                        "name": "W. Barter"
                    },
                    {
                        "name": "M. Bartolini"
                    },
                    {
                        "name": "J. Bartz"
                    },
                    {
                        "name": "J. M. Basels"
                    },
                    {
                        "name": "G. Bassi"
                    },
                    {
                        "name": "B. Batsukh"
                    },
                    {
                        "name": "A. Bay"
                    },
                    {
                        "name": "A. Beck"
                    },
                    {
                        "name": "M. Becker"
                    },
                    {
                        "name": "F. Bedeschi"
                    },
                    {
                        "name": "I. B. Bediaga"
                    },
                    {
                        "name": "S. Belin"
                    },
                    {
                        "name": "V. Bellee"
                    },
                    {
                        "name": "K. Belous"
                    },
                    {
                        "name": "I. Belov"
                    },
                    {
                        "name": "I. Belyaev"
                    },
                    {
                        "name": "G. Benane"
                    },
                    {
                        "name": "G. Bencivenni"
                    },
                    {
                        "name": "E. Ben-Haim"
                    },
                    {
                        "name": "A. Berezhnoy"
                    },
                    {
                        "name": "R. Bernet"
                    },
                    {
                        "name": "S. Bernet Andres"
                    },
                    {
                        "name": "A. Bertolin"
                    },
                    {
                        "name": "C. Betancourt"
                    },
                    {
                        "name": "F. Betti"
                    },
                    {
                        "name": "J. Bex"
                    },
                    {
                        "name": "Ia. Bezshyiko"
                    },
                    {
                        "name": "J. Bhom"
                    },
                    {
                        "name": "M. S. Bieker"
                    },
                    {
                        "name": "N. V. Biesuz"
                    },
                    {
                        "name": "P. Billoir"
                    },
                    {
                        "name": "A. Biolchini"
                    },
                    {
                        "name": "M. Birch"
                    },
                    {
                        "name": "F. C. R. Bishop"
                    },
                    {
                        "name": "A. Bitadze"
                    },
                    {
                        "name": "A. Bizzeti"
                    },
                    {
                        "name": "T. Blake"
                    },
                    {
                        "name": "F. Blanc"
                    },
                    {
                        "name": "J. E. Blank"
                    },
                    {
                        "name": "S. Blusk"
                    },
                    {
                        "name": "V. Bocharnikov"
                    },
                    {
                        "name": "J. A. Boelhauve"
                    },
                    {
                        "name": "O. Boente Garcia"
                    },
                    {
                        "name": "T. Boettcher"
                    },
                    {
                        "name": "A. Bohare"
                    },
                    {
                        "name": "A. Boldyrev"
                    },
                    {
                        "name": "C. S. Bolognani"
                    },
                    {
                        "name": "R. Bolzonella"
                    },
                    {
                        "name": "N. Bondar"
                    },
                    {
                        "name": "F. Borgato"
                    },
                    {
                        "name": "S. Borghi"
                    },
                    {
                        "name": "M. Borsato"
                    },
                    {
                        "name": "J. T. Borsuk"
                    },
                    {
                        "name": "S. A. Bouchiba"
                    },
                    {
                        "name": "T. J. V. Bowcock"
                    },
                    {
                        "name": "A. Boyer"
                    },
                    {
                        "name": "C. Bozzi"
                    },
                    {
                        "name": "A. Brea Rodriguez"
                    },
                    {
                        "name": "N. Breer"
                    },
                    {
                        "name": "J. Brodzicka"
                    },
                    {
                        "name": "A. Brossa Gonzalo"
                    },
                    {
                        "name": "J. Brown"
                    },
                    {
                        "name": "D. Brundu"
                    },
                    {
                        "name": "E. Buchanan"
                    },
                    {
                        "name": "A. Buonaura"
                    },
                    {
                        "name": "L. Buonincontri"
                    },
                    {
                        "name": "A. T. Burke"
                    },
                    {
                        "name": "C. Burr"
                    },
                    {
                        "name": "A. Butkevich"
                    },
                    {
                        "name": "J. S. Butter"
                    },
                    {
                        "name": "J. Buytaert"
                    },
                    {
                        "name": "W. Byczynski"
                    },
                    {
                        "name": "S. Cadeddu"
                    },
                    {
                        "name": "H. Cai"
                    },
                    {
                        "name": "R. Calabrese"
                    },
                    {
                        "name": "S. Calderon Ramirez"
                    },
                    {
                        "name": "L. Calefice"
                    },
                    {
                        "name": "S. Cali"
                    },
                    {
                        "name": "M. Calvi"
                    },
                    {
                        "name": "M. Calvo Gomez"
                    },
                    {
                        "name": "P. Camargo Magalhaes"
                    },
                    {
                        "name": "J. I. Cambon Bouzas"
                    },
                    {
                        "name": "P. Campana"
                    },
                    {
                        "name": "D. H. Campora Perez"
                    },
                    {
                        "name": "A. F. Campoverde Quezada"
                    },
                    {
                        "name": "S. Capelli"
                    },
                    {
                        "name": "L. Capriotti"
                    },
                    {
                        "name": "R. Caravaca-Mora"
                    },
                    {
                        "name": "A. Carbone"
                    },
                    {
                        "name": "L. Carcedo Salgado"
                    },
                    {
                        "name": "R. Cardinale"
                    },
                    {
                        "name": "A. Cardini"
                    },
                    {
                        "name": "P. Carniti"
                    },
                    {
                        "name": "L. Carus"
                    },
                    {
                        "name": "A. Casais Vidal"
                    },
                    {
                        "name": "R. Caspary"
                    },
                    {
                        "name": "G. Casse"
                    },
                    {
                        "name": "J. Castro Godinez"
                    },
                    {
                        "name": "M. Cattaneo"
                    },
                    {
                        "name": "G. Cavallero"
                    },
                    {
                        "name": "V. Cavallini"
                    },
                    {
                        "name": "S. Celani"
                    },
                    {
                        "name": "D. Cervenkov"
                    },
                    {
                        "name": "S. Cesare"
                    },
                    {
                        "name": "A. J. Chadwick"
                    },
                    {
                        "name": "I. Chahrour"
                    },
                    {
                        "name": "M. Charles"
                    },
                    {
                        "name": "Ph. Charpentier"
                    },
                    {
                        "name": "E. Chatzianagnostou"
                    },
                    {
                        "name": "C. A. Chavez Barajas"
                    },
                    {
                        "name": "M. Chefdeville"
                    },
                    {
                        "name": "C. Chen"
                    },
                    {
                        "name": "S. Chen"
                    },
                    {
                        "name": "Z. Chen"
                    },
                    {
                        "name": "A. Chernov"
                    },
                    {
                        "name": "S. Chernyshenko"
                    },
                    {
                        "name": "V. Chobanova"
                    },
                    {
                        "name": "S. Cholak"
                    },
                    {
                        "name": "M. Chrzaszcz"
                    },
                    {
                        "name": "A. Chubykin"
                    },
                    {
                        "name": "V. Chulikov"
                    },
                    {
                        "name": "P. Ciambrone"
                    },
                    {
                        "name": "X. Cid Vidal"
                    },
                    {
                        "name": "G. Ciezarek"
                    },
                    {
                        "name": "P. Cifra"
                    },
                    {
                        "name": "P. E. L. Clarke"
                    },
                    {
                        "name": "M. Clemencic"
                    },
                    {
                        "name": "H. V. Cliff"
                    },
                    {
                        "name": "J. Closier"
                    },
                    {
                        "name": "C. Cocha Toapaxi"
                    },
                    {
                        "name": "V. Coco"
                    },
                    {
                        "name": "J. Cogan"
                    },
                    {
                        "name": "E. Cogneras"
                    },
                    {
                        "name": "L. Cojocariu"
                    },
                    {
                        "name": "P. Collins"
                    },
                    {
                        "name": "T. Colombo"
                    },
                    {
                        "name": "A. Comerma-Montells"
                    },
                    {
                        "name": "L. Congedo"
                    },
                    {
                        "name": "A. Contu"
                    },
                    {
                        "name": "N. Cooke"
                    },
                    {
                        "name": "I. Corredoira"
                    },
                    {
                        "name": "A. Correia"
                    },
                    {
                        "name": "G. Corti"
                    },
                    {
                        "name": "J. Cottee Meldrum"
                    },
                    {
                        "name": "B. Couturier"
                    },
                    {
                        "name": "D. C. Craik"
                    },
                    {
                        "name": "M. Cruz Torres"
                    },
                    {
                        "name": "E. Curras Rivera"
                    },
                    {
                        "name": "R. Currie"
                    },
                    {
                        "name": "C. L. Da Silva"
                    },
                    {
                        "name": "S. Dadabaev"
                    },
                    {
                        "name": "L. Dai"
                    },
                    {
                        "name": "X. Dai"
                    },
                    {
                        "name": "E. Dall'Occo"
                    },
                    {
                        "name": "J. Dalseno"
                    },
                    {
                        "name": "C. D'Ambrosio"
                    },
                    {
                        "name": "J. Daniel"
                    },
                    {
                        "name": "A. Danilina"
                    },
                    {
                        "name": "P. d'Argent"
                    },
                    {
                        "name": "A. Davidson"
                    },
                    {
                        "name": "J. E. Davies"
                    },
                    {
                        "name": "A. Davis"
                    },
                    {
                        "name": "O. De Aguiar Francisco"
                    },
                    {
                        "name": "C. De Angelis"
                    },
                    {
                        "name": "F. De Benedetti"
                    },
                    {
                        "name": "J. de Boer"
                    },
                    {
                        "name": "K. De Bruyn"
                    },
                    {
                        "name": "S. De Capua"
                    },
                    {
                        "name": "M. De Cian"
                    },
                    {
                        "name": "U. De Freitas Carneiro Da Graca"
                    },
                    {
                        "name": "E. De Lucia"
                    },
                    {
                        "name": "J. M. De Miranda"
                    },
                    {
                        "name": "L. De Paula"
                    },
                    {
                        "name": "M. De Serio"
                    },
                    {
                        "name": "P. De Simone"
                    },
                    {
                        "name": "F. De Vellis"
                    },
                    {
                        "name": "J. A. de Vries"
                    },
                    {
                        "name": "F. Debernardis"
                    },
                    {
                        "name": "D. Decamp"
                    },
                    {
                        "name": "V. Dedu"
                    },
                    {
                        "name": "L. Del Buono"
                    },
                    {
                        "name": "B. Delaney"
                    },
                    {
                        "name": "H. -P. Dembinski"
                    },
                    {
                        "name": "J. Deng"
                    },
                    {
                        "name": "V. Denysenko"
                    },
                    {
                        "name": "O. Deschamps"
                    },
                    {
                        "name": "F. Dettori"
                    },
                    {
                        "name": "B. Dey"
                    },
                    {
                        "name": "P. Di Nezza"
                    },
                    {
                        "name": "I. Diachkov"
                    },
                    {
                        "name": "S. Didenko"
                    },
                    {
                        "name": "S. Ding"
                    },
                    {
                        "name": "L. Dittmann"
                    },
                    {
                        "name": "V. Dobishuk"
                    },
                    {
                        "name": "A. D. Docheva"
                    },
                    {
                        "name": "C. Dong"
                    },
                    {
                        "name": "A. M. Donohoe"
                    },
                    {
                        "name": "F. Dordei"
                    },
                    {
                        "name": "A. C. dos Reis"
                    },
                    {
                        "name": "A. D. Dowling"
                    },
                    {
                        "name": "W. Duan"
                    },
                    {
                        "name": "P. Duda"
                    },
                    {
                        "name": "M. W. Dudek"
                    },
                    {
                        "name": "L. Dufour"
                    },
                    {
                        "name": "V. Duk"
                    },
                    {
                        "name": "P. Durante"
                    },
                    {
                        "name": "M. M. Duras"
                    },
                    {
                        "name": "J. M. Durham"
                    },
                    {
                        "name": "O. D. Durmus"
                    },
                    {
                        "name": "A. Dziurda"
                    },
                    {
                        "name": "A. Dzyuba"
                    },
                    {
                        "name": "S. Easo"
                    },
                    {
                        "name": "E. Eckstein"
                    },
                    {
                        "name": "U. Egede"
                    },
                    {
                        "name": "A. Egorychev"
                    },
                    {
                        "name": "V. Egorychev"
                    },
                    {
                        "name": "S. Eisenhardt"
                    },
                    {
                        "name": "E. Ejopu"
                    },
                    {
                        "name": "L. Eklund"
                    },
                    {
                        "name": "M. Elashri"
                    },
                    {
                        "name": "J. Ellbracht"
                    },
                    {
                        "name": "S. Ely"
                    },
                    {
                        "name": "A. Ene"
                    },
                    {
                        "name": "E. Epple"
                    },
                    {
                        "name": "J. Eschle"
                    },
                    {
                        "name": "S. Esen"
                    },
                    {
                        "name": "T. Evans"
                    },
                    {
                        "name": "F. Fabiano"
                    },
                    {
                        "name": "L. N. Falcao"
                    },
                    {
                        "name": "Y. Fan"
                    },
                    {
                        "name": "B. Fang"
                    },
                    {
                        "name": "L. Fantini"
                    },
                    {
                        "name": "M. Faria"
                    },
                    {
                        "name": "K. Farmer"
                    },
                    {
                        "name": "D. Fazzini"
                    },
                    {
                        "name": "L. Felkowski"
                    },
                    {
                        "name": "M. Feng"
                    },
                    {
                        "name": "M. Feo"
                    },
                    {
                        "name": "M. Fernandez Gomez"
                    },
                    {
                        "name": "A. D. Fernez"
                    },
                    {
                        "name": "F. Ferrari"
                    },
                    {
                        "name": "F. Ferreira Rodrigues"
                    },
                    {
                        "name": "M. Ferrillo"
                    },
                    {
                        "name": "M. Ferro-Luzzi"
                    },
                    {
                        "name": "S. Filippov"
                    },
                    {
                        "name": "R. A. Fini"
                    },
                    {
                        "name": "M. Fiorini"
                    },
                    {
                        "name": "K. L. Fischer"
                    },
                    {
                        "name": "D. S. Fitzgerald"
                    },
                    {
                        "name": "C. Fitzpatrick"
                    },
                    {
                        "name": "F. Fleuret"
                    },
                    {
                        "name": "M. Fontana"
                    },
                    {
                        "name": "L. F. Foreman"
                    },
                    {
                        "name": "R. Forty"
                    },
                    {
                        "name": "D. Foulds-Holt"
                    },
                    {
                        "name": "M. Franco Sevilla"
                    },
                    {
                        "name": "M. Frank"
                    },
                    {
                        "name": "E. Franzoso"
                    },
                    {
                        "name": "G. Frau"
                    },
                    {
                        "name": "C. Frei"
                    },
                    {
                        "name": "D. A. Friday"
                    },
                    {
                        "name": "J. Fu"
                    },
                    {
                        "name": "Q. Führing"
                    },
                    {
                        "name": "Y. Fujii"
                    },
                    {
                        "name": "T. Fulghesu"
                    },
                    {
                        "name": "E. Gabriel"
                    },
                    {
                        "name": "G. Galati"
                    },
                    {
                        "name": "M. D. Galati"
                    },
                    {
                        "name": "A. Gallas Torreira"
                    },
                    {
                        "name": "D. Galli"
                    },
                    {
                        "name": "S. Gambetta"
                    },
                    {
                        "name": "M. Gandelman"
                    },
                    {
                        "name": "P. Gandini"
                    },
                    {
                        "name": "B. Ganie"
                    },
                    {
                        "name": "H. Gao"
                    },
                    {
                        "name": "R. Gao"
                    },
                    {
                        "name": "Y. Gao"
                    },
                    {
                        "name": "Y. Gao"
                    },
                    {
                        "name": "Y. Gao"
                    },
                    {
                        "name": "M. Garau"
                    },
                    {
                        "name": "L. M. Garcia Martin"
                    },
                    {
                        "name": "P. Garcia Moreno"
                    },
                    {
                        "name": "J. García Pardiñas"
                    },
                    {
                        "name": "K. G. Garg"
                    },
                    {
                        "name": "L. Garrido"
                    },
                    {
                        "name": "C. Gaspar"
                    },
                    {
                        "name": "R. E. Geertsema"
                    },
                    {
                        "name": "L. L. Gerken"
                    },
                    {
                        "name": "E. Gersabeck"
                    },
                    {
                        "name": "M. Gersabeck"
                    },
                    {
                        "name": "T. Gershon"
                    },
                    {
                        "name": "Z. Ghorbanimoghaddam"
                    },
                    {
                        "name": "L. Giambastiani"
                    },
                    {
                        "name": "F. I. Giasemis"
                    },
                    {
                        "name": "V. Gibson"
                    },
                    {
                        "name": "H. K. Giemza"
                    },
                    {
                        "name": "A. L. Gilman"
                    },
                    {
                        "name": "M. Giovannetti"
                    },
                    {
                        "name": "A. Gioventù"
                    },
                    {
                        "name": "P. Gironella Gironell"
                    },
                    {
                        "name": "C. Giugliano"
                    },
                    {
                        "name": "M. A. Giza"
                    },
                    {
                        "name": "E. L. Gkougkousis"
                    },
                    {
                        "name": "F. C. Glaser"
                    },
                    {
                        "name": "V. V. Gligorov"
                    },
                    {
                        "name": "C. Göbel"
                    },
                    {
                        "name": "E. Golobardes"
                    },
                    {
                        "name": "D. Golubkov"
                    },
                    {
                        "name": "A. Golutvin"
                    },
                    {
                        "name": "A. Gomes"
                    },
                    {
                        "name": "S. Gomez Fernandez"
                    },
                    {
                        "name": "F. Goncalves Abrantes"
                    },
                    {
                        "name": "M. Goncerz"
                    },
                    {
                        "name": "G. Gong"
                    },
                    {
                        "name": "J. A. Gooding"
                    },
                    {
                        "name": "I. V. Gorelov"
                    },
                    {
                        "name": "C. Gotti"
                    },
                    {
                        "name": "J. P. Grabowski"
                    },
                    {
                        "name": "L. A. Granado Cardoso"
                    },
                    {
                        "name": "E. Graugés"
                    },
                    {
                        "name": "E. Graverini"
                    },
                    {
                        "name": "L. Grazette"
                    },
                    {
                        "name": "G. Graziani"
                    },
                    {
                        "name": "A. T. Grecu"
                    },
                    {
                        "name": "L. M. Greeven"
                    },
                    {
                        "name": "N. A. Grieser"
                    },
                    {
                        "name": "L. Grillo"
                    },
                    {
                        "name": "S. Gromov"
                    },
                    {
                        "name": "C. Gu"
                    },
                    {
                        "name": "M. Guarise"
                    },
                    {
                        "name": "M. Guittiere"
                    },
                    {
                        "name": "V. Guliaeva"
                    },
                    {
                        "name": "P. A. Günther"
                    },
                    {
                        "name": "A. -K. Guseinov"
                    },
                    {
                        "name": "E. Gushchin"
                    },
                    {
                        "name": "Y. Guz"
                    },
                    {
                        "name": "T. Gys"
                    },
                    {
                        "name": "K. Habermann"
                    },
                    {
                        "name": "T. Hadavizadeh"
                    },
                    {
                        "name": "C. Hadjivasiliou"
                    },
                    {
                        "name": "G. Haefeli"
                    },
                    {
                        "name": "C. Haen"
                    },
                    {
                        "name": "J. Haimberger"
                    },
                    {
                        "name": "M. Hajheidari"
                    },
                    {
                        "name": "M. M. Halvorsen"
                    },
                    {
                        "name": "P. M. Hamilton"
                    },
                    {
                        "name": "J. Hammerich"
                    },
                    {
                        "name": "Q. Han"
                    },
                    {
                        "name": "X. Han"
                    },
                    {
                        "name": "S. Hansmann-Menzemer"
                    },
                    {
                        "name": "L. Hao"
                    },
                    {
                        "name": "N. Harnew"
                    },
                    {
                        "name": "M. Hartmann"
                    },
                    {
                        "name": "J. He"
                    },
                    {
                        "name": "F. Hemmer"
                    },
                    {
                        "name": "C. Henderson"
                    },
                    {
                        "name": "R. D. L. Henderson"
                    },
                    {
                        "name": "A. M. Hennequin"
                    },
                    {
                        "name": "K. Hennessy"
                    },
                    {
                        "name": "L. Henry"
                    },
                    {
                        "name": "J. Herd"
                    },
                    {
                        "name": "P. Herrero Gascon"
                    },
                    {
                        "name": "J. Heuel"
                    },
                    {
                        "name": "A. Hicheur"
                    },
                    {
                        "name": "G. Hijano Mendizabal"
                    },
                    {
                        "name": "D. Hill"
                    },
                    {
                        "name": "S. E. Hollitt"
                    },
                    {
                        "name": "J. Horswill"
                    },
                    {
                        "name": "R. Hou"
                    },
                    {
                        "name": "Y. Hou"
                    },
                    {
                        "name": "N. Howarth"
                    },
                    {
                        "name": "J. Hu"
                    },
                    {
                        "name": "J. Hu"
                    },
                    {
                        "name": "W. Hu"
                    },
                    {
                        "name": "X. Hu"
                    },
                    {
                        "name": "W. Huang"
                    },
                    {
                        "name": "W. Hulsbergen"
                    },
                    {
                        "name": "R. J. Hunter"
                    },
                    {
                        "name": "M. Hushchyn"
                    },
                    {
                        "name": "D. Hutchcroft"
                    },
                    {
                        "name": "D. Ilin"
                    },
                    {
                        "name": "P. Ilten"
                    },
                    {
                        "name": "A. Inglessi"
                    },
                    {
                        "name": "A. Iniukhin"
                    },
                    {
                        "name": "A. Ishteev"
                    },
                    {
                        "name": "K. Ivshin"
                    },
                    {
                        "name": "R. Jacobsson"
                    },
                    {
                        "name": "H. Jage"
                    },
                    {
                        "name": "S. J. Jaimes Elles"
                    },
                    {
                        "name": "S. Jakobsen"
                    },
                    {
                        "name": "E. Jans"
                    },
                    {
                        "name": "B. K. Jashal"
                    },
                    {
                        "name": "A. Jawahery"
                    },
                    {
                        "name": "V. Jevtic"
                    },
                    {
                        "name": "E. Jiang"
                    },
                    {
                        "name": "X. Jiang"
                    },
                    {
                        "name": "Y. Jiang"
                    },
                    {
                        "name": "Y. J. Jiang"
                    },
                    {
                        "name": "M. John"
                    },
                    {
                        "name": "D. Johnson"
                    },
                    {
                        "name": "C. R. Jones"
                    },
                    {
                        "name": "T. P. Jones"
                    },
                    {
                        "name": "S. Joshi"
                    },
                    {
                        "name": "B. Jost"
                    },
                    {
                        "name": "N. Jurik"
                    },
                    {
                        "name": "I. Juszczak"
                    },
                    {
                        "name": "D. Kaminaris"
                    },
                    {
                        "name": "S. Kandybei"
                    },
                    {
                        "name": "Y. Kang"
                    },
                    {
                        "name": "C. Kar"
                    },
                    {
                        "name": "M. Karacson"
                    },
                    {
                        "name": "D. Karpenkov"
                    },
                    {
                        "name": "A. Kauniskangas"
                    },
                    {
                        "name": "J. W. Kautz"
                    },
                    {
                        "name": "F. Keizer"
                    },
                    {
                        "name": "M. Kenzie"
                    },
                    {
                        "name": "T. Ketel"
                    },
                    {
                        "name": "B. Khanji"
                    },
                    {
                        "name": "A. Kharisova"
                    },
                    {
                        "name": "S. Kholodenko"
                    },
                    {
                        "name": "G. Khreich"
                    },
                    {
                        "name": "T. Kirn"
                    },
                    {
                        "name": "V. S. Kirsebom"
                    },
                    {
                        "name": "O. Kitouni"
                    },
                    {
                        "name": "S. Klaver"
                    },
                    {
                        "name": "N. Kleijne"
                    },
                    {
                        "name": "K. Klimaszewski"
                    },
                    {
                        "name": "M. R. Kmiec"
                    },
                    {
                        "name": "S. Koliiev"
                    },
                    {
                        "name": "L. Kolk"
                    },
                    {
                        "name": "A. Konoplyannikov"
                    },
                    {
                        "name": "P. Kopciewicz"
                    },
                    {
                        "name": "P. Koppenburg"
                    },
                    {
                        "name": "M. Korolev"
                    },
                    {
                        "name": "I. Kostiuk"
                    },
                    {
                        "name": "O. Kot"
                    },
                    {
                        "name": "S. Kotriakhova"
                    },
                    {
                        "name": "A. Kozachuk"
                    },
                    {
                        "name": "P. Kravchenko"
                    },
                    {
                        "name": "L. Kravchuk"
                    },
                    {
                        "name": "M. Kreps"
                    },
                    {
                        "name": "P. Krokovny"
                    },
                    {
                        "name": "W. Krupa"
                    },
                    {
                        "name": "W. Krzemien"
                    },
                    {
                        "name": "O. Kshyvanskyi"
                    },
                    {
                        "name": "J. Kubat"
                    },
                    {
                        "name": "S. Kubis"
                    },
                    {
                        "name": "M. Kucharczyk"
                    },
                    {
                        "name": "V. Kudryavtsev"
                    },
                    {
                        "name": "E. Kulikova"
                    },
                    {
                        "name": "A. Kupsc"
                    },
                    {
                        "name": "B. K. Kutsenko"
                    },
                    {
                        "name": "D. Lacarrere"
                    },
                    {
                        "name": "A. Lai"
                    },
                    {
                        "name": "A. Lampis"
                    },
                    {
                        "name": "D. Lancierini"
                    },
                    {
                        "name": "C. Landesa Gomez"
                    },
                    {
                        "name": "J. J. Lane"
                    },
                    {
                        "name": "R. Lane"
                    },
                    {
                        "name": "C. Langenbruch"
                    },
                    {
                        "name": "J. Langer"
                    },
                    {
                        "name": "O. Lantwin"
                    },
                    {
                        "name": "T. Latham"
                    },
                    {
                        "name": "F. Lazzari"
                    },
                    {
                        "name": "C. Lazzeroni"
                    },
                    {
                        "name": "R. Le Gac"
                    },
                    {
                        "name": "R. Lefèvre"
                    },
                    {
                        "name": "A. Leflat"
                    },
                    {
                        "name": "S. Legotin"
                    },
                    {
                        "name": "M. Lehuraux"
                    },
                    {
                        "name": "E. Lemos Cid"
                    },
                    {
                        "name": "O. Leroy"
                    },
                    {
                        "name": "T. Lesiak"
                    },
                    {
                        "name": "B. Leverington"
                    },
                    {
                        "name": "A. Li"
                    },
                    {
                        "name": "H. Li"
                    },
                    {
                        "name": "K. Li"
                    },
                    {
                        "name": "L. Li"
                    },
                    {
                        "name": "P. Li"
                    },
                    {
                        "name": "P. -R. Li"
                    },
                    {
                        "name": "Q. Li"
                    },
                    {
                        "name": "S. Li"
                    },
                    {
                        "name": "T. Li"
                    },
                    {
                        "name": "T. Li"
                    },
                    {
                        "name": "Y. Li"
                    },
                    {
                        "name": "Y. Li"
                    },
                    {
                        "name": "Z. Lian"
                    },
                    {
                        "name": "X. Liang"
                    },
                    {
                        "name": "S. Libralon"
                    },
                    {
                        "name": "C. Lin"
                    },
                    {
                        "name": "T. Lin"
                    },
                    {
                        "name": "R. Lindner"
                    },
                    {
                        "name": "V. Lisovskyi"
                    },
                    {
                        "name": "R. Litvinov"
                    },
                    {
                        "name": "F. L. Liu"
                    },
                    {
                        "name": "G. Liu"
                    },
                    {
                        "name": "K. Liu"
                    },
                    {
                        "name": "S. Liu"
                    },
                    {
                        "name": "Y. Liu"
                    },
                    {
                        "name": "Y. Liu"
                    },
                    {
                        "name": "Y. L. Liu"
                    },
                    {
                        "name": "A. Lobo Salvia"
                    },
                    {
                        "name": "A. Loi"
                    },
                    {
                        "name": "J. Lomba Castro"
                    },
                    {
                        "name": "T. Long"
                    },
                    {
                        "name": "J. H. Lopes"
                    },
                    {
                        "name": "A. Lopez Huertas"
                    },
                    {
                        "name": "S. López Soliño"
                    },
                    {
                        "name": "C. Lucarelli"
                    },
                    {
                        "name": "D. Lucchesi"
                    },
                    {
                        "name": "M. Lucio Martinez"
                    },
                    {
                        "name": "V. Lukashenko"
                    },
                    {
                        "name": "Y. Luo"
                    },
                    {
                        "name": "A. Lupato"
                    },
                    {
                        "name": "E. Luppi"
                    },
                    {
                        "name": "K. Lynch"
                    },
                    {
                        "name": "X. -R. Lyu"
                    },
                    {
                        "name": "G. M. Ma"
                    },
                    {
                        "name": "R. Ma"
                    },
                    {
                        "name": "S. Maccolini"
                    },
                    {
                        "name": "F. Machefert"
                    },
                    {
                        "name": "F. Maciuc"
                    },
                    {
                        "name": "B. Mack"
                    },
                    {
                        "name": "I. Mackay"
                    },
                    {
                        "name": "L. M. Mackey"
                    },
                    {
                        "name": "L. R. Madhan Mohan"
                    },
                    {
                        "name": "M. J. Madurai"
                    },
                    {
                        "name": "A. Maevskiy"
                    },
                    {
                        "name": "D. Magdalinski"
                    },
                    {
                        "name": "D. Maisuzenko"
                    },
                    {
                        "name": "M. W. Majewski"
                    },
                    {
                        "name": "J. J. Malczewski"
                    },
                    {
                        "name": "S. Malde"
                    },
                    {
                        "name": "L. Malentacca"
                    },
                    {
                        "name": "A. Malinin"
                    },
                    {
                        "name": "T. Maltsev"
                    },
                    {
                        "name": "G. Manca"
                    },
                    {
                        "name": "G. Mancinelli"
                    },
                    {
                        "name": "C. Mancuso"
                    },
                    {
                        "name": "R. Manera Escalero"
                    },
                    {
                        "name": "D. Manuzzi"
                    },
                    {
                        "name": "D. Marangotto"
                    },
                    {
                        "name": "J. F. Marchand"
                    },
                    {
                        "name": "R. Marchevski"
                    },
                    {
                        "name": "U. Marconi"
                    },
                    {
                        "name": "S. Mariani"
                    },
                    {
                        "name": "C. Marin Benito"
                    },
                    {
                        "name": "J. Marks"
                    },
                    {
                        "name": "A. M. Marshall"
                    },
                    {
                        "name": "G. Martelli"
                    },
                    {
                        "name": "G. Martellotti"
                    },
                    {
                        "name": "L. Martinazzoli"
                    },
                    {
                        "name": "M. Martinelli"
                    },
                    {
                        "name": "D. Martinez Santos"
                    },
                    {
                        "name": "F. Martinez Vidal"
                    },
                    {
                        "name": "A. Massafferri"
                    },
                    {
                        "name": "R. Matev"
                    },
                    {
                        "name": "A. Mathad"
                    },
                    {
                        "name": "V. Matiunin"
                    },
                    {
                        "name": "C. Matteuzzi"
                    },
                    {
                        "name": "K. R. Mattioli"
                    },
                    {
                        "name": "A. Mauri"
                    },
                    {
                        "name": "E. Maurice"
                    },
                    {
                        "name": "J. Mauricio"
                    },
                    {
                        "name": "P. Mayencourt"
                    },
                    {
                        "name": "M. Mazurek"
                    },
                    {
                        "name": "M. McCann"
                    },
                    {
                        "name": "L. Mcconnell"
                    },
                    {
                        "name": "T. H. McGrath"
                    },
                    {
                        "name": "N. T. McHugh"
                    },
                    {
                        "name": "A. McNab"
                    },
                    {
                        "name": "R. McNulty"
                    },
                    {
                        "name": "B. Meadows"
                    },
                    {
                        "name": "G. Meier"
                    },
                    {
                        "name": "D. Melnychuk"
                    },
                    {
                        "name": "F. M. Meng"
                    },
                    {
                        "name": "M. Merk"
                    },
                    {
                        "name": "A. Merli"
                    },
                    {
                        "name": "L. Meyer Garcia"
                    },
                    {
                        "name": "D. Miao"
                    },
                    {
                        "name": "H. Miao"
                    },
                    {
                        "name": "M. Mikhasenko"
                    },
                    {
                        "name": "D. A. Milanes"
                    },
                    {
                        "name": "A. Minotti"
                    },
                    {
                        "name": "E. Minucci"
                    },
                    {
                        "name": "T. Miralles"
                    },
                    {
                        "name": "B. Mitreska"
                    },
                    {
                        "name": "D. S. Mitzel"
                    },
                    {
                        "name": "A. Modak"
                    },
                    {
                        "name": "A. Mödden"
                    },
                    {
                        "name": "R. A. Mohammed"
                    },
                    {
                        "name": "R. D. Moise"
                    },
                    {
                        "name": "S. Mokhnenko"
                    },
                    {
                        "name": "T. Mombächer"
                    },
                    {
                        "name": "M. Monk"
                    },
                    {
                        "name": "S. Monteil"
                    },
                    {
                        "name": "A. Morcillo Gomez"
                    },
                    {
                        "name": "G. Morello"
                    },
                    {
                        "name": "M. J. Morello"
                    },
                    {
                        "name": "M. P. Morgenthaler"
                    },
                    {
                        "name": "A. B. Morris"
                    },
                    {
                        "name": "A. G. Morris"
                    },
                    {
                        "name": "R. Mountain"
                    },
                    {
                        "name": "H. Mu"
                    },
                    {
                        "name": "Z. M. Mu"
                    },
                    {
                        "name": "E. Muhammad"
                    },
                    {
                        "name": "F. Muheim"
                    },
                    {
                        "name": "M. Mulder"
                    },
                    {
                        "name": "K. Müller"
                    },
                    {
                        "name": "F. Muñoz-Rojas"
                    },
                    {
                        "name": "R. Murta"
                    },
                    {
                        "name": "P. Naik"
                    },
                    {
                        "name": "T. Nakada"
                    },
                    {
                        "name": "R. Nandakumar"
                    },
                    {
                        "name": "T. Nanut"
                    },
                    {
                        "name": "I. Nasteva"
                    },
                    {
                        "name": "M. Needham"
                    },
                    {
                        "name": "N. Neri"
                    },
                    {
                        "name": "S. Neubert"
                    },
                    {
                        "name": "N. Neufeld"
                    },
                    {
                        "name": "P. Neustroev"
                    },
                    {
                        "name": "J. Nicolini"
                    },
                    {
                        "name": "D. Nicotra"
                    },
                    {
                        "name": "E. M. Niel"
                    },
                    {
                        "name": "N. Nikitin"
                    },
                    {
                        "name": "Q. Niu"
                    },
                    {
                        "name": "P. Nogarolli"
                    },
                    {
                        "name": "P. Nogga"
                    },
                    {
                        "name": "N. S. Nolte"
                    },
                    {
                        "name": "C. Normand"
                    },
                    {
                        "name": "J. Novoa Fernandez"
                    },
                    {
                        "name": "G. Nowak"
                    },
                    {
                        "name": "C. Nunez"
                    },
                    {
                        "name": "H. N. Nur"
                    },
                    {
                        "name": "A. Oblakowska-Mucha"
                    },
                    {
                        "name": "V. Obraztsov"
                    },
                    {
                        "name": "T. Oeser"
                    },
                    {
                        "name": "S. Okamura"
                    },
                    {
                        "name": "A. Okhotnikov"
                    },
                    {
                        "name": "O. Okhrimenko"
                    },
                    {
                        "name": "R. Oldeman"
                    },
                    {
                        "name": "F. Oliva"
                    },
                    {
                        "name": "M. Olocco"
                    },
                    {
                        "name": "C. J. G. Onderwater"
                    },
                    {
                        "name": "R. H. O'Neil"
                    },
                    {
                        "name": "J. M. Otalora Goicochea"
                    },
                    {
                        "name": "P. Owen"
                    },
                    {
                        "name": "A. Oyanguren"
                    },
                    {
                        "name": "O. Ozcelik"
                    },
                    {
                        "name": "K. O. Padeken"
                    },
                    {
                        "name": "B. Pagare"
                    },
                    {
                        "name": "P. R. Pais"
                    },
                    {
                        "name": "T. Pajero"
                    },
                    {
                        "name": "A. Palano"
                    },
                    {
                        "name": "M. Palutan"
                    },
                    {
                        "name": "G. Panshin"
                    },
                    {
                        "name": "L. Paolucci"
                    },
                    {
                        "name": "A. Papanestis"
                    },
                    {
                        "name": "M. Pappagallo"
                    },
                    {
                        "name": "L. L. Pappalardo"
                    },
                    {
                        "name": "C. Pappenheimer"
                    },
                    {
                        "name": "C. Parkes"
                    },
                    {
                        "name": "B. Passalacqua"
                    },
                    {
                        "name": "G. Passaleva"
                    },
                    {
                        "name": "D. Passaro"
                    },
                    {
                        "name": "A. Pastore"
                    },
                    {
                        "name": "M. Patel"
                    },
                    {
                        "name": "J. Patoc"
                    },
                    {
                        "name": "C. Patrignani"
                    },
                    {
                        "name": "A. Paul"
                    },
                    {
                        "name": "C. J. Pawley"
                    },
                    {
                        "name": "A. Pellegrino"
                    },
                    {
                        "name": "J. Peng"
                    },
                    {
                        "name": "M. Pepe Altarelli"
                    },
                    {
                        "name": "S. Perazzini"
                    },
                    {
                        "name": "D. Pereima"
                    },
                    {
                        "name": "H. Pereira Da Costa"
                    },
                    {
                        "name": "A. Pereiro Castro"
                    },
                    {
                        "name": "P. Perret"
                    },
                    {
                        "name": "A. Perro"
                    },
                    {
                        "name": "K. Petridis"
                    },
                    {
                        "name": "A. Petrolini"
                    },
                    {
                        "name": "J. P. Pfaller"
                    },
                    {
                        "name": "H. Pham"
                    },
                    {
                        "name": "L. Pica"
                    },
                    {
                        "name": "M. Piccini"
                    },
                    {
                        "name": "B. Pietrzyk"
                    },
                    {
                        "name": "G. Pietrzyk"
                    },
                    {
                        "name": "D. Pinci"
                    },
                    {
                        "name": "F. Pisani"
                    },
                    {
                        "name": "M. Pizzichemi"
                    },
                    {
                        "name": "V. Placinta"
                    },
                    {
                        "name": "M. Plo Casasus"
                    },
                    {
                        "name": "F. Polci"
                    },
                    {
                        "name": "M. Poli Lener"
                    },
                    {
                        "name": "A. Poluektov"
                    },
                    {
                        "name": "N. Polukhina"
                    },
                    {
                        "name": "I. Polyakov"
                    },
                    {
                        "name": "E. Polycarpo"
                    },
                    {
                        "name": "S. Ponce"
                    },
                    {
                        "name": "D. Popov"
                    },
                    {
                        "name": "S. Poslavskii"
                    },
                    {
                        "name": "K. Prasanth"
                    },
                    {
                        "name": "C. Prouve"
                    },
                    {
                        "name": "V. Pugatch"
                    },
                    {
                        "name": "G. Punzi"
                    },
                    {
                        "name": "S. Qasim"
                    },
                    {
                        "name": "Q. Q. Qian"
                    },
                    {
                        "name": "W. Qian"
                    },
                    {
                        "name": "N. Qin"
                    },
                    {
                        "name": "S. Qu"
                    },
                    {
                        "name": "R. Quagliani"
                    },
                    {
                        "name": "R. I. Rabadan Trejo"
                    },
                    {
                        "name": "J. H. Rademacker"
                    },
                    {
                        "name": "M. Rama"
                    },
                    {
                        "name": "M. Ramírez García"
                    },
                    {
                        "name": "V. Ramos De Oliveira"
                    },
                    {
                        "name": "M. Ramos Pernas"
                    },
                    {
                        "name": "M. S. Rangel"
                    },
                    {
                        "name": "F. Ratnikov"
                    },
                    {
                        "name": "G. Raven"
                    },
                    {
                        "name": "M. Rebollo De Miguel"
                    },
                    {
                        "name": "F. Redi"
                    },
                    {
                        "name": "J. Reich"
                    },
                    {
                        "name": "F. Reiss"
                    },
                    {
                        "name": "Z. Ren"
                    },
                    {
                        "name": "P. K. Resmi"
                    },
                    {
                        "name": "R. Ribatti"
                    },
                    {
                        "name": "G. R. Ricart"
                    },
                    {
                        "name": "D. Riccardi"
                    },
                    {
                        "name": "S. Ricciardi"
                    },
                    {
                        "name": "K. Richardson"
                    },
                    {
                        "name": "M. Richardson-Slipper"
                    },
                    {
                        "name": "K. Rinnert"
                    },
                    {
                        "name": "P. Robbe"
                    },
                    {
                        "name": "G. Robertson"
                    },
                    {
                        "name": "E. Rodrigues"
                    },
                    {
                        "name": "E. Rodriguez Fernandez"
                    },
                    {
                        "name": "J. A. Rodriguez Lopez"
                    },
                    {
                        "name": "E. Rodriguez Rodriguez"
                    },
                    {
                        "name": "A. Rogovskiy"
                    },
                    {
                        "name": "D. L. Rolf"
                    },
                    {
                        "name": "P. Roloff"
                    },
                    {
                        "name": "V. Romanovskiy"
                    },
                    {
                        "name": "M. Romero Lamas"
                    },
                    {
                        "name": "A. Romero Vidal"
                    },
                    {
                        "name": "G. Romolini"
                    },
                    {
                        "name": "F. Ronchetti"
                    },
                    {
                        "name": "T. Rong"
                    },
                    {
                        "name": "M. Rotondo"
                    },
                    {
                        "name": "S. R. Roy"
                    },
                    {
                        "name": "M. S. Rudolph"
                    },
                    {
                        "name": "M. Ruiz Diaz"
                    },
                    {
                        "name": "R. A. Ruiz Fernandez"
                    },
                    {
                        "name": "J. Ruiz Vidal"
                    },
                    {
                        "name": "A. Ryzhikov"
                    },
                    {
                        "name": "J. Ryzka"
                    },
                    {
                        "name": "J. J. Saavedra-Arias"
                    },
                    {
                        "name": "J. J. Saborido Silva"
                    },
                    {
                        "name": "R. Sadek"
                    },
                    {
                        "name": "N. Sagidova"
                    },
                    {
                        "name": "D. Sahoo"
                    },
                    {
                        "name": "N. Sahoo"
                    },
                    {
                        "name": "B. Saitta"
                    },
                    {
                        "name": "M. Salomoni"
                    },
                    {
                        "name": "C. Sanchez Gras"
                    },
                    {
                        "name": "I. Sanderswood"
                    },
                    {
                        "name": "R. Santacesaria"
                    },
                    {
                        "name": "C. Santamarina Rios"
                    },
                    {
                        "name": "M. Santimaria"
                    },
                    {
                        "name": "L. Santoro"
                    },
                    {
                        "name": "E. Santovetti"
                    },
                    {
                        "name": "A. Saputi"
                    },
                    {
                        "name": "D. Saranin"
                    },
                    {
                        "name": "A. Sarnatskiy"
                    },
                    {
                        "name": "G. Sarpis"
                    },
                    {
                        "name": "M. Sarpis"
                    },
                    {
                        "name": "C. Satriano"
                    },
                    {
                        "name": "A. Satta"
                    },
                    {
                        "name": "M. Saur"
                    },
                    {
                        "name": "D. Savrina"
                    },
                    {
                        "name": "H. Sazak"
                    },
                    {
                        "name": "F. Sborzacchi"
                    },
                    {
                        "name": "L. G. Scantlebury Smead"
                    },
                    {
                        "name": "A. Scarabotto"
                    },
                    {
                        "name": "S. Schael"
                    },
                    {
                        "name": "S. Scherl"
                    },
                    {
                        "name": "M. Schiller"
                    },
                    {
                        "name": "H. Schindler"
                    },
                    {
                        "name": "M. Schmelling"
                    },
                    {
                        "name": "B. Schmidt"
                    },
                    {
                        "name": "S. Schmitt"
                    },
                    {
                        "name": "H. Schmitz"
                    },
                    {
                        "name": "O. Schneider"
                    },
                    {
                        "name": "A. Schopper"
                    },
                    {
                        "name": "N. Schulte"
                    },
                    {
                        "name": "S. Schulte"
                    },
                    {
                        "name": "M. H. Schune"
                    },
                    {
                        "name": "R. Schwemmer"
                    },
                    {
                        "name": "G. Schwering"
                    },
                    {
                        "name": "B. Sciascia"
                    },
                    {
                        "name": "A. Sciuccati"
                    },
                    {
                        "name": "S. Sellam"
                    },
                    {
                        "name": "A. Semennikov"
                    },
                    {
                        "name": "T. Senger"
                    },
                    {
                        "name": "M. Senghi Soares"
                    },
                    {
                        "name": "A. Sergi"
                    },
                    {
                        "name": "N. Serra"
                    },
                    {
                        "name": "L. Sestini"
                    },
                    {
                        "name": "A. Seuthe"
                    },
                    {
                        "name": "Y. Shang"
                    },
                    {
                        "name": "D. M. Shangase"
                    },
                    {
                        "name": "M. Shapkin"
                    },
                    {
                        "name": "R. S. Sharma"
                    },
                    {
                        "name": "I. Shchemerov"
                    },
                    {
                        "name": "L. Shchutska"
                    },
                    {
                        "name": "T. Shears"
                    },
                    {
                        "name": "L. Shekhtman"
                    },
                    {
                        "name": "Z. Shen"
                    },
                    {
                        "name": "S. Sheng"
                    },
                    {
                        "name": "V. Shevchenko"
                    },
                    {
                        "name": "B. Shi"
                    },
                    {
                        "name": "Q. Shi"
                    },
                    {
                        "name": "Y. Shimizu"
                    },
                    {
                        "name": "E. Shmanin"
                    },
                    {
                        "name": "R. Shorkin"
                    },
                    {
                        "name": "J. D. Shupperd"
                    },
                    {
                        "name": "R. Silva Coutinho"
                    },
                    {
                        "name": "G. Simi"
                    },
                    {
                        "name": "S. Simone"
                    },
                    {
                        "name": "N. Skidmore"
                    },
                    {
                        "name": "T. Skwarnicki"
                    },
                    {
                        "name": "M. W. Slater"
                    },
                    {
                        "name": "J. C. Smallwood"
                    },
                    {
                        "name": "E. Smith"
                    },
                    {
                        "name": "K. Smith"
                    },
                    {
                        "name": "M. Smith"
                    },
                    {
                        "name": "A. Snoch"
                    },
                    {
                        "name": "L. Soares Lavra"
                    },
                    {
                        "name": "M. D. Sokoloff"
                    },
                    {
                        "name": "F. J. P. Soler"
                    },
                    {
                        "name": "A. Solomin"
                    },
                    {
                        "name": "A. Solovev"
                    },
                    {
                        "name": "I. Solovyev"
                    },
                    {
                        "name": "R. Song"
                    },
                    {
                        "name": "Y. Song"
                    },
                    {
                        "name": "Y. Song"
                    },
                    {
                        "name": "Y. S. Song"
                    },
                    {
                        "name": "F. L. Souza De Almeida"
                    },
                    {
                        "name": "B. Souza De Paula"
                    },
                    {
                        "name": "E. Spadaro Norella"
                    },
                    {
                        "name": "E. Spedicato"
                    },
                    {
                        "name": "J. G. Speer"
                    },
                    {
                        "name": "E. Spiridenkov"
                    },
                    {
                        "name": "P. Spradlin"
                    },
                    {
                        "name": "V. Sriskaran"
                    },
                    {
                        "name": "F. Stagni"
                    },
                    {
                        "name": "M. Stahl"
                    },
                    {
                        "name": "S. Stahl"
                    },
                    {
                        "name": "S. Stanislaus"
                    },
                    {
                        "name": "E. N. Stein"
                    },
                    {
                        "name": "O. Steinkamp"
                    },
                    {
                        "name": "O. Stenyakin"
                    },
                    {
                        "name": "H. Stevens"
                    },
                    {
                        "name": "D. Strekalina"
                    },
                    {
                        "name": "Y. Su"
                    },
                    {
                        "name": "F. Suljik"
                    },
                    {
                        "name": "J. Sun"
                    },
                    {
                        "name": "L. Sun"
                    },
                    {
                        "name": "Y. Sun"
                    },
                    {
                        "name": "D. Sundfeld"
                    },
                    {
                        "name": "W. Sutcliffe"
                    },
                    {
                        "name": "P. N. Swallow"
                    },
                    {
                        "name": "F. Swystun"
                    },
                    {
                        "name": "A. Szabelski"
                    },
                    {
                        "name": "T. Szumlak"
                    },
                    {
                        "name": "Y. Tan"
                    },
                    {
                        "name": "M. D. Tat"
                    },
                    {
                        "name": "A. Terentev"
                    },
                    {
                        "name": "F. Terzuoli"
                    },
                    {
                        "name": "F. Teubert"
                    },
                    {
                        "name": "E. Thomas"
                    },
                    {
                        "name": "D. J. D. Thompson"
                    },
                    {
                        "name": "H. Tilquin"
                    },
                    {
                        "name": "V. Tisserand"
                    },
                    {
                        "name": "S. T'Jampens"
                    },
                    {
                        "name": "M. Tobin"
                    },
                    {
                        "name": "L. Tomassetti"
                    },
                    {
                        "name": "G. Tonani"
                    },
                    {
                        "name": "X. Tong"
                    },
                    {
                        "name": "D. Torres Machado"
                    },
                    {
                        "name": "L. Toscano"
                    },
                    {
                        "name": "D. Y. Tou"
                    },
                    {
                        "name": "C. Trippl"
                    },
                    {
                        "name": "G. Tuci"
                    },
                    {
                        "name": "N. Tuning"
                    },
                    {
                        "name": "L. H. Uecker"
                    },
                    {
                        "name": "A. Ukleja"
                    },
                    {
                        "name": "D. J. Unverzagt"
                    },
                    {
                        "name": "E. Ursov"
                    },
                    {
                        "name": "A. Usachov"
                    },
                    {
                        "name": "A. Ustyuzhanin"
                    },
                    {
                        "name": "U. Uwer"
                    },
                    {
                        "name": "V. Vagnoni"
                    },
                    {
                        "name": "G. Valenti"
                    },
                    {
                        "name": "N. Valls Canudas"
                    },
                    {
                        "name": "H. Van Hecke"
                    },
                    {
                        "name": "E. van Herwijnen"
                    },
                    {
                        "name": "C. B. Van Hulse"
                    },
                    {
                        "name": "R. Van Laak"
                    },
                    {
                        "name": "M. van Veghel"
                    },
                    {
                        "name": "G. Vasquez"
                    },
                    {
                        "name": "R. Vazquez Gomez"
                    },
                    {
                        "name": "P. Vazquez Regueiro"
                    },
                    {
                        "name": "C. Vázquez Sierra"
                    },
                    {
                        "name": "S. Vecchi"
                    },
                    {
                        "name": "J. J. Velthuis"
                    },
                    {
                        "name": "M. Veltri"
                    },
                    {
                        "name": "A. Venkateswaran"
                    },
                    {
                        "name": "M. Vesterinen"
                    },
                    {
                        "name": "M. Vieites Diaz"
                    },
                    {
                        "name": "X. Vilasis-Cardona"
                    },
                    {
                        "name": "E. Vilella Figueras"
                    },
                    {
                        "name": "A. Villa"
                    },
                    {
                        "name": "P. Vincent"
                    },
                    {
                        "name": "F. C. Volle"
                    },
                    {
                        "name": "D. vom Bruch"
                    },
                    {
                        "name": "N. Voropaev"
                    },
                    {
                        "name": "K. Vos"
                    },
                    {
                        "name": "G. Vouters"
                    },
                    {
                        "name": "C. Vrahas"
                    },
                    {
                        "name": "J. Wagner"
                    },
                    {
                        "name": "J. Walsh"
                    },
                    {
                        "name": "E. J. Walton"
                    },
                    {
                        "name": "G. Wan"
                    },
                    {
                        "name": "C. Wang"
                    },
                    {
                        "name": "G. Wang"
                    },
                    {
                        "name": "H. Wang"
                    },
                    {
                        "name": "J. Wang"
                    },
                    {
                        "name": "J. Wang"
                    },
                    {
                        "name": "J. Wang"
                    },
                    {
                        "name": "J. Wang"
                    },
                    {
                        "name": "M. Wang"
                    },
                    {
                        "name": "N. W. Wang"
                    },
                    {
                        "name": "R. Wang"
                    },
                    {
                        "name": "X. Wang"
                    },
                    {
                        "name": "X. Wang"
                    },
                    {
                        "name": "X. W. Wang"
                    },
                    {
                        "name": "Y. Wang"
                    },
                    {
                        "name": "Y. W. Wang"
                    },
                    {
                        "name": "Z. Wang"
                    },
                    {
                        "name": "Z. Wang"
                    },
                    {
                        "name": "Z. Wang"
                    },
                    {
                        "name": "J. A. Ward"
                    },
                    {
                        "name": "M. Waterlaat"
                    },
                    {
                        "name": "N. K. Watson"
                    },
                    {
                        "name": "D. Websdale"
                    },
                    {
                        "name": "Y. Wei"
                    },
                    {
                        "name": "J. Wendel"
                    },
                    {
                        "name": "B. D. C. Westhenry"
                    },
                    {
                        "name": "D. J. White"
                    },
                    {
                        "name": "M. Whitehead"
                    },
                    {
                        "name": "A. R. Wiederhold"
                    },
                    {
                        "name": "D. Wiedner"
                    },
                    {
                        "name": "G. Wilkinson"
                    },
                    {
                        "name": "M. K. Wilkinson"
                    },
                    {
                        "name": "M. Williams"
                    },
                    {
                        "name": "M. R. J. Williams"
                    },
                    {
                        "name": "R. Williams"
                    },
                    {
                        "name": "F. F. Wilson"
                    },
                    {
                        "name": "W. Wislicki"
                    },
                    {
                        "name": "M. Witek"
                    },
                    {
                        "name": "L. Witola"
                    },
                    {
                        "name": "C. P. Wong"
                    },
                    {
                        "name": "G. Wormser"
                    },
                    {
                        "name": "S. A. Wotton"
                    },
                    {
                        "name": "H. Wu"
                    },
                    {
                        "name": "J. Wu"
                    },
                    {
                        "name": "Y. Wu"
                    },
                    {
                        "name": "Z. Wu"
                    },
                    {
                        "name": "K. Wyllie"
                    },
                    {
                        "name": "S. Xian"
                    },
                    {
                        "name": "Z. Xiang"
                    },
                    {
                        "name": "Y. Xie"
                    },
                    {
                        "name": "A. Xu"
                    },
                    {
                        "name": "J. Xu"
                    },
                    {
                        "name": "L. Xu"
                    },
                    {
                        "name": "L. Xu"
                    },
                    {
                        "name": "M. Xu"
                    },
                    {
                        "name": "Z. Xu"
                    },
                    {
                        "name": "Z. Xu"
                    },
                    {
                        "name": "Z. Xu"
                    },
                    {
                        "name": "D. Yang"
                    },
                    {
                        "name": "K. Yang"
                    },
                    {
                        "name": "S. Yang"
                    },
                    {
                        "name": "X. Yang"
                    },
                    {
                        "name": "Y. Yang"
                    },
                    {
                        "name": "Z. Yang"
                    },
                    {
                        "name": "Z. Yang"
                    },
                    {
                        "name": "V. Yeroshenko"
                    },
                    {
                        "name": "H. Yeung"
                    },
                    {
                        "name": "H. Yin"
                    },
                    {
                        "name": "X. Yin"
                    },
                    {
                        "name": "C. Y. Yu"
                    },
                    {
                        "name": "J. Yu"
                    },
                    {
                        "name": "X. Yuan"
                    },
                    {
                        "name": "E. Zaffaroni"
                    },
                    {
                        "name": "M. Zavertyaev"
                    },
                    {
                        "name": "M. Zdybal"
                    },
                    {
                        "name": "C. Zeng"
                    },
                    {
                        "name": "M. Zeng"
                    },
                    {
                        "name": "C. Zhang"
                    },
                    {
                        "name": "D. Zhang"
                    },
                    {
                        "name": "J. Zhang"
                    },
                    {
                        "name": "L. Zhang"
                    },
                    {
                        "name": "S. Zhang"
                    },
                    {
                        "name": "S. Zhang"
                    },
                    {
                        "name": "Y. Zhang"
                    },
                    {
                        "name": "Y. Z. Zhang"
                    },
                    {
                        "name": "Y. Zhao"
                    },
                    {
                        "name": "A. Zharkova"
                    },
                    {
                        "name": "A. Zhelezov"
                    },
                    {
                        "name": "S. Z. Zheng"
                    },
                    {
                        "name": "X. Z. Zheng"
                    },
                    {
                        "name": "Y. Zheng"
                    },
                    {
                        "name": "T. Zhou"
                    },
                    {
                        "name": "X. Zhou"
                    },
                    {
                        "name": "Y. Zhou"
                    },
                    {
                        "name": "V. Zhovkovska"
                    },
                    {
                        "name": "L. Z. Zhu"
                    },
                    {
                        "name": "X. Zhu"
                    },
                    {
                        "name": "X. Zhu"
                    },
                    {
                        "name": "V. Zhukov"
                    },
                    {
                        "name": "J. Zhuo"
                    },
                    {
                        "name": "Q. Zou"
                    },
                    {
                        "name": "D. Zuliani"
                    },
                    {
                        "name": "G. Zunica"
                    }
                ],
                "author_detail": {
                    "name": "G. Zunica"
                },
                "author": "G. Zunica",
                "arxiv_doi": "10.1103/PhysRevD.111.012001",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevD.111.012001",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.18001v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18001v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "All figures and tables, along with machine-readable versions and any\n  supplementary material and additional information, are available at\n  https://lbfence.cern.ch/alcm/public/analysis/full-details/1800",
                "arxiv_journal_ref": "Phys. Rev. D 111 (2025) 012001",
                "arxiv_primary_category": {
                    "term": "hep-ex",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12796v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12796v1",
                "updated": "2025-01-22T10:58:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    10,
                    58,
                    4,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T10:58:04Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    10,
                    58,
                    4,
                    2,
                    22,
                    0
                ],
                "title": "Hybrid Losses for Hierarchical Embedding Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid Losses for Hierarchical Embedding Learning"
                },
                "summary": "In traditional supervised learning, the cross-entropy loss treats all\nincorrect predictions equally, ignoring the relevance or proximity of wrong\nlabels to the correct answer. By leveraging a tree hierarchy for fine-grained\nlabels, we investigate hybrid losses, such as generalised triplet and\ncross-entropy losses, to enforce similarity between labels within a multi-task\nlearning framework. We propose metrics to evaluate the embedding space\nstructure and assess the model's ability to generalise to unseen classes, that\nis, to infer similar classes for data belonging to unseen categories. Our\nexperiments on OrchideaSOL, a four-level hierarchical instrument sound dataset\nwith nearly 200 detailed categories, demonstrate that the proposed hybrid\nlosses outperform previous works in classification, retrieval, embedding space\nstructure, and generalisation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In traditional supervised learning, the cross-entropy loss treats all\nincorrect predictions equally, ignoring the relevance or proximity of wrong\nlabels to the correct answer. By leveraging a tree hierarchy for fine-grained\nlabels, we investigate hybrid losses, such as generalised triplet and\ncross-entropy losses, to enforce similarity between labels within a multi-task\nlearning framework. We propose metrics to evaluate the embedding space\nstructure and assess the model's ability to generalise to unseen classes, that\nis, to infer similar classes for data belonging to unseen categories. Our\nexperiments on OrchideaSOL, a four-level hierarchical instrument sound dataset\nwith nearly 200 detailed categories, demonstrate that the proposed hybrid\nlosses outperform previous works in classification, retrieval, embedding space\nstructure, and generalisation."
                },
                "authors": [
                    {
                        "name": "Haokun Tian"
                    },
                    {
                        "name": "Stefan Lattner"
                    },
                    {
                        "name": "Brian McFee"
                    },
                    {
                        "name": "Charalampos Saitis"
                    }
                ],
                "author_detail": {
                    "name": "Charalampos Saitis"
                },
                "author": "Charalampos Saitis",
                "arxiv_comment": "Accepted to ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12796v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12796v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12793v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12793v1",
                "updated": "2025-01-22T10:54:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    10,
                    54,
                    19,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T10:54:19Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    10,
                    54,
                    19,
                    2,
                    22,
                    0
                ],
                "title": "Revisit Self-Debugging with Self-Generated Tests for Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisit Self-Debugging with Self-Generated Tests for Code Generation"
                },
                "summary": "Large language models (LLMs) have shown significant advancements in code\ngeneration, but still face challenges on tasks beyond their basic capabilities.\nRecently, the notion of self-debugging has been proposed to boost the\nperformance of code generation by leveraging execution feedback from tests.\nDespite its promise, the availability of high-quality tests in real-world\nscenarios is limited. In this context, self-debugging with self-generated tests\nis a promising solution but lacks a full exploration of its limitations and\npractical potential. Therefore, we investigate its efficacy on diverse\nprogramming problems. To deepen our understanding, we propose two distinct\nparadigms for the process: post-execution and in-execution self-debugging.\nWithin the scope of self-contained Python programming tasks, we find that\npost-execution self-debugging struggles on basic problems but shows potential\nfor improvement on competitive ones, due to the bias introduced by\nself-generated tests. On the other hand, in-execution self-debugging enables\nLLMs to mitigate the bias by solely leveraging intermediate states during\nexecution, thereby enhancing code generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown significant advancements in code\ngeneration, but still face challenges on tasks beyond their basic capabilities.\nRecently, the notion of self-debugging has been proposed to boost the\nperformance of code generation by leveraging execution feedback from tests.\nDespite its promise, the availability of high-quality tests in real-world\nscenarios is limited. In this context, self-debugging with self-generated tests\nis a promising solution but lacks a full exploration of its limitations and\npractical potential. Therefore, we investigate its efficacy on diverse\nprogramming problems. To deepen our understanding, we propose two distinct\nparadigms for the process: post-execution and in-execution self-debugging.\nWithin the scope of self-contained Python programming tasks, we find that\npost-execution self-debugging struggles on basic problems but shows potential\nfor improvement on competitive ones, due to the bias introduced by\nself-generated tests. On the other hand, in-execution self-debugging enables\nLLMs to mitigate the bias by solely leveraging intermediate states during\nexecution, thereby enhancing code generation."
                },
                "authors": [
                    {
                        "name": "Xiancai Chen"
                    },
                    {
                        "name": "Zhengwei Tao"
                    },
                    {
                        "name": "Kechi Zhang"
                    },
                    {
                        "name": "Changzhi Zhou"
                    },
                    {
                        "name": "Wanli Gu"
                    },
                    {
                        "name": "Yuanpeng He"
                    },
                    {
                        "name": "Mengdi Zhang"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Haiyan Zhao"
                    },
                    {
                        "name": "Zhi Jin"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Jin"
                },
                "author": "Zhi Jin",
                "arxiv_comment": "Work in Progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12793v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12793v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12789v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12789v1",
                "updated": "2025-01-22T10:47:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    10,
                    47,
                    8,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T10:47:08Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    10,
                    47,
                    8,
                    2,
                    22,
                    0
                ],
                "title": "Generating Diverse Q&A Benchmarks for RAG Evaluation with DataMorgana",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating Diverse Q&A Benchmarks for RAG Evaluation with DataMorgana"
                },
                "summary": "Evaluating Retrieval-Augmented Generation (RAG) systems, especially in\ndomain-specific contexts, requires benchmarks that address the distinctive\nrequirements of the applicative scenario. Since real data can be hard to\nobtain, a common strategy is to use LLM-based methods to generate synthetic\ndata. Existing solutions are general purpose: given a document, they generate a\nquestion to build a Q&A pair. However, although the generated questions can be\nindividually good, they are typically not diverse enough to reasonably cover\nthe different ways real end-users can interact with the RAG system. We\nintroduce here DataMorgana, a tool for generating highly customizable and\ndiverse synthetic Q&A benchmarks tailored to RAG applications. DataMorgana\nenables detailed configurations of user and question categories and provides\ncontrol over their distribution within the benchmark. It uses a lightweight\ntwo-stage process, ensuring efficiency and fast iterations, while generating\nbenchmarks that reflect the expected traffic. We conduct a thorough line of\nexperiments, showing quantitatively and qualitatively that DataMorgana\nsurpasses existing tools and approaches in producing lexically, syntactically,\nand semantically diverse question sets across domain-specific and\ngeneral-knowledge corpora. DataMorgana will be made available to selected teams\nin the research community, as first beta testers, in the context of the\nupcoming SIGIR'2025 LiveRAG challenge to be announced in early February 2025.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Retrieval-Augmented Generation (RAG) systems, especially in\ndomain-specific contexts, requires benchmarks that address the distinctive\nrequirements of the applicative scenario. Since real data can be hard to\nobtain, a common strategy is to use LLM-based methods to generate synthetic\ndata. Existing solutions are general purpose: given a document, they generate a\nquestion to build a Q&A pair. However, although the generated questions can be\nindividually good, they are typically not diverse enough to reasonably cover\nthe different ways real end-users can interact with the RAG system. We\nintroduce here DataMorgana, a tool for generating highly customizable and\ndiverse synthetic Q&A benchmarks tailored to RAG applications. DataMorgana\nenables detailed configurations of user and question categories and provides\ncontrol over their distribution within the benchmark. It uses a lightweight\ntwo-stage process, ensuring efficiency and fast iterations, while generating\nbenchmarks that reflect the expected traffic. We conduct a thorough line of\nexperiments, showing quantitatively and qualitatively that DataMorgana\nsurpasses existing tools and approaches in producing lexically, syntactically,\nand semantically diverse question sets across domain-specific and\ngeneral-knowledge corpora. DataMorgana will be made available to selected teams\nin the research community, as first beta testers, in the context of the\nupcoming SIGIR'2025 LiveRAG challenge to be announced in early February 2025."
                },
                "authors": [
                    {
                        "name": "Simone Filice"
                    },
                    {
                        "name": "Guy Horowitz"
                    },
                    {
                        "name": "David Carmel"
                    },
                    {
                        "name": "Zohar Karnin"
                    },
                    {
                        "name": "Liane Lewin-Eytan"
                    },
                    {
                        "name": "Yoelle Maarek"
                    }
                ],
                "author_detail": {
                    "name": "Yoelle Maarek"
                },
                "author": "Yoelle Maarek",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12789v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12789v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03940v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03940v2",
                "updated": "2025-01-22T10:39:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    10,
                    39,
                    50,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-07T17:00:49Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    0,
                    49,
                    1,
                    7,
                    0
                ],
                "title": "Not all tokens are created equal: Perplexity Attention Weighted Networks\n  for AI generated text detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not all tokens are created equal: Perplexity Attention Weighted Networks\n  for AI generated text detection"
                },
                "summary": "The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages."
                },
                "authors": [
                    {
                        "name": "Pablo Miralles-González"
                    },
                    {
                        "name": "Javier Huertas-Tato"
                    },
                    {
                        "name": "Alejandro Martín"
                    },
                    {
                        "name": "David Camacho"
                    }
                ],
                "author_detail": {
                    "name": "David Camacho"
                },
                "author": "David Camacho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03940v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03940v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.16184v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.16184v2",
                "updated": "2025-01-22T10:30:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    10,
                    30,
                    32,
                    2,
                    22,
                    0
                ],
                "published": "2024-03-24T15:02:24Z",
                "published_parsed": [
                    2024,
                    3,
                    24,
                    15,
                    2,
                    24,
                    6,
                    84,
                    0
                ],
                "title": "Predicate Debiasing in Vision-Language Models Integration for Scene\n  Graph Generation Enhancement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicate Debiasing in Vision-Language Models Integration for Scene\n  Graph Generation Enhancement"
                },
                "summary": "Scene Graph Generation (SGG) provides basic language representation of visual\nscenes, requiring models to grasp complex and diverse semantics between\nobjects. This complexity and diversity in SGG leads to underrepresentation,\nwhere parts of triplet labels are rare or even unseen during training,\nresulting in imprecise predictions. To tackle this, we propose integrating the\npretrained Vision-language Models to enhance representation. However, due to\nthe gap between pretraining and SGG, direct inference of pretrained VLMs on SGG\nleads to severe bias, which stems from the imbalanced predicates distribution\nin the pretraining language set. To alleviate the bias, we introduce a novel LM\nEstimation to approximate the unattainable predicates distribution. Finally, we\nensemble the debiased VLMs with SGG models to enhance the representation, where\nwe design a certainty-aware indicator to score each sample and dynamically\nadjust the ensemble weights. Our training-free method effectively addresses the\npredicates bias in pretrained VLMs, enhances SGG's representation, and\nsignificantly improve the performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scene Graph Generation (SGG) provides basic language representation of visual\nscenes, requiring models to grasp complex and diverse semantics between\nobjects. This complexity and diversity in SGG leads to underrepresentation,\nwhere parts of triplet labels are rare or even unseen during training,\nresulting in imprecise predictions. To tackle this, we propose integrating the\npretrained Vision-language Models to enhance representation. However, due to\nthe gap between pretraining and SGG, direct inference of pretrained VLMs on SGG\nleads to severe bias, which stems from the imbalanced predicates distribution\nin the pretraining language set. To alleviate the bias, we introduce a novel LM\nEstimation to approximate the unattainable predicates distribution. Finally, we\nensemble the debiased VLMs with SGG models to enhance the representation, where\nwe design a certainty-aware indicator to score each sample and dynamically\nadjust the ensemble weights. Our training-free method effectively addresses the\npredicates bias in pretrained VLMs, enhances SGG's representation, and\nsignificantly improve the performance."
                },
                "authors": [
                    {
                        "name": "Yuxuan Wang"
                    },
                    {
                        "name": "Xiaoyuan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoyuan Liu"
                },
                "author": "Xiaoyuan Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.16184v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.16184v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12774v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12774v1",
                "updated": "2025-01-22T10:16:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    10,
                    16,
                    53,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T10:16:53Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    10,
                    16,
                    53,
                    2,
                    22,
                    0
                ],
                "title": "LLMs as Repositories of Factual Knowledge: Limitations and Solutions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs as Repositories of Factual Knowledge: Limitations and Solutions"
                },
                "summary": "LLMs' sources of knowledge are data snapshots containing factual information\nabout entities collected at different timestamps and from different media types\n(e.g. wikis, social media, etc.). Such unstructured knowledge is subject to\nchange due to updates through time from past to present. Equally important are\nthe inconsistencies and inaccuracies occurring in different information\nsources. Consequently, the model's knowledge about an entity may be perturbed\nwhile training over the sequence of snapshots or at inference time, resulting\nin inconsistent and inaccurate model performance. In this work, we study the\nappropriateness of Large Language Models (LLMs) as repositories of factual\nknowledge. We consider twenty-four state-of-the-art LLMs that are either\nclosed-, partially (weights), or fully (weight and training data) open-source.\nWe evaluate their reliability in responding to time-sensitive factual questions\nin terms of accuracy and consistency when prompts are perturbed. We further\nevaluate the effectiveness of state-of-the-art methods to improve LLMs'\naccuracy and consistency. We then propose \"ENtity-Aware Fine-tuning\" (ENAF), a\nsoft neurosymbolic approach aimed at providing a structured representation of\nentities during fine-tuning to improve the model's performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs' sources of knowledge are data snapshots containing factual information\nabout entities collected at different timestamps and from different media types\n(e.g. wikis, social media, etc.). Such unstructured knowledge is subject to\nchange due to updates through time from past to present. Equally important are\nthe inconsistencies and inaccuracies occurring in different information\nsources. Consequently, the model's knowledge about an entity may be perturbed\nwhile training over the sequence of snapshots or at inference time, resulting\nin inconsistent and inaccurate model performance. In this work, we study the\nappropriateness of Large Language Models (LLMs) as repositories of factual\nknowledge. We consider twenty-four state-of-the-art LLMs that are either\nclosed-, partially (weights), or fully (weight and training data) open-source.\nWe evaluate their reliability in responding to time-sensitive factual questions\nin terms of accuracy and consistency when prompts are perturbed. We further\nevaluate the effectiveness of state-of-the-art methods to improve LLMs'\naccuracy and consistency. We then propose \"ENtity-Aware Fine-tuning\" (ENAF), a\nsoft neurosymbolic approach aimed at providing a structured representation of\nentities during fine-tuning to improve the model's performance."
                },
                "authors": [
                    {
                        "name": "Seyed Mahed Mousavi"
                    },
                    {
                        "name": "Simone Alghisi"
                    },
                    {
                        "name": "Giuseppe Riccardi"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Riccardi"
                },
                "author": "Giuseppe Riccardi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12774v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12774v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12768v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12768v1",
                "updated": "2025-01-22T10:05:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    10,
                    5,
                    28,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T10:05:28Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    10,
                    5,
                    28,
                    2,
                    22,
                    0
                ],
                "title": "A Bayesian Modelling Framework with Model Comparison for Epidemics with\n  Super-Spreading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Bayesian Modelling Framework with Model Comparison for Epidemics with\n  Super-Spreading"
                },
                "summary": "The transmission dynamics of an epidemic are rarely homogeneous.\nSuper-spreading events and super-spreading individuals are two types of\nheterogeneous transmissibility. Inference of super-spreading is commonly\ncarried out on secondary case data, the expected distribution of which is known\nas the offspring distribution. However, this data is seldom available. Here we\nintroduce a multi-model framework fit to incidence time-series, data that is\nmuch more readily available. The framework consists of five discrete-time,\nstochastic, branching-process models of epidemics spread through a susceptible\npopulation. The framework includes a baseline model of homogeneous\ntransmission, a unimodal and a bimodal model for super-spreading events, as\nwell as a unimodal and a bimodal model for super-spreading individuals.\nBayesian statistics is used to infer model parameters using Markov Chain\nMonte-Carlo. Model comparison is conducted by computing Bayes factors, with\nimportance sampling used to estimate the marginal likelihood of each model.\nThis estimator is selected for its consistency and lower variance compared to\nalternatives. Application to simulated data from each model identifies the\ncorrect model for the majority of simulations and accurately infers the true\nparameters, such as the basic reproduction number. We also apply our methods to\nincidence data from the 2003 SARS outbreak and the Covid-19 pandemic. Model\nselection consistently identifies the same model and mechanism for a given\ndisease, even when using different time series. Our estimates are consistent\nwith previous studies based on secondary case data. Quantifying the\ncontribution of super-spreading to disease transmission has important\nimplications for infectious disease management and control. Our modelling\nframework is disease-agnostic and implemented as an R package, with potential\nto be a valuable tool for public health.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The transmission dynamics of an epidemic are rarely homogeneous.\nSuper-spreading events and super-spreading individuals are two types of\nheterogeneous transmissibility. Inference of super-spreading is commonly\ncarried out on secondary case data, the expected distribution of which is known\nas the offspring distribution. However, this data is seldom available. Here we\nintroduce a multi-model framework fit to incidence time-series, data that is\nmuch more readily available. The framework consists of five discrete-time,\nstochastic, branching-process models of epidemics spread through a susceptible\npopulation. The framework includes a baseline model of homogeneous\ntransmission, a unimodal and a bimodal model for super-spreading events, as\nwell as a unimodal and a bimodal model for super-spreading individuals.\nBayesian statistics is used to infer model parameters using Markov Chain\nMonte-Carlo. Model comparison is conducted by computing Bayes factors, with\nimportance sampling used to estimate the marginal likelihood of each model.\nThis estimator is selected for its consistency and lower variance compared to\nalternatives. Application to simulated data from each model identifies the\ncorrect model for the majority of simulations and accurately infers the true\nparameters, such as the basic reproduction number. We also apply our methods to\nincidence data from the 2003 SARS outbreak and the Covid-19 pandemic. Model\nselection consistently identifies the same model and mechanism for a given\ndisease, even when using different time series. Our estimates are consistent\nwith previous studies based on secondary case data. Quantifying the\ncontribution of super-spreading to disease transmission has important\nimplications for infectious disease management and control. Our modelling\nframework is disease-agnostic and implemented as an R package, with potential\nto be a valuable tool for public health."
                },
                "authors": [
                    {
                        "name": "Hannah Craddock"
                    },
                    {
                        "name": "Simon EF Spencer"
                    },
                    {
                        "name": "Xavier Didelot"
                    }
                ],
                "author_detail": {
                    "name": "Xavier Didelot"
                },
                "author": "Xavier Didelot",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12768v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12768v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.QM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12766v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12766v1",
                "updated": "2025-01-22T10:01:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    10,
                    1,
                    54,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T10:01:54Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    10,
                    1,
                    54,
                    2,
                    22,
                    0
                ],
                "title": "NExtLong: Toward Effective Long-Context Training without Long Documents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NExtLong: Toward Effective Long-Context Training without Long Documents"
                },
                "summary": "Large language models (LLMs) with extended context windows have made\nsignificant strides yet remain a challenge due to the scarcity of long\ndocuments. Existing methods tend to synthesize long-context data but lack a\nclear mechanism to reinforce the long-range dependency modeling. To address\nthis limitation, we propose NExtLong, a novel framework for synthesizing\nlong-context data through Negative document Extension. NExtLong decomposes a\ndocument into multiple meta-chunks and extends the context by interleaving hard\nnegative distractors retrieved from pretraining corpora. This approach compels\nthe model to discriminate long-range dependent context from distracting\ncontent, enhancing its ability to model long-range dependencies. Extensive\nexperiments demonstrate that NExtLong achieves significant performance\nimprovements on the HELMET and RULER benchmarks compared to existing\nlong-context synthesis approaches and leading models, which are trained on\nnon-synthetic long documents. These findings highlight NExtLong's ability to\nreduce reliance on non-synthetic long documents, making it an effective\nframework for developing advanced long-context LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with extended context windows have made\nsignificant strides yet remain a challenge due to the scarcity of long\ndocuments. Existing methods tend to synthesize long-context data but lack a\nclear mechanism to reinforce the long-range dependency modeling. To address\nthis limitation, we propose NExtLong, a novel framework for synthesizing\nlong-context data through Negative document Extension. NExtLong decomposes a\ndocument into multiple meta-chunks and extends the context by interleaving hard\nnegative distractors retrieved from pretraining corpora. This approach compels\nthe model to discriminate long-range dependent context from distracting\ncontent, enhancing its ability to model long-range dependencies. Extensive\nexperiments demonstrate that NExtLong achieves significant performance\nimprovements on the HELMET and RULER benchmarks compared to existing\nlong-context synthesis approaches and leading models, which are trained on\nnon-synthetic long documents. These findings highlight NExtLong's ability to\nreduce reliance on non-synthetic long documents, making it an effective\nframework for developing advanced long-context LLMs."
                },
                "authors": [
                    {
                        "name": "Chaochen Gao"
                    },
                    {
                        "name": "Xing Wu"
                    },
                    {
                        "name": "Zijia Lin"
                    },
                    {
                        "name": "Debing Zhang"
                    },
                    {
                        "name": "Songlin Hu"
                    }
                ],
                "author_detail": {
                    "name": "Songlin Hu"
                },
                "author": "Songlin Hu",
                "arxiv_comment": "Corresponding authors: xing wu, and songlin hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12766v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12766v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2303.14676v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2303.14676v3",
                "updated": "2025-01-22T09:50:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    9,
                    50,
                    1,
                    2,
                    22,
                    0
                ],
                "published": "2023-03-26T10:50:16Z",
                "published_parsed": [
                    2023,
                    3,
                    26,
                    10,
                    50,
                    16,
                    6,
                    85,
                    0
                ],
                "title": "PDPP: Projected Diffusion for Procedure Planning in Instructional Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PDPP: Projected Diffusion for Procedure Planning in Instructional Videos"
                },
                "summary": "In this paper, we study the problem of procedure planning in instructional\nvideos, which aims to make a plan (i.e. a sequence of actions) given the\ncurrent visual observation and the desired goal. Previous works cast this as a\nsequence modeling problem and leverage either intermediate visual observations\nor language instructions as supervision to make autoregressive planning,\nresulting in complex learning schemes and expensive annotation costs. To avoid\nintermediate supervision annotation and error accumulation caused by planning\nautoregressively, we propose a diffusion-based framework, coined as PDPP, to\ndirectly model the whole action sequence distribution with task label as\nsupervision instead. Our core idea is to treat procedure planning as a\ndistribution fitting problem under the given observations, thus transform the\nplanning problem to a sampling process from this distribution during inference.\nThe diffusion-based modeling approach also effectively addresses the\nuncertainty issue in procedure planning. Based on PDPP, we further apply joint\ntraining to our framework to generate plans with varying horizon lengths using\na single model and reduce the number of training parameters required. We\ninstantiate our PDPP with three popular diffusion models and investigate a\nseries of condition-introducing methods in our framework, including condition\nembeddings, MoEs, two-stage prediction and Classifier-Free Guidance strategy.\nFinally, we apply our PDPP to the Visual Planners for human Assistance problem\nwhich requires the goal specified in natural language rather than visual\nobservation. We conduct experiments on challenging datasets of different scales\nand our PDPP model achieves the state-of-the-art performance on multiple\nmetrics, even compared with those strongly-supervised counterparts. These\nresults further demonstrates the effectiveness and generalization ability of\nour model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we study the problem of procedure planning in instructional\nvideos, which aims to make a plan (i.e. a sequence of actions) given the\ncurrent visual observation and the desired goal. Previous works cast this as a\nsequence modeling problem and leverage either intermediate visual observations\nor language instructions as supervision to make autoregressive planning,\nresulting in complex learning schemes and expensive annotation costs. To avoid\nintermediate supervision annotation and error accumulation caused by planning\nautoregressively, we propose a diffusion-based framework, coined as PDPP, to\ndirectly model the whole action sequence distribution with task label as\nsupervision instead. Our core idea is to treat procedure planning as a\ndistribution fitting problem under the given observations, thus transform the\nplanning problem to a sampling process from this distribution during inference.\nThe diffusion-based modeling approach also effectively addresses the\nuncertainty issue in procedure planning. Based on PDPP, we further apply joint\ntraining to our framework to generate plans with varying horizon lengths using\na single model and reduce the number of training parameters required. We\ninstantiate our PDPP with three popular diffusion models and investigate a\nseries of condition-introducing methods in our framework, including condition\nembeddings, MoEs, two-stage prediction and Classifier-Free Guidance strategy.\nFinally, we apply our PDPP to the Visual Planners for human Assistance problem\nwhich requires the goal specified in natural language rather than visual\nobservation. We conduct experiments on challenging datasets of different scales\nand our PDPP model achieves the state-of-the-art performance on multiple\nmetrics, even compared with those strongly-supervised counterparts. These\nresults further demonstrates the effectiveness and generalization ability of\nour model."
                },
                "authors": [
                    {
                        "name": "Hanlin Wang"
                    },
                    {
                        "name": "Yilu Wu"
                    },
                    {
                        "name": "Sheng Guo"
                    },
                    {
                        "name": "Limin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Limin Wang"
                },
                "author": "Limin Wang",
                "arxiv_comment": "Accepted as a highlight paper at CVPR 2023. Extension accepted by\n  TPAMI. Code and trained models are available at\n  https://github.com/MCG-NJU/PDPP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2303.14676v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2303.14676v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03866v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03866v2",
                "updated": "2025-01-22T09:48:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    9,
                    48,
                    34,
                    2,
                    22,
                    0
                ],
                "published": "2024-11-06T12:22:04Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    12,
                    22,
                    4,
                    2,
                    311,
                    0
                ],
                "title": "Performance evaluation of SLAM-ASR: The Good, the Bad, the Ugly, and the\n  Way Forward",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance evaluation of SLAM-ASR: The Good, the Bad, the Ugly, and the\n  Way Forward"
                },
                "summary": "Recent research has demonstrated that training a linear connector between\nspeech foundation encoders and large language models (LLMs) enables this\narchitecture to achieve strong ASR capabilities. Despite the impressive\nresults, it remains unclear whether these simple approaches are robust enough\nacross different scenarios and speech conditions, such as domain shifts and\nspeech perturbations. In this paper, we address these questions by conducting\nvarious ablation experiments using a recent and widely adopted approach called\nSLAM-ASR. We present novel empirical findings that offer insights on how to\neffectively utilize the SLAM-ASR architecture across a wide range of settings.\nOur main findings indicate that SLAM-ASR exhibits poor performance in\ncross-domain evaluation settings. Additionally, speech perturbations on\nin-domain data, such as changes in speech rate or additive noise, can\nsignificantly degrade performance. Our findings offer critical insights for\nfine-tuning and configuring robust LLM-based ASR models, tailored to different\ndata characteristics and computational resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research has demonstrated that training a linear connector between\nspeech foundation encoders and large language models (LLMs) enables this\narchitecture to achieve strong ASR capabilities. Despite the impressive\nresults, it remains unclear whether these simple approaches are robust enough\nacross different scenarios and speech conditions, such as domain shifts and\nspeech perturbations. In this paper, we address these questions by conducting\nvarious ablation experiments using a recent and widely adopted approach called\nSLAM-ASR. We present novel empirical findings that offer insights on how to\neffectively utilize the SLAM-ASR architecture across a wide range of settings.\nOur main findings indicate that SLAM-ASR exhibits poor performance in\ncross-domain evaluation settings. Additionally, speech perturbations on\nin-domain data, such as changes in speech rate or additive noise, can\nsignificantly degrade performance. Our findings offer critical insights for\nfine-tuning and configuring robust LLM-based ASR models, tailored to different\ndata characteristics and computational resources."
                },
                "authors": [
                    {
                        "name": "Shashi Kumar"
                    },
                    {
                        "name": "Iuliia Thorbecke"
                    },
                    {
                        "name": "Sergio Burdisso"
                    },
                    {
                        "name": "Esaú Villatoro-Tello"
                    },
                    {
                        "name": "Manjunath K E"
                    },
                    {
                        "name": "Kadri Hacioğlu"
                    },
                    {
                        "name": "Pradeep Rangappa"
                    },
                    {
                        "name": "Petr Motlicek"
                    },
                    {
                        "name": "Aravind Ganapathiraju"
                    },
                    {
                        "name": "Andreas Stolcke"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Stolcke"
                },
                "author": "Andreas Stolcke",
                "arxiv_comment": "Accepted in ICASSP 2025 SALMA Workshop",
                "arxiv_journal_ref": "Proc. ICASSP Workshop on Speech and Audio Language Models (SALMA),\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03866v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03866v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12746v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12746v1",
                "updated": "2025-01-22T09:27:11Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    9,
                    27,
                    11,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T09:27:11Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    9,
                    27,
                    11,
                    2,
                    22,
                    0
                ],
                "title": "EvidenceMap: Unleashing the Power of Small Language Models with Evidence\n  Analysis for Biomedical Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EvidenceMap: Unleashing the Power of Small Language Models with Evidence\n  Analysis for Biomedical Question Answering"
                },
                "summary": "Current LLM-based approaches improve question answering performance by\nleveraging the internal reasoning abilities of models or incorporating external\nknowledge. However, when humans address professional problems, it is essential\nto explicitly analyze the multifaceted relationships from multiple pieces and\ndiverse sources of evidence to achieve better answers. In this study, we\npropose a novel generative question answering framework for the biomedical\ndomain, named EvidenceMap, which explicitly learns and incorporates evidence\nanalysis with small language models (SLMs). The framework describes an evidence\nmap for each question and fully utilizes an SLM to derive the representation of\nthe supportive evaluation, the logical correlation, and the summarization of\nthe related evidence, which facilitates an analysis-augmented generation with\nanother SLM in an autoregressive way. Extensive experiments have shown that\nintroducing an evidence analysis learning process can significantly outperform\nlarger models and popular LLM reasoning methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current LLM-based approaches improve question answering performance by\nleveraging the internal reasoning abilities of models or incorporating external\nknowledge. However, when humans address professional problems, it is essential\nto explicitly analyze the multifaceted relationships from multiple pieces and\ndiverse sources of evidence to achieve better answers. In this study, we\npropose a novel generative question answering framework for the biomedical\ndomain, named EvidenceMap, which explicitly learns and incorporates evidence\nanalysis with small language models (SLMs). The framework describes an evidence\nmap for each question and fully utilizes an SLM to derive the representation of\nthe supportive evaluation, the logical correlation, and the summarization of\nthe related evidence, which facilitates an analysis-augmented generation with\nanother SLM in an autoregressive way. Extensive experiments have shown that\nintroducing an evidence analysis learning process can significantly outperform\nlarger models and popular LLM reasoning methods."
                },
                "authors": [
                    {
                        "name": "Chang Zong"
                    },
                    {
                        "name": "Jian Wan"
                    },
                    {
                        "name": "Lei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lei Zhang"
                },
                "author": "Lei Zhang",
                "arxiv_comment": "13 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12746v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12746v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17836v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17836v2",
                "updated": "2025-01-22T09:26:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    9,
                    26,
                    42,
                    2,
                    22,
                    0
                ],
                "published": "2024-09-26T13:38:33Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    13,
                    38,
                    33,
                    3,
                    270,
                    0
                ],
                "title": "Language Models as Zero-shot Lossless Gradient Compressors: Towards\n  General Neural Parameter Prior Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models as Zero-shot Lossless Gradient Compressors: Towards\n  General Neural Parameter Prior Models"
                },
                "summary": "Despite the widespread use of statistical prior models in various fields,\nsuch models for neural network gradients have long been overlooked. The\ninherent challenge stems from their high-dimensional structures and complex\ninterdependencies, which complicate effective modeling. In this work, we\ndemonstrate the potential of large language models (LLMs) to act as gradient\npriors in a zero-shot setting. We examine the property by considering lossless\ngradient compression -- a critical application in distributed learning -- that\ndepends heavily on precise probability modeling. To achieve this, we introduce\nLM-GC, a novel method that integrates LLMs with arithmetic coding. Our\ntechnique converts plain gradients into text-like formats, enhancing token\nefficiency by up to 38 times compared to their plain representations. We ensure\nthat this data conversion maintains a close alignment with the structure of\nplain gradients and the symbols commonly recognized by LLMs. Our experiments\nindicate that LM-GC surpasses existing state-of-the-art lossless compression\nmethods, improving compression rates by 10% up to 17.2% across various datasets\nand architectures. Additionally, our approach shows promising compatibility\nwith lossy compression techniques such as quantization and sparsification.\nThese findings highlight the significant potential of LLMs as a model for\neffectively handling gradients. Code is available at\nhttps://github.com/hui-po-wang/LM-GC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the widespread use of statistical prior models in various fields,\nsuch models for neural network gradients have long been overlooked. The\ninherent challenge stems from their high-dimensional structures and complex\ninterdependencies, which complicate effective modeling. In this work, we\ndemonstrate the potential of large language models (LLMs) to act as gradient\npriors in a zero-shot setting. We examine the property by considering lossless\ngradient compression -- a critical application in distributed learning -- that\ndepends heavily on precise probability modeling. To achieve this, we introduce\nLM-GC, a novel method that integrates LLMs with arithmetic coding. Our\ntechnique converts plain gradients into text-like formats, enhancing token\nefficiency by up to 38 times compared to their plain representations. We ensure\nthat this data conversion maintains a close alignment with the structure of\nplain gradients and the symbols commonly recognized by LLMs. Our experiments\nindicate that LM-GC surpasses existing state-of-the-art lossless compression\nmethods, improving compression rates by 10% up to 17.2% across various datasets\nand architectures. Additionally, our approach shows promising compatibility\nwith lossy compression techniques such as quantization and sparsification.\nThese findings highlight the significant potential of LLMs as a model for\neffectively handling gradients. Code is available at\nhttps://github.com/hui-po-wang/LM-GC."
                },
                "authors": [
                    {
                        "name": "Hui-Po Wang"
                    },
                    {
                        "name": "Mario Fritz"
                    }
                ],
                "author_detail": {
                    "name": "Mario Fritz"
                },
                "author": "Mario Fritz",
                "arxiv_comment": "camera-ready in NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17836v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17836v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12735v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12735v1",
                "updated": "2025-01-22T09:12:09Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    9,
                    12,
                    9,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T09:12:09Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    9,
                    12,
                    9,
                    2,
                    22,
                    0
                ],
                "title": "Online Preference Alignment for Language Models via Count-based\n  Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Preference Alignment for Language Models via Count-based\n  Exploration"
                },
                "summary": "Reinforcement Learning from Human Feedback (RLHF) has shown great potential\nin fine-tuning Large Language Models (LLMs) to align with human preferences.\nExisting methods perform preference alignment from a fixed dataset, which can\nbe limited in data coverage, and the resulting reward model is hard to\ngeneralize in out-of-distribution responses. Thus, online RLHF is more\ndesirable to empower the LLM to explore outside the support of the initial\ndataset by iteratively collecting the prompt-response pairs. In this paper, we\nstudy the fundamental problem in online RLHF, i.e. \\emph{how to explore} for\nLLM. We give a theoretical motivation in linear reward assumption to show that\nan optimistic reward with an upper confidence bound (UCB) term leads to a\nprovably efficient RLHF policy. Then, we reformulate our objective to direct\npreference optimization with an exploration term, where the UCB-term can be\nconverted to a count-based exploration bonus. We further propose a practical\nalgorithm, named \\emph{Count-based Online Preference Optimization (COPO)},\nwhich leverages a simple coin-flip counting module to estimate the pseudo-count\nof a prompt-response pair in previously collected data. COPO encourages LLMs to\nbalance exploration and preference optimization in an iterative manner, which\nenlarges the exploration space and the entire data coverage of iterative LLM\npolicies. We conduct online RLHF experiments on Zephyr and Llama-3 models. The\nresults on instruction-following and standard academic benchmarks show that\nCOPO significantly increases performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning from Human Feedback (RLHF) has shown great potential\nin fine-tuning Large Language Models (LLMs) to align with human preferences.\nExisting methods perform preference alignment from a fixed dataset, which can\nbe limited in data coverage, and the resulting reward model is hard to\ngeneralize in out-of-distribution responses. Thus, online RLHF is more\ndesirable to empower the LLM to explore outside the support of the initial\ndataset by iteratively collecting the prompt-response pairs. In this paper, we\nstudy the fundamental problem in online RLHF, i.e. \\emph{how to explore} for\nLLM. We give a theoretical motivation in linear reward assumption to show that\nan optimistic reward with an upper confidence bound (UCB) term leads to a\nprovably efficient RLHF policy. Then, we reformulate our objective to direct\npreference optimization with an exploration term, where the UCB-term can be\nconverted to a count-based exploration bonus. We further propose a practical\nalgorithm, named \\emph{Count-based Online Preference Optimization (COPO)},\nwhich leverages a simple coin-flip counting module to estimate the pseudo-count\nof a prompt-response pair in previously collected data. COPO encourages LLMs to\nbalance exploration and preference optimization in an iterative manner, which\nenlarges the exploration space and the entire data coverage of iterative LLM\npolicies. We conduct online RLHF experiments on Zephyr and Llama-3 models. The\nresults on instruction-following and standard academic benchmarks show that\nCOPO significantly increases performance."
                },
                "authors": [
                    {
                        "name": "Chenjia Bai"
                    },
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Shuang Qiu"
                    },
                    {
                        "name": "Qiaosheng Zhang"
                    },
                    {
                        "name": "Kang Xu"
                    },
                    {
                        "name": "Xuelong Li"
                    }
                ],
                "author_detail": {
                    "name": "Xuelong Li"
                },
                "author": "Xuelong Li",
                "arxiv_comment": "Accepted by ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12735v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12735v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12728v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12728v1",
                "updated": "2025-01-22T09:05:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    9,
                    5,
                    1,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T09:05:01Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    9,
                    5,
                    1,
                    2,
                    22,
                    0
                ],
                "title": "A Call for Critically Rethinking and Reforming Data Analysis in\n  Empirical Software Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Call for Critically Rethinking and Reforming Data Analysis in\n  Empirical Software Engineering"
                },
                "summary": "Context: Empirical Software Engineering (ESE) drives innovation in SE through\nqualitative and quantitative studies. However, concerns about the correct\napplication of empirical methodologies have existed since the 2006 Dagstuhl\nseminar on SE. Objective: To analyze three decades of SE research, identify\nmistakes in statistical methods, and evaluate experts' ability to detect and\naddress these issues. Methods: We conducted a literature survey of ~27,000\nempirical studies, using LLMs to classify statistical methodologies as adequate\nor inadequate. Additionally, we selected 30 primary studies and held a workshop\nwith 33 ESE experts to assess their ability to identify and resolve statistical\nissues. Results: Significant statistical issues were found in the primary\nstudies, and experts showed limited ability to detect and correct these\nmethodological problems, raising concerns about the broader ESE community's\nproficiency in this area. Conclusions. Despite our study's eventual\nlimitations, its results shed light on recurring issues from promoting\ninformation copy-and-paste from past authors' works and the continuous\npublication of inadequate approaches that promote dubious results and\njeopardize the spread of the correct statistical strategies among researchers.\nBesides, it justifies further investigation into empirical rigor in software\nengineering to expose these recurring issues and establish a framework for\nreassessing our field's foundation of statistical methodology application.\nTherefore, this work calls for critically rethinking and reforming data\nanalysis in empirical software engineering, paving the way for our work soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context: Empirical Software Engineering (ESE) drives innovation in SE through\nqualitative and quantitative studies. However, concerns about the correct\napplication of empirical methodologies have existed since the 2006 Dagstuhl\nseminar on SE. Objective: To analyze three decades of SE research, identify\nmistakes in statistical methods, and evaluate experts' ability to detect and\naddress these issues. Methods: We conducted a literature survey of ~27,000\nempirical studies, using LLMs to classify statistical methodologies as adequate\nor inadequate. Additionally, we selected 30 primary studies and held a workshop\nwith 33 ESE experts to assess their ability to identify and resolve statistical\nissues. Results: Significant statistical issues were found in the primary\nstudies, and experts showed limited ability to detect and correct these\nmethodological problems, raising concerns about the broader ESE community's\nproficiency in this area. Conclusions. Despite our study's eventual\nlimitations, its results shed light on recurring issues from promoting\ninformation copy-and-paste from past authors' works and the continuous\npublication of inadequate approaches that promote dubious results and\njeopardize the spread of the correct statistical strategies among researchers.\nBesides, it justifies further investigation into empirical rigor in software\nengineering to expose these recurring issues and establish a framework for\nreassessing our field's foundation of statistical methodology application.\nTherefore, this work calls for critically rethinking and reforming data\nanalysis in empirical software engineering, paving the way for our work soon."
                },
                "authors": [
                    {
                        "name": "Matteo Esposito"
                    },
                    {
                        "name": "Mikel Robredo"
                    },
                    {
                        "name": "Murali Sridharan"
                    },
                    {
                        "name": "Guilherme Horta Travassos"
                    },
                    {
                        "name": "Rafael Peñaloza"
                    },
                    {
                        "name": "Valentina Lenarduzzi"
                    }
                ],
                "author_detail": {
                    "name": "Valentina Lenarduzzi"
                },
                "author": "Valentina Lenarduzzi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12728v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12728v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.16831v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.16831v3",
                "updated": "2025-01-22T08:45:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    8,
                    45,
                    56,
                    2,
                    22,
                    0
                ],
                "published": "2024-03-25T14:57:18Z",
                "published_parsed": [
                    2024,
                    3,
                    25,
                    14,
                    57,
                    18,
                    0,
                    85,
                    0
                ],
                "title": "UrbanVLP: Multi-Granularity Vision-Language Pretraining for Urban\n  Socioeconomic Indicator Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UrbanVLP: Multi-Granularity Vision-Language Pretraining for Urban\n  Socioeconomic Indicator Prediction"
                },
                "summary": "Urban socioeconomic indicator prediction aims to infer various metrics\nrelated to sustainable development in diverse urban landscapes using\ndata-driven methods. However, prevalent pretrained models, particularly those\nreliant on satellite imagery, face dual challenges. Firstly, concentrating\nsolely on macro-level patterns from satellite data may introduce bias, lacking\nnuanced details at micro levels, such as architectural details at a place.\nSecondly, the text generated by the precursor work UrbanCLIP, which fully\nutilizes the extensive knowledge of LLMs, frequently exhibits issues such as\nhallucination and homogenization, resulting in a lack of reliable quality. In\nresponse to these issues, we devise a novel framework entitled UrbanVLP based\non Vision-Language Pretraining. Our UrbanVLP seamlessly integrates\nmulti-granularity information from both macro (satellite) and micro\n(street-view) levels, overcoming the limitations of prior pretrained models.\nMoreover, it introduces automatic text generation and calibration, providing a\nrobust guarantee for producing high-quality text descriptions of urban imagery.\nRigorous experiments conducted across six socioeconomic indicator prediction\ntasks underscore its superior performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Urban socioeconomic indicator prediction aims to infer various metrics\nrelated to sustainable development in diverse urban landscapes using\ndata-driven methods. However, prevalent pretrained models, particularly those\nreliant on satellite imagery, face dual challenges. Firstly, concentrating\nsolely on macro-level patterns from satellite data may introduce bias, lacking\nnuanced details at micro levels, such as architectural details at a place.\nSecondly, the text generated by the precursor work UrbanCLIP, which fully\nutilizes the extensive knowledge of LLMs, frequently exhibits issues such as\nhallucination and homogenization, resulting in a lack of reliable quality. In\nresponse to these issues, we devise a novel framework entitled UrbanVLP based\non Vision-Language Pretraining. Our UrbanVLP seamlessly integrates\nmulti-granularity information from both macro (satellite) and micro\n(street-view) levels, overcoming the limitations of prior pretrained models.\nMoreover, it introduces automatic text generation and calibration, providing a\nrobust guarantee for producing high-quality text descriptions of urban imagery.\nRigorous experiments conducted across six socioeconomic indicator prediction\ntasks underscore its superior performance."
                },
                "authors": [
                    {
                        "name": "Xixuan Hao"
                    },
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Yibo Yan"
                    },
                    {
                        "name": "Siru Zhong"
                    },
                    {
                        "name": "Kun Wang"
                    },
                    {
                        "name": "Qingsong Wen"
                    },
                    {
                        "name": "Yuxuan Liang"
                    }
                ],
                "author_detail": {
                    "name": "Yuxuan Liang"
                },
                "author": "Yuxuan Liang",
                "arxiv_comment": "Accepted as a full paper by AAAI'25 - AI for Social Impact Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.16831v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.16831v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09460v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09460v3",
                "updated": "2025-01-22T08:42:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    8,
                    42,
                    13,
                    2,
                    22,
                    0
                ],
                "published": "2024-12-12T17:11:22Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    17,
                    11,
                    22,
                    3,
                    347,
                    0
                ],
                "title": "The Impact of Copyrighted Material on Large Language Models: A Norwegian\n  Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of Copyrighted Material on Large Language Models: A Norwegian\n  Perspective"
                },
                "summary": "The use of copyrighted materials in training language models raises critical\nlegal and ethical questions. This paper presents a framework for and the\nresults of empirically assessing the impact of publisher-controlled copyrighted\ncorpora on the performance of generative large language models (LLMs) for\nNorwegian. When evaluated on a diverse set of tasks, we found that adding both\nbooks and newspapers to the data mixture of LLMs tend to improve their\nperformance, while the addition of fiction works seems to be detrimental. Our\nexperiments could inform the creation of a compensation scheme for authors\nwhose works contribute to AI development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of copyrighted materials in training language models raises critical\nlegal and ethical questions. This paper presents a framework for and the\nresults of empirically assessing the impact of publisher-controlled copyrighted\ncorpora on the performance of generative large language models (LLMs) for\nNorwegian. When evaluated on a diverse set of tasks, we found that adding both\nbooks and newspapers to the data mixture of LLMs tend to improve their\nperformance, while the addition of fiction works seems to be detrimental. Our\nexperiments could inform the creation of a compensation scheme for authors\nwhose works contribute to AI development."
                },
                "authors": [
                    {
                        "name": "Javier de la Rosa"
                    },
                    {
                        "name": "Vladislav Mikhailov"
                    },
                    {
                        "name": "Lemei Zhang"
                    },
                    {
                        "name": "Freddy Wetjen"
                    },
                    {
                        "name": "David Samuel"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Rolv-Arild Braaten"
                    },
                    {
                        "name": "Petter Mæhlum"
                    },
                    {
                        "name": "Magnus Breder Birkenes"
                    },
                    {
                        "name": "Andrey Kutuzov"
                    },
                    {
                        "name": "Tita Enstad"
                    },
                    {
                        "name": "Hans Christian Farsethås"
                    },
                    {
                        "name": "Svein Arne Brygfjeld"
                    },
                    {
                        "name": "Jon Atle Gulla"
                    },
                    {
                        "name": "Stephan Oepen"
                    },
                    {
                        "name": "Erik Velldal"
                    },
                    {
                        "name": "Wilfred Østgulen"
                    },
                    {
                        "name": "Liljia Øvrelid"
                    },
                    {
                        "name": "Aslak Sira Myhre"
                    }
                ],
                "author_detail": {
                    "name": "Aslak Sira Myhre"
                },
                "author": "Aslak Sira Myhre",
                "arxiv_comment": "17 pages, 5 figures, 8 tables. Accepted at NoDaLiDa/Baltic-HLT 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09460v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09460v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.19443v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.19443v2",
                "updated": "2025-01-22T08:25:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    8,
                    25,
                    25,
                    2,
                    22,
                    0
                ],
                "published": "2024-03-28T14:15:10Z",
                "published_parsed": [
                    2024,
                    3,
                    28,
                    14,
                    15,
                    10,
                    3,
                    88,
                    0
                ],
                "title": "Mixed Preference Optimization: Reinforcement Learning with Data\n  Selection and Better Reference Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixed Preference Optimization: Reinforcement Learning with Data\n  Selection and Better Reference Model"
                },
                "summary": "Large Language Models (LLMs) have become increasingly popular due to their\nability to process and generate natural language. However, as they are trained\non massive datasets of text, LLMs can inherit harmful biases and produce\noutputs that are not aligned with human values. This paper studies two main\napproaches to LLM alignment: Reinforcement Learning with Human Feedback (RLHF)\nand contrastive learning-based methods like Direct Preference Optimization\n(DPO). By analyzing the stability and robustness of RLHF and DPO, we propose\nMPO (Mixed Preference Optimization), a novel method that mitigates the\nweaknesses of both approaches. Specifically, we propose a two-stage training\nprocedure: first train DPO on an easy dataset, and then perform RLHF on a\ndifficult set with DPO model being the reference model. Here, the easy and\ndifficult sets are constructed by a well-trained reward model that splits\nresponse pairs into those with large gaps of reward (easy), and those with\nsmall gaps (difficult). The first stage allows us to obtain a relatively\noptimal policy (LLM) model quickly, whereas the second stage refines LLM with\nonline RLHF, thus mitigating the distribution shift issue associated with DPO.\nExperiments are conducted on two public alignment datasets, namely HH-RLHF and\nTLDR, demonstrating the effectiveness of MPO, both in terms of GPT4 and human\nevaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become increasingly popular due to their\nability to process and generate natural language. However, as they are trained\non massive datasets of text, LLMs can inherit harmful biases and produce\noutputs that are not aligned with human values. This paper studies two main\napproaches to LLM alignment: Reinforcement Learning with Human Feedback (RLHF)\nand contrastive learning-based methods like Direct Preference Optimization\n(DPO). By analyzing the stability and robustness of RLHF and DPO, we propose\nMPO (Mixed Preference Optimization), a novel method that mitigates the\nweaknesses of both approaches. Specifically, we propose a two-stage training\nprocedure: first train DPO on an easy dataset, and then perform RLHF on a\ndifficult set with DPO model being the reference model. Here, the easy and\ndifficult sets are constructed by a well-trained reward model that splits\nresponse pairs into those with large gaps of reward (easy), and those with\nsmall gaps (difficult). The first stage allows us to obtain a relatively\noptimal policy (LLM) model quickly, whereas the second stage refines LLM with\nonline RLHF, thus mitigating the distribution shift issue associated with DPO.\nExperiments are conducted on two public alignment datasets, namely HH-RLHF and\nTLDR, demonstrating the effectiveness of MPO, both in terms of GPT4 and human\nevaluation."
                },
                "authors": [
                    {
                        "name": "Qi Gou"
                    },
                    {
                        "name": "Cam-Tu Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Cam-Tu Nguyen"
                },
                "author": "Cam-Tu Nguyen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.19443v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.19443v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12706v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12706v1",
                "updated": "2025-01-22T08:23:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    8,
                    23,
                    10,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T08:23:10Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    8,
                    23,
                    10,
                    2,
                    22,
                    0
                ],
                "title": "REX: Causal Discovery based on Machine Learning and Explainability\n  techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REX: Causal Discovery based on Machine Learning and Explainability\n  techniques"
                },
                "summary": "Explainability techniques hold significant potential for enhancing the causal\ndiscovery process, which is crucial for understanding complex systems in areas\nlike healthcare, economics, and artificial intelligence. However, no causal\ndiscovery methods currently incorporate explainability into their models to\nderive causal graphs. Thus, in this paper we explore this innovative approach,\nas it offers substantial potential and represents a promising new direction\nworth investigating. Specifically, we introduce REX, a causal discovery method\nthat leverages machine learning (ML) models coupled with explainability\ntechniques, specifically Shapley values, to identify and interpret significant\ncausal relationships among variables.\n  Comparative evaluations on synthetic datasets comprising continuous tabular\ndata reveal that REX outperforms state-of-the-art causal discovery methods\nacross diverse data generation processes, including non-linear and additive\nnoise models. Moreover, REX was tested on the Sachs single-cell\nprotein-signaling dataset, achieving a precision of 0.952 and recovering key\ncausal relationships with no incorrect edges. Taking together, these results\nshowcase REX's effectiveness in accurately recovering true causal structures\nwhile minimizing false positive predictions, its robustness across diverse\ndatasets, and its applicability to real-world problems. By combining ML and\nexplainability techniques with causal discovery, REX bridges the gap between\npredictive modeling and causal inference, offering an effective tool for\nunderstanding complex causal structures. REX is publicly available at\nhttps://github.com/renero/causalgraph.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explainability techniques hold significant potential for enhancing the causal\ndiscovery process, which is crucial for understanding complex systems in areas\nlike healthcare, economics, and artificial intelligence. However, no causal\ndiscovery methods currently incorporate explainability into their models to\nderive causal graphs. Thus, in this paper we explore this innovative approach,\nas it offers substantial potential and represents a promising new direction\nworth investigating. Specifically, we introduce REX, a causal discovery method\nthat leverages machine learning (ML) models coupled with explainability\ntechniques, specifically Shapley values, to identify and interpret significant\ncausal relationships among variables.\n  Comparative evaluations on synthetic datasets comprising continuous tabular\ndata reveal that REX outperforms state-of-the-art causal discovery methods\nacross diverse data generation processes, including non-linear and additive\nnoise models. Moreover, REX was tested on the Sachs single-cell\nprotein-signaling dataset, achieving a precision of 0.952 and recovering key\ncausal relationships with no incorrect edges. Taking together, these results\nshowcase REX's effectiveness in accurately recovering true causal structures\nwhile minimizing false positive predictions, its robustness across diverse\ndatasets, and its applicability to real-world problems. By combining ML and\nexplainability techniques with causal discovery, REX bridges the gap between\npredictive modeling and causal inference, offering an effective tool for\nunderstanding complex causal structures. REX is publicly available at\nhttps://github.com/renero/causalgraph."
                },
                "authors": [
                    {
                        "name": "Jesus Renero"
                    },
                    {
                        "name": "Idoia Ochoa"
                    },
                    {
                        "name": "Roberto Maestre"
                    }
                ],
                "author_detail": {
                    "name": "Roberto Maestre"
                },
                "author": "Roberto Maestre",
                "arxiv_comment": "22 pages, 30 figures, Submitted to Elsevier's Pattern Recognition",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12706v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12706v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12702v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12702v1",
                "updated": "2025-01-22T08:18:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    8,
                    18,
                    37,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T08:18:37Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    8,
                    18,
                    37,
                    2,
                    22,
                    0
                ],
                "title": "Paradigm-Based Automatic HDL Code Generation Using LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Paradigm-Based Automatic HDL Code Generation Using LLMs"
                },
                "summary": "While large language models (LLMs) have demonstrated the ability to generate\nhardware description language (HDL) code for digital circuits, they still face\nthe hallucination problem, which can result in the generation of incorrect HDL\ncode or misinterpretation of specifications. In this work, we introduce a\nhuman-expert-inspired method to mitigate the hallucination of LLMs and enhance\ntheir performance in HDL code generation. We begin by constructing specialized\nparadigm blocks that consist of several steps designed to divide and conquer\ngeneration tasks, mirroring the design methodology of human experts. These\nsteps include information extraction, human-like design flows, and the\nintegration of external tools. LLMs are then instructed to classify the type of\ncircuit in order to match it with the appropriate paradigm block and execute\nthe block to generate the HDL codes. Additionally, we propose a two-phase\nworkflow for multi-round generation, aimed at effectively improving the\ntestbench pass rate of the generated HDL codes within a limited number of\ngeneration and verification rounds. Experimental results demonstrate that our\nmethod significantly enhances the functional correctness of the generated\nVerilog code",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) have demonstrated the ability to generate\nhardware description language (HDL) code for digital circuits, they still face\nthe hallucination problem, which can result in the generation of incorrect HDL\ncode or misinterpretation of specifications. In this work, we introduce a\nhuman-expert-inspired method to mitigate the hallucination of LLMs and enhance\ntheir performance in HDL code generation. We begin by constructing specialized\nparadigm blocks that consist of several steps designed to divide and conquer\ngeneration tasks, mirroring the design methodology of human experts. These\nsteps include information extraction, human-like design flows, and the\nintegration of external tools. LLMs are then instructed to classify the type of\ncircuit in order to match it with the appropriate paradigm block and execute\nthe block to generate the HDL codes. Additionally, we propose a two-phase\nworkflow for multi-round generation, aimed at effectively improving the\ntestbench pass rate of the generated HDL codes within a limited number of\ngeneration and verification rounds. Experimental results demonstrate that our\nmethod significantly enhances the functional correctness of the generated\nVerilog code"
                },
                "authors": [
                    {
                        "name": "Wenhao Sun"
                    },
                    {
                        "name": "Bing Li"
                    },
                    {
                        "name": "Grace Li Zhang"
                    },
                    {
                        "name": "Xunzhao Yin"
                    },
                    {
                        "name": "Cheng Zhuo"
                    },
                    {
                        "name": "Ulf Schlichtmann"
                    }
                ],
                "author_detail": {
                    "name": "Ulf Schlichtmann"
                },
                "author": "Ulf Schlichtmann",
                "arxiv_comment": "accepted by ISQED2025. arXiv admin note: text overlap with\n  arXiv:2407.18326",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12702v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12702v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.17696v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.17696v6",
                "updated": "2025-01-22T08:14:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    8,
                    14,
                    59,
                    2,
                    22,
                    0
                ],
                "published": "2023-11-29T15:02:46Z",
                "published_parsed": [
                    2023,
                    11,
                    29,
                    15,
                    2,
                    46,
                    2,
                    333,
                    0
                ],
                "title": "How to Build an AI Tutor That Can Adapt to Any Course Using Knowledge\n  Graph-Enhanced Retrieval-Augmented Generation (KG-RAG)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to Build an AI Tutor That Can Adapt to Any Course Using Knowledge\n  Graph-Enhanced Retrieval-Augmented Generation (KG-RAG)"
                },
                "summary": "This paper introduces KG-RAG (Knowledge Graph-enhanced Retrieval-Augmented\nGeneration), a novel framework that addresses two critical challenges in\nLLM-based tutoring systems: information hallucination and limited\ncourse-specific adaptation. By integrating knowledge graphs with\nretrieval-augmented generation, KG-RAG provides a structured representation of\ncourse concepts and their relationships, enabling contextually grounded and\npedagogically sound responses. We implement the framework using Qwen2.5,\ndemonstrating its cost-effectiveness while maintaining high performance. The\nKG-RAG system outperformed standard RAG-based tutoring in a controlled study\nwith 76 university students (mean scores: 6.37 vs. 4.71, p<0.001, Cohen's\nd=0.86). User feedback showed strong satisfaction with answer relevance (84%\npositive) and user experience (59% positive). Our framework offers a scalable\napproach to personalized AI tutoring, ensuring response accuracy and\npedagogical coherence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces KG-RAG (Knowledge Graph-enhanced Retrieval-Augmented\nGeneration), a novel framework that addresses two critical challenges in\nLLM-based tutoring systems: information hallucination and limited\ncourse-specific adaptation. By integrating knowledge graphs with\nretrieval-augmented generation, KG-RAG provides a structured representation of\ncourse concepts and their relationships, enabling contextually grounded and\npedagogically sound responses. We implement the framework using Qwen2.5,\ndemonstrating its cost-effectiveness while maintaining high performance. The\nKG-RAG system outperformed standard RAG-based tutoring in a controlled study\nwith 76 university students (mean scores: 6.37 vs. 4.71, p<0.001, Cohen's\nd=0.86). User feedback showed strong satisfaction with answer relevance (84%\npositive) and user experience (59% positive). Our framework offers a scalable\napproach to personalized AI tutoring, ensuring response accuracy and\npedagogical coherence."
                },
                "authors": [
                    {
                        "name": "Chenxi Dong"
                    },
                    {
                        "name": "Yimin Yuan"
                    },
                    {
                        "name": "Kan Chen"
                    },
                    {
                        "name": "Shupei Cheng"
                    },
                    {
                        "name": "Chujie Wen"
                    }
                ],
                "author_detail": {
                    "name": "Chujie Wen"
                },
                "author": "Chujie Wen",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.17696v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.17696v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12698v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12698v1",
                "updated": "2025-01-22T08:14:51Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    8,
                    14,
                    51,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T08:14:51Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    8,
                    14,
                    51,
                    2,
                    22,
                    0
                ],
                "title": "Training Dialogue Systems by AI Feedback for Improving Overall Dialogue\n  Impression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training Dialogue Systems by AI Feedback for Improving Overall Dialogue\n  Impression"
                },
                "summary": "To improve user engagement during conversations with dialogue systems, we\nmust improve individual dialogue responses and dialogue impressions such as\nconsistency, personality, and empathy throughout the entire dialogue. While\nsuch dialogue systems have been developing rapidly with the help of large\nlanguage models (LLMs), reinforcement learning from AI feedback (RLAIF) has\nattracted attention to align LLM-based dialogue models for such dialogue\nimpressions. In RLAIF, a reward model based on another LLM is used to create a\ntraining signal for an LLM-based dialogue model using zero-shot/few-shot\nprompting techniques. However, evaluating an entire dialogue only by prompting\nLLMs is challenging. In this study, the supervised fine-tuning (SFT) of LLMs\nprepared reward models corresponding to 12 metrics related to the impression of\nthe entire dialogue for evaluating dialogue responses. We tuned our dialogue\nmodels using the reward model signals as feedback to improve the impression of\nthe system. The results of automatic and human evaluations showed that tuning\nthe dialogue model using our reward model corresponding to dialogue impression\nimproved the evaluation of individual metrics and the naturalness of the\ndialogue response.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To improve user engagement during conversations with dialogue systems, we\nmust improve individual dialogue responses and dialogue impressions such as\nconsistency, personality, and empathy throughout the entire dialogue. While\nsuch dialogue systems have been developing rapidly with the help of large\nlanguage models (LLMs), reinforcement learning from AI feedback (RLAIF) has\nattracted attention to align LLM-based dialogue models for such dialogue\nimpressions. In RLAIF, a reward model based on another LLM is used to create a\ntraining signal for an LLM-based dialogue model using zero-shot/few-shot\nprompting techniques. However, evaluating an entire dialogue only by prompting\nLLMs is challenging. In this study, the supervised fine-tuning (SFT) of LLMs\nprepared reward models corresponding to 12 metrics related to the impression of\nthe entire dialogue for evaluating dialogue responses. We tuned our dialogue\nmodels using the reward model signals as feedback to improve the impression of\nthe system. The results of automatic and human evaluations showed that tuning\nthe dialogue model using our reward model corresponding to dialogue impression\nimproved the evaluation of individual metrics and the naturalness of the\ndialogue response."
                },
                "authors": [
                    {
                        "name": "Kai Yoshida"
                    },
                    {
                        "name": "Masahiro Mizukami"
                    },
                    {
                        "name": "Seiya Kawano"
                    },
                    {
                        "name": "Canasai Kruengkrai"
                    },
                    {
                        "name": "Hiroaki Sugiyama"
                    },
                    {
                        "name": "Koichiro Yoshino"
                    }
                ],
                "author_detail": {
                    "name": "Koichiro Yoshino"
                },
                "author": "Koichiro Yoshino",
                "arxiv_comment": "Accepted to ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12698v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12698v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12697v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12697v1",
                "updated": "2025-01-22T08:14:11Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    8,
                    14,
                    11,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T08:14:11Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    8,
                    14,
                    11,
                    2,
                    22,
                    0
                ],
                "title": "Combining Knowledge Graph and LLMs for Enhanced Zero-shot Visual\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combining Knowledge Graph and LLMs for Enhanced Zero-shot Visual\n  Question Answering"
                },
                "summary": "Zero-shot visual question answering (ZS-VQA), an emerged critical research\narea, intends to answer visual questions without providing training samples.\nExisting research in ZS-VQA has proposed to leverage knowledge graphs or large\nlanguage models (LLMs), respectively, as external information sources to help\nVQA model comprehend images and questions. However, LLMs often struggle in\naccurately interpreting specific question meanings. Meanwhile, although\nknowledge graph has rich entity relationships, it is challenging to effectively\nconnect entities to individual image content for visual question answers. In\nthis paper, we propose a novel design to combine knowledge graph and LLMs for\nzero-shot visual question answer. Our approach uses LLMs' powerful\nunderstanding capabilities to accurately interpret image content through a\nstrategic question search mechanism. Meanwhile, the knowledge graph is used to\nexpand and connect users' queries to the image content for better visual\nquestion answering. An optimization algorithm is further used to determine the\noptimal weights for the loss functions derived from different information\nsources, towards a globally optimal set of candidate answers. Experimental\nresults on two benchmark datasets demonstrate that our model achieves\nstate-of-the-art (SOTA) performance. Both source code and benchmark data will\nbe released for public access.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-shot visual question answering (ZS-VQA), an emerged critical research\narea, intends to answer visual questions without providing training samples.\nExisting research in ZS-VQA has proposed to leverage knowledge graphs or large\nlanguage models (LLMs), respectively, as external information sources to help\nVQA model comprehend images and questions. However, LLMs often struggle in\naccurately interpreting specific question meanings. Meanwhile, although\nknowledge graph has rich entity relationships, it is challenging to effectively\nconnect entities to individual image content for visual question answers. In\nthis paper, we propose a novel design to combine knowledge graph and LLMs for\nzero-shot visual question answer. Our approach uses LLMs' powerful\nunderstanding capabilities to accurately interpret image content through a\nstrategic question search mechanism. Meanwhile, the knowledge graph is used to\nexpand and connect users' queries to the image content for better visual\nquestion answering. An optimization algorithm is further used to determine the\noptimal weights for the loss functions derived from different information\nsources, towards a globally optimal set of candidate answers. Experimental\nresults on two benchmark datasets demonstrate that our model achieves\nstate-of-the-art (SOTA) performance. Both source code and benchmark data will\nbe released for public access."
                },
                "authors": [
                    {
                        "name": "Qian Tao"
                    },
                    {
                        "name": "Xiaoyang Fan"
                    },
                    {
                        "name": "Yong Xu"
                    },
                    {
                        "name": "Xingquan Zhu"
                    },
                    {
                        "name": "Yufei Tang"
                    }
                ],
                "author_detail": {
                    "name": "Yufei Tang"
                },
                "author": "Yufei Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12697v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12697v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12690v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12690v1",
                "updated": "2025-01-22T08:02:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    8,
                    2,
                    1,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T08:02:01Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    8,
                    2,
                    1,
                    2,
                    22,
                    0
                ],
                "title": "Growth strategies for arbitrary DAG neural architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Growth strategies for arbitrary DAG neural architectures"
                },
                "summary": "Deep learning has shown impressive results obtained at the cost of training\nhuge neural networks. However, the larger the architecture, the higher the\ncomputational, financial, and environmental costs during training and\ninference. We aim at reducing both training and inference durations. We focus\non Neural Architecture Growth, which can increase the size of a small model\nwhen needed, directly during training using information from the\nbackpropagation. We expand existing work and freely grow neural networks in the\nform of any Directed Acyclic Graph by reducing expressivity bottlenecks in the\narchitecture. We explore strategies to reduce excessive computations and steer\nnetwork growth toward more parameter-efficient architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning has shown impressive results obtained at the cost of training\nhuge neural networks. However, the larger the architecture, the higher the\ncomputational, financial, and environmental costs during training and\ninference. We aim at reducing both training and inference durations. We focus\non Neural Architecture Growth, which can increase the size of a small model\nwhen needed, directly during training using information from the\nbackpropagation. We expand existing work and freely grow neural networks in the\nform of any Directed Acyclic Graph by reducing expressivity bottlenecks in the\narchitecture. We explore strategies to reduce excessive computations and steer\nnetwork growth toward more parameter-efficient architectures."
                },
                "authors": [
                    {
                        "name": "Stella Douka"
                    },
                    {
                        "name": "Manon Verbockhaven"
                    },
                    {
                        "name": "Théo Rudkiewicz"
                    },
                    {
                        "name": "Stéphane Rivaud"
                    },
                    {
                        "name": "François P Landes"
                    },
                    {
                        "name": "Sylvain Chevallier"
                    },
                    {
                        "name": "Guillaume Charpiat"
                    }
                ],
                "author_detail": {
                    "name": "Guillaume Charpiat"
                },
                "arxiv_affiliation": "LISN, TAU",
                "author": "Guillaume Charpiat",
                "arxiv_journal_ref": "ESANN 2025 - 33th European Symposium on Artificial Neural\n  Networks, Apr 2025, Bruges, Belgium",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12690v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12690v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12689v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12689v1",
                "updated": "2025-01-22T07:52:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    7,
                    52,
                    38,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T07:52:38Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    7,
                    52,
                    38,
                    2,
                    22,
                    0
                ],
                "title": "EchoLM: Accelerating LLM Serving with Real-time Knowledge Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EchoLM: Accelerating LLM Serving with Real-time Knowledge Distillation"
                },
                "summary": "Large language models (LLMs) have excelled in various applications, yet\nserving them at scale is challenging due to their substantial resource demands\nand high latency. Our real-world studies reveal that over 60% of user requests\nto LLMs have semantically similar counterparts, suggesting the potential for\nknowledge sharing among requests. However, naively caching and reusing past\nresponses leads to large quality degradation. In this paper, we introduce\nEchoLM, an in-context caching system that leverages historical requests as\nexamples to guide response generation, enabling selective offloading of\nrequests to more efficient LLMs. However, enabling this real-time knowledge\ntransfer leads to intricate tradeoffs between response quality, latency, and\nsystem throughput at scale. For a new request, EchoLM identifies similar,\nhigh-utility examples and efficiently prepends them to the input for better\nresponse. At scale, EchoLM adaptively routes requests to LLMs of varying\ncapabilities, accounting for response quality and serving loads. EchoLM employs\na cost-aware cache replay mechanism to improve example quality and coverage\noffline, maximizing cache utility and runtime efficiency. Evaluations on\nmillions of open-source requests demonstrate that EchoLM has a throughput\nimprovement of 1.4-5.9x while reducing latency by 28-71% without hurting\nresponse quality on average.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have excelled in various applications, yet\nserving them at scale is challenging due to their substantial resource demands\nand high latency. Our real-world studies reveal that over 60% of user requests\nto LLMs have semantically similar counterparts, suggesting the potential for\nknowledge sharing among requests. However, naively caching and reusing past\nresponses leads to large quality degradation. In this paper, we introduce\nEchoLM, an in-context caching system that leverages historical requests as\nexamples to guide response generation, enabling selective offloading of\nrequests to more efficient LLMs. However, enabling this real-time knowledge\ntransfer leads to intricate tradeoffs between response quality, latency, and\nsystem throughput at scale. For a new request, EchoLM identifies similar,\nhigh-utility examples and efficiently prepends them to the input for better\nresponse. At scale, EchoLM adaptively routes requests to LLMs of varying\ncapabilities, accounting for response quality and serving loads. EchoLM employs\na cost-aware cache replay mechanism to improve example quality and coverage\noffline, maximizing cache utility and runtime efficiency. Evaluations on\nmillions of open-source requests demonstrate that EchoLM has a throughput\nimprovement of 1.4-5.9x while reducing latency by 28-71% without hurting\nresponse quality on average."
                },
                "authors": [
                    {
                        "name": "Yifan Yu"
                    },
                    {
                        "name": "Yu Gan"
                    },
                    {
                        "name": "Lily Tasi"
                    },
                    {
                        "name": "Nikhil Sarda"
                    },
                    {
                        "name": "Jiaming Shen"
                    },
                    {
                        "name": "Yanqi Zhou"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "Fan Lai"
                    },
                    {
                        "name": "Henry M. Levy"
                    },
                    {
                        "name": "David Culler"
                    }
                ],
                "author_detail": {
                    "name": "David Culler"
                },
                "author": "David Culler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12689v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12689v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.05559v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.05559v2",
                "updated": "2025-01-22T07:20:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    7,
                    20,
                    4,
                    2,
                    22,
                    0
                ],
                "published": "2024-02-15T14:12:38Z",
                "published_parsed": [
                    2024,
                    2,
                    15,
                    14,
                    12,
                    38,
                    3,
                    46,
                    0
                ],
                "title": "Exploring Heterogeneity and Uncertainty for Graph-based Cognitive\n  Diagnosis Models in Intelligent Education",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Heterogeneity and Uncertainty for Graph-based Cognitive\n  Diagnosis Models in Intelligent Education"
                },
                "summary": "Graph-based Cognitive Diagnosis (CD) has attracted much research interest due\nto its strong ability on inferring students' proficiency levels on knowledge\nconcepts. While graph-based CD models have demonstrated remarkable performance,\nwe contend that they still cannot achieve optimal performance due to the\nneglect of edge heterogeneity and uncertainty. Edges involve both correct and\nincorrect response logs, indicating heterogeneity. Meanwhile, a response log\ncan have uncertain semantic meanings, e.g., a correct log can indicate true\nmastery or fortunate guessing, and a wrong log can indicate a lack of\nunderstanding or a careless mistake. In this paper, we propose an Informative\nSemantic-aware Graph-based Cognitive Diagnosis model (ISG-CD), which focuses on\nhow to utilize the heterogeneous graph in CD and minimize effects of uncertain\nedges. Specifically, to explore heterogeneity, we propose a semantic-aware\ngraph neural networks based CD model. To minimize effects of edge uncertainty,\nwe propose an Informative Edge Differentiation layer from an information\nbottleneck perspective, which suggests keeping a minimal yet sufficient\nreliable graph for CD in an unsupervised way. We formulate this process as\nmaximizing mutual information between the reliable graph and response logs,\nwhile minimizing mutual information between the reliable graph and the original\ngraph. After that, we prove that mutual information maximization can be\ntheoretically converted to the classic binary cross entropy loss function,\nwhile minimizing mutual information can be realized by the Hilbert-Schmidt\nIndependence Criterion. Finally, we adopt an alternating training strategy for\noptimizing learnable parameters of both the semantic-aware graph neural\nnetworks based CD model and the edge differentiation layer. Extensive\nexperiments on three real-world datasets have demonstrated the effectiveness of\nISG-CD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph-based Cognitive Diagnosis (CD) has attracted much research interest due\nto its strong ability on inferring students' proficiency levels on knowledge\nconcepts. While graph-based CD models have demonstrated remarkable performance,\nwe contend that they still cannot achieve optimal performance due to the\nneglect of edge heterogeneity and uncertainty. Edges involve both correct and\nincorrect response logs, indicating heterogeneity. Meanwhile, a response log\ncan have uncertain semantic meanings, e.g., a correct log can indicate true\nmastery or fortunate guessing, and a wrong log can indicate a lack of\nunderstanding or a careless mistake. In this paper, we propose an Informative\nSemantic-aware Graph-based Cognitive Diagnosis model (ISG-CD), which focuses on\nhow to utilize the heterogeneous graph in CD and minimize effects of uncertain\nedges. Specifically, to explore heterogeneity, we propose a semantic-aware\ngraph neural networks based CD model. To minimize effects of edge uncertainty,\nwe propose an Informative Edge Differentiation layer from an information\nbottleneck perspective, which suggests keeping a minimal yet sufficient\nreliable graph for CD in an unsupervised way. We formulate this process as\nmaximizing mutual information between the reliable graph and response logs,\nwhile minimizing mutual information between the reliable graph and the original\ngraph. After that, we prove that mutual information maximization can be\ntheoretically converted to the classic binary cross entropy loss function,\nwhile minimizing mutual information can be realized by the Hilbert-Schmidt\nIndependence Criterion. Finally, we adopt an alternating training strategy for\noptimizing learnable parameters of both the semantic-aware graph neural\nnetworks based CD model and the edge differentiation layer. Extensive\nexperiments on three real-world datasets have demonstrated the effectiveness of\nISG-CD."
                },
                "authors": [
                    {
                        "name": "Pengyang Shao"
                    },
                    {
                        "name": "Yonghui Yang"
                    },
                    {
                        "name": "Chen Gao"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Kun Zhang"
                    },
                    {
                        "name": "Chenyi Zhuang"
                    },
                    {
                        "name": "Le Wu"
                    },
                    {
                        "name": "Yong Li"
                    },
                    {
                        "name": "Meng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Meng Wang"
                },
                "author": "Meng Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.05559v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.05559v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17640v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17640v3",
                "updated": "2025-01-22T07:16:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    7,
                    16,
                    32,
                    2,
                    22,
                    0
                ],
                "published": "2024-09-26T08:44:38Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    8,
                    44,
                    38,
                    3,
                    270,
                    0
                ],
                "title": "T3: A Novel Zero-shot Transfer Learning Framework Iteratively Training\n  on an Assistant Task for a Target Task",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "T3: A Novel Zero-shot Transfer Learning Framework Iteratively Training\n  on an Assistant Task for a Target Task"
                },
                "summary": "Long text summarization, gradually being essential for efficiently processing\nlarge volumes of information, stays challenging for Large Language Models\n(LLMs) such as GPT and LLaMA families because of the insufficient open-sourced\ntraining datasets and the high requirement of contextual details dealing. To\naddress the issue, we design a novel zero-shot transfer learning framework,\nabbreviated as T3, to iteratively training a baseline LLM on an assistant task\nfor the target task, where the former should own richer data resources and\nshare structural or semantic similarity with the latter. In practice, T3 is\napproached to deal with the long text summarization task by utilizing question\nanswering as the assistant task, and further validated its effectiveness on the\nBBC summary, NarraSum, FairytaleQA, and NLQuAD datasets, with up to nearly 14%\nimprovement in ROUGE, 35% improvement in BLEU, and 16% improvement in Factscore\ncompared to three baseline LLMs, demonstrating its potential for more\nassistant-target task combinations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long text summarization, gradually being essential for efficiently processing\nlarge volumes of information, stays challenging for Large Language Models\n(LLMs) such as GPT and LLaMA families because of the insufficient open-sourced\ntraining datasets and the high requirement of contextual details dealing. To\naddress the issue, we design a novel zero-shot transfer learning framework,\nabbreviated as T3, to iteratively training a baseline LLM on an assistant task\nfor the target task, where the former should own richer data resources and\nshare structural or semantic similarity with the latter. In practice, T3 is\napproached to deal with the long text summarization task by utilizing question\nanswering as the assistant task, and further validated its effectiveness on the\nBBC summary, NarraSum, FairytaleQA, and NLQuAD datasets, with up to nearly 14%\nimprovement in ROUGE, 35% improvement in BLEU, and 16% improvement in Factscore\ncompared to three baseline LLMs, demonstrating its potential for more\nassistant-target task combinations."
                },
                "authors": [
                    {
                        "name": "Xindi Tong"
                    },
                    {
                        "name": "Yujin Zhu"
                    },
                    {
                        "name": "Shijian Fan"
                    },
                    {
                        "name": "Liang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Liang Xu"
                },
                "author": "Liang Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17640v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17640v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19018v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19018v3",
                "updated": "2025-01-22T06:53:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    6,
                    53,
                    56,
                    2,
                    22,
                    0
                ],
                "published": "2024-12-26T01:56:42Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    1,
                    56,
                    42,
                    3,
                    361,
                    0
                ],
                "title": "Let the Fuzzy Rule Speak: Enhancing In-context Learning Debiasing with\n  Interpretability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Let the Fuzzy Rule Speak: Enhancing In-context Learning Debiasing with\n  Interpretability"
                },
                "summary": "One of the potential failures of large language models (LLMs) is their\nimbalanced class performances in text classification tasks. With in-context\nlearning (ICL), LLMs yields good accuracy for some classes but low accuracy for\nothers. This imbalance is particularly problematic when misclassifications lead\nto user dissatisfaction or safety risks. While the root causes may lie in the\ndata, addressing them from the source through training is neither easy nor\ncost-effective. To delve deeper, the imbalance stems from certain classes\nconsistently receiving disproportionately high ICL probabilities, while others\nreceive lower probabilities, resulting in under-prediction and lower accuracy\nin the latter. Crucially, probability ranges vary in their impact on the\nimbalance, enabling precise corrections by range. Therefore, this work\nintroduces an inference-time debiasing method, FuRud (Fuzzy Rule\nOptimization-based Debiasing), to tackle this issue. FuRud addresses core\ninterpretability challenges by determining why certain classes require\ncorrections and tailoring adjustments for each sample and class probability.\nTailored corrections use fuzzy sets with triangular membership functions,\nbecause they can transform per-sample class probabilities based on probability\nranges. Each class selects one from 19 triangular membership functions, solving\na nonlinear integer programming selection problem with simulated annealing, to\nminimize class accuracy bias (COBias) and maximize overall accuracy without\nupdating LLM parameters. Notably, across seven benchmark datasets, FuRud\nreduces COBias by more than half (56%), while achieving a relative increase of\n21% in overall accuracy, outperforming state-of-the-art debiasing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One of the potential failures of large language models (LLMs) is their\nimbalanced class performances in text classification tasks. With in-context\nlearning (ICL), LLMs yields good accuracy for some classes but low accuracy for\nothers. This imbalance is particularly problematic when misclassifications lead\nto user dissatisfaction or safety risks. While the root causes may lie in the\ndata, addressing them from the source through training is neither easy nor\ncost-effective. To delve deeper, the imbalance stems from certain classes\nconsistently receiving disproportionately high ICL probabilities, while others\nreceive lower probabilities, resulting in under-prediction and lower accuracy\nin the latter. Crucially, probability ranges vary in their impact on the\nimbalance, enabling precise corrections by range. Therefore, this work\nintroduces an inference-time debiasing method, FuRud (Fuzzy Rule\nOptimization-based Debiasing), to tackle this issue. FuRud addresses core\ninterpretability challenges by determining why certain classes require\ncorrections and tailoring adjustments for each sample and class probability.\nTailored corrections use fuzzy sets with triangular membership functions,\nbecause they can transform per-sample class probabilities based on probability\nranges. Each class selects one from 19 triangular membership functions, solving\na nonlinear integer programming selection problem with simulated annealing, to\nminimize class accuracy bias (COBias) and maximize overall accuracy without\nupdating LLM parameters. Notably, across seven benchmark datasets, FuRud\nreduces COBias by more than half (56%), while achieving a relative increase of\n21% in overall accuracy, outperforming state-of-the-art debiasing methods."
                },
                "authors": [
                    {
                        "name": "Ruixi Lin"
                    },
                    {
                        "name": "Yang You"
                    }
                ],
                "author_detail": {
                    "name": "Yang You"
                },
                "author": "Yang You",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19018v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19018v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.12317v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.12317v3",
                "updated": "2025-01-22T06:32:02Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    6,
                    32,
                    2,
                    2,
                    22,
                    0
                ],
                "published": "2024-07-17T05:02:17Z",
                "published_parsed": [
                    2024,
                    7,
                    17,
                    5,
                    2,
                    17,
                    2,
                    199,
                    0
                ],
                "title": "Out of Length Text Recognition with Sub-String Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Out of Length Text Recognition with Sub-String Matching"
                },
                "summary": "Scene Text Recognition (STR) methods have demonstrated robust performance in\nword-level text recognition. However, in real applications the text image is\nsometimes long due to detected with multiple horizontal words. It triggers the\nrequirement to build long text recognition models from readily available short\n(i.e., word-level) text datasets, which has been less studied previously. In\nthis paper, we term this task Out of Length (OOL) text recognition. We\nestablish the first Long Text Benchmark (LTB) to facilitate the assessment of\ndifferent methods in long text recognition. Meanwhile, we propose a novel\nmethod called OOL Text Recognition with sub-String Matching (SMTR). SMTR\ncomprises two cross-attention-based modules: one encodes a sub-string\ncontaining multiple characters into next and previous queries, and the other\nemploys the queries to attend to the image features, matching the sub-string\nand simultaneously recognizing its next and previous character. SMTR can\nrecognize text of arbitrary length by iterating the process above. To avoid\nbeing trapped in recognizing highly similar sub-strings, we introduce a\nregularization training to compel SMTR to effectively discover subtle\ndifferences between similar sub-strings for precise matching. In addition, we\npropose an inference augmentation strategy to alleviate confusion caused by\nidentical sub-strings in the same text and improve the overall recognition\nefficiency. Extensive experimental results reveal that SMTR, even when trained\nexclusively on short text, outperforms existing methods in public short text\nbenchmarks and exhibits a clear advantage on LTB. Code:\nhttps://github.com/Topdu/OpenOCR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scene Text Recognition (STR) methods have demonstrated robust performance in\nword-level text recognition. However, in real applications the text image is\nsometimes long due to detected with multiple horizontal words. It triggers the\nrequirement to build long text recognition models from readily available short\n(i.e., word-level) text datasets, which has been less studied previously. In\nthis paper, we term this task Out of Length (OOL) text recognition. We\nestablish the first Long Text Benchmark (LTB) to facilitate the assessment of\ndifferent methods in long text recognition. Meanwhile, we propose a novel\nmethod called OOL Text Recognition with sub-String Matching (SMTR). SMTR\ncomprises two cross-attention-based modules: one encodes a sub-string\ncontaining multiple characters into next and previous queries, and the other\nemploys the queries to attend to the image features, matching the sub-string\nand simultaneously recognizing its next and previous character. SMTR can\nrecognize text of arbitrary length by iterating the process above. To avoid\nbeing trapped in recognizing highly similar sub-strings, we introduce a\nregularization training to compel SMTR to effectively discover subtle\ndifferences between similar sub-strings for precise matching. In addition, we\npropose an inference augmentation strategy to alleviate confusion caused by\nidentical sub-strings in the same text and improve the overall recognition\nefficiency. Extensive experimental results reveal that SMTR, even when trained\nexclusively on short text, outperforms existing methods in public short text\nbenchmarks and exhibits a clear advantage on LTB. Code:\nhttps://github.com/Topdu/OpenOCR."
                },
                "authors": [
                    {
                        "name": "Yongkun Du"
                    },
                    {
                        "name": "Zhineng Chen"
                    },
                    {
                        "name": "Caiyan Jia"
                    },
                    {
                        "name": "Xieping Gao"
                    },
                    {
                        "name": "Yu-Gang Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Gang Jiang"
                },
                "author": "Yu-Gang Jiang",
                "arxiv_comment": "Accepted by AAAI2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.12317v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.12317v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.01331v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.01331v3",
                "updated": "2025-01-22T06:13:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    6,
                    13,
                    41,
                    2,
                    22,
                    0
                ],
                "published": "2023-10-02T16:49:39Z",
                "published_parsed": [
                    2023,
                    10,
                    2,
                    16,
                    49,
                    39,
                    0,
                    275,
                    0
                ],
                "title": "ChoiceMates: Supporting Unfamiliar Online Decision-Making with\n  Multi-Agent Conversational Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChoiceMates: Supporting Unfamiliar Online Decision-Making with\n  Multi-Agent Conversational Interactions"
                },
                "summary": "From deciding on a PhD program to buying a new camera, unfamiliar\ndecisions--decisions without domain knowledge--are frequent and significant.\nThe complexity and uncertainty of such decisions demand unique approaches to\ninformation seeking, understanding, and decision-making. Our formative study\nhighlights that users want to start by discovering broad and relevant domain\ninformation evenly and simultaneously, quickly address emerging inquiries, and\ngain personalized standards to assess information found. We present\nChoiceMates, an interactive multi-agent system designed to address these needs\nby enabling users to engage with a dynamic set of LLM agents each presenting a\nunique experience in the domain. Unlike existing multi-agent systems that\nautomate tasks with agents, the user orchestrates agents to assist their\ndecision-making process. Our user evaluation (n=12) shows that ChoiceMates\nenables a more confident, satisfactory decision-making with better situation\nunderstanding than web search, and higher decision quality and confidence than\na commercial multi-agent framework. This work provides insights into designing\na more controllable and collaborative multi-agent system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From deciding on a PhD program to buying a new camera, unfamiliar\ndecisions--decisions without domain knowledge--are frequent and significant.\nThe complexity and uncertainty of such decisions demand unique approaches to\ninformation seeking, understanding, and decision-making. Our formative study\nhighlights that users want to start by discovering broad and relevant domain\ninformation evenly and simultaneously, quickly address emerging inquiries, and\ngain personalized standards to assess information found. We present\nChoiceMates, an interactive multi-agent system designed to address these needs\nby enabling users to engage with a dynamic set of LLM agents each presenting a\nunique experience in the domain. Unlike existing multi-agent systems that\nautomate tasks with agents, the user orchestrates agents to assist their\ndecision-making process. Our user evaluation (n=12) shows that ChoiceMates\nenables a more confident, satisfactory decision-making with better situation\nunderstanding than web search, and higher decision quality and confidence than\na commercial multi-agent framework. This work provides insights into designing\na more controllable and collaborative multi-agent system."
                },
                "authors": [
                    {
                        "name": "Jeongeon Park"
                    },
                    {
                        "name": "Bryan Min"
                    },
                    {
                        "name": "Kihoon Son"
                    },
                    {
                        "name": "Jean Y. Song"
                    },
                    {
                        "name": "Xiaojuan Ma"
                    },
                    {
                        "name": "Juho Kim"
                    }
                ],
                "author_detail": {
                    "name": "Juho Kim"
                },
                "author": "Juho Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.01331v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.01331v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.17404v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.17404v2",
                "updated": "2025-01-22T05:52:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    5,
                    52,
                    42,
                    2,
                    22,
                    0
                ],
                "published": "2024-07-24T16:36:02Z",
                "published_parsed": [
                    2024,
                    7,
                    24,
                    16,
                    36,
                    2,
                    2,
                    206,
                    0
                ],
                "title": "Grammar-based Game Description Generation using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grammar-based Game Description Generation using Large Language Models"
                },
                "summary": "Game Description Language (GDL) provides a standardized way to express\ndiverse games in a machine-readable format, enabling automated game simulation,\nand evaluation. While previous research has explored game description\ngeneration using search-based methods, generating GDL descriptions from natural\nlanguage remains a challenging task. This paper presents a novel framework that\nleverages Large Language Models (LLMs) to generate grammatically accurate game\ndescriptions from natural language. Our approach consists of two stages: first,\nwe gradually generate a minimal grammar based on GDL specifications; second, we\niteratively improve the game description through grammar-guided generation. Our\nframework employs a specialized parser that identifies valid subsequences and\ncandidate symbols from LLM responses, enabling gradual refinement of the output\nto ensure grammatical correctness. Experimental results demonstrate that our\niterative improvement approach significantly outperforms baseline methods that\ndirectly use LLM outputs. Our code is available at\nhttps://github.com/tsunehiko/ggdg",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Game Description Language (GDL) provides a standardized way to express\ndiverse games in a machine-readable format, enabling automated game simulation,\nand evaluation. While previous research has explored game description\ngeneration using search-based methods, generating GDL descriptions from natural\nlanguage remains a challenging task. This paper presents a novel framework that\nleverages Large Language Models (LLMs) to generate grammatically accurate game\ndescriptions from natural language. Our approach consists of two stages: first,\nwe gradually generate a minimal grammar based on GDL specifications; second, we\niteratively improve the game description through grammar-guided generation. Our\nframework employs a specialized parser that identifies valid subsequences and\ncandidate symbols from LLM responses, enabling gradual refinement of the output\nto ensure grammatical correctness. Experimental results demonstrate that our\niterative improvement approach significantly outperforms baseline methods that\ndirectly use LLM outputs. Our code is available at\nhttps://github.com/tsunehiko/ggdg"
                },
                "authors": [
                    {
                        "name": "Tsunehiko Tanaka"
                    },
                    {
                        "name": "Edgar Simo-Serra"
                    }
                ],
                "author_detail": {
                    "name": "Edgar Simo-Serra"
                },
                "author": "Edgar Simo-Serra",
                "arxiv_doi": "10.1109/TG.2024.3520214",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TG.2024.3520214",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.17404v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.17404v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted for publication at the IEEE Transactions on Games",
                "arxiv_journal_ref": "IEEE Transactions on Games, 2024 (early access)",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12644v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12644v1",
                "updated": "2025-01-22T05:10:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    5,
                    10,
                    47,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T05:10:47Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    5,
                    10,
                    47,
                    2,
                    22,
                    0
                ],
                "title": "Current Opinions on Memristor-Accelerated Machine Learning Hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current Opinions on Memristor-Accelerated Machine Learning Hardware"
                },
                "summary": "The unprecedented advancement of artificial intelligence has placed immense\ndemands on computing hardware, but traditional silicon-based semiconductor\ntechnologies are approaching their physical and economic limit, prompting the\nexploration of novel computing paradigms. Memristor offers a promising\nsolution, enabling in-memory analog computation and massive parallelism, which\nleads to low latency and power consumption. This manuscript reviews the current\nstatus of memristor-based machine learning accelerators, highlighting the\nmilestones achieved in developing prototype chips, that not only accelerate\nneural networks inference but also tackle other machine learning tasks. More\nimportantly, it discusses our opinion on current key challenges that remain in\nthis field, such as device variation, the need for efficient peripheral\ncircuitry, and systematic co-design and optimization. We also share our\nperspective on potential future directions, some of which address existing\nchallenges while others explore untouched territories. By addressing these\nchallenges through interdisciplinary efforts spanning device engineering,\ncircuit design, and systems architecture, memristor-based accelerators could\nsignificantly advance the capabilities of AI hardware, particularly for edge\napplications where power efficiency is paramount.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The unprecedented advancement of artificial intelligence has placed immense\ndemands on computing hardware, but traditional silicon-based semiconductor\ntechnologies are approaching their physical and economic limit, prompting the\nexploration of novel computing paradigms. Memristor offers a promising\nsolution, enabling in-memory analog computation and massive parallelism, which\nleads to low latency and power consumption. This manuscript reviews the current\nstatus of memristor-based machine learning accelerators, highlighting the\nmilestones achieved in developing prototype chips, that not only accelerate\nneural networks inference but also tackle other machine learning tasks. More\nimportantly, it discusses our opinion on current key challenges that remain in\nthis field, such as device variation, the need for efficient peripheral\ncircuitry, and systematic co-design and optimization. We also share our\nperspective on potential future directions, some of which address existing\nchallenges while others explore untouched territories. By addressing these\nchallenges through interdisciplinary efforts spanning device engineering,\ncircuit design, and systems architecture, memristor-based accelerators could\nsignificantly advance the capabilities of AI hardware, particularly for edge\napplications where power efficiency is paramount."
                },
                "authors": [
                    {
                        "name": "Mingrui Jiang"
                    },
                    {
                        "name": "Yichun Xu"
                    },
                    {
                        "name": "Zefan Li"
                    },
                    {
                        "name": "Can Li"
                    }
                ],
                "author_detail": {
                    "name": "Can Li"
                },
                "author": "Can Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12644v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12644v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00907v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00907v3",
                "updated": "2025-01-22T05:04:26Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    5,
                    4,
                    26,
                    2,
                    22,
                    0
                ],
                "published": "2024-11-01T09:22:49Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    9,
                    22,
                    49,
                    4,
                    306,
                    0
                ],
                "title": "On the Impact of White-box Deployment Strategies for Edge AI on Latency\n  and Model Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Impact of White-box Deployment Strategies for Edge AI on Latency\n  and Model Performance"
                },
                "summary": "To help MLOps engineers decide which operator to use in which deployment\nscenario, this study aims to empirically assess the accuracy vs latency\ntrade-off of white-box (training-based) and black-box operators\n(non-training-based) and their combinations in an Edge AI setup. We perform\ninference experiments including 3 white-box (i.e., QAT, Pruning, Knowledge\nDistillation), 2 black-box (i.e., Partition, SPTQ), and their combined\noperators (i.e., Distilled SPTQ, SPTQ Partition) across 3 tiers (i.e., Mobile,\nEdge, Cloud) on 4 commonly-used Computer Vision and Natural Language Processing\nmodels to identify the effective strategies, considering the perspective of\nMLOps Engineers. Our Results indicate that the combination of Distillation and\nSPTQ operators (i.e., DSPTQ) should be preferred over non-hybrid operators when\nlower latency is required in the edge at small to medium accuracy drop. Among\nthe non-hybrid operators, the Distilled operator is a better alternative in\nboth mobile and edge tiers for lower latency performance at the cost of small\nto medium accuracy loss. Moreover, the operators involving distillation show\nlower latency in resource-constrained tiers (Mobile, Edge) compared to the\noperators involving Partitioning across Mobile and Edge tiers. For textual\nsubject models, which have low input data size requirements, the Cloud tier is\na better alternative for the deployment of operators than the Mobile, Edge, or\nMobile-Edge tier (the latter being used for operators involving partitioning).\nIn contrast, for image-based subject models, which have high input data size\nrequirements, the Edge tier is a better alternative for operators than Mobile,\nEdge, or their combination.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To help MLOps engineers decide which operator to use in which deployment\nscenario, this study aims to empirically assess the accuracy vs latency\ntrade-off of white-box (training-based) and black-box operators\n(non-training-based) and their combinations in an Edge AI setup. We perform\ninference experiments including 3 white-box (i.e., QAT, Pruning, Knowledge\nDistillation), 2 black-box (i.e., Partition, SPTQ), and their combined\noperators (i.e., Distilled SPTQ, SPTQ Partition) across 3 tiers (i.e., Mobile,\nEdge, Cloud) on 4 commonly-used Computer Vision and Natural Language Processing\nmodels to identify the effective strategies, considering the perspective of\nMLOps Engineers. Our Results indicate that the combination of Distillation and\nSPTQ operators (i.e., DSPTQ) should be preferred over non-hybrid operators when\nlower latency is required in the edge at small to medium accuracy drop. Among\nthe non-hybrid operators, the Distilled operator is a better alternative in\nboth mobile and edge tiers for lower latency performance at the cost of small\nto medium accuracy loss. Moreover, the operators involving distillation show\nlower latency in resource-constrained tiers (Mobile, Edge) compared to the\noperators involving Partitioning across Mobile and Edge tiers. For textual\nsubject models, which have low input data size requirements, the Cloud tier is\na better alternative for the deployment of operators than the Mobile, Edge, or\nMobile-Edge tier (the latter being used for operators involving partitioning).\nIn contrast, for image-based subject models, which have high input data size\nrequirements, the Edge tier is a better alternative for operators than Mobile,\nEdge, or their combination."
                },
                "authors": [
                    {
                        "name": "Jaskirat Singh"
                    },
                    {
                        "name": "Bram Adams"
                    },
                    {
                        "name": "Ahmed E. Hassan"
                    }
                ],
                "author_detail": {
                    "name": "Ahmed E. Hassan"
                },
                "author": "Ahmed E. Hassan",
                "arxiv_comment": "In Approach Section, Pruning & Knowledge Distillation methods of\n  Intel Neural Compressor don't reduce the model size & improve performance,\n  respectively, unlike previous studies. There are issues exporting QAT models\n  from PyTorch to ONNX, raising concerns about our latency results",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00907v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00907v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12642v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12642v1",
                "updated": "2025-01-22T05:03:51Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    5,
                    3,
                    51,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T05:03:51Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    5,
                    3,
                    51,
                    2,
                    22,
                    0
                ],
                "title": "Training Data Attribution (TDA): Examining Its Adoption & Use Cases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training Data Attribution (TDA): Examining Its Adoption & Use Cases"
                },
                "summary": "This report investigates Training Data Attribution (TDA) and its potential\nimportance to and tractability for reducing extreme risks from AI. First, we\ndiscuss the plausibility and amount of effort it would take to bring existing\nTDA research efforts from their current state, to an efficient and accurate\ntool for TDA inference that can be run on frontier-scale LLMs. Next, we discuss\nthe numerous research benefits AI labs will expect to see from using such TDA\ntooling. Then, we discuss a key outstanding bottleneck that would limit such\nTDA tooling from being accessible publicly: AI labs' willingness to disclose\ntheir training data. We suggest ways AI labs may work around these limitations,\nand discuss the willingness of governments to mandate such access. Assuming\nthat AI labs willingly provide access to TDA inference, we then discuss what\nhigh-level societal benefits you might see. We list and discuss a series of\npolicies and systems that may be enabled by TDA. Finally, we present an\nevaluation of TDA's potential impact on mitigating large-scale risks from AI\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This report investigates Training Data Attribution (TDA) and its potential\nimportance to and tractability for reducing extreme risks from AI. First, we\ndiscuss the plausibility and amount of effort it would take to bring existing\nTDA research efforts from their current state, to an efficient and accurate\ntool for TDA inference that can be run on frontier-scale LLMs. Next, we discuss\nthe numerous research benefits AI labs will expect to see from using such TDA\ntooling. Then, we discuss a key outstanding bottleneck that would limit such\nTDA tooling from being accessible publicly: AI labs' willingness to disclose\ntheir training data. We suggest ways AI labs may work around these limitations,\nand discuss the willingness of governments to mandate such access. Assuming\nthat AI labs willingly provide access to TDA inference, we then discuss what\nhigh-level societal benefits you might see. We list and discuss a series of\npolicies and systems that may be enabled by TDA. Finally, we present an\nevaluation of TDA's potential impact on mitigating large-scale risks from AI\nsystems."
                },
                "authors": [
                    {
                        "name": "Deric Cheng"
                    },
                    {
                        "name": "Juhan Bae"
                    },
                    {
                        "name": "Justin Bullock"
                    },
                    {
                        "name": "David Kristofferson"
                    }
                ],
                "author_detail": {
                    "name": "David Kristofferson"
                },
                "author": "David Kristofferson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12642v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12642v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12634v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12634v1",
                "updated": "2025-01-22T04:42:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    4,
                    42,
                    19,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T04:42:19Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    4,
                    42,
                    19,
                    2,
                    22,
                    0
                ],
                "title": "SoMa: Identifying, Exploring, and Understanding the DRAM Communication\n  Scheduling Space for DNN Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SoMa: Identifying, Exploring, and Understanding the DRAM Communication\n  Scheduling Space for DNN Accelerators"
                },
                "summary": "Modern Deep Neural Network (DNN) accelerators are equipped with increasingly\nlarger on-chip buffers to provide more opportunities to alleviate the\nincreasingly severe DRAM bandwidth pressure. However, most existing research on\nbuffer utilization still primarily focuses on single-layer dataflow scheduling\noptimization. As buffers grow large enough to accommodate most single-layer\nweights in most networks, the impact of single-layer dataflow optimization on\nDRAM communication diminishes significantly. Therefore, developing new\nparadigms that fuse multiple layers to fully leverage the increasingly abundant\non-chip buffer resources to reduce DRAM accesses has become particularly\nimportant, yet remains an open challenge. To address this challenge, we first\nidentify the optimization opportunities in DRAM communication scheduling by\nanalyzing the drawbacks of existing works on the layer fusion paradigm and\nrecognizing the vast optimization potential in scheduling the timing of data\nprefetching from and storing to DRAM. To fully exploit these optimization\nopportunities, we develop a Tensor-centric Notation and its corresponding\nparsing method to represent different DRAM communication scheduling schemes and\ndepict the overall space of DRAM communication scheduling. Then, to thoroughly\nand efficiently explore the space of DRAM communication scheduling for diverse\naccelerators and workloads, we develop an end-to-end scheduling framework,\nSoMa, which has already been developed into a compiler for our commercial\naccelerator product. Compared with the state-of-the-art (SOTA) Cocco framework,\nSoMa achieves, on average, a 2.11x performance improvement and a 37.3%\nreduction in energy cost simultaneously. Then, we leverage SoMa to study\noptimizations for LLM, perform design space exploration (DSE), and analyze the\nDRAM communication scheduling space through a practical example, yielding\nsome..(more)",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Deep Neural Network (DNN) accelerators are equipped with increasingly\nlarger on-chip buffers to provide more opportunities to alleviate the\nincreasingly severe DRAM bandwidth pressure. However, most existing research on\nbuffer utilization still primarily focuses on single-layer dataflow scheduling\noptimization. As buffers grow large enough to accommodate most single-layer\nweights in most networks, the impact of single-layer dataflow optimization on\nDRAM communication diminishes significantly. Therefore, developing new\nparadigms that fuse multiple layers to fully leverage the increasingly abundant\non-chip buffer resources to reduce DRAM accesses has become particularly\nimportant, yet remains an open challenge. To address this challenge, we first\nidentify the optimization opportunities in DRAM communication scheduling by\nanalyzing the drawbacks of existing works on the layer fusion paradigm and\nrecognizing the vast optimization potential in scheduling the timing of data\nprefetching from and storing to DRAM. To fully exploit these optimization\nopportunities, we develop a Tensor-centric Notation and its corresponding\nparsing method to represent different DRAM communication scheduling schemes and\ndepict the overall space of DRAM communication scheduling. Then, to thoroughly\nand efficiently explore the space of DRAM communication scheduling for diverse\naccelerators and workloads, we develop an end-to-end scheduling framework,\nSoMa, which has already been developed into a compiler for our commercial\naccelerator product. Compared with the state-of-the-art (SOTA) Cocco framework,\nSoMa achieves, on average, a 2.11x performance improvement and a 37.3%\nreduction in energy cost simultaneously. Then, we leverage SoMa to study\noptimizations for LLM, perform design space exploration (DSE), and analyze the\nDRAM communication scheduling space through a practical example, yielding\nsome..(more)"
                },
                "authors": [
                    {
                        "name": "Jingwei Cai"
                    },
                    {
                        "name": "Xuan Wang"
                    },
                    {
                        "name": "Mingyu Gao"
                    },
                    {
                        "name": "Sen Peng"
                    },
                    {
                        "name": "Zijian Zhu"
                    },
                    {
                        "name": "Yuchen Wei"
                    },
                    {
                        "name": "Zuotong Wu"
                    },
                    {
                        "name": "Kaisheng Ma"
                    }
                ],
                "author_detail": {
                    "name": "Kaisheng Ma"
                },
                "arxiv_affiliation": "Tsinghua University",
                "author": "Kaisheng Ma",
                "arxiv_comment": "Accepted by 2025 IEEE International Symposium on High-Performance\n  Computer Architecture (HPCA)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12634v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12634v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07098v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07098v2",
                "updated": "2025-01-22T04:42:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    4,
                    42,
                    10,
                    2,
                    22,
                    0
                ],
                "published": "2024-11-11T16:20:27Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    16,
                    20,
                    27,
                    0,
                    316,
                    0
                ],
                "title": "A Multi-Agent Approach for REST API Testing with Semantic Graphs and\n  LLM-Driven Inputs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Multi-Agent Approach for REST API Testing with Semantic Graphs and\n  LLM-Driven Inputs"
                },
                "summary": "As modern web services increasingly rely on REST APIs, their thorough testing\nhas become crucial. Furthermore, the advent of REST API documentation\nlanguages, such as the OpenAPI Specification, has led to the emergence of many\nblack-box REST API testing tools. However, these tools often focus on\nindividual test elements in isolation (e.g., APIs, parameters, values),\nresulting in lower coverage and less effectiveness in fault detection. To\naddress these limitations, we present AutoRestTest, the first black-box tool to\nadopt a dependency-embedded multi-agent approach for REST API testing that\nintegrates multi-agent reinforcement learning (MARL) with a semantic property\ndependency graph (SPDG) and Large Language Models (LLMs). Our approach treats\nREST API testing as a separable problem, where four agents -- API, dependency,\nparameter, and value agents -- collaborate to optimize API exploration. LLMs\nhandle domain-specific value generation, the SPDG model simplifies the search\nspace for dependencies using a similarity score between API operations, and\nMARL dynamically optimizes the agents' behavior. Our evaluation of AutoRestTest\non 12 real-world REST services shows that it outperforms the four leading\nblack-box REST API testing tools, including those assisted by RESTGPT (which\ngenerates realistic test inputs using LLMs), in terms of code coverage,\noperation coverage, and fault detection. Notably, AutoRestTest is the only tool\nable to trigger an internal server error in the Spotify service. Our ablation\nstudy illustrates that each component of AutoRestTest -- the SPDG, the LLM, and\nthe agent-learning mechanism -- contributes to its overall effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As modern web services increasingly rely on REST APIs, their thorough testing\nhas become crucial. Furthermore, the advent of REST API documentation\nlanguages, such as the OpenAPI Specification, has led to the emergence of many\nblack-box REST API testing tools. However, these tools often focus on\nindividual test elements in isolation (e.g., APIs, parameters, values),\nresulting in lower coverage and less effectiveness in fault detection. To\naddress these limitations, we present AutoRestTest, the first black-box tool to\nadopt a dependency-embedded multi-agent approach for REST API testing that\nintegrates multi-agent reinforcement learning (MARL) with a semantic property\ndependency graph (SPDG) and Large Language Models (LLMs). Our approach treats\nREST API testing as a separable problem, where four agents -- API, dependency,\nparameter, and value agents -- collaborate to optimize API exploration. LLMs\nhandle domain-specific value generation, the SPDG model simplifies the search\nspace for dependencies using a similarity score between API operations, and\nMARL dynamically optimizes the agents' behavior. Our evaluation of AutoRestTest\non 12 real-world REST services shows that it outperforms the four leading\nblack-box REST API testing tools, including those assisted by RESTGPT (which\ngenerates realistic test inputs using LLMs), in terms of code coverage,\noperation coverage, and fault detection. Notably, AutoRestTest is the only tool\nable to trigger an internal server error in the Spotify service. Our ablation\nstudy illustrates that each component of AutoRestTest -- the SPDG, the LLM, and\nthe agent-learning mechanism -- contributes to its overall effectiveness."
                },
                "authors": [
                    {
                        "name": "Myeongsoo Kim"
                    },
                    {
                        "name": "Tyler Stennett"
                    },
                    {
                        "name": "Saurabh Sinha"
                    },
                    {
                        "name": "Alessandro Orso"
                    }
                ],
                "author_detail": {
                    "name": "Alessandro Orso"
                },
                "author": "Alessandro Orso",
                "arxiv_comment": "To be published in the 47th IEEE/ACM International Conference on\n  Software Engineering (ICSE 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07098v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07098v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2210.13984v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2210.13984v5",
                "updated": "2025-01-22T04:02:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    4,
                    2,
                    30,
                    2,
                    22,
                    0
                ],
                "published": "2022-10-24T07:43:59Z",
                "published_parsed": [
                    2022,
                    10,
                    24,
                    7,
                    43,
                    59,
                    0,
                    297,
                    0
                ],
                "title": "Inferring Past Human Actions in Homes with Abductive Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring Past Human Actions in Homes with Abductive Reasoning"
                },
                "summary": "Abductive reasoning aims to make the most likely inference for a given set of\nincomplete observations. In this paper, we introduce \"Abductive Past Action\nInference\", a novel research task aimed at identifying the past actions\nperformed by individuals within homes to reach specific states captured in a\nsingle image, using abductive inference. The research explores three key\nabductive inference problems: past action set prediction, past action sequence\nprediction, and abductive past action verification. We introduce several models\ntailored for abductive past action inference, including a relational graph\nneural network, a relational bilinear pooling model, and a relational\ntransformer model. Notably, the newly proposed object-relational bilinear graph\nencoder-decoder (BiGED) model emerges as the most effective among all methods\nevaluated, demonstrating good proficiency in handling the intricacies of the\nAction Genome dataset. The contributions of this research significantly advance\nthe ability of deep learning models to reason about current scene evidence and\nmake highly plausible inferences about past human actions. This advancement\nenables a deeper understanding of events and behaviors, which can enhance\ndecision-making and improve system capabilities across various real-world\napplications such as Human-Robot Interaction and Elderly Care and Health\nMonitoring. Code and data available at https://github.com/LUNAProject22/AAR",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Abductive reasoning aims to make the most likely inference for a given set of\nincomplete observations. In this paper, we introduce \"Abductive Past Action\nInference\", a novel research task aimed at identifying the past actions\nperformed by individuals within homes to reach specific states captured in a\nsingle image, using abductive inference. The research explores three key\nabductive inference problems: past action set prediction, past action sequence\nprediction, and abductive past action verification. We introduce several models\ntailored for abductive past action inference, including a relational graph\nneural network, a relational bilinear pooling model, and a relational\ntransformer model. Notably, the newly proposed object-relational bilinear graph\nencoder-decoder (BiGED) model emerges as the most effective among all methods\nevaluated, demonstrating good proficiency in handling the intricacies of the\nAction Genome dataset. The contributions of this research significantly advance\nthe ability of deep learning models to reason about current scene evidence and\nmake highly plausible inferences about past human actions. This advancement\nenables a deeper understanding of events and behaviors, which can enhance\ndecision-making and improve system capabilities across various real-world\napplications such as Human-Robot Interaction and Elderly Care and Health\nMonitoring. Code and data available at https://github.com/LUNAProject22/AAR"
                },
                "authors": [
                    {
                        "name": "Clement Tan"
                    },
                    {
                        "name": "Chai Kiat Yeo"
                    },
                    {
                        "name": "Cheston Tan"
                    },
                    {
                        "name": "Basura Fernando"
                    }
                ],
                "author_detail": {
                    "name": "Basura Fernando"
                },
                "author": "Basura Fernando",
                "arxiv_comment": "15 pages, 8 figures, Accepted to WACV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2210.13984v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2210.13984v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12619v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12619v1",
                "updated": "2025-01-22T03:57:52Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    3,
                    57,
                    52,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T03:57:52Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    3,
                    57,
                    52,
                    2,
                    22,
                    0
                ],
                "title": "Distillation Quantification for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distillation Quantification for Large Language Models"
                },
                "summary": "Model distillation is a technique for transferring knowledge from large\nlanguage models (LLMs) to smaller ones, aiming to create resource-efficient yet\nhigh-performing models. However, excessive distillation can lead to\nhomogenization, reducing diversity among models and impairing their ability to\nrobustly handle complex or novel tasks. These limitations underscore the need\nto systematically quantify the distillation process and its impact. In this\nwork, we propose a framework to evaluate and quantify model distillation. Our\nmethod addresses two key aspects: (1) Identifying identity cognition\ncontradictions to assess discrepancies in how models perceive and represent\nidentity-related information, and (2) Analyzing multi-granularity response\nsimilarities across models to measure the extent of homogenization.\nExperimental results demonstrate two key insights: (1) Well-known closed-source\nand open-source LLMs usually exhibit high distillation degrees, except for\nClaude, Doubao, and Gemini. (2) Base LLMs show higher distillation degrees\ncompared to aligned LLMs. By offering a systematic approach to improve the\ntransparency of LLM data distillation, we call for LLMs with more independent\ndevelopment and more transparent technical reports to improve LLMs' robustness\nand safety. The code and data are available under\nhttps://github.com/Aegis1863/LLMs-Distillation-Quantification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model distillation is a technique for transferring knowledge from large\nlanguage models (LLMs) to smaller ones, aiming to create resource-efficient yet\nhigh-performing models. However, excessive distillation can lead to\nhomogenization, reducing diversity among models and impairing their ability to\nrobustly handle complex or novel tasks. These limitations underscore the need\nto systematically quantify the distillation process and its impact. In this\nwork, we propose a framework to evaluate and quantify model distillation. Our\nmethod addresses two key aspects: (1) Identifying identity cognition\ncontradictions to assess discrepancies in how models perceive and represent\nidentity-related information, and (2) Analyzing multi-granularity response\nsimilarities across models to measure the extent of homogenization.\nExperimental results demonstrate two key insights: (1) Well-known closed-source\nand open-source LLMs usually exhibit high distillation degrees, except for\nClaude, Doubao, and Gemini. (2) Base LLMs show higher distillation degrees\ncompared to aligned LLMs. By offering a systematic approach to improve the\ntransparency of LLM data distillation, we call for LLMs with more independent\ndevelopment and more transparent technical reports to improve LLMs' robustness\nand safety. The code and data are available under\nhttps://github.com/Aegis1863/LLMs-Distillation-Quantification."
                },
                "authors": [
                    {
                        "name": "Sunbowen Lee"
                    },
                    {
                        "name": "Junting Zhou"
                    },
                    {
                        "name": "Chang Ao"
                    },
                    {
                        "name": "Kaige Li"
                    },
                    {
                        "name": "Xinrun Du"
                    },
                    {
                        "name": "Sirui He"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Min Yang"
                    },
                    {
                        "name": "Zhoufutu Wen"
                    },
                    {
                        "name": "Shiwen Ni"
                    }
                ],
                "author_detail": {
                    "name": "Shiwen Ni"
                },
                "author": "Shiwen Ni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12619v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12619v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12617v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12617v1",
                "updated": "2025-01-22T03:51:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    3,
                    51,
                    56,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T03:51:56Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    3,
                    51,
                    56,
                    2,
                    22,
                    0
                ],
                "title": "Deep Learning-Based Identification of Inconsistent Method Names: How Far\n  Are We?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning-Based Identification of Inconsistent Method Names: How Far\n  Are We?"
                },
                "summary": "Concise and meaningful method names are crucial for program comprehension and\nmaintenance. However, method names may become inconsistent with their\ncorresponding implementations, causing confusion and errors. Several deep\nlearning (DL)-based approaches have been proposed to identify such\ninconsistencies, with initial evaluations showing promising results. However,\nthese evaluations typically use a balanced dataset, where the number of\ninconsistent and consistent names are equal. This setup, along with flawed\ndataset construction, leads to false positives, making reported performance\nless reliable in real-world scenarios, where most method names are consistent.\nIn this paper, we present an empirical study that evaluates state-of-the-art\nDL-based methods for identifying inconsistent method names. We create a new\nbenchmark by combining automatic identification from commit histories and\nmanual developer inspections, reducing false positives. We evaluate five\nrepresentative DL approaches (one retrieval-based and four generation-based) on\nthis benchmark. Our results show that performance drops substantially when\nmoving from the balanced dataset to the new benchmark. We further conduct\nquantitative and qualitative analyses to understand the strengths and\nweaknesses of the approaches. Retrieval-based methods perform well on simple\nmethods and those with popular name sub-tokens but fail due to inefficient\nrepresentation techniques. Generation-based methods struggle with inaccurate\nsimilarity calculations and immature name generation. Based on these findings,\nwe propose improvements using contrastive learning and large language models\n(LLMs). Our study suggests that significant improvements are needed before\nthese DL approaches can be effectively applied to real-world software systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Concise and meaningful method names are crucial for program comprehension and\nmaintenance. However, method names may become inconsistent with their\ncorresponding implementations, causing confusion and errors. Several deep\nlearning (DL)-based approaches have been proposed to identify such\ninconsistencies, with initial evaluations showing promising results. However,\nthese evaluations typically use a balanced dataset, where the number of\ninconsistent and consistent names are equal. This setup, along with flawed\ndataset construction, leads to false positives, making reported performance\nless reliable in real-world scenarios, where most method names are consistent.\nIn this paper, we present an empirical study that evaluates state-of-the-art\nDL-based methods for identifying inconsistent method names. We create a new\nbenchmark by combining automatic identification from commit histories and\nmanual developer inspections, reducing false positives. We evaluate five\nrepresentative DL approaches (one retrieval-based and four generation-based) on\nthis benchmark. Our results show that performance drops substantially when\nmoving from the balanced dataset to the new benchmark. We further conduct\nquantitative and qualitative analyses to understand the strengths and\nweaknesses of the approaches. Retrieval-based methods perform well on simple\nmethods and those with popular name sub-tokens but fail due to inefficient\nrepresentation techniques. Generation-based methods struggle with inaccurate\nsimilarity calculations and immature name generation. Based on these findings,\nwe propose improvements using contrastive learning and large language models\n(LLMs). Our study suggests that significant improvements are needed before\nthese DL approaches can be effectively applied to real-world software systems."
                },
                "authors": [
                    {
                        "name": "Taiming Wang"
                    },
                    {
                        "name": "Yuxia Zhang"
                    },
                    {
                        "name": "Lin Jiang"
                    },
                    {
                        "name": "Yi Tang"
                    },
                    {
                        "name": "Guangjie Li"
                    },
                    {
                        "name": "Hui Liu"
                    }
                ],
                "author_detail": {
                    "name": "Hui Liu"
                },
                "author": "Hui Liu",
                "arxiv_doi": "10.1007/s10664-024-10592-z",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s10664-024-10592-z",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.12617v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12617v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Empirical Software Engineering, 2025, 30(1): 31",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.10528v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.10528v3",
                "updated": "2025-01-22T03:50:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    3,
                    50,
                    58,
                    2,
                    22,
                    0
                ],
                "published": "2024-02-16T09:29:50Z",
                "published_parsed": [
                    2024,
                    2,
                    16,
                    9,
                    29,
                    50,
                    4,
                    47,
                    0
                ],
                "title": "Can We Verify Step by Step for Incorrect Answer Detection?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can We Verify Step by Step for Incorrect Answer Detection?"
                },
                "summary": "Chain-of-Thought (CoT) prompting has marked a significant advancement in\nenhancing the reasoning capabilities of large language models (LLMs). Previous\nstudies have developed various extensions of CoT, which focus primarily on\nenhancing end-task performance. In addition, there has been research on\nassessing the quality of reasoning chains in CoT. This raises an intriguing\nquestion: Is it possible to predict the accuracy of LLM outputs by scrutinizing\nthe reasoning chains they generate? To answer this research question, we\nintroduce a benchmark, R2PE, designed specifically to explore the relationship\nbetween reasoning chains and performance in various reasoning tasks spanning\nfive different domains. This benchmark aims to measure the falsehood of the\nfinal output of LLMs based on the reasoning steps. To make full use of\ninformation in multiple reasoning chains, we propose the process discernibility\nscore (PDS) framework that beats the answer-checking baseline by a large\nmargin. Concretely, this resulted in an average of $5.1\\%$ increase in the F1\nscore and $2.97\\%$ improvement in AUC-PR across all 45 subsets within R2PE. We\nfurther demonstrate our PDS's efficacy in advancing open-domain QA accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) prompting has marked a significant advancement in\nenhancing the reasoning capabilities of large language models (LLMs). Previous\nstudies have developed various extensions of CoT, which focus primarily on\nenhancing end-task performance. In addition, there has been research on\nassessing the quality of reasoning chains in CoT. This raises an intriguing\nquestion: Is it possible to predict the accuracy of LLM outputs by scrutinizing\nthe reasoning chains they generate? To answer this research question, we\nintroduce a benchmark, R2PE, designed specifically to explore the relationship\nbetween reasoning chains and performance in various reasoning tasks spanning\nfive different domains. This benchmark aims to measure the falsehood of the\nfinal output of LLMs based on the reasoning steps. To make full use of\ninformation in multiple reasoning chains, we propose the process discernibility\nscore (PDS) framework that beats the answer-checking baseline by a large\nmargin. Concretely, this resulted in an average of $5.1\\%$ increase in the F1\nscore and $2.97\\%$ improvement in AUC-PR across all 45 subsets within R2PE. We\nfurther demonstrate our PDS's efficacy in advancing open-domain QA accuracy."
                },
                "authors": [
                    {
                        "name": "Xin Xu"
                    },
                    {
                        "name": "Shizhe Diao"
                    },
                    {
                        "name": "Can Yang"
                    },
                    {
                        "name": "Yang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yang Wang"
                },
                "author": "Yang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.10528v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.10528v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14219v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14219v2",
                "updated": "2025-01-22T03:33:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    3,
                    33,
                    25,
                    2,
                    22,
                    0
                ],
                "published": "2024-12-18T14:11:15Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    14,
                    11,
                    15,
                    2,
                    353,
                    0
                ],
                "title": "A Survey on Inference Optimization Techniques for Mixture of Experts\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Inference Optimization Techniques for Mixture of Experts\n  Models"
                },
                "summary": "The emergence of large-scale Mixture of Experts (MoE) models represents a\nsignificant advancement in artificial intelligence, offering enhanced model\ncapacity and computational efficiency through conditional computation. However,\ndeploying and running inference on these models presents significant challenges\nin computational resources, latency, and energy efficiency. This comprehensive\nsurvey analyzes optimization techniques for MoE models across the entire system\nstack. We first establish a taxonomical framework that categorizes optimization\napproaches into model-level, system-level, and hardware-level optimizations. At\nthe model level, we examine architectural innovations including efficient\nexpert design, attention mechanisms, various compression techniques such as\npruning, quantization, and knowledge distillation, as well as algorithm\nimprovement including dynamic routing strategies and expert merging methods. At\nthe system level, we investigate distributed computing approaches, load\nbalancing mechanisms, and efficient scheduling algorithms that enable scalable\ndeployment. Furthermore, we delve into hardware-specific optimizations and\nco-design strategies that maximize throughput and energy efficiency. This\nsurvey provides both a structured overview of existing solutions and identifies\nkey challenges and promising research directions in MoE inference optimization.\nTo facilitate ongoing updates and the sharing of cutting-edge advances in MoE\ninference optimization research, we have established a repository accessible at\nhttps://github.com/MoE-Inf/awesome-moe-inference/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of large-scale Mixture of Experts (MoE) models represents a\nsignificant advancement in artificial intelligence, offering enhanced model\ncapacity and computational efficiency through conditional computation. However,\ndeploying and running inference on these models presents significant challenges\nin computational resources, latency, and energy efficiency. This comprehensive\nsurvey analyzes optimization techniques for MoE models across the entire system\nstack. We first establish a taxonomical framework that categorizes optimization\napproaches into model-level, system-level, and hardware-level optimizations. At\nthe model level, we examine architectural innovations including efficient\nexpert design, attention mechanisms, various compression techniques such as\npruning, quantization, and knowledge distillation, as well as algorithm\nimprovement including dynamic routing strategies and expert merging methods. At\nthe system level, we investigate distributed computing approaches, load\nbalancing mechanisms, and efficient scheduling algorithms that enable scalable\ndeployment. Furthermore, we delve into hardware-specific optimizations and\nco-design strategies that maximize throughput and energy efficiency. This\nsurvey provides both a structured overview of existing solutions and identifies\nkey challenges and promising research directions in MoE inference optimization.\nTo facilitate ongoing updates and the sharing of cutting-edge advances in MoE\ninference optimization research, we have established a repository accessible at\nhttps://github.com/MoE-Inf/awesome-moe-inference/."
                },
                "authors": [
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Peng Tang"
                    },
                    {
                        "name": "Wenfeng Wang"
                    },
                    {
                        "name": "Yuhang Ren"
                    },
                    {
                        "name": "Xiaofeng Hou"
                    },
                    {
                        "name": "Pheng-Ann Heng"
                    },
                    {
                        "name": "Minyi Guo"
                    },
                    {
                        "name": "Chao Li"
                    }
                ],
                "author_detail": {
                    "name": "Chao Li"
                },
                "author": "Chao Li",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14219v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14219v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12608v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12608v1",
                "updated": "2025-01-22T03:24:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    3,
                    24,
                    36,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T03:24:36Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    3,
                    24,
                    36,
                    2,
                    22,
                    0
                ],
                "title": "Inferring the cosmological constant in early Universe only by\n  gravitational waves",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring the cosmological constant in early Universe only by\n  gravitational waves"
                },
                "summary": "The expansion of the Universe is accelerating which can be interpreted as due\nto the cosmological constant $\\Lambda$. In this study, we investigate the\nbehavior of gravitational waves in the presence of a cosmological constant in\nthe early universe. We rigorously analyze the merger rate of binary primordial\nblack holes (PBHs) and the corresponding signal-to-noise ratio within the\nframework of Laser Interferometer Space Antenna (LISA). We find that binary\nPBHs with a total mass of $M_{\\mathrm{tot}}=1000M_{\\odot}$ and a redshift\nlarger than $z=500$ are the ideal system for studying the effect of the\ncosmological constant through LISA. By computing the fisher information matrix,\nwe establish that the cosmological constant can be effectively constrained.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The expansion of the Universe is accelerating which can be interpreted as due\nto the cosmological constant $\\Lambda$. In this study, we investigate the\nbehavior of gravitational waves in the presence of a cosmological constant in\nthe early universe. We rigorously analyze the merger rate of binary primordial\nblack holes (PBHs) and the corresponding signal-to-noise ratio within the\nframework of Laser Interferometer Space Antenna (LISA). We find that binary\nPBHs with a total mass of $M_{\\mathrm{tot}}=1000M_{\\odot}$ and a redshift\nlarger than $z=500$ are the ideal system for studying the effect of the\ncosmological constant through LISA. By computing the fisher information matrix,\nwe establish that the cosmological constant can be effectively constrained."
                },
                "authors": [
                    {
                        "name": "Song Li"
                    },
                    {
                        "name": "Wen-Biao Han"
                    }
                ],
                "author_detail": {
                    "name": "Wen-Biao Han"
                },
                "author": "Wen-Biao Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12608v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12608v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12599v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12599v1",
                "updated": "2025-01-22T02:48:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    2,
                    48,
                    14,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T02:48:14Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    2,
                    48,
                    14,
                    2,
                    22,
                    0
                ],
                "title": "Kimi k1.5: Scaling Reinforcement Learning with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kimi k1.5: Scaling Reinforcement Learning with LLMs"
                },
                "summary": "Language model pretraining with next token prediction has proved effective\nfor scaling compute but is limited to the amount of available training data.\nScaling reinforcement learning (RL) unlocks a new axis for the continued\nimprovement of artificial intelligence, with the promise that large language\nmodels (LLMs) can scale their training data by learning to explore with\nrewards. However, prior published work has not produced competitive results. In\nlight of this, we report on the training practice of Kimi k1.5, our latest\nmulti-modal LLM trained with RL, including its RL training techniques,\nmulti-modal data recipes, and infrastructure optimization. Long context scaling\nand improved policy optimization methods are key ingredients of our approach,\nwhich establishes a simplistic, effective RL framework without relying on more\ncomplex techniques such as Monte Carlo tree search, value functions, and\nprocess reward models. Notably, our system achieves state-of-the-art reasoning\nperformance across multiple benchmarks and modalities -- e.g., 77.5 on AIME,\n96.2 on MATH 500, 94-th percentile on Codeforces, 74.9 on MathVista -- matching\nOpenAI's o1. Moreover, we present effective long2short methods that use\nlong-CoT techniques to improve short-CoT models, yielding state-of-the-art\nshort-CoT reasoning results -- e.g., 60.8 on AIME, 94.6 on MATH500, 47.3 on\nLiveCodeBench -- outperforming existing short-CoT models such as GPT-4o and\nClaude Sonnet 3.5 by a large margin (up to +550%).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language model pretraining with next token prediction has proved effective\nfor scaling compute but is limited to the amount of available training data.\nScaling reinforcement learning (RL) unlocks a new axis for the continued\nimprovement of artificial intelligence, with the promise that large language\nmodels (LLMs) can scale their training data by learning to explore with\nrewards. However, prior published work has not produced competitive results. In\nlight of this, we report on the training practice of Kimi k1.5, our latest\nmulti-modal LLM trained with RL, including its RL training techniques,\nmulti-modal data recipes, and infrastructure optimization. Long context scaling\nand improved policy optimization methods are key ingredients of our approach,\nwhich establishes a simplistic, effective RL framework without relying on more\ncomplex techniques such as Monte Carlo tree search, value functions, and\nprocess reward models. Notably, our system achieves state-of-the-art reasoning\nperformance across multiple benchmarks and modalities -- e.g., 77.5 on AIME,\n96.2 on MATH 500, 94-th percentile on Codeforces, 74.9 on MathVista -- matching\nOpenAI's o1. Moreover, we present effective long2short methods that use\nlong-CoT techniques to improve short-CoT models, yielding state-of-the-art\nshort-CoT reasoning results -- e.g., 60.8 on AIME, 94.6 on MATH500, 47.3 on\nLiveCodeBench -- outperforming existing short-CoT models such as GPT-4o and\nClaude Sonnet 3.5 by a large margin (up to +550%)."
                },
                "authors": [
                    {
                        "name": "Kimi Team"
                    },
                    {
                        "name": "Angang Du"
                    },
                    {
                        "name": "Bofei Gao"
                    },
                    {
                        "name": "Bowei Xing"
                    },
                    {
                        "name": "Changjiu Jiang"
                    },
                    {
                        "name": "Cheng Chen"
                    },
                    {
                        "name": "Cheng Li"
                    },
                    {
                        "name": "Chenjun Xiao"
                    },
                    {
                        "name": "Chenzhuang Du"
                    },
                    {
                        "name": "Chonghua Liao"
                    },
                    {
                        "name": "Chuning Tang"
                    },
                    {
                        "name": "Congcong Wang"
                    },
                    {
                        "name": "Dehao Zhang"
                    },
                    {
                        "name": "Enming Yuan"
                    },
                    {
                        "name": "Enzhe Lu"
                    },
                    {
                        "name": "Fengxiang Tang"
                    },
                    {
                        "name": "Flood Sung"
                    },
                    {
                        "name": "Guangda Wei"
                    },
                    {
                        "name": "Guokun Lai"
                    },
                    {
                        "name": "Haiqing Guo"
                    },
                    {
                        "name": "Han Zhu"
                    },
                    {
                        "name": "Hao Ding"
                    },
                    {
                        "name": "Hao Hu"
                    },
                    {
                        "name": "Hao Yang"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Haotian Yao"
                    },
                    {
                        "name": "Haotian Zhao"
                    },
                    {
                        "name": "Haoyu Lu"
                    },
                    {
                        "name": "Haoze Li"
                    },
                    {
                        "name": "Haozhen Yu"
                    },
                    {
                        "name": "Hongcheng Gao"
                    },
                    {
                        "name": "Huabin Zheng"
                    },
                    {
                        "name": "Huan Yuan"
                    },
                    {
                        "name": "Jia Chen"
                    },
                    {
                        "name": "Jianhang Guo"
                    },
                    {
                        "name": "Jianlin Su"
                    },
                    {
                        "name": "Jianzhou Wang"
                    },
                    {
                        "name": "Jie Zhao"
                    },
                    {
                        "name": "Jin Zhang"
                    },
                    {
                        "name": "Jingyuan Liu"
                    },
                    {
                        "name": "Junjie Yan"
                    },
                    {
                        "name": "Junyan Wu"
                    },
                    {
                        "name": "Lidong Shi"
                    },
                    {
                        "name": "Ling Ye"
                    },
                    {
                        "name": "Longhui Yu"
                    },
                    {
                        "name": "Mengnan Dong"
                    },
                    {
                        "name": "Neo Zhang"
                    },
                    {
                        "name": "Ningchen Ma"
                    },
                    {
                        "name": "Qiwei Pan"
                    },
                    {
                        "name": "Qucheng Gong"
                    },
                    {
                        "name": "Shaowei Liu"
                    },
                    {
                        "name": "Shengling Ma"
                    },
                    {
                        "name": "Shupeng Wei"
                    },
                    {
                        "name": "Sihan Cao"
                    },
                    {
                        "name": "Siying Huang"
                    },
                    {
                        "name": "Tao Jiang"
                    },
                    {
                        "name": "Weihao Gao"
                    },
                    {
                        "name": "Weimin Xiong"
                    },
                    {
                        "name": "Weiran He"
                    },
                    {
                        "name": "Weixiao Huang"
                    },
                    {
                        "name": "Wenhao Wu"
                    },
                    {
                        "name": "Wenyang He"
                    },
                    {
                        "name": "Xianghui Wei"
                    },
                    {
                        "name": "Xianqing Jia"
                    },
                    {
                        "name": "Xingzhe Wu"
                    },
                    {
                        "name": "Xinran Xu"
                    },
                    {
                        "name": "Xinxing Zu"
                    },
                    {
                        "name": "Xinyu Zhou"
                    },
                    {
                        "name": "Xuehai Pan"
                    },
                    {
                        "name": "Y. Charles"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Yangyang Hu"
                    },
                    {
                        "name": "Yangyang Liu"
                    },
                    {
                        "name": "Yanru Chen"
                    },
                    {
                        "name": "Yejie Wang"
                    },
                    {
                        "name": "Yibo Liu"
                    },
                    {
                        "name": "Yidao Qin"
                    },
                    {
                        "name": "Yifeng Liu"
                    },
                    {
                        "name": "Ying Yang"
                    },
                    {
                        "name": "Yiping Bao"
                    },
                    {
                        "name": "Yulun Du"
                    },
                    {
                        "name": "Yuxin Wu"
                    },
                    {
                        "name": "Yuzhi Wang"
                    },
                    {
                        "name": "Zaida Zhou"
                    },
                    {
                        "name": "Zhaoji Wang"
                    },
                    {
                        "name": "Zhaowei Li"
                    },
                    {
                        "name": "Zhen Zhu"
                    },
                    {
                        "name": "Zheng Zhang"
                    },
                    {
                        "name": "Zhexu Wang"
                    },
                    {
                        "name": "Zhilin Yang"
                    },
                    {
                        "name": "Zhiqi Huang"
                    },
                    {
                        "name": "Zihao Huang"
                    },
                    {
                        "name": "Ziyao Xu"
                    },
                    {
                        "name": "Zonghan Yang"
                    }
                ],
                "author_detail": {
                    "name": "Zonghan Yang"
                },
                "author": "Zonghan Yang",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12599v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12599v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02406v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02406v2",
                "updated": "2025-01-22T02:43:21Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    2,
                    43,
                    21,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-04T23:51:43Z",
                "published_parsed": [
                    2025,
                    1,
                    4,
                    23,
                    51,
                    43,
                    5,
                    4,
                    0
                ],
                "title": "Zero-Shot Statistical Tests for LLM-Generated Text Detection using\n  Finite Sample Concentration Inequalities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Shot Statistical Tests for LLM-Generated Text Detection using\n  Finite Sample Concentration Inequalities"
                },
                "summary": "Verifying the provenance of content is crucial to the function of many\norganizations, e.g., educational institutions, social media platforms, firms,\netc. This problem is becoming increasingly difficult as text generated by Large\nLanguage Models (LLMs) becomes almost indistinguishable from human-generated\ncontent. In addition, many institutions utilize in-house LLMs and want to\nensure that external, non-sanctioned LLMs do not produce content within the\ninstitution. In this paper, we answer the following question: Given a piece of\ntext, can we identify whether it was produced by LLM $A$ or $B$ (where $B$ can\nbe a human)? We model LLM-generated text as a sequential stochastic process\nwith complete dependence on history and design zero-shot statistical tests to\ndistinguish between (i) the text generated by two different sets of LLMs $A$\n(in-house) and $B$ (non-sanctioned) and also (ii) LLM-generated and\nhuman-generated texts. We prove that the type I and type II errors for our\ntests decrease exponentially in the text length. In designing our tests, we\nderive concentration inequalities on the difference between log-perplexity and\nthe average entropy of the string under $A$. Specifically, for a given string,\nwe demonstrate that if the string is generated by $A$, the log-perplexity of\nthe string under $A$ converges to the average entropy of the string under $A$,\nexcept with an exponentially small probability in string length. We also show\nthat if $B$ generates the text, except with an exponentially small probability\nin string length, the log-perplexity of the string under $A$ converges to the\naverage cross-entropy of $B$ and $A$. Lastly, we present preliminary\nexperimental results to support our theoretical results. By enabling guaranteed\n(with high probability) finding of the origin of harmful LLM-generated text\nwith arbitrary size, we can help combat misinformation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Verifying the provenance of content is crucial to the function of many\norganizations, e.g., educational institutions, social media platforms, firms,\netc. This problem is becoming increasingly difficult as text generated by Large\nLanguage Models (LLMs) becomes almost indistinguishable from human-generated\ncontent. In addition, many institutions utilize in-house LLMs and want to\nensure that external, non-sanctioned LLMs do not produce content within the\ninstitution. In this paper, we answer the following question: Given a piece of\ntext, can we identify whether it was produced by LLM $A$ or $B$ (where $B$ can\nbe a human)? We model LLM-generated text as a sequential stochastic process\nwith complete dependence on history and design zero-shot statistical tests to\ndistinguish between (i) the text generated by two different sets of LLMs $A$\n(in-house) and $B$ (non-sanctioned) and also (ii) LLM-generated and\nhuman-generated texts. We prove that the type I and type II errors for our\ntests decrease exponentially in the text length. In designing our tests, we\nderive concentration inequalities on the difference between log-perplexity and\nthe average entropy of the string under $A$. Specifically, for a given string,\nwe demonstrate that if the string is generated by $A$, the log-perplexity of\nthe string under $A$ converges to the average entropy of the string under $A$,\nexcept with an exponentially small probability in string length. We also show\nthat if $B$ generates the text, except with an exponentially small probability\nin string length, the log-perplexity of the string under $A$ converges to the\naverage cross-entropy of $B$ and $A$. Lastly, we present preliminary\nexperimental results to support our theoretical results. By enabling guaranteed\n(with high probability) finding of the origin of harmful LLM-generated text\nwith arbitrary size, we can help combat misinformation."
                },
                "authors": [
                    {
                        "name": "Tara Radvand"
                    },
                    {
                        "name": "Mojtaba Abdolmaleki"
                    },
                    {
                        "name": "Mohamed Mostagir"
                    },
                    {
                        "name": "Ambuj Tewari"
                    }
                ],
                "author_detail": {
                    "name": "Ambuj Tewari"
                },
                "author": "Ambuj Tewari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02406v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02406v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15023v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15023v2",
                "updated": "2025-01-22T02:32:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    2,
                    32,
                    41,
                    2,
                    22,
                    0
                ],
                "published": "2024-10-19T07:41:16Z",
                "published_parsed": [
                    2024,
                    10,
                    19,
                    7,
                    41,
                    16,
                    5,
                    293,
                    0
                ],
                "title": "PaperWave: Listening to Research Papers as Conversational Podcasts\n  Scripted by LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PaperWave: Listening to Research Papers as Conversational Podcasts\n  Scripted by LLM"
                },
                "summary": "Listening to audio content, such as podcasts and audiobooks, is one way for\npeople to engage with knowledge. Listening affords people more mobility than\nreading by seeing, thereby broadening their learning opportunities. This study\nexplores the potential applications of large language models (LLMs) to adapt\ntext documents to audio content and addresses the lack of listening-friendly\nmaterials for niche content, such as research papers. LLMs can generate scripts\nof audio content in various styles tailored to specific needs, such as\nfull-content duration or speech types (monologue or dialogue). To explore this\npotential, we developed PaperWave as a prototype that transforms academic paper\nPDFs into conversational podcasts. Our two-month investigation, involving 11\nparticipants (including the authors), employed an autobiographical design, a\nfield study, and a design workshop. The findings highlight the importance of\nconsidering listener interaction with their environment when designing\ndocument-to-audio systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Listening to audio content, such as podcasts and audiobooks, is one way for\npeople to engage with knowledge. Listening affords people more mobility than\nreading by seeing, thereby broadening their learning opportunities. This study\nexplores the potential applications of large language models (LLMs) to adapt\ntext documents to audio content and addresses the lack of listening-friendly\nmaterials for niche content, such as research papers. LLMs can generate scripts\nof audio content in various styles tailored to specific needs, such as\nfull-content duration or speech types (monologue or dialogue). To explore this\npotential, we developed PaperWave as a prototype that transforms academic paper\nPDFs into conversational podcasts. Our two-month investigation, involving 11\nparticipants (including the authors), employed an autobiographical design, a\nfield study, and a design workshop. The findings highlight the importance of\nconsidering listener interaction with their environment when designing\ndocument-to-audio systems."
                },
                "authors": [
                    {
                        "name": "Yuchi Yahagi"
                    },
                    {
                        "name": "Rintaro Chujo"
                    },
                    {
                        "name": "Yuga Harada"
                    },
                    {
                        "name": "Changyo Han"
                    },
                    {
                        "name": "Kohei Sugiyama"
                    },
                    {
                        "name": "Takeshi Naemura"
                    }
                ],
                "author_detail": {
                    "name": "Takeshi Naemura"
                },
                "author": "Takeshi Naemura",
                "arxiv_doi": "10.1145/3706599.3706664",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3706599.3706664",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.15023v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15023v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12573v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12573v1",
                "updated": "2025-01-22T01:41:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    1,
                    41,
                    5,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T01:41:05Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    1,
                    41,
                    5,
                    2,
                    22,
                    0
                ],
                "title": "Leveraging LLMs to Create a Haptic Devices' Recommendation System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging LLMs to Create a Haptic Devices' Recommendation System"
                },
                "summary": "Haptic technology has seen significant growth, yet a lack of awareness of\nexisting haptic device design knowledge hinders development. This paper\naddresses these limitations by leveraging advancements in Large Language Models\n(LLMs) to develop a haptic agent, focusing specifically on Grounded Force\nFeedback (GFF) devices recommendation. Our approach involves automating the\ncreation of a structured haptic device database using information from research\npapers and product specifications. This database enables the recommendation of\nrelevant GFF devices based on user queries. To ensure precise and contextually\nrelevant recommendations, the system employs a dynamic retrieval method that\ncombines both conditional and semantic searches. Benchmarking against the\nestablished UEQ and existing haptic device searching tools, the proposed haptic\nrecommendation agent ranks in the top 10\\% across all UEQ categories with mean\ndifferences favoring the agent in nearly all subscales, and maintains no\nsignificant performance bias across different user groups, showcasing superior\nusability and user satisfaction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Haptic technology has seen significant growth, yet a lack of awareness of\nexisting haptic device design knowledge hinders development. This paper\naddresses these limitations by leveraging advancements in Large Language Models\n(LLMs) to develop a haptic agent, focusing specifically on Grounded Force\nFeedback (GFF) devices recommendation. Our approach involves automating the\ncreation of a structured haptic device database using information from research\npapers and product specifications. This database enables the recommendation of\nrelevant GFF devices based on user queries. To ensure precise and contextually\nrelevant recommendations, the system employs a dynamic retrieval method that\ncombines both conditional and semantic searches. Benchmarking against the\nestablished UEQ and existing haptic device searching tools, the proposed haptic\nrecommendation agent ranks in the top 10\\% across all UEQ categories with mean\ndifferences favoring the agent in nearly all subscales, and maintains no\nsignificant performance bias across different user groups, showcasing superior\nusability and user satisfaction."
                },
                "authors": [
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Haiwei Dong"
                    },
                    {
                        "name": "Abdulmotaleb El Saddik"
                    }
                ],
                "author_detail": {
                    "name": "Abdulmotaleb El Saddik"
                },
                "author": "Abdulmotaleb El Saddik",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12573v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12573v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09879v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09879v2",
                "updated": "2025-01-22T01:38:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    1,
                    38,
                    36,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-16T23:31:49Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    23,
                    31,
                    49,
                    3,
                    16,
                    0
                ],
                "title": "Testing Refactoring Engine via Historical Bug Report driven LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Testing Refactoring Engine via Historical Bug Report driven LLM"
                },
                "summary": "Refactoring is the process of restructuring existing code without changing\nits external behavior while improving its internal structure. Refactoring\nengines are integral components of modern Integrated Development Environments\n(IDEs) and can automate or semi-automate this process to enhance code\nreadability, reduce complexity, and improve the maintainability of software\nproducts. Similar to traditional software systems such as compilers,\nrefactoring engines may also contain bugs that can lead to unexpected\nbehaviors. In this paper, we propose a novel approach called RETESTER, a\nLLM-based framework for automated refactoring engine testing. Specifically, by\nusing input program structure templates extracted from historical bug reports\nand input program characteristics that are error-prone, we design\nchain-of-thought (CoT) prompts to perform refactoring-preserving\ntransformations. The generated variants are then tested on the latest version\nof refactoring engines using differential testing. We evaluate RETESTER on two\nmost popular modern refactoring engines (i.e., ECLIPSE, and INTELLIJ IDEA). It\nsuccessfully revealed 18 new bugs in the latest version of those refactoring\nengines. By the time we submit our paper, seven of them were confirmed by their\ndevelopers, and three were fixed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Refactoring is the process of restructuring existing code without changing\nits external behavior while improving its internal structure. Refactoring\nengines are integral components of modern Integrated Development Environments\n(IDEs) and can automate or semi-automate this process to enhance code\nreadability, reduce complexity, and improve the maintainability of software\nproducts. Similar to traditional software systems such as compilers,\nrefactoring engines may also contain bugs that can lead to unexpected\nbehaviors. In this paper, we propose a novel approach called RETESTER, a\nLLM-based framework for automated refactoring engine testing. Specifically, by\nusing input program structure templates extracted from historical bug reports\nand input program characteristics that are error-prone, we design\nchain-of-thought (CoT) prompts to perform refactoring-preserving\ntransformations. The generated variants are then tested on the latest version\nof refactoring engines using differential testing. We evaluate RETESTER on two\nmost popular modern refactoring engines (i.e., ECLIPSE, and INTELLIJ IDEA). It\nsuccessfully revealed 18 new bugs in the latest version of those refactoring\nengines. By the time we submit our paper, seven of them were confirmed by their\ndevelopers, and three were fixed."
                },
                "authors": [
                    {
                        "name": "Haibo Wang"
                    },
                    {
                        "name": "Zhuolin Xu"
                    },
                    {
                        "name": "Shin Hwei Tan"
                    }
                ],
                "author_detail": {
                    "name": "Shin Hwei Tan"
                },
                "author": "Shin Hwei Tan",
                "arxiv_comment": "Accepted at the 2nd ACM international conference on AI Foundation\n  Models and Software Engineering (FORGE 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09879v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09879v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2501.13106v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13106v1",
                "updated": "2025-01-22T18:59:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    18,
                    59,
                    46,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T18:59:46Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    18,
                    59,
                    46,
                    2,
                    22,
                    0
                ],
                "title": "VideoLLaMA 3: Frontier Multimodal Foundation Models for Image and Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VideoLLaMA 3: Frontier Multimodal Foundation Models for Image and Video\n  Understanding"
                },
                "summary": "In this paper, we propose VideoLLaMA3, a more advanced multimodal foundation\nmodel for image and video understanding. The core design philosophy of\nVideoLLaMA3 is vision-centric. The meaning of \"vision-centric\" is two-fold: the\nvision-centric training paradigm and vision-centric framework design. The key\ninsight of our vision-centric training paradigm is that high-quality image-text\ndata is crucial for both image and video understanding. Instead of preparing\nmassive video-text datasets, we focus on constructing large-scale and\nhigh-quality image-text datasets. VideoLLaMA3 has four training stages: 1)\nvision-centric alignment stage, which warms up the vision encoder and\nprojector; 2) vision-language pretraining stage, which jointly tunes the vision\nencoder, projector, and LLM with large-scale image-text data covering multiple\ntypes (including scene images, documents, charts) as well as text-only data. 3)\nmulti-task fine-tuning stage, which incorporates image-text SFT data for\ndownstream tasks and video-text data to establish a foundation for video\nunderstanding. 4) video-centric fine-tuning, which further improves the model's\ncapability in video understanding. As for the framework design, to better\ncapture fine-grained details in images, the pretrained vision encoder is\nadapted to encode images of varying sizes into vision tokens with corresponding\nnumbers, rather than a fixed number of tokens. For video inputs, we reduce the\nnumber of vision tokens according to their similarity so that the\nrepresentation of videos will be more precise and compact. Benefit from\nvision-centric designs, VideoLLaMA3 achieves compelling performances in both\nimage and video understanding benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose VideoLLaMA3, a more advanced multimodal foundation\nmodel for image and video understanding. The core design philosophy of\nVideoLLaMA3 is vision-centric. The meaning of \"vision-centric\" is two-fold: the\nvision-centric training paradigm and vision-centric framework design. The key\ninsight of our vision-centric training paradigm is that high-quality image-text\ndata is crucial for both image and video understanding. Instead of preparing\nmassive video-text datasets, we focus on constructing large-scale and\nhigh-quality image-text datasets. VideoLLaMA3 has four training stages: 1)\nvision-centric alignment stage, which warms up the vision encoder and\nprojector; 2) vision-language pretraining stage, which jointly tunes the vision\nencoder, projector, and LLM with large-scale image-text data covering multiple\ntypes (including scene images, documents, charts) as well as text-only data. 3)\nmulti-task fine-tuning stage, which incorporates image-text SFT data for\ndownstream tasks and video-text data to establish a foundation for video\nunderstanding. 4) video-centric fine-tuning, which further improves the model's\ncapability in video understanding. As for the framework design, to better\ncapture fine-grained details in images, the pretrained vision encoder is\nadapted to encode images of varying sizes into vision tokens with corresponding\nnumbers, rather than a fixed number of tokens. For video inputs, we reduce the\nnumber of vision tokens according to their similarity so that the\nrepresentation of videos will be more precise and compact. Benefit from\nvision-centric designs, VideoLLaMA3 achieves compelling performances in both\nimage and video understanding benchmarks."
                },
                "authors": [
                    {
                        "name": "Boqiang Zhang"
                    },
                    {
                        "name": "Kehan Li"
                    },
                    {
                        "name": "Zesen Cheng"
                    },
                    {
                        "name": "Zhiqiang Hu"
                    },
                    {
                        "name": "Yuqian Yuan"
                    },
                    {
                        "name": "Guanzheng Chen"
                    },
                    {
                        "name": "Sicong Leng"
                    },
                    {
                        "name": "Yuming Jiang"
                    },
                    {
                        "name": "Hang Zhang"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Peng Jin"
                    },
                    {
                        "name": "Wenqi Zhang"
                    },
                    {
                        "name": "Fan Wang"
                    },
                    {
                        "name": "Lidong Bing"
                    },
                    {
                        "name": "Deli Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Deli Zhao"
                },
                "author": "Deli Zhao",
                "arxiv_comment": "BZ, KL, ZC, ZH, YY, GC, SL, YJ, HZ, and XL contributed equally to\n  this project. Code: https://github.com/DAMO-NLP-SG/VideoLLaMA3",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13106v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13106v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13080v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13080v1",
                "updated": "2025-01-22T18:40:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    18,
                    40,
                    57,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T18:40:57Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    18,
                    40,
                    57,
                    2,
                    22,
                    0
                ],
                "title": "Refining Input Guardrails: Enhancing LLM-as-a-Judge Efficiency Through\n  Chain-of-Thought Fine-Tuning and Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Refining Input Guardrails: Enhancing LLM-as-a-Judge Efficiency Through\n  Chain-of-Thought Fine-Tuning and Alignment"
                },
                "summary": "Large Language Models (LLMs) have demonstrated powerful capabilities that\nrender them valuable in different applications, including conversational AI\nproducts. It is paramount to ensure the security and reliability of these\nproducts by mitigating their vulnerabilities towards malicious user\ninteractions, which can lead to the exposure of great risks and reputational\nrepercussions. In this work, we present a comprehensive study on the efficacy\nof fine-tuning and aligning Chain-of-Thought (CoT) responses of different LLMs\nthat serve as input moderation guardrails. We systematically explore various\ntuning methods by leveraging a small set of training data to adapt these models\nas proxy defense mechanisms to detect malicious inputs and provide a reasoning\nfor their verdicts, thereby preventing the exploitation of conversational\nagents. We rigorously evaluate the efficacy and robustness of different tuning\nstrategies to generalize across diverse adversarial and malicious query types.\nOur experimental results outline the potential of alignment processes tailored\nto a varied range of harmful input queries, even with constrained data\nresources. These techniques significantly enhance the safety of conversational\nAI systems and provide a feasible framework for deploying more secure and\ntrustworthy AI-driven interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated powerful capabilities that\nrender them valuable in different applications, including conversational AI\nproducts. It is paramount to ensure the security and reliability of these\nproducts by mitigating their vulnerabilities towards malicious user\ninteractions, which can lead to the exposure of great risks and reputational\nrepercussions. In this work, we present a comprehensive study on the efficacy\nof fine-tuning and aligning Chain-of-Thought (CoT) responses of different LLMs\nthat serve as input moderation guardrails. We systematically explore various\ntuning methods by leveraging a small set of training data to adapt these models\nas proxy defense mechanisms to detect malicious inputs and provide a reasoning\nfor their verdicts, thereby preventing the exploitation of conversational\nagents. We rigorously evaluate the efficacy and robustness of different tuning\nstrategies to generalize across diverse adversarial and malicious query types.\nOur experimental results outline the potential of alignment processes tailored\nto a varied range of harmful input queries, even with constrained data\nresources. These techniques significantly enhance the safety of conversational\nAI systems and provide a feasible framework for deploying more secure and\ntrustworthy AI-driven interactions."
                },
                "authors": [
                    {
                        "name": "Melissa Kazemi Rad"
                    },
                    {
                        "name": "Huy Nghiem"
                    },
                    {
                        "name": "Andy Luo"
                    },
                    {
                        "name": "Sahil Wadhwa"
                    },
                    {
                        "name": "Mohammad Sorower"
                    },
                    {
                        "name": "Stephen Rawls"
                    }
                ],
                "author_detail": {
                    "name": "Stephen Rawls"
                },
                "author": "Stephen Rawls",
                "arxiv_comment": "16 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13080v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13080v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13052v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13052v1",
                "updated": "2025-01-22T18:01:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    18,
                    1,
                    24,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T18:01:24Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    18,
                    1,
                    24,
                    2,
                    22,
                    0
                ],
                "title": "One-Class Domain Adaptation via Meta-Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One-Class Domain Adaptation via Meta-Learning"
                },
                "summary": "The deployment of IoT (Internet of Things) sensor-based machine learning\nmodels in industrial systems for anomaly classification tasks poses significant\nchallenges due to distribution shifts, as the training data acquired in\ncontrolled laboratory settings may significantly differ from real-time data in\nproduction environments. Furthermore, many real-world applications cannot\nprovide a substantial number of labeled examples for each anomalous class in\nevery new environment. It is therefore crucial to develop adaptable machine\nlearning models that can be effectively transferred from one environment to\nanother, enabling rapid adaptation using normal operational data. We extended\nthis problem setting to an arbitrary classification task and formulated the\none-class domain adaptation (OC-DA) problem setting. We took a meta-learning\napproach to tackle the challenge of OC-DA, and proposed a task sampling\nstrategy to adapt any bi-level meta-learning algorithm to OC-DA. We modified\nthe well-established model-agnostic meta-learning (MAML) algorithm and\nintroduced the OC-DA MAML algorithm. We provided a theoretical analysis showing\nthat OC-DA MAML optimizes for meta-parameters that enable rapid one-class\nadaptation across domains. The OC-DA MAML algorithm is evaluated on the\nRainbow-MNIST meta-learning benchmark and on a real-world dataset of\nvibration-based sensor readings. The results show that OC-DA MAML significantly\nimproves the performance on the target domains and outperforms MAML using the\nstandard task sampling strategy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of IoT (Internet of Things) sensor-based machine learning\nmodels in industrial systems for anomaly classification tasks poses significant\nchallenges due to distribution shifts, as the training data acquired in\ncontrolled laboratory settings may significantly differ from real-time data in\nproduction environments. Furthermore, many real-world applications cannot\nprovide a substantial number of labeled examples for each anomalous class in\nevery new environment. It is therefore crucial to develop adaptable machine\nlearning models that can be effectively transferred from one environment to\nanother, enabling rapid adaptation using normal operational data. We extended\nthis problem setting to an arbitrary classification task and formulated the\none-class domain adaptation (OC-DA) problem setting. We took a meta-learning\napproach to tackle the challenge of OC-DA, and proposed a task sampling\nstrategy to adapt any bi-level meta-learning algorithm to OC-DA. We modified\nthe well-established model-agnostic meta-learning (MAML) algorithm and\nintroduced the OC-DA MAML algorithm. We provided a theoretical analysis showing\nthat OC-DA MAML optimizes for meta-parameters that enable rapid one-class\nadaptation across domains. The OC-DA MAML algorithm is evaluated on the\nRainbow-MNIST meta-learning benchmark and on a real-world dataset of\nvibration-based sensor readings. The results show that OC-DA MAML significantly\nimproves the performance on the target domains and outperforms MAML using the\nstandard task sampling strategy."
                },
                "authors": [
                    {
                        "name": "Stephanie Holly"
                    },
                    {
                        "name": "Thomas Bierweiler"
                    },
                    {
                        "name": "Stefan von Dosky"
                    },
                    {
                        "name": "Ahmed Frikha"
                    },
                    {
                        "name": "Clemens Heitzinger"
                    },
                    {
                        "name": "Jana Eder"
                    }
                ],
                "author_detail": {
                    "name": "Jana Eder"
                },
                "author": "Jana Eder",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13052v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13052v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11223v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11223v2",
                "updated": "2025-01-22T17:49:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    17,
                    49,
                    37,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-20T02:16:19Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    2,
                    16,
                    19,
                    0,
                    20,
                    0
                ],
                "title": "Reasoning Language Models: A Blueprint",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Language Models: A Blueprint"
                },
                "summary": "Reasoning language models (RLMs), also known as Large Reasoning Models\n(LRMs), such as OpenAI's o1 and o3, DeepSeek-V3, and Alibaba's QwQ, have\nredefined AI's problem-solving capabilities by extending LLMs with advanced\nreasoning mechanisms. Yet, their high costs, proprietary nature, and complex\narchitectures - uniquely combining Reinforcement Learning (RL), search\nheuristics, and LLMs - present accessibility and scalability challenges. To\naddress these, we propose a comprehensive blueprint that organizes RLM\ncomponents into a modular framework, based on a survey and analysis of all RLM\nworks. This blueprint incorporates diverse reasoning structures (chains, trees,\ngraphs, and nested forms), reasoning strategies (e.g., Monte Carlo Tree Search,\nBeam Search), RL concepts (policy, value models and others), supervision\nschemes (Outcome-Based and Process-Based Supervision), and other related\nconcepts (e.g., Test-Time Compute, Retrieval-Augmented Generation, agent\ntools). We provide detailed mathematical formulations and algorithmic\nspecifications to simplify RLM implementation. By showing how schemes like\nLLaMA-Berry, QwQ, Journey Learning, and Graph of Thoughts fit as special cases,\nwe demonstrate the blueprint's versatility and unifying potential. To\nillustrate its utility, we introduce x1, a modular implementation for rapid RLM\nprototyping and experimentation. Using x1 and a literature review, we provide\nkey insights, such as multi-phase training for policy and value models, and the\nimportance of familiar training distributions. Finally, we discuss scalable RLM\ncloud deployments and we outline how RLMs can integrate with a broader LLM\necosystem. Our work demystifies RLM construction, democratizes advanced\nreasoning capabilities, and fosters innovation, aiming to mitigate the gap\nbetween \"rich AI\" and \"poor AI\" by lowering barriers to RLM development and\nexperimentation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning language models (RLMs), also known as Large Reasoning Models\n(LRMs), such as OpenAI's o1 and o3, DeepSeek-V3, and Alibaba's QwQ, have\nredefined AI's problem-solving capabilities by extending LLMs with advanced\nreasoning mechanisms. Yet, their high costs, proprietary nature, and complex\narchitectures - uniquely combining Reinforcement Learning (RL), search\nheuristics, and LLMs - present accessibility and scalability challenges. To\naddress these, we propose a comprehensive blueprint that organizes RLM\ncomponents into a modular framework, based on a survey and analysis of all RLM\nworks. This blueprint incorporates diverse reasoning structures (chains, trees,\ngraphs, and nested forms), reasoning strategies (e.g., Monte Carlo Tree Search,\nBeam Search), RL concepts (policy, value models and others), supervision\nschemes (Outcome-Based and Process-Based Supervision), and other related\nconcepts (e.g., Test-Time Compute, Retrieval-Augmented Generation, agent\ntools). We provide detailed mathematical formulations and algorithmic\nspecifications to simplify RLM implementation. By showing how schemes like\nLLaMA-Berry, QwQ, Journey Learning, and Graph of Thoughts fit as special cases,\nwe demonstrate the blueprint's versatility and unifying potential. To\nillustrate its utility, we introduce x1, a modular implementation for rapid RLM\nprototyping and experimentation. Using x1 and a literature review, we provide\nkey insights, such as multi-phase training for policy and value models, and the\nimportance of familiar training distributions. Finally, we discuss scalable RLM\ncloud deployments and we outline how RLMs can integrate with a broader LLM\necosystem. Our work demystifies RLM construction, democratizes advanced\nreasoning capabilities, and fosters innovation, aiming to mitigate the gap\nbetween \"rich AI\" and \"poor AI\" by lowering barriers to RLM development and\nexperimentation."
                },
                "authors": [
                    {
                        "name": "Maciej Besta"
                    },
                    {
                        "name": "Julia Barth"
                    },
                    {
                        "name": "Eric Schreiber"
                    },
                    {
                        "name": "Ales Kubicek"
                    },
                    {
                        "name": "Afonso Catarino"
                    },
                    {
                        "name": "Robert Gerstenberger"
                    },
                    {
                        "name": "Piotr Nyczyk"
                    },
                    {
                        "name": "Patrick Iff"
                    },
                    {
                        "name": "Yueling Li"
                    },
                    {
                        "name": "Sam Houliston"
                    },
                    {
                        "name": "Tomasz Sternal"
                    },
                    {
                        "name": "Marcin Copik"
                    },
                    {
                        "name": "Grzegorz Kwaśniewski"
                    },
                    {
                        "name": "Jürgen Müller"
                    },
                    {
                        "name": "Łukasz Flis"
                    },
                    {
                        "name": "Hannes Eberhard"
                    },
                    {
                        "name": "Hubert Niewiadomski"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11223v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11223v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13042v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13042v1",
                "updated": "2025-01-22T17:44:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    17,
                    44,
                    1,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T17:44:01Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    17,
                    44,
                    1,
                    2,
                    22,
                    0
                ],
                "title": "Does Table Source Matter? Benchmarking and Improving Multimodal\n  Scientific Table Understanding and Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does Table Source Matter? Benchmarking and Improving Multimodal\n  Scientific Table Understanding and Reasoning"
                },
                "summary": "Recent large language models (LLMs) have advanced table understanding\ncapabilities but rely on converting tables into text sequences. While\nmultimodal large language models (MLLMs) enable direct visual processing, they\nface limitations in handling scientific tables due to fixed input image\nresolutions and insufficient numerical reasoning capabilities. We present a\ncomprehensive framework for multimodal scientific table understanding and\nreasoning with dynamic input image resolutions. Our framework consists of three\nkey components: (1) MMSci-Pre, a domain-specific table structure learning\ndataset of 52K scientific table structure recognition samples, (2) MMSci-Ins,\nan instruction tuning dataset with 12K samples across three table-based tasks,\nand (3) MMSci-Eval, a benchmark with 3,114 testing samples specifically\ndesigned to evaluate numerical reasoning capabilities. Extensive experiments\ndemonstrate that our domain-specific approach with 52K scientific table images\nachieves superior performance compared to 150K general-domain tables,\nhighlighting the importance of data quality over quantity. Our proposed\ntable-based MLLMs with dynamic input resolutions show significant improvements\nin both general table understanding and numerical reasoning capabilities, with\nstrong generalisation to held-out datasets. Our code and data are publicly\navailable at https://github.com/Bernard-Yang/MMSci_Table.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language models (LLMs) have advanced table understanding\ncapabilities but rely on converting tables into text sequences. While\nmultimodal large language models (MLLMs) enable direct visual processing, they\nface limitations in handling scientific tables due to fixed input image\nresolutions and insufficient numerical reasoning capabilities. We present a\ncomprehensive framework for multimodal scientific table understanding and\nreasoning with dynamic input image resolutions. Our framework consists of three\nkey components: (1) MMSci-Pre, a domain-specific table structure learning\ndataset of 52K scientific table structure recognition samples, (2) MMSci-Ins,\nan instruction tuning dataset with 12K samples across three table-based tasks,\nand (3) MMSci-Eval, a benchmark with 3,114 testing samples specifically\ndesigned to evaluate numerical reasoning capabilities. Extensive experiments\ndemonstrate that our domain-specific approach with 52K scientific table images\nachieves superior performance compared to 150K general-domain tables,\nhighlighting the importance of data quality over quantity. Our proposed\ntable-based MLLMs with dynamic input resolutions show significant improvements\nin both general table understanding and numerical reasoning capabilities, with\nstrong generalisation to held-out datasets. Our code and data are publicly\navailable at https://github.com/Bernard-Yang/MMSci_Table."
                },
                "authors": [
                    {
                        "name": "Bohao Yang"
                    },
                    {
                        "name": "Yingji Zhang"
                    },
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "André Freitas"
                    },
                    {
                        "name": "Chenghua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Chenghua Lin"
                },
                "author": "Chenghua Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13042v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13042v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21358v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21358v3",
                "updated": "2025-01-22T17:21:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    17,
                    21,
                    5,
                    2,
                    22,
                    0
                ],
                "published": "2024-10-28T17:35:59Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    35,
                    59,
                    0,
                    302,
                    0
                ],
                "title": "\"We do use it, but not how hearing people think\": How the Deaf and Hard\n  of Hearing Community Uses Large Language Model Tools",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"We do use it, but not how hearing people think\": How the Deaf and Hard\n  of Hearing Community Uses Large Language Model Tools"
                },
                "summary": "Generative AI tools, particularly those utilizing large language models\n(LLMs), are increasingly used in everyday contexts. While these tools enhance\nproductivity and accessibility, little is known about how Deaf and Hard of\nHearing (DHH) individuals engage with them or the challenges they face when\nusing them. This paper presents a mixed-method study exploring how the DHH\ncommunity uses Text AI tools like ChatGPT to reduce communication barriers and\nenhance information access. We surveyed 80 DHH participants and conducted\ninterviews with 11 participants. Our findings reveal important benefits, such\nas eased communication and bridging Deaf and hearing cultures, alongside\nchallenges like lack of American Sign Language (ASL) support and Deaf cultural\nunderstanding. We highlight unique usage patterns, propose inclusive design\nrecommendations, and outline future research directions to improve Text AI\naccessibility for the DHH community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI tools, particularly those utilizing large language models\n(LLMs), are increasingly used in everyday contexts. While these tools enhance\nproductivity and accessibility, little is known about how Deaf and Hard of\nHearing (DHH) individuals engage with them or the challenges they face when\nusing them. This paper presents a mixed-method study exploring how the DHH\ncommunity uses Text AI tools like ChatGPT to reduce communication barriers and\nenhance information access. We surveyed 80 DHH participants and conducted\ninterviews with 11 participants. Our findings reveal important benefits, such\nas eased communication and bridging Deaf and hearing cultures, alongside\nchallenges like lack of American Sign Language (ASL) support and Deaf cultural\nunderstanding. We highlight unique usage patterns, propose inclusive design\nrecommendations, and outline future research directions to improve Text AI\naccessibility for the DHH community."
                },
                "authors": [
                    {
                        "name": "Shuxu Huffman"
                    },
                    {
                        "name": "Si Chen"
                    },
                    {
                        "name": "Kelly Avery Mack"
                    },
                    {
                        "name": "Haotian Su"
                    },
                    {
                        "name": "Qi Wang"
                    },
                    {
                        "name": "Raja Kushalnagar"
                    }
                ],
                "author_detail": {
                    "name": "Raja Kushalnagar"
                },
                "author": "Raja Kushalnagar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21358v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21358v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16086v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16086v2",
                "updated": "2025-01-22T17:18:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    17,
                    18,
                    15,
                    2,
                    22,
                    0
                ],
                "published": "2024-12-20T17:33:50Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    17,
                    33,
                    50,
                    4,
                    355,
                    0
                ],
                "title": "Towards Interpretable Radiology Report Generation via Concept\n  Bottlenecks using a Multi-Agentic RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Interpretable Radiology Report Generation via Concept\n  Bottlenecks using a Multi-Agentic RAG"
                },
                "summary": "Deep learning has advanced medical image classification, but interpretability\nchallenges hinder its clinical adoption. This study enhances interpretability\nin Chest X-ray (CXR) classification by using concept bottleneck models (CBMs)\nand a multi-agent Retrieval-Augmented Generation (RAG) system for report\ngeneration. By modeling relationships between visual features and clinical\nconcepts, we create interpretable concept vectors that guide a multi-agent RAG\nsystem to generate radiology reports, enhancing clinical relevance,\nexplainability, and transparency. Evaluation of the generated reports using an\nLLM-as-a-judge confirmed the interpretability and clinical utility of our\nmodel's outputs. On the COVID-QU dataset, our model achieved 81% classification\naccuracy and demonstrated robust report generation performance, with five key\nmetrics ranging between 84% and 90%. This interpretable multi-agent framework\nbridges the gap between high-performance AI and the explainability required for\nreliable AI-driven CXR analysis in clinical settings. Our code is available at\nhttps://github.com/tifat58/IRR-with-CBM-RAG.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning has advanced medical image classification, but interpretability\nchallenges hinder its clinical adoption. This study enhances interpretability\nin Chest X-ray (CXR) classification by using concept bottleneck models (CBMs)\nand a multi-agent Retrieval-Augmented Generation (RAG) system for report\ngeneration. By modeling relationships between visual features and clinical\nconcepts, we create interpretable concept vectors that guide a multi-agent RAG\nsystem to generate radiology reports, enhancing clinical relevance,\nexplainability, and transparency. Evaluation of the generated reports using an\nLLM-as-a-judge confirmed the interpretability and clinical utility of our\nmodel's outputs. On the COVID-QU dataset, our model achieved 81% classification\naccuracy and demonstrated robust report generation performance, with five key\nmetrics ranging between 84% and 90%. This interpretable multi-agent framework\nbridges the gap between high-performance AI and the explainability required for\nreliable AI-driven CXR analysis in clinical settings. Our code is available at\nhttps://github.com/tifat58/IRR-with-CBM-RAG.git."
                },
                "authors": [
                    {
                        "name": "Hasan Md Tusfiqur Alam"
                    },
                    {
                        "name": "Devansh Srivastav"
                    },
                    {
                        "name": "Md Abdul Kadir"
                    },
                    {
                        "name": "Daniel Sonntag"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Sonntag"
                },
                "author": "Daniel Sonntag",
                "arxiv_comment": "Accepted in the 47th European Conference for Information Retrieval\n  (ECIR) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16086v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16086v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13018v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13018v1",
                "updated": "2025-01-22T17:05:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    17,
                    5,
                    38,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T17:05:38Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    17,
                    5,
                    38,
                    2,
                    22,
                    0
                ],
                "title": "Multi-Objective Hyperparameter Selection via Hypothesis Testing on\n  Reliability Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Objective Hyperparameter Selection via Hypothesis Testing on\n  Reliability Graphs"
                },
                "summary": "In sensitive application domains, multi-objective hyperparameter selection\ncan ensure the reliability of AI models prior to deployment, while optimizing\nauxiliary performance metrics. The state-of-the-art Pareto Testing (PT) method\nguarantees statistical reliability constraints by adopting a multiple\nhypothesis testing framework. In PT, hyperparameters are validated one at a\ntime, following a data-driven order determined by expected reliability levels.\nThis paper introduces a novel framework for multi-objective hyperparameter\nselection that captures the interdependencies among the reliability levels of\ndifferent hyperparameter configurations using a directed acyclic graph (DAG),\nwhich is termed the reliability graph (RG). The RG is constructed based on\nprior information and data by using the Bradley-Terry model. The proposed\napproach, RG-based PT (RG-PT), leverages the RG to enable the efficient,\nparallel testing of multiple hyperparameters at the same reliability level. By\nintegrating False Discovery Rate (FDR) control, RG-PT ensures robust\nstatistical reliability guarantees and is shown via experiments across diverse\ndomains to consistently yield superior solutions for multi-objective\ncalibration problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In sensitive application domains, multi-objective hyperparameter selection\ncan ensure the reliability of AI models prior to deployment, while optimizing\nauxiliary performance metrics. The state-of-the-art Pareto Testing (PT) method\nguarantees statistical reliability constraints by adopting a multiple\nhypothesis testing framework. In PT, hyperparameters are validated one at a\ntime, following a data-driven order determined by expected reliability levels.\nThis paper introduces a novel framework for multi-objective hyperparameter\nselection that captures the interdependencies among the reliability levels of\ndifferent hyperparameter configurations using a directed acyclic graph (DAG),\nwhich is termed the reliability graph (RG). The RG is constructed based on\nprior information and data by using the Bradley-Terry model. The proposed\napproach, RG-based PT (RG-PT), leverages the RG to enable the efficient,\nparallel testing of multiple hyperparameters at the same reliability level. By\nintegrating False Discovery Rate (FDR) control, RG-PT ensures robust\nstatistical reliability guarantees and is shown via experiments across diverse\ndomains to consistently yield superior solutions for multi-objective\ncalibration problems."
                },
                "authors": [
                    {
                        "name": "Amirmohammad Farzaneh"
                    },
                    {
                        "name": "Osvaldo Simeone"
                    }
                ],
                "author_detail": {
                    "name": "Osvaldo Simeone"
                },
                "author": "Osvaldo Simeone",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13018v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13018v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13011v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13011v1",
                "updated": "2025-01-22T16:53:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    16,
                    53,
                    8,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T16:53:08Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    16,
                    53,
                    8,
                    2,
                    22,
                    0
                ],
                "title": "MONA: Myopic Optimization with Non-myopic Approval Can Mitigate\n  Multi-step Reward Hacking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MONA: Myopic Optimization with Non-myopic Approval Can Mitigate\n  Multi-step Reward Hacking"
                },
                "summary": "Future advanced AI systems may learn sophisticated strategies through\nreinforcement learning (RL) that humans cannot understand well enough to safely\nevaluate. We propose a training method which avoids agents learning undesired\nmulti-step plans that receive high reward (multi-step \"reward hacks\") even if\nhumans are not able to detect that the behaviour is undesired. The method,\nMyopic Optimization with Non-myopic Approval (MONA), works by combining\nshort-sighted optimization with far-sighted reward. We demonstrate that MONA\ncan prevent multi-step reward hacking that ordinary RL causes, even without\nbeing able to detect the reward hacking and without any extra information that\nordinary RL does not get access to. We study MONA empirically in three settings\nwhich model different misalignment failure modes including 2-step environments\nwith LLMs representing delegated oversight and encoded reasoning and\nlonger-horizon gridworld environments representing sensor tampering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Future advanced AI systems may learn sophisticated strategies through\nreinforcement learning (RL) that humans cannot understand well enough to safely\nevaluate. We propose a training method which avoids agents learning undesired\nmulti-step plans that receive high reward (multi-step \"reward hacks\") even if\nhumans are not able to detect that the behaviour is undesired. The method,\nMyopic Optimization with Non-myopic Approval (MONA), works by combining\nshort-sighted optimization with far-sighted reward. We demonstrate that MONA\ncan prevent multi-step reward hacking that ordinary RL causes, even without\nbeing able to detect the reward hacking and without any extra information that\nordinary RL does not get access to. We study MONA empirically in three settings\nwhich model different misalignment failure modes including 2-step environments\nwith LLMs representing delegated oversight and encoded reasoning and\nlonger-horizon gridworld environments representing sensor tampering."
                },
                "authors": [
                    {
                        "name": "Sebastian Farquhar"
                    },
                    {
                        "name": "Vikrant Varma"
                    },
                    {
                        "name": "David Lindner"
                    },
                    {
                        "name": "David Elson"
                    },
                    {
                        "name": "Caleb Biddulph"
                    },
                    {
                        "name": "Ian Goodfellow"
                    },
                    {
                        "name": "Rohin Shah"
                    }
                ],
                "author_detail": {
                    "name": "Rohin Shah"
                },
                "author": "Rohin Shah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13011v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13011v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13007v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13007v1",
                "updated": "2025-01-22T16:49:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    16,
                    49,
                    37,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T16:49:37Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    16,
                    49,
                    37,
                    2,
                    22,
                    0
                ],
                "title": "Pairwise RM: Perform Best-of-N Sampling with Knockout Tournament",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pairwise RM: Perform Best-of-N Sampling with Knockout Tournament"
                },
                "summary": "Best-of-N (BoN) sampling, a common strategy for test-time scaling of Large\nLanguage Models (LLMs), relies on reward models to select the best candidate\nsolution from multiple generations. However, traditional reward models often\nassign arbitrary and inconsistent scores, limiting their effectiveness. To\naddress this, we propose a Pairwise Reward Model (Pairwise RM) combined with a\nknockout tournament for BoN sampling. Instead of assigning absolute scores,\ngiven one math problem, Pairwise RM evaluates two candidate solutions'\ncorrectness simultaneously. This approach eliminates the need for arbitrary\nscoring and enables cross-validation of solutions through parallel comparison.\nIn the knockout tournament, Pairwise RM conducts pairwise comparisons between\ncandidate solutions and eliminates the incorrect ones iteratively. We construct\n\\ourdataset, a large-scale dataset of 443K pairwise comparisons derived from\nNumiaMath and annotated using \\texttt{gemini-1.5-flash}, and train the Pairwise\nRM via supervised fine-tuning. Experiments on MATH-500 and the Olympiad Bench\ndemonstrate significant improvements over traditional discriminative reward\nmodels. And a 40\\% to 60\\% relative improvement is achieved on the top 50\\%\nchallenging problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Best-of-N (BoN) sampling, a common strategy for test-time scaling of Large\nLanguage Models (LLMs), relies on reward models to select the best candidate\nsolution from multiple generations. However, traditional reward models often\nassign arbitrary and inconsistent scores, limiting their effectiveness. To\naddress this, we propose a Pairwise Reward Model (Pairwise RM) combined with a\nknockout tournament for BoN sampling. Instead of assigning absolute scores,\ngiven one math problem, Pairwise RM evaluates two candidate solutions'\ncorrectness simultaneously. This approach eliminates the need for arbitrary\nscoring and enables cross-validation of solutions through parallel comparison.\nIn the knockout tournament, Pairwise RM conducts pairwise comparisons between\ncandidate solutions and eliminates the incorrect ones iteratively. We construct\n\\ourdataset, a large-scale dataset of 443K pairwise comparisons derived from\nNumiaMath and annotated using \\texttt{gemini-1.5-flash}, and train the Pairwise\nRM via supervised fine-tuning. Experiments on MATH-500 and the Olympiad Bench\ndemonstrate significant improvements over traditional discriminative reward\nmodels. And a 40\\% to 60\\% relative improvement is achieved on the top 50\\%\nchallenging problems."
                },
                "authors": [
                    {
                        "name": "Yantao Liu"
                    },
                    {
                        "name": "Zijun Yao"
                    },
                    {
                        "name": "Rui Min"
                    },
                    {
                        "name": "Yixin Cao"
                    },
                    {
                        "name": "Lei Hou"
                    },
                    {
                        "name": "Juanzi Li"
                    }
                ],
                "author_detail": {
                    "name": "Juanzi Li"
                },
                "author": "Juanzi Li",
                "arxiv_comment": "in progress work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13007v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13007v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12988v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12988v1",
                "updated": "2025-01-22T16:20:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    16,
                    20,
                    47,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T16:20:47Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    16,
                    20,
                    47,
                    2,
                    22,
                    0
                ],
                "title": "Large Language Model-Based Semantic Communication System for Image\n  Transmission",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model-Based Semantic Communication System for Image\n  Transmission"
                },
                "summary": "The remarkable success of Large Language Models (LLMs) in understanding and\ngenerating various data types, such as images and text, has demonstrated their\nability to process and extract semantic information across diverse domains.\nThis transformative capability lays the foundation for semantic communications,\nenabling highly efficient and intelligent communication systems. In this work,\nwe present a novel OFDM-based semantic communication framework for image\ntransmission. We propose an innovative semantic encoder design that leverages\nthe ability of LLMs to extract the meaning of transmitted data rather than\nfocusing on its raw representation. On the receiver side, we design an\nLLM-based semantic decoder capable of comprehending context and generating the\nmost appropriate representation to fit the given context. We evaluate our\nproposed system under different scenarios, including Urban Macro-cell\nenvironments with varying speed ranges. The evaluation metrics demonstrate that\nour proposed system reduces the data size 4250 times, while achieving a higher\ndata rate compared to conventional communication methods. This approach offers\na robust and scalable solution to unlock the full potential of 6G connectivity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The remarkable success of Large Language Models (LLMs) in understanding and\ngenerating various data types, such as images and text, has demonstrated their\nability to process and extract semantic information across diverse domains.\nThis transformative capability lays the foundation for semantic communications,\nenabling highly efficient and intelligent communication systems. In this work,\nwe present a novel OFDM-based semantic communication framework for image\ntransmission. We propose an innovative semantic encoder design that leverages\nthe ability of LLMs to extract the meaning of transmitted data rather than\nfocusing on its raw representation. On the receiver side, we design an\nLLM-based semantic decoder capable of comprehending context and generating the\nmost appropriate representation to fit the given context. We evaluate our\nproposed system under different scenarios, including Urban Macro-cell\nenvironments with varying speed ranges. The evaluation metrics demonstrate that\nour proposed system reduces the data size 4250 times, while achieving a higher\ndata rate compared to conventional communication methods. This approach offers\na robust and scalable solution to unlock the full potential of 6G connectivity."
                },
                "authors": [
                    {
                        "name": "Soheyb Ribouh"
                    },
                    {
                        "name": "Osama Saleem"
                    }
                ],
                "author_detail": {
                    "name": "Osama Saleem"
                },
                "author": "Osama Saleem",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12988v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12988v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12983v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12983v1",
                "updated": "2025-01-22T16:12:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    16,
                    12,
                    38,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T16:12:38Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    16,
                    12,
                    38,
                    2,
                    22,
                    0
                ],
                "title": "LLM4WM: Adapting LLM for Wireless Multi-Tasking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM4WM: Adapting LLM for Wireless Multi-Tasking"
                },
                "summary": "The wireless channel is fundamental to communication, encompassing numerous\ntasks collectively referred to as channel-associated tasks. These tasks can\nleverage joint learning based on channel characteristics to share\nrepresentations and enhance system design. To capitalize on this advantage,\nLLM4WM is proposed--a large language model (LLM) multi-task fine-tuning\nframework specifically tailored for channel-associated tasks. This framework\nutilizes a Mixture of Experts with Low-Rank Adaptation (MoE-LoRA) approach for\nmulti-task fine-tuning, enabling the transfer of the pre-trained LLM's general\nknowledge to these tasks. Given the unique characteristics of wireless channel\ndata, preprocessing modules, adapter modules, and multi-task output layers are\ndesigned to align the channel data with the LLM's semantic feature space.\nExperiments on a channel-associated multi-task dataset demonstrate that LLM4WM\noutperforms existing methodologies in both full-sample and few-shot\nevaluations, owing to its robust multi-task joint modeling and transfer\nlearning capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The wireless channel is fundamental to communication, encompassing numerous\ntasks collectively referred to as channel-associated tasks. These tasks can\nleverage joint learning based on channel characteristics to share\nrepresentations and enhance system design. To capitalize on this advantage,\nLLM4WM is proposed--a large language model (LLM) multi-task fine-tuning\nframework specifically tailored for channel-associated tasks. This framework\nutilizes a Mixture of Experts with Low-Rank Adaptation (MoE-LoRA) approach for\nmulti-task fine-tuning, enabling the transfer of the pre-trained LLM's general\nknowledge to these tasks. Given the unique characteristics of wireless channel\ndata, preprocessing modules, adapter modules, and multi-task output layers are\ndesigned to align the channel data with the LLM's semantic feature space.\nExperiments on a channel-associated multi-task dataset demonstrate that LLM4WM\noutperforms existing methodologies in both full-sample and few-shot\nevaluations, owing to its robust multi-task joint modeling and transfer\nlearning capabilities."
                },
                "authors": [
                    {
                        "name": "Xuanyu Liu"
                    },
                    {
                        "name": "Shijian Gao"
                    },
                    {
                        "name": "Boxun Liu"
                    },
                    {
                        "name": "Xiang Cheng"
                    },
                    {
                        "name": "Liuqing Yang"
                    }
                ],
                "author_detail": {
                    "name": "Liuqing Yang"
                },
                "author": "Liuqing Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12983v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12983v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12980v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12980v1",
                "updated": "2025-01-22T16:07:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    16,
                    7,
                    24,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T16:07:24Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    16,
                    7,
                    24,
                    2,
                    22,
                    0
                ],
                "title": "Implicit Causality-biases in humans and LLMs as a tool for benchmarking\n  LLM discourse capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implicit Causality-biases in humans and LLMs as a tool for benchmarking\n  LLM discourse capabilities"
                },
                "summary": "In this paper, we compare data generated with mono- and multilingual LLMs\nspanning a range of model sizes with data provided by human participants in an\nexperimental setting investigating well-established discourse biases. Beyond\nthe comparison as such, we aim to develop a benchmark to assess the\ncapabilities of LLMs with discourse biases as a robust proxy for more general\ndiscourse understanding capabilities. More specifically, we investigated\nImplicit Causality verbs, for which psycholinguistic research has found\nparticipants to display biases with regard to three phenomena:\\ the\nestablishment of (i) coreference relations (Experiment 1), (ii) coherence\nrelations (Experiment 2), and (iii) the use of particular referring expressions\n(Experiments 3 and 4). With regard to coreference biases we found only the\nlargest monolingual LLM (German Bloom 6.4B) to display more human-like biases.\nFor coherence relation, no LLM displayed the explanation bias usually found for\nhumans. For referring expressions, all LLMs displayed a preference for\nreferring to subject arguments with simpler forms than to objects. However, no\nbias effect on referring expression was found, as opposed to recent studies\ninvestigating human biases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we compare data generated with mono- and multilingual LLMs\nspanning a range of model sizes with data provided by human participants in an\nexperimental setting investigating well-established discourse biases. Beyond\nthe comparison as such, we aim to develop a benchmark to assess the\ncapabilities of LLMs with discourse biases as a robust proxy for more general\ndiscourse understanding capabilities. More specifically, we investigated\nImplicit Causality verbs, for which psycholinguistic research has found\nparticipants to display biases with regard to three phenomena:\\ the\nestablishment of (i) coreference relations (Experiment 1), (ii) coherence\nrelations (Experiment 2), and (iii) the use of particular referring expressions\n(Experiments 3 and 4). With regard to coreference biases we found only the\nlargest monolingual LLM (German Bloom 6.4B) to display more human-like biases.\nFor coherence relation, no LLM displayed the explanation bias usually found for\nhumans. For referring expressions, all LLMs displayed a preference for\nreferring to subject arguments with simpler forms than to objects. However, no\nbias effect on referring expression was found, as opposed to recent studies\ninvestigating human biases."
                },
                "authors": [
                    {
                        "name": "Florian Kankowski"
                    },
                    {
                        "name": "Torgrim Solstad"
                    },
                    {
                        "name": "Sina Zarriess"
                    },
                    {
                        "name": "Oliver Bott"
                    }
                ],
                "author_detail": {
                    "name": "Oliver Bott"
                },
                "author": "Oliver Bott",
                "arxiv_comment": "38 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12980v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12980v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12975v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12975v1",
                "updated": "2025-01-22T15:59:44Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    15,
                    59,
                    44,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T15:59:44Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    15,
                    59,
                    44,
                    2,
                    22,
                    0
                ],
                "title": "OnionEval: An Unified Evaluation of Fact-conflicting Hallucination for\n  Small-Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OnionEval: An Unified Evaluation of Fact-conflicting Hallucination for\n  Small-Large Language Models"
                },
                "summary": "Large Language Models (LLMs) are highly capable but require significant\ncomputational resources for both training and inference. Within the LLM family,\nsmaller models (those with fewer than 10 billion parameters) also perform well\nacross various tasks. However, these smaller models share similar limitations\nto their larger counterparts, including the tendency to hallucinate. Despite\nthe existence of many benchmarks to evaluate hallucination in LLMs, few have\nspecifically focused on small LLMs (SLLMs). Additionally, SLLMs show widely\nvarying performance across different benchmarks. In this paper, we introduce\nOnionEval, a multi-layer structured framework with a specific metric called the\ncontext-influence score (CI), designed to effectively assess the\nfact-conflicting hallucination tendencies of small LLMs across different\ncontextual levels. Our experimental results reveal a key feature of SLLMs: they\nexcel in factual analysis but face challenges with context reasoning. Further\ninvestigation shows that a simple Chain-of-Thought strategy can significantly\nreduce these limitations, improving the practical usefulness of SLLMs in\nreal-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are highly capable but require significant\ncomputational resources for both training and inference. Within the LLM family,\nsmaller models (those with fewer than 10 billion parameters) also perform well\nacross various tasks. However, these smaller models share similar limitations\nto their larger counterparts, including the tendency to hallucinate. Despite\nthe existence of many benchmarks to evaluate hallucination in LLMs, few have\nspecifically focused on small LLMs (SLLMs). Additionally, SLLMs show widely\nvarying performance across different benchmarks. In this paper, we introduce\nOnionEval, a multi-layer structured framework with a specific metric called the\ncontext-influence score (CI), designed to effectively assess the\nfact-conflicting hallucination tendencies of small LLMs across different\ncontextual levels. Our experimental results reveal a key feature of SLLMs: they\nexcel in factual analysis but face challenges with context reasoning. Further\ninvestigation shows that a simple Chain-of-Thought strategy can significantly\nreduce these limitations, improving the practical usefulness of SLLMs in\nreal-world applications."
                },
                "authors": [
                    {
                        "name": "Chongren Sun"
                    },
                    {
                        "name": "Yuran Li"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Benoit Boulet"
                    }
                ],
                "author_detail": {
                    "name": "Benoit Boulet"
                },
                "author": "Benoit Boulet",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12975v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12975v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12972v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12972v1",
                "updated": "2025-01-22T15:57:29Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    15,
                    57,
                    29,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T15:57:29Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    15,
                    57,
                    29,
                    2,
                    22,
                    0
                ],
                "title": "Accessible Smart Contracts Verification: Synthesizing Formal Models with\n  Tamed LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accessible Smart Contracts Verification: Synthesizing Formal Models with\n  Tamed LLMs"
                },
                "summary": "When blockchain systems are said to be trustless, what this really means is\nthat all the trust is put into software. Thus, there are strong incentives to\nensure blockchain software is correct -- vulnerabilities here cost millions and\nbreak businesses. One of the most powerful ways of establishing software\ncorrectness is by using formal methods. Approaches based on formal methods,\nhowever, induce a significant overhead in terms of time and expertise required\nto successfully employ them. Our work addresses this critical disadvantage by\nautomating the creation of a formal model -- a mathematical abstraction of the\nsoftware system -- which is often a core task when employing formal methods. We\nperform model synthesis in three phases: we first transpile the code into model\nstubs; then we \"fill in the blanks\" using a large language model (LLM);\nfinally, we iteratively repair the generated model, on both syntactical and\nsemantical level. In this way, we significantly reduce the amount of time\nnecessary to create formal models and increase accessibility of valuable\nsoftware verification methods that rely on them. The practical context of our\nwork was reducing the time-to-value of using formal models for correctness\naudits of smart contracts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When blockchain systems are said to be trustless, what this really means is\nthat all the trust is put into software. Thus, there are strong incentives to\nensure blockchain software is correct -- vulnerabilities here cost millions and\nbreak businesses. One of the most powerful ways of establishing software\ncorrectness is by using formal methods. Approaches based on formal methods,\nhowever, induce a significant overhead in terms of time and expertise required\nto successfully employ them. Our work addresses this critical disadvantage by\nautomating the creation of a formal model -- a mathematical abstraction of the\nsoftware system -- which is often a core task when employing formal methods. We\nperform model synthesis in three phases: we first transpile the code into model\nstubs; then we \"fill in the blanks\" using a large language model (LLM);\nfinally, we iteratively repair the generated model, on both syntactical and\nsemantical level. In this way, we significantly reduce the amount of time\nnecessary to create formal models and increase accessibility of valuable\nsoftware verification methods that rely on them. The practical context of our\nwork was reducing the time-to-value of using formal models for correctness\naudits of smart contracts."
                },
                "authors": [
                    {
                        "name": "Jan Corazza"
                    },
                    {
                        "name": "Ivan Gavran"
                    },
                    {
                        "name": "Gabriela Moreira"
                    },
                    {
                        "name": "Daniel Neider"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Neider"
                },
                "author": "Daniel Neider",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12972v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12972v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12959v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12959v1",
                "updated": "2025-01-22T15:33:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    15,
                    33,
                    17,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T15:33:17Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    15,
                    33,
                    17,
                    2,
                    22,
                    0
                ],
                "title": "Efficient Prompt Compression with Evaluator Heads for Long-Context\n  Transformer Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Prompt Compression with Evaluator Heads for Long-Context\n  Transformer Inference"
                },
                "summary": "Although applications involving long-context inputs are crucial for the\neffective utilization of large language models (LLMs), they also result in\nincreased computational costs and reduced performance. To address this\nchallenge, we propose an efficient, training-free prompt compression method\nthat retains key information within compressed prompts. We identify specific\nattention heads in transformer-based LLMs, which we designate as evaluator\nheads, that are capable of selecting tokens in long inputs that are most\nsignificant for inference. Building on this discovery, we develop EHPC, an\nEvaluator Head-based Prompt Compression method, which enables LLMs to rapidly\n\"skim through\" input prompts by leveraging only the first few layers with\nevaluator heads during the pre-filling stage, subsequently passing only the\nimportant tokens to the model for inference. EHPC achieves state-of-the-art\nresults across two mainstream benchmarks: prompt compression and long-context\ninference acceleration. Consequently, it effectively reduces the complexity and\ncosts associated with commercial API calls. We further demonstrate that EHPC\nattains competitive results compared to key-value cache-based acceleration\nmethods, thereby highlighting its potential to enhance the efficiency of LLMs\nfor long-context tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although applications involving long-context inputs are crucial for the\neffective utilization of large language models (LLMs), they also result in\nincreased computational costs and reduced performance. To address this\nchallenge, we propose an efficient, training-free prompt compression method\nthat retains key information within compressed prompts. We identify specific\nattention heads in transformer-based LLMs, which we designate as evaluator\nheads, that are capable of selecting tokens in long inputs that are most\nsignificant for inference. Building on this discovery, we develop EHPC, an\nEvaluator Head-based Prompt Compression method, which enables LLMs to rapidly\n\"skim through\" input prompts by leveraging only the first few layers with\nevaluator heads during the pre-filling stage, subsequently passing only the\nimportant tokens to the model for inference. EHPC achieves state-of-the-art\nresults across two mainstream benchmarks: prompt compression and long-context\ninference acceleration. Consequently, it effectively reduces the complexity and\ncosts associated with commercial API calls. We further demonstrate that EHPC\nattains competitive results compared to key-value cache-based acceleration\nmethods, thereby highlighting its potential to enhance the efficiency of LLMs\nfor long-context tasks."
                },
                "authors": [
                    {
                        "name": "Weizhi Fei"
                    },
                    {
                        "name": "Xueyan Niu"
                    },
                    {
                        "name": "Guoqing Xie"
                    },
                    {
                        "name": "Yingqing Liu"
                    },
                    {
                        "name": "Bo Bai"
                    },
                    {
                        "name": "Wei Han"
                    }
                ],
                "author_detail": {
                    "name": "Wei Han"
                },
                "author": "Wei Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12959v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12959v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12956v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12956v1",
                "updated": "2025-01-22T15:29:09Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    15,
                    29,
                    9,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T15:29:09Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    15,
                    29,
                    9,
                    2,
                    22,
                    0
                ],
                "title": "GANQ: GPU-Adaptive Non-Uniform Quantization for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GANQ: GPU-Adaptive Non-Uniform Quantization for Large Language Models"
                },
                "summary": "Large Language Models (LLMs) face significant deployment challenges due to\ntheir substantial resource requirements. While low-bit quantized weights can\nreduce memory usage and improve inference efficiency, current hardware lacks\nnative support for mixed-precision General Matrix Multiplication (mpGEMM),\nresulting in inefficient dequantization-based implementations. Moreover,\nuniform quantization methods often fail to capture weight distributions\nadequately, leading to performance degradation. We propose GANQ (GPU-Adaptive\nNon-Uniform Quantization), a layer-wise post-training non-uniform quantization\nframework optimized for hardware-efficient lookup table-based mpGEMM. GANQ\nachieves superior quantization performance by utilizing a training-free,\nGPU-adaptive optimization algorithm to efficiently reduce layer-wise\nquantization errors. Extensive experiments demonstrate GANQ's ability to reduce\nthe perplexity gap from the FP16 baseline compared to state-of-the-art methods\nfor both 3-bit and 4-bit quantization. Furthermore, when deployed on a single\nNVIDIA RTX 4090 GPU, GANQ's quantized models achieve up to 2.57$\\times$ speedup\nover the baseline, advancing memory and inference efficiency in LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) face significant deployment challenges due to\ntheir substantial resource requirements. While low-bit quantized weights can\nreduce memory usage and improve inference efficiency, current hardware lacks\nnative support for mixed-precision General Matrix Multiplication (mpGEMM),\nresulting in inefficient dequantization-based implementations. Moreover,\nuniform quantization methods often fail to capture weight distributions\nadequately, leading to performance degradation. We propose GANQ (GPU-Adaptive\nNon-Uniform Quantization), a layer-wise post-training non-uniform quantization\nframework optimized for hardware-efficient lookup table-based mpGEMM. GANQ\nachieves superior quantization performance by utilizing a training-free,\nGPU-adaptive optimization algorithm to efficiently reduce layer-wise\nquantization errors. Extensive experiments demonstrate GANQ's ability to reduce\nthe perplexity gap from the FP16 baseline compared to state-of-the-art methods\nfor both 3-bit and 4-bit quantization. Furthermore, when deployed on a single\nNVIDIA RTX 4090 GPU, GANQ's quantized models achieve up to 2.57$\\times$ speedup\nover the baseline, advancing memory and inference efficiency in LLM deployment."
                },
                "authors": [
                    {
                        "name": "Pengxiang Zhao"
                    },
                    {
                        "name": "Xiaoming Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoming Yuan"
                },
                "author": "Xiaoming Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12956v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12956v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12948v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12948v1",
                "updated": "2025-01-22T15:19:35Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    15,
                    19,
                    35,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T15:19:35Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    15,
                    19,
                    35,
                    2,
                    22,
                    0
                ],
                "title": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via\n  Reinforcement Learning"
                },
                "summary": "We introduce our first-generation reasoning models, DeepSeek-R1-Zero and\nDeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement\nlearning (RL) without supervised fine-tuning (SFT) as a preliminary step,\ndemonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero\nnaturally emerges with numerous powerful and intriguing reasoning behaviors.\nHowever, it encounters challenges such as poor readability, and language\nmixing. To address these issues and further enhance reasoning performance, we\nintroduce DeepSeek-R1, which incorporates multi-stage training and cold-start\ndata before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217\non reasoning tasks. To support the research community, we open-source\nDeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B,\n70B) distilled from DeepSeek-R1 based on Qwen and Llama.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce our first-generation reasoning models, DeepSeek-R1-Zero and\nDeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement\nlearning (RL) without supervised fine-tuning (SFT) as a preliminary step,\ndemonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero\nnaturally emerges with numerous powerful and intriguing reasoning behaviors.\nHowever, it encounters challenges such as poor readability, and language\nmixing. To address these issues and further enhance reasoning performance, we\nintroduce DeepSeek-R1, which incorporates multi-stage training and cold-start\ndata before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217\non reasoning tasks. To support the research community, we open-source\nDeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B,\n70B) distilled from DeepSeek-R1 based on Qwen and Llama."
                },
                "authors": [
                    {
                        "name": "DeepSeek-AI"
                    },
                    {
                        "name": "Daya Guo"
                    },
                    {
                        "name": "Dejian Yang"
                    },
                    {
                        "name": "Haowei Zhang"
                    },
                    {
                        "name": "Junxiao Song"
                    },
                    {
                        "name": "Ruoyu Zhang"
                    },
                    {
                        "name": "Runxin Xu"
                    },
                    {
                        "name": "Qihao Zhu"
                    },
                    {
                        "name": "Shirong Ma"
                    },
                    {
                        "name": "Peiyi Wang"
                    },
                    {
                        "name": "Xiao Bi"
                    },
                    {
                        "name": "Xiaokang Zhang"
                    },
                    {
                        "name": "Xingkai Yu"
                    },
                    {
                        "name": "Yu Wu"
                    },
                    {
                        "name": "Z. F. Wu"
                    },
                    {
                        "name": "Zhibin Gou"
                    },
                    {
                        "name": "Zhihong Shao"
                    },
                    {
                        "name": "Zhuoshu Li"
                    },
                    {
                        "name": "Ziyi Gao"
                    },
                    {
                        "name": "Aixin Liu"
                    },
                    {
                        "name": "Bing Xue"
                    },
                    {
                        "name": "Bingxuan Wang"
                    },
                    {
                        "name": "Bochao Wu"
                    },
                    {
                        "name": "Bei Feng"
                    },
                    {
                        "name": "Chengda Lu"
                    },
                    {
                        "name": "Chenggang Zhao"
                    },
                    {
                        "name": "Chengqi Deng"
                    },
                    {
                        "name": "Chenyu Zhang"
                    },
                    {
                        "name": "Chong Ruan"
                    },
                    {
                        "name": "Damai Dai"
                    },
                    {
                        "name": "Deli Chen"
                    },
                    {
                        "name": "Dongjie Ji"
                    },
                    {
                        "name": "Erhang Li"
                    },
                    {
                        "name": "Fangyun Lin"
                    },
                    {
                        "name": "Fucong Dai"
                    },
                    {
                        "name": "Fuli Luo"
                    },
                    {
                        "name": "Guangbo Hao"
                    },
                    {
                        "name": "Guanting Chen"
                    },
                    {
                        "name": "Guowei Li"
                    },
                    {
                        "name": "H. Zhang"
                    },
                    {
                        "name": "Han Bao"
                    },
                    {
                        "name": "Hanwei Xu"
                    },
                    {
                        "name": "Haocheng Wang"
                    },
                    {
                        "name": "Honghui Ding"
                    },
                    {
                        "name": "Huajian Xin"
                    },
                    {
                        "name": "Huazuo Gao"
                    },
                    {
                        "name": "Hui Qu"
                    },
                    {
                        "name": "Hui Li"
                    },
                    {
                        "name": "Jianzhong Guo"
                    },
                    {
                        "name": "Jiashi Li"
                    },
                    {
                        "name": "Jiawei Wang"
                    },
                    {
                        "name": "Jingchang Chen"
                    },
                    {
                        "name": "Jingyang Yuan"
                    },
                    {
                        "name": "Junjie Qiu"
                    },
                    {
                        "name": "Junlong Li"
                    },
                    {
                        "name": "J. L. Cai"
                    },
                    {
                        "name": "Jiaqi Ni"
                    },
                    {
                        "name": "Jian Liang"
                    },
                    {
                        "name": "Jin Chen"
                    },
                    {
                        "name": "Kai Dong"
                    },
                    {
                        "name": "Kai Hu"
                    },
                    {
                        "name": "Kaige Gao"
                    },
                    {
                        "name": "Kang Guan"
                    },
                    {
                        "name": "Kexin Huang"
                    },
                    {
                        "name": "Kuai Yu"
                    },
                    {
                        "name": "Lean Wang"
                    },
                    {
                        "name": "Lecong Zhang"
                    },
                    {
                        "name": "Liang Zhao"
                    },
                    {
                        "name": "Litong Wang"
                    },
                    {
                        "name": "Liyue Zhang"
                    },
                    {
                        "name": "Lei Xu"
                    },
                    {
                        "name": "Leyi Xia"
                    },
                    {
                        "name": "Mingchuan Zhang"
                    },
                    {
                        "name": "Minghua Zhang"
                    },
                    {
                        "name": "Minghui Tang"
                    },
                    {
                        "name": "Meng Li"
                    },
                    {
                        "name": "Miaojun Wang"
                    },
                    {
                        "name": "Mingming Li"
                    },
                    {
                        "name": "Ning Tian"
                    },
                    {
                        "name": "Panpan Huang"
                    },
                    {
                        "name": "Peng Zhang"
                    },
                    {
                        "name": "Qiancheng Wang"
                    },
                    {
                        "name": "Qinyu Chen"
                    },
                    {
                        "name": "Qiushi Du"
                    },
                    {
                        "name": "Ruiqi Ge"
                    },
                    {
                        "name": "Ruisong Zhang"
                    },
                    {
                        "name": "Ruizhe Pan"
                    },
                    {
                        "name": "Runji Wang"
                    },
                    {
                        "name": "R. J. Chen"
                    },
                    {
                        "name": "R. L. Jin"
                    },
                    {
                        "name": "Ruyi Chen"
                    },
                    {
                        "name": "Shanghao Lu"
                    },
                    {
                        "name": "Shangyan Zhou"
                    },
                    {
                        "name": "Shanhuang Chen"
                    },
                    {
                        "name": "Shengfeng Ye"
                    },
                    {
                        "name": "Shiyu Wang"
                    },
                    {
                        "name": "Shuiping Yu"
                    },
                    {
                        "name": "Shunfeng Zhou"
                    },
                    {
                        "name": "Shuting Pan"
                    },
                    {
                        "name": "S. S. Li"
                    },
                    {
                        "name": "Shuang Zhou"
                    },
                    {
                        "name": "Shaoqing Wu"
                    },
                    {
                        "name": "Shengfeng Ye"
                    },
                    {
                        "name": "Tao Yun"
                    },
                    {
                        "name": "Tian Pei"
                    },
                    {
                        "name": "Tianyu Sun"
                    },
                    {
                        "name": "T. Wang"
                    },
                    {
                        "name": "Wangding Zeng"
                    },
                    {
                        "name": "Wanjia Zhao"
                    },
                    {
                        "name": "Wen Liu"
                    },
                    {
                        "name": "Wenfeng Liang"
                    },
                    {
                        "name": "Wenjun Gao"
                    },
                    {
                        "name": "Wenqin Yu"
                    },
                    {
                        "name": "Wentao Zhang"
                    },
                    {
                        "name": "W. L. Xiao"
                    },
                    {
                        "name": "Wei An"
                    },
                    {
                        "name": "Xiaodong Liu"
                    },
                    {
                        "name": "Xiaohan Wang"
                    },
                    {
                        "name": "Xiaokang Chen"
                    },
                    {
                        "name": "Xiaotao Nie"
                    },
                    {
                        "name": "Xin Cheng"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Xin Xie"
                    },
                    {
                        "name": "Xingchao Liu"
                    },
                    {
                        "name": "Xinyu Yang"
                    },
                    {
                        "name": "Xinyuan Li"
                    },
                    {
                        "name": "Xuecheng Su"
                    },
                    {
                        "name": "Xuheng Lin"
                    },
                    {
                        "name": "X. Q. Li"
                    },
                    {
                        "name": "Xiangyue Jin"
                    },
                    {
                        "name": "Xiaojin Shen"
                    },
                    {
                        "name": "Xiaosha Chen"
                    },
                    {
                        "name": "Xiaowen Sun"
                    },
                    {
                        "name": "Xiaoxiang Wang"
                    },
                    {
                        "name": "Xinnan Song"
                    },
                    {
                        "name": "Xinyi Zhou"
                    },
                    {
                        "name": "Xianzu Wang"
                    },
                    {
                        "name": "Xinxia Shan"
                    },
                    {
                        "name": "Y. K. Li"
                    },
                    {
                        "name": "Y. Q. Wang"
                    },
                    {
                        "name": "Y. X. Wei"
                    },
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Yanhong Xu"
                    },
                    {
                        "name": "Yao Li"
                    },
                    {
                        "name": "Yao Zhao"
                    },
                    {
                        "name": "Yaofeng Sun"
                    },
                    {
                        "name": "Yaohui Wang"
                    },
                    {
                        "name": "Yi Yu"
                    },
                    {
                        "name": "Yichao Zhang"
                    },
                    {
                        "name": "Yifan Shi"
                    },
                    {
                        "name": "Yiliang Xiong"
                    },
                    {
                        "name": "Ying He"
                    },
                    {
                        "name": "Yishi Piao"
                    },
                    {
                        "name": "Yisong Wang"
                    },
                    {
                        "name": "Yixuan Tan"
                    },
                    {
                        "name": "Yiyang Ma"
                    },
                    {
                        "name": "Yiyuan Liu"
                    },
                    {
                        "name": "Yongqiang Guo"
                    },
                    {
                        "name": "Yuan Ou"
                    },
                    {
                        "name": "Yuduan Wang"
                    },
                    {
                        "name": "Yue Gong"
                    },
                    {
                        "name": "Yuheng Zou"
                    },
                    {
                        "name": "Yujia He"
                    },
                    {
                        "name": "Yunfan Xiong"
                    },
                    {
                        "name": "Yuxiang Luo"
                    },
                    {
                        "name": "Yuxiang You"
                    },
                    {
                        "name": "Yuxuan Liu"
                    },
                    {
                        "name": "Yuyang Zhou"
                    },
                    {
                        "name": "Y. X. Zhu"
                    },
                    {
                        "name": "Yanhong Xu"
                    },
                    {
                        "name": "Yanping Huang"
                    },
                    {
                        "name": "Yaohui Li"
                    },
                    {
                        "name": "Yi Zheng"
                    },
                    {
                        "name": "Yuchen Zhu"
                    },
                    {
                        "name": "Yunxian Ma"
                    },
                    {
                        "name": "Ying Tang"
                    },
                    {
                        "name": "Yukun Zha"
                    },
                    {
                        "name": "Yuting Yan"
                    },
                    {
                        "name": "Z. Z. Ren"
                    },
                    {
                        "name": "Zehui Ren"
                    },
                    {
                        "name": "Zhangli Sha"
                    },
                    {
                        "name": "Zhe Fu"
                    },
                    {
                        "name": "Zhean Xu"
                    },
                    {
                        "name": "Zhenda Xie"
                    },
                    {
                        "name": "Zhengyan Zhang"
                    },
                    {
                        "name": "Zhewen Hao"
                    },
                    {
                        "name": "Zhicheng Ma"
                    },
                    {
                        "name": "Zhigang Yan"
                    },
                    {
                        "name": "Zhiyu Wu"
                    },
                    {
                        "name": "Zihui Gu"
                    },
                    {
                        "name": "Zijia Zhu"
                    },
                    {
                        "name": "Zijun Liu"
                    },
                    {
                        "name": "Zilin Li"
                    },
                    {
                        "name": "Ziwei Xie"
                    },
                    {
                        "name": "Ziyang Song"
                    },
                    {
                        "name": "Zizheng Pan"
                    },
                    {
                        "name": "Zhen Huang"
                    },
                    {
                        "name": "Zhipeng Xu"
                    },
                    {
                        "name": "Zhongyu Zhang"
                    },
                    {
                        "name": "Zhen Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zhen Zhang"
                },
                "author": "Zhen Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12948v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12948v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01253v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01253v5",
                "updated": "2025-01-22T15:09:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    15,
                    9,
                    58,
                    2,
                    22,
                    0
                ],
                "published": "2024-12-02T08:22:56Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    8,
                    22,
                    56,
                    0,
                    337,
                    0
                ],
                "title": "Yi-Lightning Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yi-Lightning Technical Report"
                },
                "summary": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com."
                },
                "authors": [
                    {
                        "name": "Alan Wake"
                    },
                    {
                        "name": "Bei Chen"
                    },
                    {
                        "name": "C. X. Lv"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Chengen Huang"
                    },
                    {
                        "name": "Chenglin Cai"
                    },
                    {
                        "name": "Chujie Zheng"
                    },
                    {
                        "name": "Daniel Cooper"
                    },
                    {
                        "name": "Fan Zhou"
                    },
                    {
                        "name": "Feng Hu"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Guoyin Wang"
                    },
                    {
                        "name": "Heng Ji"
                    },
                    {
                        "name": "Howard Qiu"
                    },
                    {
                        "name": "Jiangcheng Zhu"
                    },
                    {
                        "name": "Jun Tian"
                    },
                    {
                        "name": "Katherine Su"
                    },
                    {
                        "name": "Lihuan Zhang"
                    },
                    {
                        "name": "Liying Li"
                    },
                    {
                        "name": "Ming Song"
                    },
                    {
                        "name": "Mou Li"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Qicheng Hu"
                    },
                    {
                        "name": "Shawn Wang"
                    },
                    {
                        "name": "Shijun Zhou"
                    },
                    {
                        "name": "Shiming Yang"
                    },
                    {
                        "name": "Shiyong Li"
                    },
                    {
                        "name": "Tianhang Zhu"
                    },
                    {
                        "name": "Wen Xie"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Xiang He"
                    },
                    {
                        "name": "Xiaobo Chen"
                    },
                    {
                        "name": "Xiaohui Hu"
                    },
                    {
                        "name": "Xiaoyi Ren"
                    },
                    {
                        "name": "Xinyao Niu"
                    },
                    {
                        "name": "Yanpeng Li"
                    },
                    {
                        "name": "Yongke Zhao"
                    },
                    {
                        "name": "Yongzhen Luo"
                    },
                    {
                        "name": "Yuchi Xu"
                    },
                    {
                        "name": "Yuxuan Sha"
                    },
                    {
                        "name": "Zhaodong Yan"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Zirui Zhang"
                    },
                    {
                        "name": "Zonghong Dai"
                    }
                ],
                "author_detail": {
                    "name": "Zonghong Dai"
                },
                "author": "Zonghong Dai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01253v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01253v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12934v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12934v1",
                "updated": "2025-01-22T15:04:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    15,
                    4,
                    13,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T15:04:13Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    15,
                    4,
                    13,
                    2,
                    22,
                    0
                ],
                "title": "Correctness Assessment of Code Generated by Large Language Models Using\n  Internal Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Correctness Assessment of Code Generated by Large Language Models Using\n  Internal Representations"
                },
                "summary": "Ensuring the correctness of code generated by Large Language Models (LLMs)\npresents a significant challenge in AI-driven software development. Existing\napproaches predominantly rely on black-box (closed-box) approaches that\nevaluate correctness post-generation, failing to utilize the rich insights\nembedded in the LLMs' internal states during code generation. In this paper, we\nintroduce OPENIA, a novel white-box (open-box) framework that leverages these\ninternal representations to assess the correctness of LLM-generated code.\nOPENIA systematically analyzes the intermediate states of representative\nopen-source LLMs specialized for code, including DeepSeek-Coder, CodeLlama, and\nMagicCoder, across diverse code generation benchmarks. Our empirical analysis\nreveals that these internal representations encode latent information, which\nstrongly correlates with the correctness of the generated code. Building on\nthese insights, OPENIA uses a white-box/open-box approach to make informed\npredictions about code correctness, offering significant advantages in\nadaptability and robustness over traditional classification-based methods and\nzero-shot approaches. Experimental results demonstrate that OPENIA consistently\noutperforms baseline models, achieving higher accuracy, precision, recall, and\nF1-Scores with up to a 2X improvement in standalone code generation and a 46%\nenhancement in repository-specific scenarios. By unlocking the potential of\nin-process signals, OPENIA paves the way for more proactive and efficient\nquality assurance mechanisms in LLM-assisted code generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring the correctness of code generated by Large Language Models (LLMs)\npresents a significant challenge in AI-driven software development. Existing\napproaches predominantly rely on black-box (closed-box) approaches that\nevaluate correctness post-generation, failing to utilize the rich insights\nembedded in the LLMs' internal states during code generation. In this paper, we\nintroduce OPENIA, a novel white-box (open-box) framework that leverages these\ninternal representations to assess the correctness of LLM-generated code.\nOPENIA systematically analyzes the intermediate states of representative\nopen-source LLMs specialized for code, including DeepSeek-Coder, CodeLlama, and\nMagicCoder, across diverse code generation benchmarks. Our empirical analysis\nreveals that these internal representations encode latent information, which\nstrongly correlates with the correctness of the generated code. Building on\nthese insights, OPENIA uses a white-box/open-box approach to make informed\npredictions about code correctness, offering significant advantages in\nadaptability and robustness over traditional classification-based methods and\nzero-shot approaches. Experimental results demonstrate that OPENIA consistently\noutperforms baseline models, achieving higher accuracy, precision, recall, and\nF1-Scores with up to a 2X improvement in standalone code generation and a 46%\nenhancement in repository-specific scenarios. By unlocking the potential of\nin-process signals, OPENIA paves the way for more proactive and efficient\nquality assurance mechanisms in LLM-assisted code generation."
                },
                "authors": [
                    {
                        "name": "Tuan-Dung Bui"
                    },
                    {
                        "name": "Thanh Trong Vu"
                    },
                    {
                        "name": "Thu-Trang Nguyen"
                    },
                    {
                        "name": "Son Nguyen"
                    },
                    {
                        "name": "Hieu Dinh Vo"
                    }
                ],
                "author_detail": {
                    "name": "Hieu Dinh Vo"
                },
                "author": "Hieu Dinh Vo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12934v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12934v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19291v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19291v2",
                "updated": "2025-01-22T14:50:33Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    14,
                    50,
                    33,
                    2,
                    22,
                    0
                ],
                "published": "2024-12-26T17:34:26Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    17,
                    34,
                    26,
                    3,
                    361,
                    0
                ],
                "title": "RAG with Differential Privacy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAG with Differential Privacy"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has emerged as the dominant technique to\nprovide \\emph{Large Language Models} (LLM) with fresh and relevant context,\nmitigating the risk of hallucinations and improving the overall quality of\nresponses in environments with large and fast moving knowledge bases. However,\nthe integration of external documents into the generation process raises\nsignificant privacy concerns. Indeed, when added to a prompt, it is not\npossible to guarantee a response will not inadvertently expose confidential\ndata, leading to potential breaches of privacy and ethical dilemmas. This paper\nexplores a practical solution to this problem suitable to general knowledge\nextraction from personal data. It shows \\emph{differentially private token\ngeneration} is a viable approach to private RAG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has emerged as the dominant technique to\nprovide \\emph{Large Language Models} (LLM) with fresh and relevant context,\nmitigating the risk of hallucinations and improving the overall quality of\nresponses in environments with large and fast moving knowledge bases. However,\nthe integration of external documents into the generation process raises\nsignificant privacy concerns. Indeed, when added to a prompt, it is not\npossible to guarantee a response will not inadvertently expose confidential\ndata, leading to potential breaches of privacy and ethical dilemmas. This paper\nexplores a practical solution to this problem suitable to general knowledge\nextraction from personal data. It shows \\emph{differentially private token\ngeneration} is a viable approach to private RAG."
                },
                "authors": [
                    {
                        "name": "Nicolas Grislain"
                    }
                ],
                "author_detail": {
                    "name": "Nicolas Grislain"
                },
                "author": "Nicolas Grislain",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19291v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19291v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12909v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12909v1",
                "updated": "2025-01-22T14:36:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    14,
                    36,
                    30,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T14:36:30Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    14,
                    36,
                    30,
                    2,
                    22,
                    0
                ],
                "title": "FilmAgent: A Multi-Agent Framework for End-to-End Film Automation in\n  Virtual 3D Spaces",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FilmAgent: A Multi-Agent Framework for End-to-End Film Automation in\n  Virtual 3D Spaces"
                },
                "summary": "Virtual film production requires intricate decision-making processes,\nincluding scriptwriting, virtual cinematography, and precise actor positioning\nand actions. Motivated by recent advances in automated decision-making with\nlanguage agent-based societies, this paper introduces FilmAgent, a novel\nLLM-based multi-agent collaborative framework for end-to-end film automation in\nour constructed 3D virtual spaces. FilmAgent simulates various crew roles,\nincluding directors, screenwriters, actors, and cinematographers, and covers\nkey stages of a film production workflow: (1) idea development transforms\nbrainstormed ideas into structured story outlines; (2) scriptwriting elaborates\non dialogue and character actions for each scene; (3) cinematography determines\nthe camera setups for each shot. A team of agents collaborates through\niterative feedback and revisions, thereby verifying intermediate scripts and\nreducing hallucinations. We evaluate the generated videos on 15 ideas and 4 key\naspects. Human evaluation shows that FilmAgent outperforms all baselines across\nall aspects and scores 3.98 out of 5 on average, showing the feasibility of\nmulti-agent collaboration in filmmaking. Further analysis reveals that\nFilmAgent, despite using the less advanced GPT-4o model, surpasses the\nsingle-agent o1, showing the advantage of a well-coordinated multi-agent\nsystem. Lastly, we discuss the complementary strengths and weaknesses of\nOpenAI's text-to-video model Sora and our FilmAgent in filmmaking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Virtual film production requires intricate decision-making processes,\nincluding scriptwriting, virtual cinematography, and precise actor positioning\nand actions. Motivated by recent advances in automated decision-making with\nlanguage agent-based societies, this paper introduces FilmAgent, a novel\nLLM-based multi-agent collaborative framework for end-to-end film automation in\nour constructed 3D virtual spaces. FilmAgent simulates various crew roles,\nincluding directors, screenwriters, actors, and cinematographers, and covers\nkey stages of a film production workflow: (1) idea development transforms\nbrainstormed ideas into structured story outlines; (2) scriptwriting elaborates\non dialogue and character actions for each scene; (3) cinematography determines\nthe camera setups for each shot. A team of agents collaborates through\niterative feedback and revisions, thereby verifying intermediate scripts and\nreducing hallucinations. We evaluate the generated videos on 15 ideas and 4 key\naspects. Human evaluation shows that FilmAgent outperforms all baselines across\nall aspects and scores 3.98 out of 5 on average, showing the feasibility of\nmulti-agent collaboration in filmmaking. Further analysis reveals that\nFilmAgent, despite using the less advanced GPT-4o model, surpasses the\nsingle-agent o1, showing the advantage of a well-coordinated multi-agent\nsystem. Lastly, we discuss the complementary strengths and weaknesses of\nOpenAI's text-to-video model Sora and our FilmAgent in filmmaking."
                },
                "authors": [
                    {
                        "name": "Zhenran Xu"
                    },
                    {
                        "name": "Longyue Wang"
                    },
                    {
                        "name": "Jifang Wang"
                    },
                    {
                        "name": "Zhouyi Li"
                    },
                    {
                        "name": "Senbao Shi"
                    },
                    {
                        "name": "Xue Yang"
                    },
                    {
                        "name": "Yiyu Wang"
                    },
                    {
                        "name": "Baotian Hu"
                    },
                    {
                        "name": "Jun Yu"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "Work in progress. Project Page: https://filmagent.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12909v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12909v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10994v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10994v2",
                "updated": "2025-01-22T14:33:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    14,
                    33,
                    23,
                    2,
                    22,
                    0
                ],
                "published": "2024-06-24T12:09:34Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    12,
                    9,
                    34,
                    0,
                    176,
                    0
                ],
                "title": "Panza: Design and Analysis of a Fully-Local Personalized Text Writing\n  Assistant",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Panza: Design and Analysis of a Fully-Local Personalized Text Writing\n  Assistant"
                },
                "summary": "The availability of powerful open-source large language models (LLMs) opens\nexciting use cases, such as automated personal assistants that adapt to the\nuser's unique data and demands. Two key requirements for such assistants are\npersonalization - in the sense that the assistant should reflect the user's own\nwriting style - and privacy - users may prefer to always store their personal\ndata locally, on their own computing device. In this application paper, we\npresent a new design and evaluation for such an automated assistant, for the\nspecific use case of email generation, which we call Panza. Specifically, Panza\ncan be trained and deployed locally on commodity hardware, and is personalized\nto the user's writing style. Panza's personalization features are based on a\ncombination of fine-tuning using a variant of the Reverse Instructions\ntechnique together with Retrieval-Augmented Generation (RAG). We demonstrate\nthat this combination allows us to fine-tune an LLM to better reflect a user's\nwriting style using limited data, while executing on extremely limited\nresources, e.g. on a free Google Colab instance. Our key methodological\ncontribution is what we believe to be the first detailed study of evaluation\nmetrics for this personalized writing task, and of how different choices of\nsystem components - e.g. the use of RAG and of different fine-tuning approaches\n- impact the system's performance. We are releasing the full Panza code as well\nas a new \"David\" personalized email dataset licensed for research use, both\navailable on https://github.com/IST-DASLab/PanzaMail.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The availability of powerful open-source large language models (LLMs) opens\nexciting use cases, such as automated personal assistants that adapt to the\nuser's unique data and demands. Two key requirements for such assistants are\npersonalization - in the sense that the assistant should reflect the user's own\nwriting style - and privacy - users may prefer to always store their personal\ndata locally, on their own computing device. In this application paper, we\npresent a new design and evaluation for such an automated assistant, for the\nspecific use case of email generation, which we call Panza. Specifically, Panza\ncan be trained and deployed locally on commodity hardware, and is personalized\nto the user's writing style. Panza's personalization features are based on a\ncombination of fine-tuning using a variant of the Reverse Instructions\ntechnique together with Retrieval-Augmented Generation (RAG). We demonstrate\nthat this combination allows us to fine-tune an LLM to better reflect a user's\nwriting style using limited data, while executing on extremely limited\nresources, e.g. on a free Google Colab instance. Our key methodological\ncontribution is what we believe to be the first detailed study of evaluation\nmetrics for this personalized writing task, and of how different choices of\nsystem components - e.g. the use of RAG and of different fine-tuning approaches\n- impact the system's performance. We are releasing the full Panza code as well\nas a new \"David\" personalized email dataset licensed for research use, both\navailable on https://github.com/IST-DASLab/PanzaMail."
                },
                "authors": [
                    {
                        "name": "Armand Nicolicioiu"
                    },
                    {
                        "name": "Eugenia Iofinova"
                    },
                    {
                        "name": "Eldar Kurtic"
                    },
                    {
                        "name": "Mahdi Nikdan"
                    },
                    {
                        "name": "Andrei Panferov"
                    },
                    {
                        "name": "Ilia Markov"
                    },
                    {
                        "name": "Nir Shavit"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "arxiv_comment": "Panza is available at https://github.com/IST-DASLab/PanzaMail",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10994v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10994v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11935v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11935v2",
                "updated": "2025-01-22T14:31:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    14,
                    31,
                    48,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-21T07:16:18Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    7,
                    16,
                    18,
                    1,
                    21,
                    0
                ],
                "title": "Web vs. LLMs: An Empirical Study of Learning Behaviors of CS2 Students",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Web vs. LLMs: An Empirical Study of Learning Behaviors of CS2 Students"
                },
                "summary": "LLMs such as ChatGPT have been widely adopted by students in higher education\nas tools for learning programming and related concepts. However, it remains\nunclear how effective students are and what strategies students use while\nlearning with LLMs. Since the majority of students' experiences in online\nself-learning have come through using search engines such as Google, evaluating\nAI tools in this context can help us address these gaps. In this mixed methods\nresearch, we conducted an exploratory within-subjects study to understand how\nCS2 students learn programming concepts using both LLMs as well as traditional\nonline methods such as educational websites and videos to examine how students\napproach learning within and across both scenarios. We discovered that students\nfound it easier to learn a more difficult concept using traditional methods\nthan using ChatGPT. We also found that students ask fewer follow-ups and use\nmore keyword-based queries for search engines while their prompts to LLMs tend\nto explicitly ask for information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs such as ChatGPT have been widely adopted by students in higher education\nas tools for learning programming and related concepts. However, it remains\nunclear how effective students are and what strategies students use while\nlearning with LLMs. Since the majority of students' experiences in online\nself-learning have come through using search engines such as Google, evaluating\nAI tools in this context can help us address these gaps. In this mixed methods\nresearch, we conducted an exploratory within-subjects study to understand how\nCS2 students learn programming concepts using both LLMs as well as traditional\nonline methods such as educational websites and videos to examine how students\napproach learning within and across both scenarios. We discovered that students\nfound it easier to learn a more difficult concept using traditional methods\nthan using ChatGPT. We also found that students ask fewer follow-ups and use\nmore keyword-based queries for search engines while their prompts to LLMs tend\nto explicitly ask for information."
                },
                "authors": [
                    {
                        "name": "Aayush Kumar"
                    },
                    {
                        "name": "Daniel Prol"
                    },
                    {
                        "name": "Amin Alipour"
                    },
                    {
                        "name": "Sruti Srinivasa Ragavan"
                    }
                ],
                "author_detail": {
                    "name": "Sruti Srinivasa Ragavan"
                },
                "author": "Sruti Srinivasa Ragavan",
                "arxiv_comment": "7 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11935v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11935v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12904v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12904v1",
                "updated": "2025-01-22T14:30:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    14,
                    30,
                    40,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T14:30:40Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    14,
                    30,
                    40,
                    2,
                    22,
                    0
                ],
                "title": "A Functional Software Reference Architecture for LLM-Integrated Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Functional Software Reference Architecture for LLM-Integrated Systems"
                },
                "summary": "The integration of large language models into software systems is\ntransforming capabilities such as natural language understanding,\ndecision-making, and autonomous task execution. However, the absence of a\ncommonly accepted software reference architecture hinders systematic reasoning\nabout their design and quality attributes. This gap makes it challenging to\naddress critical concerns like privacy, security, modularity, and\ninteroperability, which are increasingly important as these systems grow in\ncomplexity and societal impact. In this paper, we describe our\n\\textit{emerging} results for a preliminary functional reference architecture\nas a conceptual framework to address these challenges and guide the design,\nevaluation, and evolution of large language model-integrated systems. We\nidentify key architectural concerns for these systems, informed by current\nresearch and practice. We then evaluate how the architecture addresses these\nconcerns and validate its applicability using three open-source large language\nmodel-integrated systems in computer vision, text processing, and coding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of large language models into software systems is\ntransforming capabilities such as natural language understanding,\ndecision-making, and autonomous task execution. However, the absence of a\ncommonly accepted software reference architecture hinders systematic reasoning\nabout their design and quality attributes. This gap makes it challenging to\naddress critical concerns like privacy, security, modularity, and\ninteroperability, which are increasingly important as these systems grow in\ncomplexity and societal impact. In this paper, we describe our\n\\textit{emerging} results for a preliminary functional reference architecture\nas a conceptual framework to address these challenges and guide the design,\nevaluation, and evolution of large language model-integrated systems. We\nidentify key architectural concerns for these systems, informed by current\nresearch and practice. We then evaluate how the architecture addresses these\nconcerns and validate its applicability using three open-source large language\nmodel-integrated systems in computer vision, text processing, and coding."
                },
                "authors": [
                    {
                        "name": "Alessio Bucaioni"
                    },
                    {
                        "name": "Martin Weyssow"
                    },
                    {
                        "name": "Junda He"
                    },
                    {
                        "name": "Yunbo Lyu"
                    },
                    {
                        "name": "David Lo"
                    }
                ],
                "author_detail": {
                    "name": "David Lo"
                },
                "author": "David Lo",
                "arxiv_comment": "Accepted for publication at the 22nd IEEE International Conference on\n  Software Architecture (ICSA 2025) - New and Emerging Ideas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12904v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12904v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12895v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12895v1",
                "updated": "2025-01-22T14:15:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    14,
                    15,
                    46,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T14:15:46Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    14,
                    15,
                    46,
                    2,
                    22,
                    0
                ],
                "title": "Test-Time Preference Optimization: On-the-Fly Alignment via Iterative\n  Textual Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-Time Preference Optimization: On-the-Fly Alignment via Iterative\n  Textual Feedback"
                },
                "summary": "Large language models (LLMs) demonstrate impressive performance but lack the\nflexibility to adapt to human preferences quickly without retraining. In this\nwork, we introduce Test-time Preference Optimization (TPO), a framework that\naligns LLM outputs with human preferences during inference, removing the need\nto update model parameters. Rather than relying on purely numerical rewards,\nTPO translates reward signals into textual critiques and uses them as textual\nrewards to iteratively refine its response. Evaluations on benchmarks covering\ninstruction following, preference alignment, safety, and mathematics reveal\nthat TPO progressively improves alignment with human preferences. Notably,\nafter only a few TPO steps, the initially unaligned Llama-3.1-70B-SFT model can\nsurpass the aligned counterpart, Llama-3.1-70B-Instruct. Furthermore, TPO\nscales efficiently with both the search width and depth during inference.\nThrough case studies, we illustrate how TPO exploits the innate capacity of LLM\nto interpret and act upon reward signals. Our findings establish TPO as a\npractical, lightweight alternative for test-time preference optimization,\nachieving alignment on the fly. Our code is publicly available at\nhttps://github.com/yafuly/TPO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate impressive performance but lack the\nflexibility to adapt to human preferences quickly without retraining. In this\nwork, we introduce Test-time Preference Optimization (TPO), a framework that\naligns LLM outputs with human preferences during inference, removing the need\nto update model parameters. Rather than relying on purely numerical rewards,\nTPO translates reward signals into textual critiques and uses them as textual\nrewards to iteratively refine its response. Evaluations on benchmarks covering\ninstruction following, preference alignment, safety, and mathematics reveal\nthat TPO progressively improves alignment with human preferences. Notably,\nafter only a few TPO steps, the initially unaligned Llama-3.1-70B-SFT model can\nsurpass the aligned counterpart, Llama-3.1-70B-Instruct. Furthermore, TPO\nscales efficiently with both the search width and depth during inference.\nThrough case studies, we illustrate how TPO exploits the innate capacity of LLM\nto interpret and act upon reward signals. Our findings establish TPO as a\npractical, lightweight alternative for test-time preference optimization,\nachieving alignment on the fly. Our code is publicly available at\nhttps://github.com/yafuly/TPO."
                },
                "authors": [
                    {
                        "name": "Yafu Li"
                    },
                    {
                        "name": "Xuyang Hu"
                    },
                    {
                        "name": "Xiaoye Qu"
                    },
                    {
                        "name": "Linjie Li"
                    },
                    {
                        "name": "Yu Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Yu Cheng"
                },
                "author": "Yu Cheng",
                "arxiv_comment": "43 pages; work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12895v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12895v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11036v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11036v2",
                "updated": "2025-01-22T13:53:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    13,
                    53,
                    59,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-19T13:06:51Z",
                "published_parsed": [
                    2025,
                    1,
                    19,
                    13,
                    6,
                    51,
                    6,
                    19,
                    0
                ],
                "title": "LF-Steering: Latent Feature Activation Steering for Enhancing Semantic\n  Consistency in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LF-Steering: Latent Feature Activation Steering for Enhancing Semantic\n  Consistency in Large Language Models"
                },
                "summary": "Large Language Models (LLMs) often generate inconsistent responses when\nprompted with semantically equivalent paraphrased inputs. Recently, activation\nsteering, a technique that modulates LLMs' behaviours by adjusting their latent\nrepresentations during inference time, has been explored to improve the\nsemantic consistency of LLMs. However, these methods typically operate at the\nmodel component level, such as layer hidden states or attention head outputs.\nThey face a challenge due to the ``polysemanticity issue'', where the model\ncomponents of LLMs typically encode multiple entangled features, making precise\nsteering difficult. To address this challenge, we drill down to feature-level\nrepresentations and propose LF-Steering, a novel activation steering approach\nto precisely identify latent feature representations responsible for semantic\ninconsistency. More specifically, our method maps the hidden states of the\nrelevant transformer layer into a sparsely activated, high-dimensional feature\nspace based on a sparse autoencoder (SAE), ensuring model steering based on\ndecoupled feature representations with minimal interference. Comprehensive\nexperiments on NLU and NLG datasets demonstrate the effectiveness of our method\nin enhancing semantic consistency, resulting in significant performance gains\nfor various NLU and NLG tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) often generate inconsistent responses when\nprompted with semantically equivalent paraphrased inputs. Recently, activation\nsteering, a technique that modulates LLMs' behaviours by adjusting their latent\nrepresentations during inference time, has been explored to improve the\nsemantic consistency of LLMs. However, these methods typically operate at the\nmodel component level, such as layer hidden states or attention head outputs.\nThey face a challenge due to the ``polysemanticity issue'', where the model\ncomponents of LLMs typically encode multiple entangled features, making precise\nsteering difficult. To address this challenge, we drill down to feature-level\nrepresentations and propose LF-Steering, a novel activation steering approach\nto precisely identify latent feature representations responsible for semantic\ninconsistency. More specifically, our method maps the hidden states of the\nrelevant transformer layer into a sparsely activated, high-dimensional feature\nspace based on a sparse autoencoder (SAE), ensuring model steering based on\ndecoupled feature representations with minimal interference. Comprehensive\nexperiments on NLU and NLG datasets demonstrate the effectiveness of our method\nin enhancing semantic consistency, resulting in significant performance gains\nfor various NLU and NLG tasks."
                },
                "authors": [
                    {
                        "name": "Jingyuan Yang"
                    },
                    {
                        "name": "Rongjun Li"
                    },
                    {
                        "name": "Weixuan Wang"
                    },
                    {
                        "name": "Ziyu Zhou"
                    },
                    {
                        "name": "Zhiyong Feng"
                    },
                    {
                        "name": "Wei Peng"
                    }
                ],
                "author_detail": {
                    "name": "Wei Peng"
                },
                "author": "Wei Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11036v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11036v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12883v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12883v1",
                "updated": "2025-01-22T13:44:44Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    13,
                    44,
                    44,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T13:44:44Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    13,
                    44,
                    44,
                    2,
                    22,
                    0
                ],
                "title": "Generative AI Misuse Potential in Cyber Security Education: A Case Study\n  of a UK Degree Program",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI Misuse Potential in Cyber Security Education: A Case Study\n  of a UK Degree Program"
                },
                "summary": "Recent advances in generative artificial intelligence (AI), such as ChatGPT,\nGoogle Gemini, and other large language models (LLMs), pose significant\nchallenges to upholding academic integrity in higher education. This paper\ninvestigates the susceptibility of a Master's-level cyber security degree\nprogram at a UK Russell Group university, accredited by a leading national\nbody, to LLM misuse. Through the application and extension of a quantitative\nassessment framework, we identify a high exposure to misuse, particularly in\nindependent project- and report-based assessments. Contributing factors,\nincluding block teaching and a predominantly international cohort, are\nhighlighted as potential amplifiers of these vulnerabilities. To address these\nchallenges, we discuss the adoption of LLM-resistant assessments, detection\ntools, and the importance of fostering an ethical learning environment. These\napproaches aim to uphold academic standards while preparing students for the\ncomplexities of real-world cyber security.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in generative artificial intelligence (AI), such as ChatGPT,\nGoogle Gemini, and other large language models (LLMs), pose significant\nchallenges to upholding academic integrity in higher education. This paper\ninvestigates the susceptibility of a Master's-level cyber security degree\nprogram at a UK Russell Group university, accredited by a leading national\nbody, to LLM misuse. Through the application and extension of a quantitative\nassessment framework, we identify a high exposure to misuse, particularly in\nindependent project- and report-based assessments. Contributing factors,\nincluding block teaching and a predominantly international cohort, are\nhighlighted as potential amplifiers of these vulnerabilities. To address these\nchallenges, we discuss the adoption of LLM-resistant assessments, detection\ntools, and the importance of fostering an ethical learning environment. These\napproaches aim to uphold academic standards while preparing students for the\ncomplexities of real-world cyber security."
                },
                "authors": [
                    {
                        "name": "Carlton Shepherd"
                    }
                ],
                "author_detail": {
                    "name": "Carlton Shepherd"
                },
                "author": "Carlton Shepherd",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12883v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12883v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09957v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09957v2",
                "updated": "2025-01-22T13:42:26Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    13,
                    42,
                    26,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-17T05:19:14Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    5,
                    19,
                    14,
                    4,
                    17,
                    0
                ],
                "title": "FRAG: A Flexible Modular Framework for Retrieval-Augmented Generation\n  based on Knowledge Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FRAG: A Flexible Modular Framework for Retrieval-Augmented Generation\n  based on Knowledge Graphs"
                },
                "summary": "To mitigate the hallucination and knowledge deficiency in large language\nmodels (LLMs), Knowledge Graph (KG)-based Retrieval-Augmented Generation (RAG)\nhas shown promising potential by utilizing KGs as external resource to enhance\nLLMs reasoning. However, existing KG-RAG approaches struggle with a trade-off\nbetween flexibility and retrieval quality. Modular methods prioritize\nflexibility by avoiding the use of KG-fine-tuned models during retrieval,\nleading to fixed retrieval strategies and suboptimal retrieval quality.\nConversely, coupled methods embed KG information within models to improve\nretrieval quality, but at the expense of flexibility. In this paper, we propose\na novel flexible modular KG-RAG framework, termed FRAG, which synergizes the\nadvantages of both approaches. FRAG estimates the hop range of reasoning paths\nbased solely on the query and classify it as either simple or complex. To match\nthe complexity of the query, tailored pipelines are applied to ensure efficient\nand accurate reasoning path retrieval, thus fostering the final reasoning\nprocess. By using the query text instead of the KG to infer the structural\ninformation of reasoning paths and employing adaptable retrieval strategies,\nFRAG improves retrieval quality while maintaining flexibility. Moreover, FRAG\ndoes not require extra LLMs fine-tuning or calls, significantly boosting\nefficiency and conserving resources. Extensive experiments show that FRAG\nachieves state-of-the-art performance with high efficiency and low resource\nconsumption.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To mitigate the hallucination and knowledge deficiency in large language\nmodels (LLMs), Knowledge Graph (KG)-based Retrieval-Augmented Generation (RAG)\nhas shown promising potential by utilizing KGs as external resource to enhance\nLLMs reasoning. However, existing KG-RAG approaches struggle with a trade-off\nbetween flexibility and retrieval quality. Modular methods prioritize\nflexibility by avoiding the use of KG-fine-tuned models during retrieval,\nleading to fixed retrieval strategies and suboptimal retrieval quality.\nConversely, coupled methods embed KG information within models to improve\nretrieval quality, but at the expense of flexibility. In this paper, we propose\na novel flexible modular KG-RAG framework, termed FRAG, which synergizes the\nadvantages of both approaches. FRAG estimates the hop range of reasoning paths\nbased solely on the query and classify it as either simple or complex. To match\nthe complexity of the query, tailored pipelines are applied to ensure efficient\nand accurate reasoning path retrieval, thus fostering the final reasoning\nprocess. By using the query text instead of the KG to infer the structural\ninformation of reasoning paths and employing adaptable retrieval strategies,\nFRAG improves retrieval quality while maintaining flexibility. Moreover, FRAG\ndoes not require extra LLMs fine-tuning or calls, significantly boosting\nefficiency and conserving resources. Extensive experiments show that FRAG\nachieves state-of-the-art performance with high efficiency and low resource\nconsumption."
                },
                "authors": [
                    {
                        "name": "Zengyi Gao"
                    },
                    {
                        "name": "Yukun Cao"
                    },
                    {
                        "name": "Hairu Wang"
                    },
                    {
                        "name": "Ao Ke"
                    },
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Xike Xie"
                    },
                    {
                        "name": "S Kevin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "S Kevin Zhou"
                },
                "author": "S Kevin Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09957v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09957v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12878v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12878v1",
                "updated": "2025-01-22T13:38:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    13,
                    38,
                    49,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T13:38:49Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    13,
                    38,
                    49,
                    2,
                    22,
                    0
                ],
                "title": "$μ$OpTime: Statically Reducing the Execution Time of Microbenchmark\n  Suites Using Stability Metrics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "$μ$OpTime: Statically Reducing the Execution Time of Microbenchmark\n  Suites Using Stability Metrics"
                },
                "summary": "Performance regressions have a tremendous impact on the quality of software.\nOne way to catch regressions before they reach production is executing\nperformance tests before deployment, e.g., using microbenchmarks, which measure\nperformance at subroutine level. In projects with many microbenchmarks, this\nmay take several hours due to repeated execution to get accurate results,\ndisqualifying them from frequent use in CI/CD pipelines.\n  We propose $\\mu$OpTime, a static approach to reduce the execution time of\nmicrobenchmark suites by configuring the number of repetitions for each\nmicrobenchmark. Based on the results of a full, previous microbenchmark suite\nrun, $\\mu$OpTime determines the minimal number of (measurement) repetitions\nwith statistical stability metrics that still lead to accurate results.\n  We evaluate $\\mu$OpTime with an experimental study on 14 open-source projects\nwritten in two programming languages and five stability metrics. Our results\nshow that (i) $\\mu$OpTime reduces the total suite execution time (measurement\nphase) by up to 95.83% (Go) and 94.17% (Java), (ii) the choice of stability\nmetric depends on the project and programming language, (iii) microbenchmark\nwarmup phases have to be considered for Java projects (potentially leading to\nhigher reductions), and (iv) $\\mu$OpTime can be used to reliably detect\nperformance regressions in CI/CD pipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance regressions have a tremendous impact on the quality of software.\nOne way to catch regressions before they reach production is executing\nperformance tests before deployment, e.g., using microbenchmarks, which measure\nperformance at subroutine level. In projects with many microbenchmarks, this\nmay take several hours due to repeated execution to get accurate results,\ndisqualifying them from frequent use in CI/CD pipelines.\n  We propose $\\mu$OpTime, a static approach to reduce the execution time of\nmicrobenchmark suites by configuring the number of repetitions for each\nmicrobenchmark. Based on the results of a full, previous microbenchmark suite\nrun, $\\mu$OpTime determines the minimal number of (measurement) repetitions\nwith statistical stability metrics that still lead to accurate results.\n  We evaluate $\\mu$OpTime with an experimental study on 14 open-source projects\nwritten in two programming languages and five stability metrics. Our results\nshow that (i) $\\mu$OpTime reduces the total suite execution time (measurement\nphase) by up to 95.83% (Go) and 94.17% (Java), (ii) the choice of stability\nmetric depends on the project and programming language, (iii) microbenchmark\nwarmup phases have to be considered for Java projects (potentially leading to\nhigher reductions), and (iv) $\\mu$OpTime can be used to reliably detect\nperformance regressions in CI/CD pipelines."
                },
                "authors": [
                    {
                        "name": "Nils Japke"
                    },
                    {
                        "name": "Martin Grambow"
                    },
                    {
                        "name": "Christoph Laaber"
                    },
                    {
                        "name": "David Bermbach"
                    }
                ],
                "author_detail": {
                    "name": "David Bermbach"
                },
                "author": "David Bermbach",
                "arxiv_comment": "Accepted for publication in ACM Transactions on Software Engineering\n  and Methodology",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12878v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12878v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12877v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12877v1",
                "updated": "2025-01-22T13:36:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    13,
                    36,
                    46,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T13:36:46Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    13,
                    36,
                    46,
                    2,
                    22,
                    0
                ],
                "title": "WisdomBot: Tuning Large Language Models with Artificial Intelligence\n  Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WisdomBot: Tuning Large Language Models with Artificial Intelligence\n  Knowledge"
                },
                "summary": "Large language models (LLMs) have emerged as powerful tools in natural\nlanguage processing (NLP), showing a promising future of artificial generated\nintelligence (AGI). Despite their notable performance in the general domain,\nLLMs have remained suboptimal in the field of education, owing to the unique\nchallenges presented by this domain, such as the need for more specialized\nknowledge, the requirement for personalized learning experiences, and the\nnecessity for concise explanations of complex concepts. To address these\nissues, this paper presents a novel LLM for education named WisdomBot, which\ncombines the power of LLMs with educational theories, enabling their seamless\nintegration into educational contexts. To be specific, we harness\nself-instructed knowledge concepts and instructions under the guidance of\nBloom's Taxonomy as training data. To further enhance the accuracy and\nprofessionalism of model's response on factual questions, we introduce two key\nenhancements during inference, i.e., local knowledge base retrieval\naugmentation and search engine retrieval augmentation during inference. We\nsubstantiate the effectiveness of our approach by applying it to several\nChinese LLMs, thereby showcasing that the fine-tuned models can generate more\nreliable and professional responses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have emerged as powerful tools in natural\nlanguage processing (NLP), showing a promising future of artificial generated\nintelligence (AGI). Despite their notable performance in the general domain,\nLLMs have remained suboptimal in the field of education, owing to the unique\nchallenges presented by this domain, such as the need for more specialized\nknowledge, the requirement for personalized learning experiences, and the\nnecessity for concise explanations of complex concepts. To address these\nissues, this paper presents a novel LLM for education named WisdomBot, which\ncombines the power of LLMs with educational theories, enabling their seamless\nintegration into educational contexts. To be specific, we harness\nself-instructed knowledge concepts and instructions under the guidance of\nBloom's Taxonomy as training data. To further enhance the accuracy and\nprofessionalism of model's response on factual questions, we introduce two key\nenhancements during inference, i.e., local knowledge base retrieval\naugmentation and search engine retrieval augmentation during inference. We\nsubstantiate the effectiveness of our approach by applying it to several\nChinese LLMs, thereby showcasing that the fine-tuned models can generate more\nreliable and professional responses."
                },
                "authors": [
                    {
                        "name": "Jingyuan Chen"
                    },
                    {
                        "name": "Tao Wu"
                    },
                    {
                        "name": "Wei Ji"
                    },
                    {
                        "name": "Fei Wu"
                    }
                ],
                "author_detail": {
                    "name": "Fei Wu"
                },
                "author": "Fei Wu",
                "arxiv_comment": "Frontiers of Digital Education",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12877v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12877v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03948v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03948v2",
                "updated": "2025-01-22T13:35:26Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    13,
                    35,
                    26,
                    2,
                    22,
                    0
                ],
                "published": "2024-11-06T14:29:49Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    14,
                    29,
                    49,
                    2,
                    311,
                    0
                ],
                "title": "Long-Form Text-to-Music Generation with Adaptive Prompts: A Case of\n  Study in Tabletop Role-Playing Games Soundtracks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-Form Text-to-Music Generation with Adaptive Prompts: A Case of\n  Study in Tabletop Role-Playing Games Soundtracks"
                },
                "summary": "This paper investigates the capabilities of text-to-audio music generation\nmodels in producing long-form music with prompts that change over time,\nfocusing on soundtrack generation for Tabletop Role-Playing Games (TRPGs). We\nintroduce Babel Bardo, a system that uses Large Language Models (LLMs) to\ntransform speech transcriptions into music descriptions for controlling a\ntext-to-music model. Four versions of Babel Bardo were compared in two TRPG\ncampaigns: a baseline using direct speech transcriptions, and three LLM-based\nversions with varying approaches to music description generation. Evaluations\nconsidered audio quality, story alignment, and transition smoothness. Results\nindicate that detailed music descriptions improve audio quality while\nmaintaining consistency across consecutive descriptions enhances story\nalignment and transition smoothness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the capabilities of text-to-audio music generation\nmodels in producing long-form music with prompts that change over time,\nfocusing on soundtrack generation for Tabletop Role-Playing Games (TRPGs). We\nintroduce Babel Bardo, a system that uses Large Language Models (LLMs) to\ntransform speech transcriptions into music descriptions for controlling a\ntext-to-music model. Four versions of Babel Bardo were compared in two TRPG\ncampaigns: a baseline using direct speech transcriptions, and three LLM-based\nversions with varying approaches to music description generation. Evaluations\nconsidered audio quality, story alignment, and transition smoothness. Results\nindicate that detailed music descriptions improve audio quality while\nmaintaining consistency across consecutive descriptions enhances story\nalignment and transition smoothness."
                },
                "authors": [
                    {
                        "name": "Felipe Marra"
                    },
                    {
                        "name": "Lucas N. Ferreira"
                    }
                ],
                "author_detail": {
                    "name": "Lucas N. Ferreira"
                },
                "author": "Lucas N. Ferreira",
                "arxiv_comment": "Paper accepted at the LAMIR 2024 workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03948v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03948v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11885v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11885v2",
                "updated": "2025-01-22T13:32:29Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    13,
                    32,
                    29,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-21T04:40:43Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    4,
                    40,
                    43,
                    1,
                    21,
                    0
                ],
                "title": "Med-R$^2$: Crafting Trustworthy LLM Physicians through Retrieval and\n  Reasoning of Evidence-Based Medicine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Med-R$^2$: Crafting Trustworthy LLM Physicians through Retrieval and\n  Reasoning of Evidence-Based Medicine"
                },
                "summary": "In recent years, Large Language Models (LLMs) have exhibited remarkable\ncapabilities in clinical scenarios. However, despite their potential, existing\nworks face challenges when applying LLMs to medical settings. Strategies\nrelying on training with medical datasets are highly cost-intensive and may\nsuffer from outdated training data. Leveraging external knowledge bases is a\nsuitable alternative, yet it faces obstacles such as limited retrieval\nprecision and poor effectiveness in answer extraction. These issues\ncollectively prevent LLMs from demonstrating the expected level of proficiency\nin mastering medical expertise. To address these challenges, we introduce\nMed-R^2, a novel LLM physician framework that adheres to the Evidence-Based\nMedicine (EBM) process, efficiently integrating retrieval mechanisms as well as\nthe selection and reasoning processes of evidence, thereby enhancing the\nproblem-solving capabilities of LLMs in healthcare scenarios and fostering a\ntrustworthy LLM physician. Our comprehensive experiments indicate that Med-R^2\nachieves a 14.87\\% improvement over vanilla RAG methods and even a 3.59\\%\nenhancement compared to fine-tuning strategies, without incurring additional\ntraining costs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Large Language Models (LLMs) have exhibited remarkable\ncapabilities in clinical scenarios. However, despite their potential, existing\nworks face challenges when applying LLMs to medical settings. Strategies\nrelying on training with medical datasets are highly cost-intensive and may\nsuffer from outdated training data. Leveraging external knowledge bases is a\nsuitable alternative, yet it faces obstacles such as limited retrieval\nprecision and poor effectiveness in answer extraction. These issues\ncollectively prevent LLMs from demonstrating the expected level of proficiency\nin mastering medical expertise. To address these challenges, we introduce\nMed-R^2, a novel LLM physician framework that adheres to the Evidence-Based\nMedicine (EBM) process, efficiently integrating retrieval mechanisms as well as\nthe selection and reasoning processes of evidence, thereby enhancing the\nproblem-solving capabilities of LLMs in healthcare scenarios and fostering a\ntrustworthy LLM physician. Our comprehensive experiments indicate that Med-R^2\nachieves a 14.87\\% improvement over vanilla RAG methods and even a 3.59\\%\nenhancement compared to fine-tuning strategies, without incurring additional\ntraining costs."
                },
                "authors": [
                    {
                        "name": "Keer Lu"
                    },
                    {
                        "name": "Zheng Liang"
                    },
                    {
                        "name": "Da Pan"
                    },
                    {
                        "name": "Shusen Zhang"
                    },
                    {
                        "name": "Xin Wu"
                    },
                    {
                        "name": "Weipeng Chen"
                    },
                    {
                        "name": "Zenan Zhou"
                    },
                    {
                        "name": "Guosheng Dong"
                    },
                    {
                        "name": "Bin Cui"
                    },
                    {
                        "name": "Wentao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wentao Zhang"
                },
                "author": "Wentao Zhang",
                "arxiv_comment": "We need to add some experiments",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11885v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11885v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12862v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12862v1",
                "updated": "2025-01-22T13:14:02Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    13,
                    14,
                    2,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T13:14:02Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    13,
                    14,
                    2,
                    2,
                    22,
                    0
                ],
                "title": "Mutation-Guided LLM-based Test Generation at Meta",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mutation-Guided LLM-based Test Generation at Meta"
                },
                "summary": "This paper describes Meta's ACH system for mutation-guided LLM-based test\ngeneration. ACH generates relatively few mutants (aka simulated faults),\ncompared to traditional mutation testing. Instead, it focuses on generating\ncurrently undetected faults that are specific to an issue of concern. From\nthese currently uncaught faults, ACH generates tests that can catch them,\nthereby `killing' the mutants and consequently hardening the platform against\nregressions. We use privacy concerns to illustrate our approach, but ACH can\nharden code against {\\em any} type of regression. In total, ACH was applied to\n10,795 Android Kotlin classes in 7 software platforms deployed by Meta, from\nwhich it generated 9,095 mutants and 571 privacy-hardening test cases. ACH also\ndeploys an LLM-based equivalent mutant detection agent that achieves a\nprecision of 0.79 and a recall of 0.47 (rising to 0.95 and 0.96 with simple\npre-processing). ACH was used by Messenger and WhatsApp test-a-thons where\nengineers accepted 73% of its tests, judging 36% to privacy relevant. We\nconclude that ACH hardens code against specific concerns and that, even when\nits tests do not directly tackle the specific concern, engineers find them\nuseful for their other benefits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper describes Meta's ACH system for mutation-guided LLM-based test\ngeneration. ACH generates relatively few mutants (aka simulated faults),\ncompared to traditional mutation testing. Instead, it focuses on generating\ncurrently undetected faults that are specific to an issue of concern. From\nthese currently uncaught faults, ACH generates tests that can catch them,\nthereby `killing' the mutants and consequently hardening the platform against\nregressions. We use privacy concerns to illustrate our approach, but ACH can\nharden code against {\\em any} type of regression. In total, ACH was applied to\n10,795 Android Kotlin classes in 7 software platforms deployed by Meta, from\nwhich it generated 9,095 mutants and 571 privacy-hardening test cases. ACH also\ndeploys an LLM-based equivalent mutant detection agent that achieves a\nprecision of 0.79 and a recall of 0.47 (rising to 0.95 and 0.96 with simple\npre-processing). ACH was used by Messenger and WhatsApp test-a-thons where\nengineers accepted 73% of its tests, judging 36% to privacy relevant. We\nconclude that ACH hardens code against specific concerns and that, even when\nits tests do not directly tackle the specific concern, engineers find them\nuseful for their other benefits."
                },
                "authors": [
                    {
                        "name": "Christopher Foster"
                    },
                    {
                        "name": "Abhishek Gulati"
                    },
                    {
                        "name": "Mark Harman"
                    },
                    {
                        "name": "Inna Harper"
                    },
                    {
                        "name": "Ke Mao"
                    },
                    {
                        "name": "Jillian Ritchey"
                    },
                    {
                        "name": "Hervé Robert"
                    },
                    {
                        "name": "Shubho Sengupta"
                    }
                ],
                "author_detail": {
                    "name": "Shubho Sengupta"
                },
                "author": "Shubho Sengupta",
                "arxiv_comment": "Submitted to FSE 2025 Industry Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12862v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12862v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12851v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12851v1",
                "updated": "2025-01-22T12:59:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    12,
                    59,
                    8,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T12:59:08Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    12,
                    59,
                    8,
                    2,
                    22,
                    0
                ],
                "title": "ACEBench: Who Wins the Match Point in Tool Learning?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ACEBench: Who Wins the Match Point in Tool Learning?"
                },
                "summary": "Large language models (LLMs) have demonstrated significant potential in\ndecision-making and reasoning, especially when combined with various tools to\neffectively solve complex problems. However, existing evaluation systems for\nassessing LLM function calling capabilities have several limitations: (1)\nlimited evaluation scenarios, lacking assessments in real multi-turn dialogue\ncontexts; (2) narrow evaluation dimensions, lacking detailed assessments for\nfine-grained function calls; (3) relying on LLMs or real API executions for\nresult evaluation, which introduces significant overhead. To address these\nissues, we propose a comprehensive evaluation system named ACEBench. This\nsystem is meticulously designed to encompass a wide spectrum of function\ncalling scenarios. Moreover, it categorizes these scenarios into three primary\ntypes according to the evaluation methodology: Normal, Special, and Agent.\nNormal evaluates function calls in basic scenarios; Special evaluates function\ncalls in scenarios with vague or incomplete instructions; Agent introduces\nmulti-agent interactions to simulate function calling evaluation in real-world\nmulti-turn interactions. We conducted extensive experiments on ACEBench,\nanalyzing various LLMs in-depth and performing a more granular analysis of\nerror causes across different data types.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated significant potential in\ndecision-making and reasoning, especially when combined with various tools to\neffectively solve complex problems. However, existing evaluation systems for\nassessing LLM function calling capabilities have several limitations: (1)\nlimited evaluation scenarios, lacking assessments in real multi-turn dialogue\ncontexts; (2) narrow evaluation dimensions, lacking detailed assessments for\nfine-grained function calls; (3) relying on LLMs or real API executions for\nresult evaluation, which introduces significant overhead. To address these\nissues, we propose a comprehensive evaluation system named ACEBench. This\nsystem is meticulously designed to encompass a wide spectrum of function\ncalling scenarios. Moreover, it categorizes these scenarios into three primary\ntypes according to the evaluation methodology: Normal, Special, and Agent.\nNormal evaluates function calls in basic scenarios; Special evaluates function\ncalls in scenarios with vague or incomplete instructions; Agent introduces\nmulti-agent interactions to simulate function calling evaluation in real-world\nmulti-turn interactions. We conducted extensive experiments on ACEBench,\nanalyzing various LLMs in-depth and performing a more granular analysis of\nerror causes across different data types."
                },
                "authors": [
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Xinlong Hao"
                    },
                    {
                        "name": "Weiwen Liu"
                    },
                    {
                        "name": "Xu Huang"
                    },
                    {
                        "name": "Xingshan Zeng"
                    },
                    {
                        "name": "Shuai Yu"
                    },
                    {
                        "name": "Dexun Li"
                    },
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Weinan Gan"
                    },
                    {
                        "name": "Yuefeng Huang"
                    },
                    {
                        "name": "Xinzhi Wang"
                    },
                    {
                        "name": "Defu Lian"
                    },
                    {
                        "name": "Baoqun Yin"
                    },
                    {
                        "name": "Yasheng Wang"
                    },
                    {
                        "name": "Wu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Wu Liu"
                },
                "author": "Wu Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12851v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12851v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12835v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12835v1",
                "updated": "2025-01-22T12:21:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    12,
                    21,
                    17,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T12:21:17Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    12,
                    21,
                    17,
                    2,
                    22,
                    0
                ],
                "title": "Adaptive Retrieval Without Self-Knowledge? Bringing Uncertainty Back\n  Home",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Retrieval Without Self-Knowledge? Bringing Uncertainty Back\n  Home"
                },
                "summary": "Retrieval Augmented Generation (RAG) improves correctness of Question\nAnswering (QA) and addresses hallucinations in Large Language Models (LLMs),\nyet greatly increase computational costs. Besides, RAG is not always needed as\nmay introduce irrelevant information. Recent adaptive retrieval methods\nintegrate LLMs' intrinsic knowledge with external information appealing to LLM\nself-knowledge, but they often neglect efficiency evaluations and comparisons\nwith uncertainty estimation techniques. We bridge this gap by conducting a\ncomprehensive analysis of 35 adaptive retrieval methods, including 8 recent\napproaches and 27 uncertainty estimation techniques, across 6 datasets using 10\nmetrics for QA performance, self-knowledge, and efficiency. Our findings show\nthat uncertainty estimation techniques often outperform complex pipelines in\nterms of efficiency and self-knowledge, while maintaining comparable QA\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval Augmented Generation (RAG) improves correctness of Question\nAnswering (QA) and addresses hallucinations in Large Language Models (LLMs),\nyet greatly increase computational costs. Besides, RAG is not always needed as\nmay introduce irrelevant information. Recent adaptive retrieval methods\nintegrate LLMs' intrinsic knowledge with external information appealing to LLM\nself-knowledge, but they often neglect efficiency evaluations and comparisons\nwith uncertainty estimation techniques. We bridge this gap by conducting a\ncomprehensive analysis of 35 adaptive retrieval methods, including 8 recent\napproaches and 27 uncertainty estimation techniques, across 6 datasets using 10\nmetrics for QA performance, self-knowledge, and efficiency. Our findings show\nthat uncertainty estimation techniques often outperform complex pipelines in\nterms of efficiency and self-knowledge, while maintaining comparable QA\nperformance."
                },
                "authors": [
                    {
                        "name": "Viktor Moskvoretskii"
                    },
                    {
                        "name": "Maria Lysyuk"
                    },
                    {
                        "name": "Mikhail Salnikov"
                    },
                    {
                        "name": "Nikolay Ivanov"
                    },
                    {
                        "name": "Sergey Pletenev"
                    },
                    {
                        "name": "Daria Galimzianova"
                    },
                    {
                        "name": "Nikita Krayko"
                    },
                    {
                        "name": "Vasily Konovalov"
                    },
                    {
                        "name": "Irina Nikishina"
                    },
                    {
                        "name": "Alexander Panchenko"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Panchenko"
                },
                "author": "Alexander Panchenko",
                "arxiv_comment": "The code and data will be published soon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12835v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12835v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12826v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12826v1",
                "updated": "2025-01-22T12:06:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    12,
                    6,
                    16,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T12:06:16Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    12,
                    6,
                    16,
                    2,
                    22,
                    0
                ],
                "title": "Open or Closed LLM for Lesser-Resourced Languages? Lessons from Greek",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open or Closed LLM for Lesser-Resourced Languages? Lessons from Greek"
                },
                "summary": "Natural Language Processing (NLP) for lesser-resourced languages faces\npersistent challenges, including limited datasets, inherited biases from\nhigh-resource languages, and the need for domain-specific solutions. This study\naddresses these gaps for Modern Greek through three key contributions. First,\nwe evaluate the performance of open-source (Llama-70b) and closed-source\n(GPT-4o mini) large language models (LLMs) on seven core NLP tasks with dataset\navailability, revealing task-specific strengths, weaknesses, and parity in\ntheir performance. Second, we expand the scope of Greek NLP by reframing\nAuthorship Attribution as a tool to assess potential data usage by LLMs in\npre-training, with high 0-shot accuracy suggesting ethical implications for\ndata provenance. Third, we showcase a legal NLP case study, where a Summarize,\nTranslate, and Embed (STE) methodology outperforms the traditional TF-IDF\napproach for clustering \\emph{long} legal texts. Together, these contributions\nprovide a roadmap to advance NLP in lesser-resourced languages, bridging gaps\nin model evaluation, task innovation, and real-world impact.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural Language Processing (NLP) for lesser-resourced languages faces\npersistent challenges, including limited datasets, inherited biases from\nhigh-resource languages, and the need for domain-specific solutions. This study\naddresses these gaps for Modern Greek through three key contributions. First,\nwe evaluate the performance of open-source (Llama-70b) and closed-source\n(GPT-4o mini) large language models (LLMs) on seven core NLP tasks with dataset\navailability, revealing task-specific strengths, weaknesses, and parity in\ntheir performance. Second, we expand the scope of Greek NLP by reframing\nAuthorship Attribution as a tool to assess potential data usage by LLMs in\npre-training, with high 0-shot accuracy suggesting ethical implications for\ndata provenance. Third, we showcase a legal NLP case study, where a Summarize,\nTranslate, and Embed (STE) methodology outperforms the traditional TF-IDF\napproach for clustering \\emph{long} legal texts. Together, these contributions\nprovide a roadmap to advance NLP in lesser-resourced languages, bridging gaps\nin model evaluation, task innovation, and real-world impact."
                },
                "authors": [
                    {
                        "name": "John Pavlopoulos"
                    },
                    {
                        "name": "Juli Bakagianni"
                    },
                    {
                        "name": "Kanella Pouli"
                    },
                    {
                        "name": "Maria Gavriilidou"
                    }
                ],
                "author_detail": {
                    "name": "Maria Gavriilidou"
                },
                "author": "Maria Gavriilidou",
                "arxiv_comment": "NLP, Modern Greek, benchmark, machine learning, language resources",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12826v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12826v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02518v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02518v2",
                "updated": "2025-01-22T11:49:44Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    11,
                    49,
                    44,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-05T12:15:02Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    12,
                    15,
                    2,
                    6,
                    5,
                    0
                ],
                "title": "CHAIR -- Classifier of Hallucination as Improver",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CHAIR -- Classifier of Hallucination as Improver"
                },
                "summary": "In this work, we introduce CHAIR (Classifier of Hallucination As ImproveR), a\nsupervised framework for detecting hallucinations by analyzing internal logits\nfrom each layer of every token. Our method extracts a compact set of features\nsuch as maximum, minimum, mean, standard deviation, and slope-from the token\nlogits across all layers, enabling effective hallucination detection without\noverfitting. Experiments on TruthfulQA and MMLU datasets demonstrate that CHAIR\nsignificantly improves detection accuracy, particularly in zero-shot scenarios,\nshowcasing its robustness and generalizability. Beyond hallucination detection,\nCHAIR highlights the potential of using internal representations for designing\nadvanced decoding strategies. By leveraging patterns in logits, we suggest that\nmore sophisticated models and adaptive decoding methods could further reduce\nhallucinations and enhance text completion quality. CHAIR not only offers a\npractical solution for detecting hallucinations but also lays the groundwork\nfor exploring richer representations in LLMs to improve their factuality and\ncoherence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we introduce CHAIR (Classifier of Hallucination As ImproveR), a\nsupervised framework for detecting hallucinations by analyzing internal logits\nfrom each layer of every token. Our method extracts a compact set of features\nsuch as maximum, minimum, mean, standard deviation, and slope-from the token\nlogits across all layers, enabling effective hallucination detection without\noverfitting. Experiments on TruthfulQA and MMLU datasets demonstrate that CHAIR\nsignificantly improves detection accuracy, particularly in zero-shot scenarios,\nshowcasing its robustness and generalizability. Beyond hallucination detection,\nCHAIR highlights the potential of using internal representations for designing\nadvanced decoding strategies. By leveraging patterns in logits, we suggest that\nmore sophisticated models and adaptive decoding methods could further reduce\nhallucinations and enhance text completion quality. CHAIR not only offers a\npractical solution for detecting hallucinations but also lays the groundwork\nfor exploring richer representations in LLMs to improve their factuality and\ncoherence."
                },
                "authors": [
                    {
                        "name": "Ao Sun"
                    }
                ],
                "author_detail": {
                    "name": "Ao Sun"
                },
                "author": "Ao Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02518v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02518v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.18373v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.18373v3",
                "updated": "2025-01-22T11:17:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    11,
                    17,
                    23,
                    2,
                    22,
                    0
                ],
                "published": "2024-04-29T02:23:53Z",
                "published_parsed": [
                    2024,
                    4,
                    29,
                    2,
                    23,
                    53,
                    0,
                    120,
                    0
                ],
                "title": "6G comprehensive intelligence: network operations and optimization based\n  on Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "6G comprehensive intelligence: network operations and optimization based\n  on Large Language Models"
                },
                "summary": "The sixth generation mobile communication standard (6G) can promote the\ndevelopment of Industrial Internet and Internet of Things (IoT). To achieve\ncomprehensive intelligent development of the network and provide customers with\nhigher quality personalized services. This paper proposes a network performance\noptimization and intelligent operation network architecture based on Large\nLanguage Model (LLM), aiming to build a comprehensive intelligent 6G network\nsystem. The Large Language Model, with more parameters and stronger learning\nability, can more accurately capture patterns and features in data, which can\nachieve more accurate content output and high intelligence and provide strong\nsupport for related research such as network data security, privacy protection,\nand health assessment. This paper also presents the design framework of a\nnetwork health assessment system based on LLM and focuses on its potential\napplication value, through the case of network health management system, it is\nfully demonstrated that the 6G intelligent network system based on LLM has\nimportant practical significance for the comprehensive realization of\nintelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The sixth generation mobile communication standard (6G) can promote the\ndevelopment of Industrial Internet and Internet of Things (IoT). To achieve\ncomprehensive intelligent development of the network and provide customers with\nhigher quality personalized services. This paper proposes a network performance\noptimization and intelligent operation network architecture based on Large\nLanguage Model (LLM), aiming to build a comprehensive intelligent 6G network\nsystem. The Large Language Model, with more parameters and stronger learning\nability, can more accurately capture patterns and features in data, which can\nachieve more accurate content output and high intelligence and provide strong\nsupport for related research such as network data security, privacy protection,\nand health assessment. This paper also presents the design framework of a\nnetwork health assessment system based on LLM and focuses on its potential\napplication value, through the case of network health management system, it is\nfully demonstrated that the 6G intelligent network system based on LLM has\nimportant practical significance for the comprehensive realization of\nintelligence."
                },
                "authors": [
                    {
                        "name": "Sifan Long"
                    },
                    {
                        "name": "Fengxiao Tang"
                    },
                    {
                        "name": "Yangfan Li"
                    },
                    {
                        "name": "Tiao Tan"
                    },
                    {
                        "name": "Zhengjie Jin"
                    },
                    {
                        "name": "Ming Zhao"
                    },
                    {
                        "name": "Nei Kato"
                    }
                ],
                "author_detail": {
                    "name": "Nei Kato"
                },
                "author": "Nei Kato",
                "arxiv_doi": "10.1109/MNET.2024.3470774",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/MNET.2024.3470774",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2404.18373v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.18373v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This paper has been published on the IEEE network, and the relevant\n  information is as follows: 8 pages, 5 figures, 15 preferences,",
                "arxiv_journal_ref": "IEEE Network (2024)",
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "94-03",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12793v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12793v1",
                "updated": "2025-01-22T10:54:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    10,
                    54,
                    19,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T10:54:19Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    10,
                    54,
                    19,
                    2,
                    22,
                    0
                ],
                "title": "Revisit Self-Debugging with Self-Generated Tests for Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisit Self-Debugging with Self-Generated Tests for Code Generation"
                },
                "summary": "Large language models (LLMs) have shown significant advancements in code\ngeneration, but still face challenges on tasks beyond their basic capabilities.\nRecently, the notion of self-debugging has been proposed to boost the\nperformance of code generation by leveraging execution feedback from tests.\nDespite its promise, the availability of high-quality tests in real-world\nscenarios is limited. In this context, self-debugging with self-generated tests\nis a promising solution but lacks a full exploration of its limitations and\npractical potential. Therefore, we investigate its efficacy on diverse\nprogramming problems. To deepen our understanding, we propose two distinct\nparadigms for the process: post-execution and in-execution self-debugging.\nWithin the scope of self-contained Python programming tasks, we find that\npost-execution self-debugging struggles on basic problems but shows potential\nfor improvement on competitive ones, due to the bias introduced by\nself-generated tests. On the other hand, in-execution self-debugging enables\nLLMs to mitigate the bias by solely leveraging intermediate states during\nexecution, thereby enhancing code generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown significant advancements in code\ngeneration, but still face challenges on tasks beyond their basic capabilities.\nRecently, the notion of self-debugging has been proposed to boost the\nperformance of code generation by leveraging execution feedback from tests.\nDespite its promise, the availability of high-quality tests in real-world\nscenarios is limited. In this context, self-debugging with self-generated tests\nis a promising solution but lacks a full exploration of its limitations and\npractical potential. Therefore, we investigate its efficacy on diverse\nprogramming problems. To deepen our understanding, we propose two distinct\nparadigms for the process: post-execution and in-execution self-debugging.\nWithin the scope of self-contained Python programming tasks, we find that\npost-execution self-debugging struggles on basic problems but shows potential\nfor improvement on competitive ones, due to the bias introduced by\nself-generated tests. On the other hand, in-execution self-debugging enables\nLLMs to mitigate the bias by solely leveraging intermediate states during\nexecution, thereby enhancing code generation."
                },
                "authors": [
                    {
                        "name": "Xiancai Chen"
                    },
                    {
                        "name": "Zhengwei Tao"
                    },
                    {
                        "name": "Kechi Zhang"
                    },
                    {
                        "name": "Changzhi Zhou"
                    },
                    {
                        "name": "Wanli Gu"
                    },
                    {
                        "name": "Yuanpeng He"
                    },
                    {
                        "name": "Mengdi Zhang"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Haiyan Zhao"
                    },
                    {
                        "name": "Zhi Jin"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Jin"
                },
                "author": "Zhi Jin",
                "arxiv_comment": "Work in Progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12793v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12793v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12789v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12789v1",
                "updated": "2025-01-22T10:47:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    10,
                    47,
                    8,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T10:47:08Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    10,
                    47,
                    8,
                    2,
                    22,
                    0
                ],
                "title": "Generating Diverse Q&A Benchmarks for RAG Evaluation with DataMorgana",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating Diverse Q&A Benchmarks for RAG Evaluation with DataMorgana"
                },
                "summary": "Evaluating Retrieval-Augmented Generation (RAG) systems, especially in\ndomain-specific contexts, requires benchmarks that address the distinctive\nrequirements of the applicative scenario. Since real data can be hard to\nobtain, a common strategy is to use LLM-based methods to generate synthetic\ndata. Existing solutions are general purpose: given a document, they generate a\nquestion to build a Q&A pair. However, although the generated questions can be\nindividually good, they are typically not diverse enough to reasonably cover\nthe different ways real end-users can interact with the RAG system. We\nintroduce here DataMorgana, a tool for generating highly customizable and\ndiverse synthetic Q&A benchmarks tailored to RAG applications. DataMorgana\nenables detailed configurations of user and question categories and provides\ncontrol over their distribution within the benchmark. It uses a lightweight\ntwo-stage process, ensuring efficiency and fast iterations, while generating\nbenchmarks that reflect the expected traffic. We conduct a thorough line of\nexperiments, showing quantitatively and qualitatively that DataMorgana\nsurpasses existing tools and approaches in producing lexically, syntactically,\nand semantically diverse question sets across domain-specific and\ngeneral-knowledge corpora. DataMorgana will be made available to selected teams\nin the research community, as first beta testers, in the context of the\nupcoming SIGIR'2025 LiveRAG challenge to be announced in early February 2025.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Retrieval-Augmented Generation (RAG) systems, especially in\ndomain-specific contexts, requires benchmarks that address the distinctive\nrequirements of the applicative scenario. Since real data can be hard to\nobtain, a common strategy is to use LLM-based methods to generate synthetic\ndata. Existing solutions are general purpose: given a document, they generate a\nquestion to build a Q&A pair. However, although the generated questions can be\nindividually good, they are typically not diverse enough to reasonably cover\nthe different ways real end-users can interact with the RAG system. We\nintroduce here DataMorgana, a tool for generating highly customizable and\ndiverse synthetic Q&A benchmarks tailored to RAG applications. DataMorgana\nenables detailed configurations of user and question categories and provides\ncontrol over their distribution within the benchmark. It uses a lightweight\ntwo-stage process, ensuring efficiency and fast iterations, while generating\nbenchmarks that reflect the expected traffic. We conduct a thorough line of\nexperiments, showing quantitatively and qualitatively that DataMorgana\nsurpasses existing tools and approaches in producing lexically, syntactically,\nand semantically diverse question sets across domain-specific and\ngeneral-knowledge corpora. DataMorgana will be made available to selected teams\nin the research community, as first beta testers, in the context of the\nupcoming SIGIR'2025 LiveRAG challenge to be announced in early February 2025."
                },
                "authors": [
                    {
                        "name": "Simone Filice"
                    },
                    {
                        "name": "Guy Horowitz"
                    },
                    {
                        "name": "David Carmel"
                    },
                    {
                        "name": "Zohar Karnin"
                    },
                    {
                        "name": "Liane Lewin-Eytan"
                    },
                    {
                        "name": "Yoelle Maarek"
                    }
                ],
                "author_detail": {
                    "name": "Yoelle Maarek"
                },
                "author": "Yoelle Maarek",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12789v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12789v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03940v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03940v2",
                "updated": "2025-01-22T10:39:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    10,
                    39,
                    50,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-07T17:00:49Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    0,
                    49,
                    1,
                    7,
                    0
                ],
                "title": "Not all tokens are created equal: Perplexity Attention Weighted Networks\n  for AI generated text detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not all tokens are created equal: Perplexity Attention Weighted Networks\n  for AI generated text detection"
                },
                "summary": "The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages."
                },
                "authors": [
                    {
                        "name": "Pablo Miralles-González"
                    },
                    {
                        "name": "Javier Huertas-Tato"
                    },
                    {
                        "name": "Alejandro Martín"
                    },
                    {
                        "name": "David Camacho"
                    }
                ],
                "author_detail": {
                    "name": "David Camacho"
                },
                "author": "David Camacho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03940v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03940v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12783v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12783v1",
                "updated": "2025-01-22T10:35:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    10,
                    35,
                    36,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T10:35:36Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    10,
                    35,
                    36,
                    2,
                    22,
                    0
                ],
                "title": "Cost Optimization for Serverless Edge Computing with Budget Constraints\n  using Deep Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cost Optimization for Serverless Edge Computing with Budget Constraints\n  using Deep Reinforcement Learning"
                },
                "summary": "Serverless computing adopts a pay-as-you-go billing model where applications\nare executed in stateless and shortlived containers triggered by events,\nresulting in a reduction of monetary costs and resource utilization. However,\nexisting platforms do not provide an upper bound for the billing model which\nmakes the overall cost unpredictable, precluding many organizations from\nmanaging their budgets. Due to the diverse ranges of serverless functions and\nthe heterogeneous capacity of edge devices, it is challenging to receive\nnear-optimal solutions for deployment cost in a polynomial time. In this paper,\nwe investigated the function scheduling problem with a budget constraint for\nserverless computing in wireless networks. Users and IoT devices are sending\nrequests to edge nodes, improving the latency perceived by users. We propose\ntwo online scheduling algorithms based on reinforcement learning, incorporating\nseveral important characteristics of serverless functions. Via extensive\nsimulations, we justify the superiority of the proposed algorithm by comparing\nwith an ILP solver (Midaco). Our results indicate that the proposed algorithms\nefficiently approximate the results of Midaco within a factor of 1.03 while our\ndecision-making time is 5 orders of magnitude less than that of Midaco.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless computing adopts a pay-as-you-go billing model where applications\nare executed in stateless and shortlived containers triggered by events,\nresulting in a reduction of monetary costs and resource utilization. However,\nexisting platforms do not provide an upper bound for the billing model which\nmakes the overall cost unpredictable, precluding many organizations from\nmanaging their budgets. Due to the diverse ranges of serverless functions and\nthe heterogeneous capacity of edge devices, it is challenging to receive\nnear-optimal solutions for deployment cost in a polynomial time. In this paper,\nwe investigated the function scheduling problem with a budget constraint for\nserverless computing in wireless networks. Users and IoT devices are sending\nrequests to edge nodes, improving the latency perceived by users. We propose\ntwo online scheduling algorithms based on reinforcement learning, incorporating\nseveral important characteristics of serverless functions. Via extensive\nsimulations, we justify the superiority of the proposed algorithm by comparing\nwith an ILP solver (Midaco). Our results indicate that the proposed algorithms\nefficiently approximate the results of Midaco within a factor of 1.03 while our\ndecision-making time is 5 orders of magnitude less than that of Midaco."
                },
                "authors": [
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Peiyuan Guan"
                    },
                    {
                        "name": "Ziru Chen"
                    },
                    {
                        "name": "Amir Taherkordi"
                    },
                    {
                        "name": "Fen Hou"
                    },
                    {
                        "name": "Lin X. Cai"
                    }
                ],
                "author_detail": {
                    "name": "Lin X. Cai"
                },
                "author": "Lin X. Cai",
                "arxiv_comment": "This paper has been accepted by IEEE ICC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12783v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12783v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12774v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12774v1",
                "updated": "2025-01-22T10:16:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    10,
                    16,
                    53,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T10:16:53Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    10,
                    16,
                    53,
                    2,
                    22,
                    0
                ],
                "title": "LLMs as Repositories of Factual Knowledge: Limitations and Solutions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs as Repositories of Factual Knowledge: Limitations and Solutions"
                },
                "summary": "LLMs' sources of knowledge are data snapshots containing factual information\nabout entities collected at different timestamps and from different media types\n(e.g. wikis, social media, etc.). Such unstructured knowledge is subject to\nchange due to updates through time from past to present. Equally important are\nthe inconsistencies and inaccuracies occurring in different information\nsources. Consequently, the model's knowledge about an entity may be perturbed\nwhile training over the sequence of snapshots or at inference time, resulting\nin inconsistent and inaccurate model performance. In this work, we study the\nappropriateness of Large Language Models (LLMs) as repositories of factual\nknowledge. We consider twenty-four state-of-the-art LLMs that are either\nclosed-, partially (weights), or fully (weight and training data) open-source.\nWe evaluate their reliability in responding to time-sensitive factual questions\nin terms of accuracy and consistency when prompts are perturbed. We further\nevaluate the effectiveness of state-of-the-art methods to improve LLMs'\naccuracy and consistency. We then propose \"ENtity-Aware Fine-tuning\" (ENAF), a\nsoft neurosymbolic approach aimed at providing a structured representation of\nentities during fine-tuning to improve the model's performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs' sources of knowledge are data snapshots containing factual information\nabout entities collected at different timestamps and from different media types\n(e.g. wikis, social media, etc.). Such unstructured knowledge is subject to\nchange due to updates through time from past to present. Equally important are\nthe inconsistencies and inaccuracies occurring in different information\nsources. Consequently, the model's knowledge about an entity may be perturbed\nwhile training over the sequence of snapshots or at inference time, resulting\nin inconsistent and inaccurate model performance. In this work, we study the\nappropriateness of Large Language Models (LLMs) as repositories of factual\nknowledge. We consider twenty-four state-of-the-art LLMs that are either\nclosed-, partially (weights), or fully (weight and training data) open-source.\nWe evaluate their reliability in responding to time-sensitive factual questions\nin terms of accuracy and consistency when prompts are perturbed. We further\nevaluate the effectiveness of state-of-the-art methods to improve LLMs'\naccuracy and consistency. We then propose \"ENtity-Aware Fine-tuning\" (ENAF), a\nsoft neurosymbolic approach aimed at providing a structured representation of\nentities during fine-tuning to improve the model's performance."
                },
                "authors": [
                    {
                        "name": "Seyed Mahed Mousavi"
                    },
                    {
                        "name": "Simone Alghisi"
                    },
                    {
                        "name": "Giuseppe Riccardi"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Riccardi"
                },
                "author": "Giuseppe Riccardi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12774v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12774v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12766v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12766v1",
                "updated": "2025-01-22T10:01:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    10,
                    1,
                    54,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T10:01:54Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    10,
                    1,
                    54,
                    2,
                    22,
                    0
                ],
                "title": "NExtLong: Toward Effective Long-Context Training without Long Documents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NExtLong: Toward Effective Long-Context Training without Long Documents"
                },
                "summary": "Large language models (LLMs) with extended context windows have made\nsignificant strides yet remain a challenge due to the scarcity of long\ndocuments. Existing methods tend to synthesize long-context data but lack a\nclear mechanism to reinforce the long-range dependency modeling. To address\nthis limitation, we propose NExtLong, a novel framework for synthesizing\nlong-context data through Negative document Extension. NExtLong decomposes a\ndocument into multiple meta-chunks and extends the context by interleaving hard\nnegative distractors retrieved from pretraining corpora. This approach compels\nthe model to discriminate long-range dependent context from distracting\ncontent, enhancing its ability to model long-range dependencies. Extensive\nexperiments demonstrate that NExtLong achieves significant performance\nimprovements on the HELMET and RULER benchmarks compared to existing\nlong-context synthesis approaches and leading models, which are trained on\nnon-synthetic long documents. These findings highlight NExtLong's ability to\nreduce reliance on non-synthetic long documents, making it an effective\nframework for developing advanced long-context LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with extended context windows have made\nsignificant strides yet remain a challenge due to the scarcity of long\ndocuments. Existing methods tend to synthesize long-context data but lack a\nclear mechanism to reinforce the long-range dependency modeling. To address\nthis limitation, we propose NExtLong, a novel framework for synthesizing\nlong-context data through Negative document Extension. NExtLong decomposes a\ndocument into multiple meta-chunks and extends the context by interleaving hard\nnegative distractors retrieved from pretraining corpora. This approach compels\nthe model to discriminate long-range dependent context from distracting\ncontent, enhancing its ability to model long-range dependencies. Extensive\nexperiments demonstrate that NExtLong achieves significant performance\nimprovements on the HELMET and RULER benchmarks compared to existing\nlong-context synthesis approaches and leading models, which are trained on\nnon-synthetic long documents. These findings highlight NExtLong's ability to\nreduce reliance on non-synthetic long documents, making it an effective\nframework for developing advanced long-context LLMs."
                },
                "authors": [
                    {
                        "name": "Chaochen Gao"
                    },
                    {
                        "name": "Xing Wu"
                    },
                    {
                        "name": "Zijia Lin"
                    },
                    {
                        "name": "Debing Zhang"
                    },
                    {
                        "name": "Songlin Hu"
                    }
                ],
                "author_detail": {
                    "name": "Songlin Hu"
                },
                "author": "Songlin Hu",
                "arxiv_comment": "Corresponding authors: xing wu, and songlin hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12766v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12766v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03866v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03866v2",
                "updated": "2025-01-22T09:48:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    9,
                    48,
                    34,
                    2,
                    22,
                    0
                ],
                "published": "2024-11-06T12:22:04Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    12,
                    22,
                    4,
                    2,
                    311,
                    0
                ],
                "title": "Performance evaluation of SLAM-ASR: The Good, the Bad, the Ugly, and the\n  Way Forward",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance evaluation of SLAM-ASR: The Good, the Bad, the Ugly, and the\n  Way Forward"
                },
                "summary": "Recent research has demonstrated that training a linear connector between\nspeech foundation encoders and large language models (LLMs) enables this\narchitecture to achieve strong ASR capabilities. Despite the impressive\nresults, it remains unclear whether these simple approaches are robust enough\nacross different scenarios and speech conditions, such as domain shifts and\nspeech perturbations. In this paper, we address these questions by conducting\nvarious ablation experiments using a recent and widely adopted approach called\nSLAM-ASR. We present novel empirical findings that offer insights on how to\neffectively utilize the SLAM-ASR architecture across a wide range of settings.\nOur main findings indicate that SLAM-ASR exhibits poor performance in\ncross-domain evaluation settings. Additionally, speech perturbations on\nin-domain data, such as changes in speech rate or additive noise, can\nsignificantly degrade performance. Our findings offer critical insights for\nfine-tuning and configuring robust LLM-based ASR models, tailored to different\ndata characteristics and computational resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research has demonstrated that training a linear connector between\nspeech foundation encoders and large language models (LLMs) enables this\narchitecture to achieve strong ASR capabilities. Despite the impressive\nresults, it remains unclear whether these simple approaches are robust enough\nacross different scenarios and speech conditions, such as domain shifts and\nspeech perturbations. In this paper, we address these questions by conducting\nvarious ablation experiments using a recent and widely adopted approach called\nSLAM-ASR. We present novel empirical findings that offer insights on how to\neffectively utilize the SLAM-ASR architecture across a wide range of settings.\nOur main findings indicate that SLAM-ASR exhibits poor performance in\ncross-domain evaluation settings. Additionally, speech perturbations on\nin-domain data, such as changes in speech rate or additive noise, can\nsignificantly degrade performance. Our findings offer critical insights for\nfine-tuning and configuring robust LLM-based ASR models, tailored to different\ndata characteristics and computational resources."
                },
                "authors": [
                    {
                        "name": "Shashi Kumar"
                    },
                    {
                        "name": "Iuliia Thorbecke"
                    },
                    {
                        "name": "Sergio Burdisso"
                    },
                    {
                        "name": "Esaú Villatoro-Tello"
                    },
                    {
                        "name": "Manjunath K E"
                    },
                    {
                        "name": "Kadri Hacioğlu"
                    },
                    {
                        "name": "Pradeep Rangappa"
                    },
                    {
                        "name": "Petr Motlicek"
                    },
                    {
                        "name": "Aravind Ganapathiraju"
                    },
                    {
                        "name": "Andreas Stolcke"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Stolcke"
                },
                "author": "Andreas Stolcke",
                "arxiv_comment": "Accepted in ICASSP 2025 SALMA Workshop",
                "arxiv_journal_ref": "Proc. ICASSP Workshop on Speech and Audio Language Models (SALMA),\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03866v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03866v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12746v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12746v1",
                "updated": "2025-01-22T09:27:11Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    9,
                    27,
                    11,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T09:27:11Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    9,
                    27,
                    11,
                    2,
                    22,
                    0
                ],
                "title": "EvidenceMap: Unleashing the Power of Small Language Models with Evidence\n  Analysis for Biomedical Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EvidenceMap: Unleashing the Power of Small Language Models with Evidence\n  Analysis for Biomedical Question Answering"
                },
                "summary": "Current LLM-based approaches improve question answering performance by\nleveraging the internal reasoning abilities of models or incorporating external\nknowledge. However, when humans address professional problems, it is essential\nto explicitly analyze the multifaceted relationships from multiple pieces and\ndiverse sources of evidence to achieve better answers. In this study, we\npropose a novel generative question answering framework for the biomedical\ndomain, named EvidenceMap, which explicitly learns and incorporates evidence\nanalysis with small language models (SLMs). The framework describes an evidence\nmap for each question and fully utilizes an SLM to derive the representation of\nthe supportive evaluation, the logical correlation, and the summarization of\nthe related evidence, which facilitates an analysis-augmented generation with\nanother SLM in an autoregressive way. Extensive experiments have shown that\nintroducing an evidence analysis learning process can significantly outperform\nlarger models and popular LLM reasoning methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current LLM-based approaches improve question answering performance by\nleveraging the internal reasoning abilities of models or incorporating external\nknowledge. However, when humans address professional problems, it is essential\nto explicitly analyze the multifaceted relationships from multiple pieces and\ndiverse sources of evidence to achieve better answers. In this study, we\npropose a novel generative question answering framework for the biomedical\ndomain, named EvidenceMap, which explicitly learns and incorporates evidence\nanalysis with small language models (SLMs). The framework describes an evidence\nmap for each question and fully utilizes an SLM to derive the representation of\nthe supportive evaluation, the logical correlation, and the summarization of\nthe related evidence, which facilitates an analysis-augmented generation with\nanother SLM in an autoregressive way. Extensive experiments have shown that\nintroducing an evidence analysis learning process can significantly outperform\nlarger models and popular LLM reasoning methods."
                },
                "authors": [
                    {
                        "name": "Chang Zong"
                    },
                    {
                        "name": "Jian Wan"
                    },
                    {
                        "name": "Lei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lei Zhang"
                },
                "author": "Lei Zhang",
                "arxiv_comment": "13 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12746v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12746v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17836v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17836v2",
                "updated": "2025-01-22T09:26:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    9,
                    26,
                    42,
                    2,
                    22,
                    0
                ],
                "published": "2024-09-26T13:38:33Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    13,
                    38,
                    33,
                    3,
                    270,
                    0
                ],
                "title": "Language Models as Zero-shot Lossless Gradient Compressors: Towards\n  General Neural Parameter Prior Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models as Zero-shot Lossless Gradient Compressors: Towards\n  General Neural Parameter Prior Models"
                },
                "summary": "Despite the widespread use of statistical prior models in various fields,\nsuch models for neural network gradients have long been overlooked. The\ninherent challenge stems from their high-dimensional structures and complex\ninterdependencies, which complicate effective modeling. In this work, we\ndemonstrate the potential of large language models (LLMs) to act as gradient\npriors in a zero-shot setting. We examine the property by considering lossless\ngradient compression -- a critical application in distributed learning -- that\ndepends heavily on precise probability modeling. To achieve this, we introduce\nLM-GC, a novel method that integrates LLMs with arithmetic coding. Our\ntechnique converts plain gradients into text-like formats, enhancing token\nefficiency by up to 38 times compared to their plain representations. We ensure\nthat this data conversion maintains a close alignment with the structure of\nplain gradients and the symbols commonly recognized by LLMs. Our experiments\nindicate that LM-GC surpasses existing state-of-the-art lossless compression\nmethods, improving compression rates by 10% up to 17.2% across various datasets\nand architectures. Additionally, our approach shows promising compatibility\nwith lossy compression techniques such as quantization and sparsification.\nThese findings highlight the significant potential of LLMs as a model for\neffectively handling gradients. Code is available at\nhttps://github.com/hui-po-wang/LM-GC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the widespread use of statistical prior models in various fields,\nsuch models for neural network gradients have long been overlooked. The\ninherent challenge stems from their high-dimensional structures and complex\ninterdependencies, which complicate effective modeling. In this work, we\ndemonstrate the potential of large language models (LLMs) to act as gradient\npriors in a zero-shot setting. We examine the property by considering lossless\ngradient compression -- a critical application in distributed learning -- that\ndepends heavily on precise probability modeling. To achieve this, we introduce\nLM-GC, a novel method that integrates LLMs with arithmetic coding. Our\ntechnique converts plain gradients into text-like formats, enhancing token\nefficiency by up to 38 times compared to their plain representations. We ensure\nthat this data conversion maintains a close alignment with the structure of\nplain gradients and the symbols commonly recognized by LLMs. Our experiments\nindicate that LM-GC surpasses existing state-of-the-art lossless compression\nmethods, improving compression rates by 10% up to 17.2% across various datasets\nand architectures. Additionally, our approach shows promising compatibility\nwith lossy compression techniques such as quantization and sparsification.\nThese findings highlight the significant potential of LLMs as a model for\neffectively handling gradients. Code is available at\nhttps://github.com/hui-po-wang/LM-GC."
                },
                "authors": [
                    {
                        "name": "Hui-Po Wang"
                    },
                    {
                        "name": "Mario Fritz"
                    }
                ],
                "author_detail": {
                    "name": "Mario Fritz"
                },
                "author": "Mario Fritz",
                "arxiv_comment": "camera-ready in NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17836v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17836v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12735v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12735v1",
                "updated": "2025-01-22T09:12:09Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    9,
                    12,
                    9,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T09:12:09Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    9,
                    12,
                    9,
                    2,
                    22,
                    0
                ],
                "title": "Online Preference Alignment for Language Models via Count-based\n  Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Preference Alignment for Language Models via Count-based\n  Exploration"
                },
                "summary": "Reinforcement Learning from Human Feedback (RLHF) has shown great potential\nin fine-tuning Large Language Models (LLMs) to align with human preferences.\nExisting methods perform preference alignment from a fixed dataset, which can\nbe limited in data coverage, and the resulting reward model is hard to\ngeneralize in out-of-distribution responses. Thus, online RLHF is more\ndesirable to empower the LLM to explore outside the support of the initial\ndataset by iteratively collecting the prompt-response pairs. In this paper, we\nstudy the fundamental problem in online RLHF, i.e. \\emph{how to explore} for\nLLM. We give a theoretical motivation in linear reward assumption to show that\nan optimistic reward with an upper confidence bound (UCB) term leads to a\nprovably efficient RLHF policy. Then, we reformulate our objective to direct\npreference optimization with an exploration term, where the UCB-term can be\nconverted to a count-based exploration bonus. We further propose a practical\nalgorithm, named \\emph{Count-based Online Preference Optimization (COPO)},\nwhich leverages a simple coin-flip counting module to estimate the pseudo-count\nof a prompt-response pair in previously collected data. COPO encourages LLMs to\nbalance exploration and preference optimization in an iterative manner, which\nenlarges the exploration space and the entire data coverage of iterative LLM\npolicies. We conduct online RLHF experiments on Zephyr and Llama-3 models. The\nresults on instruction-following and standard academic benchmarks show that\nCOPO significantly increases performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning from Human Feedback (RLHF) has shown great potential\nin fine-tuning Large Language Models (LLMs) to align with human preferences.\nExisting methods perform preference alignment from a fixed dataset, which can\nbe limited in data coverage, and the resulting reward model is hard to\ngeneralize in out-of-distribution responses. Thus, online RLHF is more\ndesirable to empower the LLM to explore outside the support of the initial\ndataset by iteratively collecting the prompt-response pairs. In this paper, we\nstudy the fundamental problem in online RLHF, i.e. \\emph{how to explore} for\nLLM. We give a theoretical motivation in linear reward assumption to show that\nan optimistic reward with an upper confidence bound (UCB) term leads to a\nprovably efficient RLHF policy. Then, we reformulate our objective to direct\npreference optimization with an exploration term, where the UCB-term can be\nconverted to a count-based exploration bonus. We further propose a practical\nalgorithm, named \\emph{Count-based Online Preference Optimization (COPO)},\nwhich leverages a simple coin-flip counting module to estimate the pseudo-count\nof a prompt-response pair in previously collected data. COPO encourages LLMs to\nbalance exploration and preference optimization in an iterative manner, which\nenlarges the exploration space and the entire data coverage of iterative LLM\npolicies. We conduct online RLHF experiments on Zephyr and Llama-3 models. The\nresults on instruction-following and standard academic benchmarks show that\nCOPO significantly increases performance."
                },
                "authors": [
                    {
                        "name": "Chenjia Bai"
                    },
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Shuang Qiu"
                    },
                    {
                        "name": "Qiaosheng Zhang"
                    },
                    {
                        "name": "Kang Xu"
                    },
                    {
                        "name": "Xuelong Li"
                    }
                ],
                "author_detail": {
                    "name": "Xuelong Li"
                },
                "author": "Xuelong Li",
                "arxiv_comment": "Accepted by ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12735v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12735v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12728v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12728v1",
                "updated": "2025-01-22T09:05:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    9,
                    5,
                    1,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T09:05:01Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    9,
                    5,
                    1,
                    2,
                    22,
                    0
                ],
                "title": "A Call for Critically Rethinking and Reforming Data Analysis in\n  Empirical Software Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Call for Critically Rethinking and Reforming Data Analysis in\n  Empirical Software Engineering"
                },
                "summary": "Context: Empirical Software Engineering (ESE) drives innovation in SE through\nqualitative and quantitative studies. However, concerns about the correct\napplication of empirical methodologies have existed since the 2006 Dagstuhl\nseminar on SE. Objective: To analyze three decades of SE research, identify\nmistakes in statistical methods, and evaluate experts' ability to detect and\naddress these issues. Methods: We conducted a literature survey of ~27,000\nempirical studies, using LLMs to classify statistical methodologies as adequate\nor inadequate. Additionally, we selected 30 primary studies and held a workshop\nwith 33 ESE experts to assess their ability to identify and resolve statistical\nissues. Results: Significant statistical issues were found in the primary\nstudies, and experts showed limited ability to detect and correct these\nmethodological problems, raising concerns about the broader ESE community's\nproficiency in this area. Conclusions. Despite our study's eventual\nlimitations, its results shed light on recurring issues from promoting\ninformation copy-and-paste from past authors' works and the continuous\npublication of inadequate approaches that promote dubious results and\njeopardize the spread of the correct statistical strategies among researchers.\nBesides, it justifies further investigation into empirical rigor in software\nengineering to expose these recurring issues and establish a framework for\nreassessing our field's foundation of statistical methodology application.\nTherefore, this work calls for critically rethinking and reforming data\nanalysis in empirical software engineering, paving the way for our work soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context: Empirical Software Engineering (ESE) drives innovation in SE through\nqualitative and quantitative studies. However, concerns about the correct\napplication of empirical methodologies have existed since the 2006 Dagstuhl\nseminar on SE. Objective: To analyze three decades of SE research, identify\nmistakes in statistical methods, and evaluate experts' ability to detect and\naddress these issues. Methods: We conducted a literature survey of ~27,000\nempirical studies, using LLMs to classify statistical methodologies as adequate\nor inadequate. Additionally, we selected 30 primary studies and held a workshop\nwith 33 ESE experts to assess their ability to identify and resolve statistical\nissues. Results: Significant statistical issues were found in the primary\nstudies, and experts showed limited ability to detect and correct these\nmethodological problems, raising concerns about the broader ESE community's\nproficiency in this area. Conclusions. Despite our study's eventual\nlimitations, its results shed light on recurring issues from promoting\ninformation copy-and-paste from past authors' works and the continuous\npublication of inadequate approaches that promote dubious results and\njeopardize the spread of the correct statistical strategies among researchers.\nBesides, it justifies further investigation into empirical rigor in software\nengineering to expose these recurring issues and establish a framework for\nreassessing our field's foundation of statistical methodology application.\nTherefore, this work calls for critically rethinking and reforming data\nanalysis in empirical software engineering, paving the way for our work soon."
                },
                "authors": [
                    {
                        "name": "Matteo Esposito"
                    },
                    {
                        "name": "Mikel Robredo"
                    },
                    {
                        "name": "Murali Sridharan"
                    },
                    {
                        "name": "Guilherme Horta Travassos"
                    },
                    {
                        "name": "Rafael Peñaloza"
                    },
                    {
                        "name": "Valentina Lenarduzzi"
                    }
                ],
                "author_detail": {
                    "name": "Valentina Lenarduzzi"
                },
                "author": "Valentina Lenarduzzi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12728v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12728v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12725v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12725v1",
                "updated": "2025-01-22T08:55:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    8,
                    55,
                    28,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T08:55:28Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    8,
                    55,
                    28,
                    2,
                    22,
                    0
                ],
                "title": "Online Rack Placement in Large-Scale Data Centers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Rack Placement in Large-Scale Data Centers"
                },
                "summary": "This paper optimizes the configuration of large-scale data centers toward\ncost-effective, reliable and sustainable cloud supply chains. We formulate an\ninteger optimization model that optimizes the placement of racks of servers\nwithin a data center to maximize demand coverage, adhere to space, power and\ncooling restrictions, and pace resource utilization for future demand. We also\ndefine a tractable single-sample online approximation (SSOA) approach to\nmulti-stage stochastic optimization, which approximates unknown parameters with\na single realization and re-optimizes decisions dynamically. Theoretical\nresults provide strong performance guarantees of SSOA in the canonical online\ngeneralized assignment and online bin packing settings. Computational results\nusing real-world data show that our optimization approach can enhance\nutilization and reduce power stranding in data centers. Following iterative\nimprovements in collaboration with data center managers, our algorithm has been\npackaged into a software solution deployed in Microsoft's data centers\nworldwide. Deployment data indicate a significant increase in adoption, leading\nto improved power utilization, multi-million-dollar annual cost savings, and\nconcomitant savings in greenhouse gas emissions. Ultimately, this paper\nconstitutes one of the first large-scale deployments of a decision-making tool\nin data centers, contributing an interactive decision-making process at the\nhuman-machine interface.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper optimizes the configuration of large-scale data centers toward\ncost-effective, reliable and sustainable cloud supply chains. We formulate an\ninteger optimization model that optimizes the placement of racks of servers\nwithin a data center to maximize demand coverage, adhere to space, power and\ncooling restrictions, and pace resource utilization for future demand. We also\ndefine a tractable single-sample online approximation (SSOA) approach to\nmulti-stage stochastic optimization, which approximates unknown parameters with\na single realization and re-optimizes decisions dynamically. Theoretical\nresults provide strong performance guarantees of SSOA in the canonical online\ngeneralized assignment and online bin packing settings. Computational results\nusing real-world data show that our optimization approach can enhance\nutilization and reduce power stranding in data centers. Following iterative\nimprovements in collaboration with data center managers, our algorithm has been\npackaged into a software solution deployed in Microsoft's data centers\nworldwide. Deployment data indicate a significant increase in adoption, leading\nto improved power utilization, multi-million-dollar annual cost savings, and\nconcomitant savings in greenhouse gas emissions. Ultimately, this paper\nconstitutes one of the first large-scale deployments of a decision-making tool\nin data centers, contributing an interactive decision-making process at the\nhuman-machine interface."
                },
                "authors": [
                    {
                        "name": "Saumil Baxi"
                    },
                    {
                        "name": "Kayla Cummings"
                    },
                    {
                        "name": "Alexandre Jacquillat"
                    },
                    {
                        "name": "Sean Lo"
                    },
                    {
                        "name": "Rob McDonald"
                    },
                    {
                        "name": "Konstantina Mellou"
                    },
                    {
                        "name": "Ishai Menache"
                    },
                    {
                        "name": "Marco Molinaro"
                    }
                ],
                "author_detail": {
                    "name": "Marco Molinaro"
                },
                "author": "Marco Molinaro",
                "arxiv_comment": "52 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12725v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12725v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "90C11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.16831v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.16831v3",
                "updated": "2025-01-22T08:45:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    8,
                    45,
                    56,
                    2,
                    22,
                    0
                ],
                "published": "2024-03-25T14:57:18Z",
                "published_parsed": [
                    2024,
                    3,
                    25,
                    14,
                    57,
                    18,
                    0,
                    85,
                    0
                ],
                "title": "UrbanVLP: Multi-Granularity Vision-Language Pretraining for Urban\n  Socioeconomic Indicator Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UrbanVLP: Multi-Granularity Vision-Language Pretraining for Urban\n  Socioeconomic Indicator Prediction"
                },
                "summary": "Urban socioeconomic indicator prediction aims to infer various metrics\nrelated to sustainable development in diverse urban landscapes using\ndata-driven methods. However, prevalent pretrained models, particularly those\nreliant on satellite imagery, face dual challenges. Firstly, concentrating\nsolely on macro-level patterns from satellite data may introduce bias, lacking\nnuanced details at micro levels, such as architectural details at a place.\nSecondly, the text generated by the precursor work UrbanCLIP, which fully\nutilizes the extensive knowledge of LLMs, frequently exhibits issues such as\nhallucination and homogenization, resulting in a lack of reliable quality. In\nresponse to these issues, we devise a novel framework entitled UrbanVLP based\non Vision-Language Pretraining. Our UrbanVLP seamlessly integrates\nmulti-granularity information from both macro (satellite) and micro\n(street-view) levels, overcoming the limitations of prior pretrained models.\nMoreover, it introduces automatic text generation and calibration, providing a\nrobust guarantee for producing high-quality text descriptions of urban imagery.\nRigorous experiments conducted across six socioeconomic indicator prediction\ntasks underscore its superior performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Urban socioeconomic indicator prediction aims to infer various metrics\nrelated to sustainable development in diverse urban landscapes using\ndata-driven methods. However, prevalent pretrained models, particularly those\nreliant on satellite imagery, face dual challenges. Firstly, concentrating\nsolely on macro-level patterns from satellite data may introduce bias, lacking\nnuanced details at micro levels, such as architectural details at a place.\nSecondly, the text generated by the precursor work UrbanCLIP, which fully\nutilizes the extensive knowledge of LLMs, frequently exhibits issues such as\nhallucination and homogenization, resulting in a lack of reliable quality. In\nresponse to these issues, we devise a novel framework entitled UrbanVLP based\non Vision-Language Pretraining. Our UrbanVLP seamlessly integrates\nmulti-granularity information from both macro (satellite) and micro\n(street-view) levels, overcoming the limitations of prior pretrained models.\nMoreover, it introduces automatic text generation and calibration, providing a\nrobust guarantee for producing high-quality text descriptions of urban imagery.\nRigorous experiments conducted across six socioeconomic indicator prediction\ntasks underscore its superior performance."
                },
                "authors": [
                    {
                        "name": "Xixuan Hao"
                    },
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Yibo Yan"
                    },
                    {
                        "name": "Siru Zhong"
                    },
                    {
                        "name": "Kun Wang"
                    },
                    {
                        "name": "Qingsong Wen"
                    },
                    {
                        "name": "Yuxuan Liang"
                    }
                ],
                "author_detail": {
                    "name": "Yuxuan Liang"
                },
                "author": "Yuxuan Liang",
                "arxiv_comment": "Accepted as a full paper by AAAI'25 - AI for Social Impact Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.16831v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.16831v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09460v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09460v3",
                "updated": "2025-01-22T08:42:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    8,
                    42,
                    13,
                    2,
                    22,
                    0
                ],
                "published": "2024-12-12T17:11:22Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    17,
                    11,
                    22,
                    3,
                    347,
                    0
                ],
                "title": "The Impact of Copyrighted Material on Large Language Models: A Norwegian\n  Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of Copyrighted Material on Large Language Models: A Norwegian\n  Perspective"
                },
                "summary": "The use of copyrighted materials in training language models raises critical\nlegal and ethical questions. This paper presents a framework for and the\nresults of empirically assessing the impact of publisher-controlled copyrighted\ncorpora on the performance of generative large language models (LLMs) for\nNorwegian. When evaluated on a diverse set of tasks, we found that adding both\nbooks and newspapers to the data mixture of LLMs tend to improve their\nperformance, while the addition of fiction works seems to be detrimental. Our\nexperiments could inform the creation of a compensation scheme for authors\nwhose works contribute to AI development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of copyrighted materials in training language models raises critical\nlegal and ethical questions. This paper presents a framework for and the\nresults of empirically assessing the impact of publisher-controlled copyrighted\ncorpora on the performance of generative large language models (LLMs) for\nNorwegian. When evaluated on a diverse set of tasks, we found that adding both\nbooks and newspapers to the data mixture of LLMs tend to improve their\nperformance, while the addition of fiction works seems to be detrimental. Our\nexperiments could inform the creation of a compensation scheme for authors\nwhose works contribute to AI development."
                },
                "authors": [
                    {
                        "name": "Javier de la Rosa"
                    },
                    {
                        "name": "Vladislav Mikhailov"
                    },
                    {
                        "name": "Lemei Zhang"
                    },
                    {
                        "name": "Freddy Wetjen"
                    },
                    {
                        "name": "David Samuel"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Rolv-Arild Braaten"
                    },
                    {
                        "name": "Petter Mæhlum"
                    },
                    {
                        "name": "Magnus Breder Birkenes"
                    },
                    {
                        "name": "Andrey Kutuzov"
                    },
                    {
                        "name": "Tita Enstad"
                    },
                    {
                        "name": "Hans Christian Farsethås"
                    },
                    {
                        "name": "Svein Arne Brygfjeld"
                    },
                    {
                        "name": "Jon Atle Gulla"
                    },
                    {
                        "name": "Stephan Oepen"
                    },
                    {
                        "name": "Erik Velldal"
                    },
                    {
                        "name": "Wilfred Østgulen"
                    },
                    {
                        "name": "Liljia Øvrelid"
                    },
                    {
                        "name": "Aslak Sira Myhre"
                    }
                ],
                "author_detail": {
                    "name": "Aslak Sira Myhre"
                },
                "author": "Aslak Sira Myhre",
                "arxiv_comment": "17 pages, 5 figures, 8 tables. Accepted at NoDaLiDa/Baltic-HLT 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09460v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09460v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.19443v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.19443v2",
                "updated": "2025-01-22T08:25:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    8,
                    25,
                    25,
                    2,
                    22,
                    0
                ],
                "published": "2024-03-28T14:15:10Z",
                "published_parsed": [
                    2024,
                    3,
                    28,
                    14,
                    15,
                    10,
                    3,
                    88,
                    0
                ],
                "title": "Mixed Preference Optimization: Reinforcement Learning with Data\n  Selection and Better Reference Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixed Preference Optimization: Reinforcement Learning with Data\n  Selection and Better Reference Model"
                },
                "summary": "Large Language Models (LLMs) have become increasingly popular due to their\nability to process and generate natural language. However, as they are trained\non massive datasets of text, LLMs can inherit harmful biases and produce\noutputs that are not aligned with human values. This paper studies two main\napproaches to LLM alignment: Reinforcement Learning with Human Feedback (RLHF)\nand contrastive learning-based methods like Direct Preference Optimization\n(DPO). By analyzing the stability and robustness of RLHF and DPO, we propose\nMPO (Mixed Preference Optimization), a novel method that mitigates the\nweaknesses of both approaches. Specifically, we propose a two-stage training\nprocedure: first train DPO on an easy dataset, and then perform RLHF on a\ndifficult set with DPO model being the reference model. Here, the easy and\ndifficult sets are constructed by a well-trained reward model that splits\nresponse pairs into those with large gaps of reward (easy), and those with\nsmall gaps (difficult). The first stage allows us to obtain a relatively\noptimal policy (LLM) model quickly, whereas the second stage refines LLM with\nonline RLHF, thus mitigating the distribution shift issue associated with DPO.\nExperiments are conducted on two public alignment datasets, namely HH-RLHF and\nTLDR, demonstrating the effectiveness of MPO, both in terms of GPT4 and human\nevaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become increasingly popular due to their\nability to process and generate natural language. However, as they are trained\non massive datasets of text, LLMs can inherit harmful biases and produce\noutputs that are not aligned with human values. This paper studies two main\napproaches to LLM alignment: Reinforcement Learning with Human Feedback (RLHF)\nand contrastive learning-based methods like Direct Preference Optimization\n(DPO). By analyzing the stability and robustness of RLHF and DPO, we propose\nMPO (Mixed Preference Optimization), a novel method that mitigates the\nweaknesses of both approaches. Specifically, we propose a two-stage training\nprocedure: first train DPO on an easy dataset, and then perform RLHF on a\ndifficult set with DPO model being the reference model. Here, the easy and\ndifficult sets are constructed by a well-trained reward model that splits\nresponse pairs into those with large gaps of reward (easy), and those with\nsmall gaps (difficult). The first stage allows us to obtain a relatively\noptimal policy (LLM) model quickly, whereas the second stage refines LLM with\nonline RLHF, thus mitigating the distribution shift issue associated with DPO.\nExperiments are conducted on two public alignment datasets, namely HH-RLHF and\nTLDR, demonstrating the effectiveness of MPO, both in terms of GPT4 and human\nevaluation."
                },
                "authors": [
                    {
                        "name": "Qi Gou"
                    },
                    {
                        "name": "Cam-Tu Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Cam-Tu Nguyen"
                },
                "author": "Cam-Tu Nguyen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.19443v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.19443v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12702v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12702v1",
                "updated": "2025-01-22T08:18:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    8,
                    18,
                    37,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T08:18:37Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    8,
                    18,
                    37,
                    2,
                    22,
                    0
                ],
                "title": "Paradigm-Based Automatic HDL Code Generation Using LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Paradigm-Based Automatic HDL Code Generation Using LLMs"
                },
                "summary": "While large language models (LLMs) have demonstrated the ability to generate\nhardware description language (HDL) code for digital circuits, they still face\nthe hallucination problem, which can result in the generation of incorrect HDL\ncode or misinterpretation of specifications. In this work, we introduce a\nhuman-expert-inspired method to mitigate the hallucination of LLMs and enhance\ntheir performance in HDL code generation. We begin by constructing specialized\nparadigm blocks that consist of several steps designed to divide and conquer\ngeneration tasks, mirroring the design methodology of human experts. These\nsteps include information extraction, human-like design flows, and the\nintegration of external tools. LLMs are then instructed to classify the type of\ncircuit in order to match it with the appropriate paradigm block and execute\nthe block to generate the HDL codes. Additionally, we propose a two-phase\nworkflow for multi-round generation, aimed at effectively improving the\ntestbench pass rate of the generated HDL codes within a limited number of\ngeneration and verification rounds. Experimental results demonstrate that our\nmethod significantly enhances the functional correctness of the generated\nVerilog code",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) have demonstrated the ability to generate\nhardware description language (HDL) code for digital circuits, they still face\nthe hallucination problem, which can result in the generation of incorrect HDL\ncode or misinterpretation of specifications. In this work, we introduce a\nhuman-expert-inspired method to mitigate the hallucination of LLMs and enhance\ntheir performance in HDL code generation. We begin by constructing specialized\nparadigm blocks that consist of several steps designed to divide and conquer\ngeneration tasks, mirroring the design methodology of human experts. These\nsteps include information extraction, human-like design flows, and the\nintegration of external tools. LLMs are then instructed to classify the type of\ncircuit in order to match it with the appropriate paradigm block and execute\nthe block to generate the HDL codes. Additionally, we propose a two-phase\nworkflow for multi-round generation, aimed at effectively improving the\ntestbench pass rate of the generated HDL codes within a limited number of\ngeneration and verification rounds. Experimental results demonstrate that our\nmethod significantly enhances the functional correctness of the generated\nVerilog code"
                },
                "authors": [
                    {
                        "name": "Wenhao Sun"
                    },
                    {
                        "name": "Bing Li"
                    },
                    {
                        "name": "Grace Li Zhang"
                    },
                    {
                        "name": "Xunzhao Yin"
                    },
                    {
                        "name": "Cheng Zhuo"
                    },
                    {
                        "name": "Ulf Schlichtmann"
                    }
                ],
                "author_detail": {
                    "name": "Ulf Schlichtmann"
                },
                "author": "Ulf Schlichtmann",
                "arxiv_comment": "accepted by ISQED2025. arXiv admin note: text overlap with\n  arXiv:2407.18326",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12702v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12702v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.17696v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.17696v6",
                "updated": "2025-01-22T08:14:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    8,
                    14,
                    59,
                    2,
                    22,
                    0
                ],
                "published": "2023-11-29T15:02:46Z",
                "published_parsed": [
                    2023,
                    11,
                    29,
                    15,
                    2,
                    46,
                    2,
                    333,
                    0
                ],
                "title": "How to Build an AI Tutor That Can Adapt to Any Course Using Knowledge\n  Graph-Enhanced Retrieval-Augmented Generation (KG-RAG)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to Build an AI Tutor That Can Adapt to Any Course Using Knowledge\n  Graph-Enhanced Retrieval-Augmented Generation (KG-RAG)"
                },
                "summary": "This paper introduces KG-RAG (Knowledge Graph-enhanced Retrieval-Augmented\nGeneration), a novel framework that addresses two critical challenges in\nLLM-based tutoring systems: information hallucination and limited\ncourse-specific adaptation. By integrating knowledge graphs with\nretrieval-augmented generation, KG-RAG provides a structured representation of\ncourse concepts and their relationships, enabling contextually grounded and\npedagogically sound responses. We implement the framework using Qwen2.5,\ndemonstrating its cost-effectiveness while maintaining high performance. The\nKG-RAG system outperformed standard RAG-based tutoring in a controlled study\nwith 76 university students (mean scores: 6.37 vs. 4.71, p<0.001, Cohen's\nd=0.86). User feedback showed strong satisfaction with answer relevance (84%\npositive) and user experience (59% positive). Our framework offers a scalable\napproach to personalized AI tutoring, ensuring response accuracy and\npedagogical coherence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces KG-RAG (Knowledge Graph-enhanced Retrieval-Augmented\nGeneration), a novel framework that addresses two critical challenges in\nLLM-based tutoring systems: information hallucination and limited\ncourse-specific adaptation. By integrating knowledge graphs with\nretrieval-augmented generation, KG-RAG provides a structured representation of\ncourse concepts and their relationships, enabling contextually grounded and\npedagogically sound responses. We implement the framework using Qwen2.5,\ndemonstrating its cost-effectiveness while maintaining high performance. The\nKG-RAG system outperformed standard RAG-based tutoring in a controlled study\nwith 76 university students (mean scores: 6.37 vs. 4.71, p<0.001, Cohen's\nd=0.86). User feedback showed strong satisfaction with answer relevance (84%\npositive) and user experience (59% positive). Our framework offers a scalable\napproach to personalized AI tutoring, ensuring response accuracy and\npedagogical coherence."
                },
                "authors": [
                    {
                        "name": "Chenxi Dong"
                    },
                    {
                        "name": "Yimin Yuan"
                    },
                    {
                        "name": "Kan Chen"
                    },
                    {
                        "name": "Shupei Cheng"
                    },
                    {
                        "name": "Chujie Wen"
                    }
                ],
                "author_detail": {
                    "name": "Chujie Wen"
                },
                "author": "Chujie Wen",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.17696v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.17696v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12698v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12698v1",
                "updated": "2025-01-22T08:14:51Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    8,
                    14,
                    51,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T08:14:51Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    8,
                    14,
                    51,
                    2,
                    22,
                    0
                ],
                "title": "Training Dialogue Systems by AI Feedback for Improving Overall Dialogue\n  Impression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training Dialogue Systems by AI Feedback for Improving Overall Dialogue\n  Impression"
                },
                "summary": "To improve user engagement during conversations with dialogue systems, we\nmust improve individual dialogue responses and dialogue impressions such as\nconsistency, personality, and empathy throughout the entire dialogue. While\nsuch dialogue systems have been developing rapidly with the help of large\nlanguage models (LLMs), reinforcement learning from AI feedback (RLAIF) has\nattracted attention to align LLM-based dialogue models for such dialogue\nimpressions. In RLAIF, a reward model based on another LLM is used to create a\ntraining signal for an LLM-based dialogue model using zero-shot/few-shot\nprompting techniques. However, evaluating an entire dialogue only by prompting\nLLMs is challenging. In this study, the supervised fine-tuning (SFT) of LLMs\nprepared reward models corresponding to 12 metrics related to the impression of\nthe entire dialogue for evaluating dialogue responses. We tuned our dialogue\nmodels using the reward model signals as feedback to improve the impression of\nthe system. The results of automatic and human evaluations showed that tuning\nthe dialogue model using our reward model corresponding to dialogue impression\nimproved the evaluation of individual metrics and the naturalness of the\ndialogue response.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To improve user engagement during conversations with dialogue systems, we\nmust improve individual dialogue responses and dialogue impressions such as\nconsistency, personality, and empathy throughout the entire dialogue. While\nsuch dialogue systems have been developing rapidly with the help of large\nlanguage models (LLMs), reinforcement learning from AI feedback (RLAIF) has\nattracted attention to align LLM-based dialogue models for such dialogue\nimpressions. In RLAIF, a reward model based on another LLM is used to create a\ntraining signal for an LLM-based dialogue model using zero-shot/few-shot\nprompting techniques. However, evaluating an entire dialogue only by prompting\nLLMs is challenging. In this study, the supervised fine-tuning (SFT) of LLMs\nprepared reward models corresponding to 12 metrics related to the impression of\nthe entire dialogue for evaluating dialogue responses. We tuned our dialogue\nmodels using the reward model signals as feedback to improve the impression of\nthe system. The results of automatic and human evaluations showed that tuning\nthe dialogue model using our reward model corresponding to dialogue impression\nimproved the evaluation of individual metrics and the naturalness of the\ndialogue response."
                },
                "authors": [
                    {
                        "name": "Kai Yoshida"
                    },
                    {
                        "name": "Masahiro Mizukami"
                    },
                    {
                        "name": "Seiya Kawano"
                    },
                    {
                        "name": "Canasai Kruengkrai"
                    },
                    {
                        "name": "Hiroaki Sugiyama"
                    },
                    {
                        "name": "Koichiro Yoshino"
                    }
                ],
                "author_detail": {
                    "name": "Koichiro Yoshino"
                },
                "author": "Koichiro Yoshino",
                "arxiv_comment": "Accepted to ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12698v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12698v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12697v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12697v1",
                "updated": "2025-01-22T08:14:11Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    8,
                    14,
                    11,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T08:14:11Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    8,
                    14,
                    11,
                    2,
                    22,
                    0
                ],
                "title": "Combining Knowledge Graph and LLMs for Enhanced Zero-shot Visual\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combining Knowledge Graph and LLMs for Enhanced Zero-shot Visual\n  Question Answering"
                },
                "summary": "Zero-shot visual question answering (ZS-VQA), an emerged critical research\narea, intends to answer visual questions without providing training samples.\nExisting research in ZS-VQA has proposed to leverage knowledge graphs or large\nlanguage models (LLMs), respectively, as external information sources to help\nVQA model comprehend images and questions. However, LLMs often struggle in\naccurately interpreting specific question meanings. Meanwhile, although\nknowledge graph has rich entity relationships, it is challenging to effectively\nconnect entities to individual image content for visual question answers. In\nthis paper, we propose a novel design to combine knowledge graph and LLMs for\nzero-shot visual question answer. Our approach uses LLMs' powerful\nunderstanding capabilities to accurately interpret image content through a\nstrategic question search mechanism. Meanwhile, the knowledge graph is used to\nexpand and connect users' queries to the image content for better visual\nquestion answering. An optimization algorithm is further used to determine the\noptimal weights for the loss functions derived from different information\nsources, towards a globally optimal set of candidate answers. Experimental\nresults on two benchmark datasets demonstrate that our model achieves\nstate-of-the-art (SOTA) performance. Both source code and benchmark data will\nbe released for public access.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-shot visual question answering (ZS-VQA), an emerged critical research\narea, intends to answer visual questions without providing training samples.\nExisting research in ZS-VQA has proposed to leverage knowledge graphs or large\nlanguage models (LLMs), respectively, as external information sources to help\nVQA model comprehend images and questions. However, LLMs often struggle in\naccurately interpreting specific question meanings. Meanwhile, although\nknowledge graph has rich entity relationships, it is challenging to effectively\nconnect entities to individual image content for visual question answers. In\nthis paper, we propose a novel design to combine knowledge graph and LLMs for\nzero-shot visual question answer. Our approach uses LLMs' powerful\nunderstanding capabilities to accurately interpret image content through a\nstrategic question search mechanism. Meanwhile, the knowledge graph is used to\nexpand and connect users' queries to the image content for better visual\nquestion answering. An optimization algorithm is further used to determine the\noptimal weights for the loss functions derived from different information\nsources, towards a globally optimal set of candidate answers. Experimental\nresults on two benchmark datasets demonstrate that our model achieves\nstate-of-the-art (SOTA) performance. Both source code and benchmark data will\nbe released for public access."
                },
                "authors": [
                    {
                        "name": "Qian Tao"
                    },
                    {
                        "name": "Xiaoyang Fan"
                    },
                    {
                        "name": "Yong Xu"
                    },
                    {
                        "name": "Xingquan Zhu"
                    },
                    {
                        "name": "Yufei Tang"
                    }
                ],
                "author_detail": {
                    "name": "Yufei Tang"
                },
                "author": "Yufei Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12697v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12697v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12696v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12696v1",
                "updated": "2025-01-22T08:09:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    8,
                    9,
                    1,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T08:09:01Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    8,
                    9,
                    1,
                    2,
                    22,
                    0
                ],
                "title": "SoundSpring: Loss-Resilient Audio Transceiver with Dual-Functional\n  Masked Language Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SoundSpring: Loss-Resilient Audio Transceiver with Dual-Functional\n  Masked Language Modeling"
                },
                "summary": "In this paper, we propose \"SoundSpring\", a cutting-edge error-resilient audio\ntransceiver that marries the robustness benefits of joint source-channel coding\n(JSCC) while also being compatible with current digital communication systems.\nUnlike recent deep JSCC transceivers, which learn to directly map audio signals\nto analog channel-input symbols via neural networks, our SoundSpring adopts the\nlayered architecture that delineates audio compression from digital coded\ntransmission, but it sufficiently exploits the impressive in-context predictive\ncapabilities of large language (foundation) models. Integrated with the\ncasual-order mask learning strategy, our single model operates on the latent\nfeature domain and serve dual-functionalities: as efficient audio compressors\nat the transmitter and as effective mechanisms for packet loss concealment at\nthe receiver. By jointly optimizing towards both audio compression efficiency\nand transmission error resiliency, we show that mask-learned language models\nare indeed powerful contextual predictors, and our dual-functional compression\nand concealment framework offers fresh perspectives on the application of\nfoundation language models in audio communication. Through extensive\nexperimental evaluations, we establish that SoundSpring apparently outperforms\ncontemporary audio transmission systems in terms of signal fidelity metrics and\nperceptual quality scores. These new findings not only advocate for the\npractical deployment of SoundSpring in learning-based audio communication\nsystems but also inspire the development of future audio semantic transceivers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose \"SoundSpring\", a cutting-edge error-resilient audio\ntransceiver that marries the robustness benefits of joint source-channel coding\n(JSCC) while also being compatible with current digital communication systems.\nUnlike recent deep JSCC transceivers, which learn to directly map audio signals\nto analog channel-input symbols via neural networks, our SoundSpring adopts the\nlayered architecture that delineates audio compression from digital coded\ntransmission, but it sufficiently exploits the impressive in-context predictive\ncapabilities of large language (foundation) models. Integrated with the\ncasual-order mask learning strategy, our single model operates on the latent\nfeature domain and serve dual-functionalities: as efficient audio compressors\nat the transmitter and as effective mechanisms for packet loss concealment at\nthe receiver. By jointly optimizing towards both audio compression efficiency\nand transmission error resiliency, we show that mask-learned language models\nare indeed powerful contextual predictors, and our dual-functional compression\nand concealment framework offers fresh perspectives on the application of\nfoundation language models in audio communication. Through extensive\nexperimental evaluations, we establish that SoundSpring apparently outperforms\ncontemporary audio transmission systems in terms of signal fidelity metrics and\nperceptual quality scores. These new findings not only advocate for the\npractical deployment of SoundSpring in learning-based audio communication\nsystems but also inspire the development of future audio semantic transceivers."
                },
                "authors": [
                    {
                        "name": "Shengshi Yao"
                    },
                    {
                        "name": "Jincheng Dai"
                    },
                    {
                        "name": "Xiaoqi Qin"
                    },
                    {
                        "name": "Sixian Wang"
                    },
                    {
                        "name": "Siye Wang"
                    },
                    {
                        "name": "Kai Niu"
                    },
                    {
                        "name": "Ping Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ping Zhang"
                },
                "author": "Ping Zhang",
                "arxiv_doi": "10.1109/JSAC.2025.3531406",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/JSAC.2025.3531406",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.12696v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12696v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "To appear in IEEE JSAC",
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12689v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12689v1",
                "updated": "2025-01-22T07:52:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    7,
                    52,
                    38,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T07:52:38Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    7,
                    52,
                    38,
                    2,
                    22,
                    0
                ],
                "title": "EchoLM: Accelerating LLM Serving with Real-time Knowledge Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EchoLM: Accelerating LLM Serving with Real-time Knowledge Distillation"
                },
                "summary": "Large language models (LLMs) have excelled in various applications, yet\nserving them at scale is challenging due to their substantial resource demands\nand high latency. Our real-world studies reveal that over 60% of user requests\nto LLMs have semantically similar counterparts, suggesting the potential for\nknowledge sharing among requests. However, naively caching and reusing past\nresponses leads to large quality degradation. In this paper, we introduce\nEchoLM, an in-context caching system that leverages historical requests as\nexamples to guide response generation, enabling selective offloading of\nrequests to more efficient LLMs. However, enabling this real-time knowledge\ntransfer leads to intricate tradeoffs between response quality, latency, and\nsystem throughput at scale. For a new request, EchoLM identifies similar,\nhigh-utility examples and efficiently prepends them to the input for better\nresponse. At scale, EchoLM adaptively routes requests to LLMs of varying\ncapabilities, accounting for response quality and serving loads. EchoLM employs\na cost-aware cache replay mechanism to improve example quality and coverage\noffline, maximizing cache utility and runtime efficiency. Evaluations on\nmillions of open-source requests demonstrate that EchoLM has a throughput\nimprovement of 1.4-5.9x while reducing latency by 28-71% without hurting\nresponse quality on average.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have excelled in various applications, yet\nserving them at scale is challenging due to their substantial resource demands\nand high latency. Our real-world studies reveal that over 60% of user requests\nto LLMs have semantically similar counterparts, suggesting the potential for\nknowledge sharing among requests. However, naively caching and reusing past\nresponses leads to large quality degradation. In this paper, we introduce\nEchoLM, an in-context caching system that leverages historical requests as\nexamples to guide response generation, enabling selective offloading of\nrequests to more efficient LLMs. However, enabling this real-time knowledge\ntransfer leads to intricate tradeoffs between response quality, latency, and\nsystem throughput at scale. For a new request, EchoLM identifies similar,\nhigh-utility examples and efficiently prepends them to the input for better\nresponse. At scale, EchoLM adaptively routes requests to LLMs of varying\ncapabilities, accounting for response quality and serving loads. EchoLM employs\na cost-aware cache replay mechanism to improve example quality and coverage\noffline, maximizing cache utility and runtime efficiency. Evaluations on\nmillions of open-source requests demonstrate that EchoLM has a throughput\nimprovement of 1.4-5.9x while reducing latency by 28-71% without hurting\nresponse quality on average."
                },
                "authors": [
                    {
                        "name": "Yifan Yu"
                    },
                    {
                        "name": "Yu Gan"
                    },
                    {
                        "name": "Lily Tasi"
                    },
                    {
                        "name": "Nikhil Sarda"
                    },
                    {
                        "name": "Jiaming Shen"
                    },
                    {
                        "name": "Yanqi Zhou"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "Fan Lai"
                    },
                    {
                        "name": "Henry M. Levy"
                    },
                    {
                        "name": "David Culler"
                    }
                ],
                "author_detail": {
                    "name": "David Culler"
                },
                "author": "David Culler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12689v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12689v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17640v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17640v3",
                "updated": "2025-01-22T07:16:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    7,
                    16,
                    32,
                    2,
                    22,
                    0
                ],
                "published": "2024-09-26T08:44:38Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    8,
                    44,
                    38,
                    3,
                    270,
                    0
                ],
                "title": "T3: A Novel Zero-shot Transfer Learning Framework Iteratively Training\n  on an Assistant Task for a Target Task",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "T3: A Novel Zero-shot Transfer Learning Framework Iteratively Training\n  on an Assistant Task for a Target Task"
                },
                "summary": "Long text summarization, gradually being essential for efficiently processing\nlarge volumes of information, stays challenging for Large Language Models\n(LLMs) such as GPT and LLaMA families because of the insufficient open-sourced\ntraining datasets and the high requirement of contextual details dealing. To\naddress the issue, we design a novel zero-shot transfer learning framework,\nabbreviated as T3, to iteratively training a baseline LLM on an assistant task\nfor the target task, where the former should own richer data resources and\nshare structural or semantic similarity with the latter. In practice, T3 is\napproached to deal with the long text summarization task by utilizing question\nanswering as the assistant task, and further validated its effectiveness on the\nBBC summary, NarraSum, FairytaleQA, and NLQuAD datasets, with up to nearly 14%\nimprovement in ROUGE, 35% improvement in BLEU, and 16% improvement in Factscore\ncompared to three baseline LLMs, demonstrating its potential for more\nassistant-target task combinations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long text summarization, gradually being essential for efficiently processing\nlarge volumes of information, stays challenging for Large Language Models\n(LLMs) such as GPT and LLaMA families because of the insufficient open-sourced\ntraining datasets and the high requirement of contextual details dealing. To\naddress the issue, we design a novel zero-shot transfer learning framework,\nabbreviated as T3, to iteratively training a baseline LLM on an assistant task\nfor the target task, where the former should own richer data resources and\nshare structural or semantic similarity with the latter. In practice, T3 is\napproached to deal with the long text summarization task by utilizing question\nanswering as the assistant task, and further validated its effectiveness on the\nBBC summary, NarraSum, FairytaleQA, and NLQuAD datasets, with up to nearly 14%\nimprovement in ROUGE, 35% improvement in BLEU, and 16% improvement in Factscore\ncompared to three baseline LLMs, demonstrating its potential for more\nassistant-target task combinations."
                },
                "authors": [
                    {
                        "name": "Xindi Tong"
                    },
                    {
                        "name": "Yujin Zhu"
                    },
                    {
                        "name": "Shijian Fan"
                    },
                    {
                        "name": "Liang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Liang Xu"
                },
                "author": "Liang Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17640v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17640v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19018v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19018v3",
                "updated": "2025-01-22T06:53:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    6,
                    53,
                    56,
                    2,
                    22,
                    0
                ],
                "published": "2024-12-26T01:56:42Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    1,
                    56,
                    42,
                    3,
                    361,
                    0
                ],
                "title": "Let the Fuzzy Rule Speak: Enhancing In-context Learning Debiasing with\n  Interpretability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Let the Fuzzy Rule Speak: Enhancing In-context Learning Debiasing with\n  Interpretability"
                },
                "summary": "One of the potential failures of large language models (LLMs) is their\nimbalanced class performances in text classification tasks. With in-context\nlearning (ICL), LLMs yields good accuracy for some classes but low accuracy for\nothers. This imbalance is particularly problematic when misclassifications lead\nto user dissatisfaction or safety risks. While the root causes may lie in the\ndata, addressing them from the source through training is neither easy nor\ncost-effective. To delve deeper, the imbalance stems from certain classes\nconsistently receiving disproportionately high ICL probabilities, while others\nreceive lower probabilities, resulting in under-prediction and lower accuracy\nin the latter. Crucially, probability ranges vary in their impact on the\nimbalance, enabling precise corrections by range. Therefore, this work\nintroduces an inference-time debiasing method, FuRud (Fuzzy Rule\nOptimization-based Debiasing), to tackle this issue. FuRud addresses core\ninterpretability challenges by determining why certain classes require\ncorrections and tailoring adjustments for each sample and class probability.\nTailored corrections use fuzzy sets with triangular membership functions,\nbecause they can transform per-sample class probabilities based on probability\nranges. Each class selects one from 19 triangular membership functions, solving\na nonlinear integer programming selection problem with simulated annealing, to\nminimize class accuracy bias (COBias) and maximize overall accuracy without\nupdating LLM parameters. Notably, across seven benchmark datasets, FuRud\nreduces COBias by more than half (56%), while achieving a relative increase of\n21% in overall accuracy, outperforming state-of-the-art debiasing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One of the potential failures of large language models (LLMs) is their\nimbalanced class performances in text classification tasks. With in-context\nlearning (ICL), LLMs yields good accuracy for some classes but low accuracy for\nothers. This imbalance is particularly problematic when misclassifications lead\nto user dissatisfaction or safety risks. While the root causes may lie in the\ndata, addressing them from the source through training is neither easy nor\ncost-effective. To delve deeper, the imbalance stems from certain classes\nconsistently receiving disproportionately high ICL probabilities, while others\nreceive lower probabilities, resulting in under-prediction and lower accuracy\nin the latter. Crucially, probability ranges vary in their impact on the\nimbalance, enabling precise corrections by range. Therefore, this work\nintroduces an inference-time debiasing method, FuRud (Fuzzy Rule\nOptimization-based Debiasing), to tackle this issue. FuRud addresses core\ninterpretability challenges by determining why certain classes require\ncorrections and tailoring adjustments for each sample and class probability.\nTailored corrections use fuzzy sets with triangular membership functions,\nbecause they can transform per-sample class probabilities based on probability\nranges. Each class selects one from 19 triangular membership functions, solving\na nonlinear integer programming selection problem with simulated annealing, to\nminimize class accuracy bias (COBias) and maximize overall accuracy without\nupdating LLM parameters. Notably, across seven benchmark datasets, FuRud\nreduces COBias by more than half (56%), while achieving a relative increase of\n21% in overall accuracy, outperforming state-of-the-art debiasing methods."
                },
                "authors": [
                    {
                        "name": "Ruixi Lin"
                    },
                    {
                        "name": "Yang You"
                    }
                ],
                "author_detail": {
                    "name": "Yang You"
                },
                "author": "Yang You",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19018v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19018v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.01331v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.01331v3",
                "updated": "2025-01-22T06:13:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    6,
                    13,
                    41,
                    2,
                    22,
                    0
                ],
                "published": "2023-10-02T16:49:39Z",
                "published_parsed": [
                    2023,
                    10,
                    2,
                    16,
                    49,
                    39,
                    0,
                    275,
                    0
                ],
                "title": "ChoiceMates: Supporting Unfamiliar Online Decision-Making with\n  Multi-Agent Conversational Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChoiceMates: Supporting Unfamiliar Online Decision-Making with\n  Multi-Agent Conversational Interactions"
                },
                "summary": "From deciding on a PhD program to buying a new camera, unfamiliar\ndecisions--decisions without domain knowledge--are frequent and significant.\nThe complexity and uncertainty of such decisions demand unique approaches to\ninformation seeking, understanding, and decision-making. Our formative study\nhighlights that users want to start by discovering broad and relevant domain\ninformation evenly and simultaneously, quickly address emerging inquiries, and\ngain personalized standards to assess information found. We present\nChoiceMates, an interactive multi-agent system designed to address these needs\nby enabling users to engage with a dynamic set of LLM agents each presenting a\nunique experience in the domain. Unlike existing multi-agent systems that\nautomate tasks with agents, the user orchestrates agents to assist their\ndecision-making process. Our user evaluation (n=12) shows that ChoiceMates\nenables a more confident, satisfactory decision-making with better situation\nunderstanding than web search, and higher decision quality and confidence than\na commercial multi-agent framework. This work provides insights into designing\na more controllable and collaborative multi-agent system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From deciding on a PhD program to buying a new camera, unfamiliar\ndecisions--decisions without domain knowledge--are frequent and significant.\nThe complexity and uncertainty of such decisions demand unique approaches to\ninformation seeking, understanding, and decision-making. Our formative study\nhighlights that users want to start by discovering broad and relevant domain\ninformation evenly and simultaneously, quickly address emerging inquiries, and\ngain personalized standards to assess information found. We present\nChoiceMates, an interactive multi-agent system designed to address these needs\nby enabling users to engage with a dynamic set of LLM agents each presenting a\nunique experience in the domain. Unlike existing multi-agent systems that\nautomate tasks with agents, the user orchestrates agents to assist their\ndecision-making process. Our user evaluation (n=12) shows that ChoiceMates\nenables a more confident, satisfactory decision-making with better situation\nunderstanding than web search, and higher decision quality and confidence than\na commercial multi-agent framework. This work provides insights into designing\na more controllable and collaborative multi-agent system."
                },
                "authors": [
                    {
                        "name": "Jeongeon Park"
                    },
                    {
                        "name": "Bryan Min"
                    },
                    {
                        "name": "Kihoon Son"
                    },
                    {
                        "name": "Jean Y. Song"
                    },
                    {
                        "name": "Xiaojuan Ma"
                    },
                    {
                        "name": "Juho Kim"
                    }
                ],
                "author_detail": {
                    "name": "Juho Kim"
                },
                "author": "Juho Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.01331v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.01331v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.17404v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.17404v2",
                "updated": "2025-01-22T05:52:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    5,
                    52,
                    42,
                    2,
                    22,
                    0
                ],
                "published": "2024-07-24T16:36:02Z",
                "published_parsed": [
                    2024,
                    7,
                    24,
                    16,
                    36,
                    2,
                    2,
                    206,
                    0
                ],
                "title": "Grammar-based Game Description Generation using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grammar-based Game Description Generation using Large Language Models"
                },
                "summary": "Game Description Language (GDL) provides a standardized way to express\ndiverse games in a machine-readable format, enabling automated game simulation,\nand evaluation. While previous research has explored game description\ngeneration using search-based methods, generating GDL descriptions from natural\nlanguage remains a challenging task. This paper presents a novel framework that\nleverages Large Language Models (LLMs) to generate grammatically accurate game\ndescriptions from natural language. Our approach consists of two stages: first,\nwe gradually generate a minimal grammar based on GDL specifications; second, we\niteratively improve the game description through grammar-guided generation. Our\nframework employs a specialized parser that identifies valid subsequences and\ncandidate symbols from LLM responses, enabling gradual refinement of the output\nto ensure grammatical correctness. Experimental results demonstrate that our\niterative improvement approach significantly outperforms baseline methods that\ndirectly use LLM outputs. Our code is available at\nhttps://github.com/tsunehiko/ggdg",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Game Description Language (GDL) provides a standardized way to express\ndiverse games in a machine-readable format, enabling automated game simulation,\nand evaluation. While previous research has explored game description\ngeneration using search-based methods, generating GDL descriptions from natural\nlanguage remains a challenging task. This paper presents a novel framework that\nleverages Large Language Models (LLMs) to generate grammatically accurate game\ndescriptions from natural language. Our approach consists of two stages: first,\nwe gradually generate a minimal grammar based on GDL specifications; second, we\niteratively improve the game description through grammar-guided generation. Our\nframework employs a specialized parser that identifies valid subsequences and\ncandidate symbols from LLM responses, enabling gradual refinement of the output\nto ensure grammatical correctness. Experimental results demonstrate that our\niterative improvement approach significantly outperforms baseline methods that\ndirectly use LLM outputs. Our code is available at\nhttps://github.com/tsunehiko/ggdg"
                },
                "authors": [
                    {
                        "name": "Tsunehiko Tanaka"
                    },
                    {
                        "name": "Edgar Simo-Serra"
                    }
                ],
                "author_detail": {
                    "name": "Edgar Simo-Serra"
                },
                "author": "Edgar Simo-Serra",
                "arxiv_doi": "10.1109/TG.2024.3520214",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TG.2024.3520214",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.17404v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.17404v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted for publication at the IEEE Transactions on Games",
                "arxiv_journal_ref": "IEEE Transactions on Games, 2024 (early access)",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11820v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11820v2",
                "updated": "2025-01-22T05:42:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    5,
                    42,
                    45,
                    2,
                    22,
                    0
                ],
                "published": "2024-08-02T22:40:20Z",
                "published_parsed": [
                    2024,
                    8,
                    2,
                    22,
                    40,
                    20,
                    4,
                    215,
                    0
                ],
                "title": "Responsible AI Question Bank: A Comprehensive Tool for AI Risk\n  Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Responsible AI Question Bank: A Comprehensive Tool for AI Risk\n  Assessment"
                },
                "summary": "The rapid growth of Artificial Intelligence (AI) has underscored the urgent\nneed for responsible AI practices. Despite increasing interest, a comprehensive\nAI risk assessment toolkit remains lacking. This study introduces our\nResponsible AI (RAI) Question Bank, a comprehensive framework and tool designed\nto support diverse AI initiatives. By integrating AI ethics principles such as\nfairness, transparency, and accountability into a structured question format,\nthe RAI Question Bank aids in identifying potential risks, aligning with\nemerging regulations like the EU AI Act, and enhancing overall AI governance. A\nkey benefit of the RAI Question Bank is its systematic approach to linking\nlower-level risk questions to higher-level ones and related themes, preventing\nsiloed assessments and ensuring a cohesive evaluation process. Case studies\nillustrate the practical application of the RAI Question Bank in assessing AI\nprojects, from evaluating risk factors to informing decision-making processes.\nThe study also demonstrates how the RAI Question Bank can be used to ensure\ncompliance with standards, mitigate risks, and promote the development of\ntrustworthy AI systems. This work advances RAI by providing organizations with\na valuable tool to navigate the complexities of ethical AI development and\ndeployment while ensuring comprehensive risk management.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of Artificial Intelligence (AI) has underscored the urgent\nneed for responsible AI practices. Despite increasing interest, a comprehensive\nAI risk assessment toolkit remains lacking. This study introduces our\nResponsible AI (RAI) Question Bank, a comprehensive framework and tool designed\nto support diverse AI initiatives. By integrating AI ethics principles such as\nfairness, transparency, and accountability into a structured question format,\nthe RAI Question Bank aids in identifying potential risks, aligning with\nemerging regulations like the EU AI Act, and enhancing overall AI governance. A\nkey benefit of the RAI Question Bank is its systematic approach to linking\nlower-level risk questions to higher-level ones and related themes, preventing\nsiloed assessments and ensuring a cohesive evaluation process. Case studies\nillustrate the practical application of the RAI Question Bank in assessing AI\nprojects, from evaluating risk factors to informing decision-making processes.\nThe study also demonstrates how the RAI Question Bank can be used to ensure\ncompliance with standards, mitigate risks, and promote the development of\ntrustworthy AI systems. This work advances RAI by providing organizations with\na valuable tool to navigate the complexities of ethical AI development and\ndeployment while ensuring comprehensive risk management."
                },
                "authors": [
                    {
                        "name": "Sung Une Lee"
                    },
                    {
                        "name": "Harsha Perera"
                    },
                    {
                        "name": "Yue Liu"
                    },
                    {
                        "name": "Boming Xia"
                    },
                    {
                        "name": "Qinghua Lu"
                    },
                    {
                        "name": "Liming Zhu"
                    },
                    {
                        "name": "Olivier Salvado"
                    },
                    {
                        "name": "Jon Whittle"
                    }
                ],
                "author_detail": {
                    "name": "Jon Whittle"
                },
                "author": "Jon Whittle",
                "arxiv_comment": "30 pages, 6 tables, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11820v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11820v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00907v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00907v3",
                "updated": "2025-01-22T05:04:26Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    5,
                    4,
                    26,
                    2,
                    22,
                    0
                ],
                "published": "2024-11-01T09:22:49Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    9,
                    22,
                    49,
                    4,
                    306,
                    0
                ],
                "title": "On the Impact of White-box Deployment Strategies for Edge AI on Latency\n  and Model Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Impact of White-box Deployment Strategies for Edge AI on Latency\n  and Model Performance"
                },
                "summary": "To help MLOps engineers decide which operator to use in which deployment\nscenario, this study aims to empirically assess the accuracy vs latency\ntrade-off of white-box (training-based) and black-box operators\n(non-training-based) and their combinations in an Edge AI setup. We perform\ninference experiments including 3 white-box (i.e., QAT, Pruning, Knowledge\nDistillation), 2 black-box (i.e., Partition, SPTQ), and their combined\noperators (i.e., Distilled SPTQ, SPTQ Partition) across 3 tiers (i.e., Mobile,\nEdge, Cloud) on 4 commonly-used Computer Vision and Natural Language Processing\nmodels to identify the effective strategies, considering the perspective of\nMLOps Engineers. Our Results indicate that the combination of Distillation and\nSPTQ operators (i.e., DSPTQ) should be preferred over non-hybrid operators when\nlower latency is required in the edge at small to medium accuracy drop. Among\nthe non-hybrid operators, the Distilled operator is a better alternative in\nboth mobile and edge tiers for lower latency performance at the cost of small\nto medium accuracy loss. Moreover, the operators involving distillation show\nlower latency in resource-constrained tiers (Mobile, Edge) compared to the\noperators involving Partitioning across Mobile and Edge tiers. For textual\nsubject models, which have low input data size requirements, the Cloud tier is\na better alternative for the deployment of operators than the Mobile, Edge, or\nMobile-Edge tier (the latter being used for operators involving partitioning).\nIn contrast, for image-based subject models, which have high input data size\nrequirements, the Edge tier is a better alternative for operators than Mobile,\nEdge, or their combination.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To help MLOps engineers decide which operator to use in which deployment\nscenario, this study aims to empirically assess the accuracy vs latency\ntrade-off of white-box (training-based) and black-box operators\n(non-training-based) and their combinations in an Edge AI setup. We perform\ninference experiments including 3 white-box (i.e., QAT, Pruning, Knowledge\nDistillation), 2 black-box (i.e., Partition, SPTQ), and their combined\noperators (i.e., Distilled SPTQ, SPTQ Partition) across 3 tiers (i.e., Mobile,\nEdge, Cloud) on 4 commonly-used Computer Vision and Natural Language Processing\nmodels to identify the effective strategies, considering the perspective of\nMLOps Engineers. Our Results indicate that the combination of Distillation and\nSPTQ operators (i.e., DSPTQ) should be preferred over non-hybrid operators when\nlower latency is required in the edge at small to medium accuracy drop. Among\nthe non-hybrid operators, the Distilled operator is a better alternative in\nboth mobile and edge tiers for lower latency performance at the cost of small\nto medium accuracy loss. Moreover, the operators involving distillation show\nlower latency in resource-constrained tiers (Mobile, Edge) compared to the\noperators involving Partitioning across Mobile and Edge tiers. For textual\nsubject models, which have low input data size requirements, the Cloud tier is\na better alternative for the deployment of operators than the Mobile, Edge, or\nMobile-Edge tier (the latter being used for operators involving partitioning).\nIn contrast, for image-based subject models, which have high input data size\nrequirements, the Edge tier is a better alternative for operators than Mobile,\nEdge, or their combination."
                },
                "authors": [
                    {
                        "name": "Jaskirat Singh"
                    },
                    {
                        "name": "Bram Adams"
                    },
                    {
                        "name": "Ahmed E. Hassan"
                    }
                ],
                "author_detail": {
                    "name": "Ahmed E. Hassan"
                },
                "author": "Ahmed E. Hassan",
                "arxiv_comment": "In Approach Section, Pruning & Knowledge Distillation methods of\n  Intel Neural Compressor don't reduce the model size & improve performance,\n  respectively, unlike previous studies. There are issues exporting QAT models\n  from PyTorch to ONNX, raising concerns about our latency results",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00907v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00907v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12642v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12642v1",
                "updated": "2025-01-22T05:03:51Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    5,
                    3,
                    51,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T05:03:51Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    5,
                    3,
                    51,
                    2,
                    22,
                    0
                ],
                "title": "Training Data Attribution (TDA): Examining Its Adoption & Use Cases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training Data Attribution (TDA): Examining Its Adoption & Use Cases"
                },
                "summary": "This report investigates Training Data Attribution (TDA) and its potential\nimportance to and tractability for reducing extreme risks from AI. First, we\ndiscuss the plausibility and amount of effort it would take to bring existing\nTDA research efforts from their current state, to an efficient and accurate\ntool for TDA inference that can be run on frontier-scale LLMs. Next, we discuss\nthe numerous research benefits AI labs will expect to see from using such TDA\ntooling. Then, we discuss a key outstanding bottleneck that would limit such\nTDA tooling from being accessible publicly: AI labs' willingness to disclose\ntheir training data. We suggest ways AI labs may work around these limitations,\nand discuss the willingness of governments to mandate such access. Assuming\nthat AI labs willingly provide access to TDA inference, we then discuss what\nhigh-level societal benefits you might see. We list and discuss a series of\npolicies and systems that may be enabled by TDA. Finally, we present an\nevaluation of TDA's potential impact on mitigating large-scale risks from AI\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This report investigates Training Data Attribution (TDA) and its potential\nimportance to and tractability for reducing extreme risks from AI. First, we\ndiscuss the plausibility and amount of effort it would take to bring existing\nTDA research efforts from their current state, to an efficient and accurate\ntool for TDA inference that can be run on frontier-scale LLMs. Next, we discuss\nthe numerous research benefits AI labs will expect to see from using such TDA\ntooling. Then, we discuss a key outstanding bottleneck that would limit such\nTDA tooling from being accessible publicly: AI labs' willingness to disclose\ntheir training data. We suggest ways AI labs may work around these limitations,\nand discuss the willingness of governments to mandate such access. Assuming\nthat AI labs willingly provide access to TDA inference, we then discuss what\nhigh-level societal benefits you might see. We list and discuss a series of\npolicies and systems that may be enabled by TDA. Finally, we present an\nevaluation of TDA's potential impact on mitigating large-scale risks from AI\nsystems."
                },
                "authors": [
                    {
                        "name": "Deric Cheng"
                    },
                    {
                        "name": "Juhan Bae"
                    },
                    {
                        "name": "Justin Bullock"
                    },
                    {
                        "name": "David Kristofferson"
                    }
                ],
                "author_detail": {
                    "name": "David Kristofferson"
                },
                "author": "David Kristofferson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12642v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12642v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12634v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12634v1",
                "updated": "2025-01-22T04:42:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    4,
                    42,
                    19,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T04:42:19Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    4,
                    42,
                    19,
                    2,
                    22,
                    0
                ],
                "title": "SoMa: Identifying, Exploring, and Understanding the DRAM Communication\n  Scheduling Space for DNN Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SoMa: Identifying, Exploring, and Understanding the DRAM Communication\n  Scheduling Space for DNN Accelerators"
                },
                "summary": "Modern Deep Neural Network (DNN) accelerators are equipped with increasingly\nlarger on-chip buffers to provide more opportunities to alleviate the\nincreasingly severe DRAM bandwidth pressure. However, most existing research on\nbuffer utilization still primarily focuses on single-layer dataflow scheduling\noptimization. As buffers grow large enough to accommodate most single-layer\nweights in most networks, the impact of single-layer dataflow optimization on\nDRAM communication diminishes significantly. Therefore, developing new\nparadigms that fuse multiple layers to fully leverage the increasingly abundant\non-chip buffer resources to reduce DRAM accesses has become particularly\nimportant, yet remains an open challenge. To address this challenge, we first\nidentify the optimization opportunities in DRAM communication scheduling by\nanalyzing the drawbacks of existing works on the layer fusion paradigm and\nrecognizing the vast optimization potential in scheduling the timing of data\nprefetching from and storing to DRAM. To fully exploit these optimization\nopportunities, we develop a Tensor-centric Notation and its corresponding\nparsing method to represent different DRAM communication scheduling schemes and\ndepict the overall space of DRAM communication scheduling. Then, to thoroughly\nand efficiently explore the space of DRAM communication scheduling for diverse\naccelerators and workloads, we develop an end-to-end scheduling framework,\nSoMa, which has already been developed into a compiler for our commercial\naccelerator product. Compared with the state-of-the-art (SOTA) Cocco framework,\nSoMa achieves, on average, a 2.11x performance improvement and a 37.3%\nreduction in energy cost simultaneously. Then, we leverage SoMa to study\noptimizations for LLM, perform design space exploration (DSE), and analyze the\nDRAM communication scheduling space through a practical example, yielding\nsome..(more)",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Deep Neural Network (DNN) accelerators are equipped with increasingly\nlarger on-chip buffers to provide more opportunities to alleviate the\nincreasingly severe DRAM bandwidth pressure. However, most existing research on\nbuffer utilization still primarily focuses on single-layer dataflow scheduling\noptimization. As buffers grow large enough to accommodate most single-layer\nweights in most networks, the impact of single-layer dataflow optimization on\nDRAM communication diminishes significantly. Therefore, developing new\nparadigms that fuse multiple layers to fully leverage the increasingly abundant\non-chip buffer resources to reduce DRAM accesses has become particularly\nimportant, yet remains an open challenge. To address this challenge, we first\nidentify the optimization opportunities in DRAM communication scheduling by\nanalyzing the drawbacks of existing works on the layer fusion paradigm and\nrecognizing the vast optimization potential in scheduling the timing of data\nprefetching from and storing to DRAM. To fully exploit these optimization\nopportunities, we develop a Tensor-centric Notation and its corresponding\nparsing method to represent different DRAM communication scheduling schemes and\ndepict the overall space of DRAM communication scheduling. Then, to thoroughly\nand efficiently explore the space of DRAM communication scheduling for diverse\naccelerators and workloads, we develop an end-to-end scheduling framework,\nSoMa, which has already been developed into a compiler for our commercial\naccelerator product. Compared with the state-of-the-art (SOTA) Cocco framework,\nSoMa achieves, on average, a 2.11x performance improvement and a 37.3%\nreduction in energy cost simultaneously. Then, we leverage SoMa to study\noptimizations for LLM, perform design space exploration (DSE), and analyze the\nDRAM communication scheduling space through a practical example, yielding\nsome..(more)"
                },
                "authors": [
                    {
                        "name": "Jingwei Cai"
                    },
                    {
                        "name": "Xuan Wang"
                    },
                    {
                        "name": "Mingyu Gao"
                    },
                    {
                        "name": "Sen Peng"
                    },
                    {
                        "name": "Zijian Zhu"
                    },
                    {
                        "name": "Yuchen Wei"
                    },
                    {
                        "name": "Zuotong Wu"
                    },
                    {
                        "name": "Kaisheng Ma"
                    }
                ],
                "author_detail": {
                    "name": "Kaisheng Ma"
                },
                "arxiv_affiliation": "Tsinghua University",
                "author": "Kaisheng Ma",
                "arxiv_comment": "Accepted by 2025 IEEE International Symposium on High-Performance\n  Computer Architecture (HPCA)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12634v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12634v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07098v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07098v2",
                "updated": "2025-01-22T04:42:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    4,
                    42,
                    10,
                    2,
                    22,
                    0
                ],
                "published": "2024-11-11T16:20:27Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    16,
                    20,
                    27,
                    0,
                    316,
                    0
                ],
                "title": "A Multi-Agent Approach for REST API Testing with Semantic Graphs and\n  LLM-Driven Inputs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Multi-Agent Approach for REST API Testing with Semantic Graphs and\n  LLM-Driven Inputs"
                },
                "summary": "As modern web services increasingly rely on REST APIs, their thorough testing\nhas become crucial. Furthermore, the advent of REST API documentation\nlanguages, such as the OpenAPI Specification, has led to the emergence of many\nblack-box REST API testing tools. However, these tools often focus on\nindividual test elements in isolation (e.g., APIs, parameters, values),\nresulting in lower coverage and less effectiveness in fault detection. To\naddress these limitations, we present AutoRestTest, the first black-box tool to\nadopt a dependency-embedded multi-agent approach for REST API testing that\nintegrates multi-agent reinforcement learning (MARL) with a semantic property\ndependency graph (SPDG) and Large Language Models (LLMs). Our approach treats\nREST API testing as a separable problem, where four agents -- API, dependency,\nparameter, and value agents -- collaborate to optimize API exploration. LLMs\nhandle domain-specific value generation, the SPDG model simplifies the search\nspace for dependencies using a similarity score between API operations, and\nMARL dynamically optimizes the agents' behavior. Our evaluation of AutoRestTest\non 12 real-world REST services shows that it outperforms the four leading\nblack-box REST API testing tools, including those assisted by RESTGPT (which\ngenerates realistic test inputs using LLMs), in terms of code coverage,\noperation coverage, and fault detection. Notably, AutoRestTest is the only tool\nable to trigger an internal server error in the Spotify service. Our ablation\nstudy illustrates that each component of AutoRestTest -- the SPDG, the LLM, and\nthe agent-learning mechanism -- contributes to its overall effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As modern web services increasingly rely on REST APIs, their thorough testing\nhas become crucial. Furthermore, the advent of REST API documentation\nlanguages, such as the OpenAPI Specification, has led to the emergence of many\nblack-box REST API testing tools. However, these tools often focus on\nindividual test elements in isolation (e.g., APIs, parameters, values),\nresulting in lower coverage and less effectiveness in fault detection. To\naddress these limitations, we present AutoRestTest, the first black-box tool to\nadopt a dependency-embedded multi-agent approach for REST API testing that\nintegrates multi-agent reinforcement learning (MARL) with a semantic property\ndependency graph (SPDG) and Large Language Models (LLMs). Our approach treats\nREST API testing as a separable problem, where four agents -- API, dependency,\nparameter, and value agents -- collaborate to optimize API exploration. LLMs\nhandle domain-specific value generation, the SPDG model simplifies the search\nspace for dependencies using a similarity score between API operations, and\nMARL dynamically optimizes the agents' behavior. Our evaluation of AutoRestTest\non 12 real-world REST services shows that it outperforms the four leading\nblack-box REST API testing tools, including those assisted by RESTGPT (which\ngenerates realistic test inputs using LLMs), in terms of code coverage,\noperation coverage, and fault detection. Notably, AutoRestTest is the only tool\nable to trigger an internal server error in the Spotify service. Our ablation\nstudy illustrates that each component of AutoRestTest -- the SPDG, the LLM, and\nthe agent-learning mechanism -- contributes to its overall effectiveness."
                },
                "authors": [
                    {
                        "name": "Myeongsoo Kim"
                    },
                    {
                        "name": "Tyler Stennett"
                    },
                    {
                        "name": "Saurabh Sinha"
                    },
                    {
                        "name": "Alessandro Orso"
                    }
                ],
                "author_detail": {
                    "name": "Alessandro Orso"
                },
                "author": "Alessandro Orso",
                "arxiv_comment": "To be published in the 47th IEEE/ACM International Conference on\n  Software Engineering (ICSE 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07098v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07098v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12632v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12632v1",
                "updated": "2025-01-22T04:36:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    4,
                    36,
                    17,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T04:36:17Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    4,
                    36,
                    17,
                    2,
                    22,
                    0
                ],
                "title": "TeD-Loc: Text Distillation for Weakly Supervised Object Localization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TeD-Loc: Text Distillation for Weakly Supervised Object Localization"
                },
                "summary": "Weakly supervised object localization (WSOL) using classification models\ntrained with only image-class labels remains an important challenge in computer\nvision. Given their reliance on classification objectives, traditional WSOL\nmethods like class activation mapping focus on the most discriminative object\nparts, often missing the full spatial extent. In contrast, recent WSOL methods\nbased on vision-language models like CLIP require ground truth classes or\nexternal classifiers to produce a localization map, limiting their deployment\nin downstream tasks. Moreover, methods like GenPromp attempt to address these\nissues but introduce considerable complexity due to their reliance on\nconditional denoising processes and intricate prompt learning. This paper\nintroduces Text Distillation for Localization (TeD-Loc), an approach that\ndirectly distills knowledge from CLIP text embeddings into the model backbone\nand produces patch-level localization. Multiple instance learning of these\nimage patches allows for accurate localization and classification using one\nmodel without requiring external classifiers. Such integration of textual and\nvisual modalities addresses the longstanding challenge of achieving accurate\nlocalization and classification concurrently, as WSOL methods in the literature\ntypically converge at different epochs. Extensive experiments show that\nleveraging text embeddings and localization cues provides a cost-effective WSOL\nmodel. TeD-Loc improves Top-1 LOC accuracy over state-of-the-art models by\nabout 5% on both CUB and ILSVRC datasets, while significantly reducing\ncomputational complexity compared to GenPromp.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Weakly supervised object localization (WSOL) using classification models\ntrained with only image-class labels remains an important challenge in computer\nvision. Given their reliance on classification objectives, traditional WSOL\nmethods like class activation mapping focus on the most discriminative object\nparts, often missing the full spatial extent. In contrast, recent WSOL methods\nbased on vision-language models like CLIP require ground truth classes or\nexternal classifiers to produce a localization map, limiting their deployment\nin downstream tasks. Moreover, methods like GenPromp attempt to address these\nissues but introduce considerable complexity due to their reliance on\nconditional denoising processes and intricate prompt learning. This paper\nintroduces Text Distillation for Localization (TeD-Loc), an approach that\ndirectly distills knowledge from CLIP text embeddings into the model backbone\nand produces patch-level localization. Multiple instance learning of these\nimage patches allows for accurate localization and classification using one\nmodel without requiring external classifiers. Such integration of textual and\nvisual modalities addresses the longstanding challenge of achieving accurate\nlocalization and classification concurrently, as WSOL methods in the literature\ntypically converge at different epochs. Extensive experiments show that\nleveraging text embeddings and localization cues provides a cost-effective WSOL\nmodel. TeD-Loc improves Top-1 LOC accuracy over state-of-the-art models by\nabout 5% on both CUB and ILSVRC datasets, while significantly reducing\ncomputational complexity compared to GenPromp."
                },
                "authors": [
                    {
                        "name": "Shakeeb Murtaza"
                    },
                    {
                        "name": "Soufiane Belharbi"
                    },
                    {
                        "name": "Marco Pedersoli"
                    },
                    {
                        "name": "Eric Granger"
                    }
                ],
                "author_detail": {
                    "name": "Eric Granger"
                },
                "author": "Eric Granger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12632v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12632v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12619v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12619v1",
                "updated": "2025-01-22T03:57:52Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    3,
                    57,
                    52,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T03:57:52Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    3,
                    57,
                    52,
                    2,
                    22,
                    0
                ],
                "title": "Distillation Quantification for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distillation Quantification for Large Language Models"
                },
                "summary": "Model distillation is a technique for transferring knowledge from large\nlanguage models (LLMs) to smaller ones, aiming to create resource-efficient yet\nhigh-performing models. However, excessive distillation can lead to\nhomogenization, reducing diversity among models and impairing their ability to\nrobustly handle complex or novel tasks. These limitations underscore the need\nto systematically quantify the distillation process and its impact. In this\nwork, we propose a framework to evaluate and quantify model distillation. Our\nmethod addresses two key aspects: (1) Identifying identity cognition\ncontradictions to assess discrepancies in how models perceive and represent\nidentity-related information, and (2) Analyzing multi-granularity response\nsimilarities across models to measure the extent of homogenization.\nExperimental results demonstrate two key insights: (1) Well-known closed-source\nand open-source LLMs usually exhibit high distillation degrees, except for\nClaude, Doubao, and Gemini. (2) Base LLMs show higher distillation degrees\ncompared to aligned LLMs. By offering a systematic approach to improve the\ntransparency of LLM data distillation, we call for LLMs with more independent\ndevelopment and more transparent technical reports to improve LLMs' robustness\nand safety. The code and data are available under\nhttps://github.com/Aegis1863/LLMs-Distillation-Quantification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model distillation is a technique for transferring knowledge from large\nlanguage models (LLMs) to smaller ones, aiming to create resource-efficient yet\nhigh-performing models. However, excessive distillation can lead to\nhomogenization, reducing diversity among models and impairing their ability to\nrobustly handle complex or novel tasks. These limitations underscore the need\nto systematically quantify the distillation process and its impact. In this\nwork, we propose a framework to evaluate and quantify model distillation. Our\nmethod addresses two key aspects: (1) Identifying identity cognition\ncontradictions to assess discrepancies in how models perceive and represent\nidentity-related information, and (2) Analyzing multi-granularity response\nsimilarities across models to measure the extent of homogenization.\nExperimental results demonstrate two key insights: (1) Well-known closed-source\nand open-source LLMs usually exhibit high distillation degrees, except for\nClaude, Doubao, and Gemini. (2) Base LLMs show higher distillation degrees\ncompared to aligned LLMs. By offering a systematic approach to improve the\ntransparency of LLM data distillation, we call for LLMs with more independent\ndevelopment and more transparent technical reports to improve LLMs' robustness\nand safety. The code and data are available under\nhttps://github.com/Aegis1863/LLMs-Distillation-Quantification."
                },
                "authors": [
                    {
                        "name": "Sunbowen Lee"
                    },
                    {
                        "name": "Junting Zhou"
                    },
                    {
                        "name": "Chang Ao"
                    },
                    {
                        "name": "Kaige Li"
                    },
                    {
                        "name": "Xinrun Du"
                    },
                    {
                        "name": "Sirui He"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Min Yang"
                    },
                    {
                        "name": "Zhoufutu Wen"
                    },
                    {
                        "name": "Shiwen Ni"
                    }
                ],
                "author_detail": {
                    "name": "Shiwen Ni"
                },
                "author": "Shiwen Ni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12619v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12619v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12617v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12617v1",
                "updated": "2025-01-22T03:51:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    3,
                    51,
                    56,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T03:51:56Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    3,
                    51,
                    56,
                    2,
                    22,
                    0
                ],
                "title": "Deep Learning-Based Identification of Inconsistent Method Names: How Far\n  Are We?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning-Based Identification of Inconsistent Method Names: How Far\n  Are We?"
                },
                "summary": "Concise and meaningful method names are crucial for program comprehension and\nmaintenance. However, method names may become inconsistent with their\ncorresponding implementations, causing confusion and errors. Several deep\nlearning (DL)-based approaches have been proposed to identify such\ninconsistencies, with initial evaluations showing promising results. However,\nthese evaluations typically use a balanced dataset, where the number of\ninconsistent and consistent names are equal. This setup, along with flawed\ndataset construction, leads to false positives, making reported performance\nless reliable in real-world scenarios, where most method names are consistent.\nIn this paper, we present an empirical study that evaluates state-of-the-art\nDL-based methods for identifying inconsistent method names. We create a new\nbenchmark by combining automatic identification from commit histories and\nmanual developer inspections, reducing false positives. We evaluate five\nrepresentative DL approaches (one retrieval-based and four generation-based) on\nthis benchmark. Our results show that performance drops substantially when\nmoving from the balanced dataset to the new benchmark. We further conduct\nquantitative and qualitative analyses to understand the strengths and\nweaknesses of the approaches. Retrieval-based methods perform well on simple\nmethods and those with popular name sub-tokens but fail due to inefficient\nrepresentation techniques. Generation-based methods struggle with inaccurate\nsimilarity calculations and immature name generation. Based on these findings,\nwe propose improvements using contrastive learning and large language models\n(LLMs). Our study suggests that significant improvements are needed before\nthese DL approaches can be effectively applied to real-world software systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Concise and meaningful method names are crucial for program comprehension and\nmaintenance. However, method names may become inconsistent with their\ncorresponding implementations, causing confusion and errors. Several deep\nlearning (DL)-based approaches have been proposed to identify such\ninconsistencies, with initial evaluations showing promising results. However,\nthese evaluations typically use a balanced dataset, where the number of\ninconsistent and consistent names are equal. This setup, along with flawed\ndataset construction, leads to false positives, making reported performance\nless reliable in real-world scenarios, where most method names are consistent.\nIn this paper, we present an empirical study that evaluates state-of-the-art\nDL-based methods for identifying inconsistent method names. We create a new\nbenchmark by combining automatic identification from commit histories and\nmanual developer inspections, reducing false positives. We evaluate five\nrepresentative DL approaches (one retrieval-based and four generation-based) on\nthis benchmark. Our results show that performance drops substantially when\nmoving from the balanced dataset to the new benchmark. We further conduct\nquantitative and qualitative analyses to understand the strengths and\nweaknesses of the approaches. Retrieval-based methods perform well on simple\nmethods and those with popular name sub-tokens but fail due to inefficient\nrepresentation techniques. Generation-based methods struggle with inaccurate\nsimilarity calculations and immature name generation. Based on these findings,\nwe propose improvements using contrastive learning and large language models\n(LLMs). Our study suggests that significant improvements are needed before\nthese DL approaches can be effectively applied to real-world software systems."
                },
                "authors": [
                    {
                        "name": "Taiming Wang"
                    },
                    {
                        "name": "Yuxia Zhang"
                    },
                    {
                        "name": "Lin Jiang"
                    },
                    {
                        "name": "Yi Tang"
                    },
                    {
                        "name": "Guangjie Li"
                    },
                    {
                        "name": "Hui Liu"
                    }
                ],
                "author_detail": {
                    "name": "Hui Liu"
                },
                "author": "Hui Liu",
                "arxiv_doi": "10.1007/s10664-024-10592-z",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s10664-024-10592-z",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.12617v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12617v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Empirical Software Engineering, 2025, 30(1): 31",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.10528v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.10528v3",
                "updated": "2025-01-22T03:50:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    3,
                    50,
                    58,
                    2,
                    22,
                    0
                ],
                "published": "2024-02-16T09:29:50Z",
                "published_parsed": [
                    2024,
                    2,
                    16,
                    9,
                    29,
                    50,
                    4,
                    47,
                    0
                ],
                "title": "Can We Verify Step by Step for Incorrect Answer Detection?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can We Verify Step by Step for Incorrect Answer Detection?"
                },
                "summary": "Chain-of-Thought (CoT) prompting has marked a significant advancement in\nenhancing the reasoning capabilities of large language models (LLMs). Previous\nstudies have developed various extensions of CoT, which focus primarily on\nenhancing end-task performance. In addition, there has been research on\nassessing the quality of reasoning chains in CoT. This raises an intriguing\nquestion: Is it possible to predict the accuracy of LLM outputs by scrutinizing\nthe reasoning chains they generate? To answer this research question, we\nintroduce a benchmark, R2PE, designed specifically to explore the relationship\nbetween reasoning chains and performance in various reasoning tasks spanning\nfive different domains. This benchmark aims to measure the falsehood of the\nfinal output of LLMs based on the reasoning steps. To make full use of\ninformation in multiple reasoning chains, we propose the process discernibility\nscore (PDS) framework that beats the answer-checking baseline by a large\nmargin. Concretely, this resulted in an average of $5.1\\%$ increase in the F1\nscore and $2.97\\%$ improvement in AUC-PR across all 45 subsets within R2PE. We\nfurther demonstrate our PDS's efficacy in advancing open-domain QA accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) prompting has marked a significant advancement in\nenhancing the reasoning capabilities of large language models (LLMs). Previous\nstudies have developed various extensions of CoT, which focus primarily on\nenhancing end-task performance. In addition, there has been research on\nassessing the quality of reasoning chains in CoT. This raises an intriguing\nquestion: Is it possible to predict the accuracy of LLM outputs by scrutinizing\nthe reasoning chains they generate? To answer this research question, we\nintroduce a benchmark, R2PE, designed specifically to explore the relationship\nbetween reasoning chains and performance in various reasoning tasks spanning\nfive different domains. This benchmark aims to measure the falsehood of the\nfinal output of LLMs based on the reasoning steps. To make full use of\ninformation in multiple reasoning chains, we propose the process discernibility\nscore (PDS) framework that beats the answer-checking baseline by a large\nmargin. Concretely, this resulted in an average of $5.1\\%$ increase in the F1\nscore and $2.97\\%$ improvement in AUC-PR across all 45 subsets within R2PE. We\nfurther demonstrate our PDS's efficacy in advancing open-domain QA accuracy."
                },
                "authors": [
                    {
                        "name": "Xin Xu"
                    },
                    {
                        "name": "Shizhe Diao"
                    },
                    {
                        "name": "Can Yang"
                    },
                    {
                        "name": "Yang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yang Wang"
                },
                "author": "Yang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.10528v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.10528v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14219v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14219v2",
                "updated": "2025-01-22T03:33:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    3,
                    33,
                    25,
                    2,
                    22,
                    0
                ],
                "published": "2024-12-18T14:11:15Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    14,
                    11,
                    15,
                    2,
                    353,
                    0
                ],
                "title": "A Survey on Inference Optimization Techniques for Mixture of Experts\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Inference Optimization Techniques for Mixture of Experts\n  Models"
                },
                "summary": "The emergence of large-scale Mixture of Experts (MoE) models represents a\nsignificant advancement in artificial intelligence, offering enhanced model\ncapacity and computational efficiency through conditional computation. However,\ndeploying and running inference on these models presents significant challenges\nin computational resources, latency, and energy efficiency. This comprehensive\nsurvey analyzes optimization techniques for MoE models across the entire system\nstack. We first establish a taxonomical framework that categorizes optimization\napproaches into model-level, system-level, and hardware-level optimizations. At\nthe model level, we examine architectural innovations including efficient\nexpert design, attention mechanisms, various compression techniques such as\npruning, quantization, and knowledge distillation, as well as algorithm\nimprovement including dynamic routing strategies and expert merging methods. At\nthe system level, we investigate distributed computing approaches, load\nbalancing mechanisms, and efficient scheduling algorithms that enable scalable\ndeployment. Furthermore, we delve into hardware-specific optimizations and\nco-design strategies that maximize throughput and energy efficiency. This\nsurvey provides both a structured overview of existing solutions and identifies\nkey challenges and promising research directions in MoE inference optimization.\nTo facilitate ongoing updates and the sharing of cutting-edge advances in MoE\ninference optimization research, we have established a repository accessible at\nhttps://github.com/MoE-Inf/awesome-moe-inference/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of large-scale Mixture of Experts (MoE) models represents a\nsignificant advancement in artificial intelligence, offering enhanced model\ncapacity and computational efficiency through conditional computation. However,\ndeploying and running inference on these models presents significant challenges\nin computational resources, latency, and energy efficiency. This comprehensive\nsurvey analyzes optimization techniques for MoE models across the entire system\nstack. We first establish a taxonomical framework that categorizes optimization\napproaches into model-level, system-level, and hardware-level optimizations. At\nthe model level, we examine architectural innovations including efficient\nexpert design, attention mechanisms, various compression techniques such as\npruning, quantization, and knowledge distillation, as well as algorithm\nimprovement including dynamic routing strategies and expert merging methods. At\nthe system level, we investigate distributed computing approaches, load\nbalancing mechanisms, and efficient scheduling algorithms that enable scalable\ndeployment. Furthermore, we delve into hardware-specific optimizations and\nco-design strategies that maximize throughput and energy efficiency. This\nsurvey provides both a structured overview of existing solutions and identifies\nkey challenges and promising research directions in MoE inference optimization.\nTo facilitate ongoing updates and the sharing of cutting-edge advances in MoE\ninference optimization research, we have established a repository accessible at\nhttps://github.com/MoE-Inf/awesome-moe-inference/."
                },
                "authors": [
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Peng Tang"
                    },
                    {
                        "name": "Wenfeng Wang"
                    },
                    {
                        "name": "Yuhang Ren"
                    },
                    {
                        "name": "Xiaofeng Hou"
                    },
                    {
                        "name": "Pheng-Ann Heng"
                    },
                    {
                        "name": "Minyi Guo"
                    },
                    {
                        "name": "Chao Li"
                    }
                ],
                "author_detail": {
                    "name": "Chao Li"
                },
                "author": "Chao Li",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14219v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14219v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12599v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12599v1",
                "updated": "2025-01-22T02:48:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    2,
                    48,
                    14,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T02:48:14Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    2,
                    48,
                    14,
                    2,
                    22,
                    0
                ],
                "title": "Kimi k1.5: Scaling Reinforcement Learning with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kimi k1.5: Scaling Reinforcement Learning with LLMs"
                },
                "summary": "Language model pretraining with next token prediction has proved effective\nfor scaling compute but is limited to the amount of available training data.\nScaling reinforcement learning (RL) unlocks a new axis for the continued\nimprovement of artificial intelligence, with the promise that large language\nmodels (LLMs) can scale their training data by learning to explore with\nrewards. However, prior published work has not produced competitive results. In\nlight of this, we report on the training practice of Kimi k1.5, our latest\nmulti-modal LLM trained with RL, including its RL training techniques,\nmulti-modal data recipes, and infrastructure optimization. Long context scaling\nand improved policy optimization methods are key ingredients of our approach,\nwhich establishes a simplistic, effective RL framework without relying on more\ncomplex techniques such as Monte Carlo tree search, value functions, and\nprocess reward models. Notably, our system achieves state-of-the-art reasoning\nperformance across multiple benchmarks and modalities -- e.g., 77.5 on AIME,\n96.2 on MATH 500, 94-th percentile on Codeforces, 74.9 on MathVista -- matching\nOpenAI's o1. Moreover, we present effective long2short methods that use\nlong-CoT techniques to improve short-CoT models, yielding state-of-the-art\nshort-CoT reasoning results -- e.g., 60.8 on AIME, 94.6 on MATH500, 47.3 on\nLiveCodeBench -- outperforming existing short-CoT models such as GPT-4o and\nClaude Sonnet 3.5 by a large margin (up to +550%).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language model pretraining with next token prediction has proved effective\nfor scaling compute but is limited to the amount of available training data.\nScaling reinforcement learning (RL) unlocks a new axis for the continued\nimprovement of artificial intelligence, with the promise that large language\nmodels (LLMs) can scale their training data by learning to explore with\nrewards. However, prior published work has not produced competitive results. In\nlight of this, we report on the training practice of Kimi k1.5, our latest\nmulti-modal LLM trained with RL, including its RL training techniques,\nmulti-modal data recipes, and infrastructure optimization. Long context scaling\nand improved policy optimization methods are key ingredients of our approach,\nwhich establishes a simplistic, effective RL framework without relying on more\ncomplex techniques such as Monte Carlo tree search, value functions, and\nprocess reward models. Notably, our system achieves state-of-the-art reasoning\nperformance across multiple benchmarks and modalities -- e.g., 77.5 on AIME,\n96.2 on MATH 500, 94-th percentile on Codeforces, 74.9 on MathVista -- matching\nOpenAI's o1. Moreover, we present effective long2short methods that use\nlong-CoT techniques to improve short-CoT models, yielding state-of-the-art\nshort-CoT reasoning results -- e.g., 60.8 on AIME, 94.6 on MATH500, 47.3 on\nLiveCodeBench -- outperforming existing short-CoT models such as GPT-4o and\nClaude Sonnet 3.5 by a large margin (up to +550%)."
                },
                "authors": [
                    {
                        "name": "Kimi Team"
                    },
                    {
                        "name": "Angang Du"
                    },
                    {
                        "name": "Bofei Gao"
                    },
                    {
                        "name": "Bowei Xing"
                    },
                    {
                        "name": "Changjiu Jiang"
                    },
                    {
                        "name": "Cheng Chen"
                    },
                    {
                        "name": "Cheng Li"
                    },
                    {
                        "name": "Chenjun Xiao"
                    },
                    {
                        "name": "Chenzhuang Du"
                    },
                    {
                        "name": "Chonghua Liao"
                    },
                    {
                        "name": "Chuning Tang"
                    },
                    {
                        "name": "Congcong Wang"
                    },
                    {
                        "name": "Dehao Zhang"
                    },
                    {
                        "name": "Enming Yuan"
                    },
                    {
                        "name": "Enzhe Lu"
                    },
                    {
                        "name": "Fengxiang Tang"
                    },
                    {
                        "name": "Flood Sung"
                    },
                    {
                        "name": "Guangda Wei"
                    },
                    {
                        "name": "Guokun Lai"
                    },
                    {
                        "name": "Haiqing Guo"
                    },
                    {
                        "name": "Han Zhu"
                    },
                    {
                        "name": "Hao Ding"
                    },
                    {
                        "name": "Hao Hu"
                    },
                    {
                        "name": "Hao Yang"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Haotian Yao"
                    },
                    {
                        "name": "Haotian Zhao"
                    },
                    {
                        "name": "Haoyu Lu"
                    },
                    {
                        "name": "Haoze Li"
                    },
                    {
                        "name": "Haozhen Yu"
                    },
                    {
                        "name": "Hongcheng Gao"
                    },
                    {
                        "name": "Huabin Zheng"
                    },
                    {
                        "name": "Huan Yuan"
                    },
                    {
                        "name": "Jia Chen"
                    },
                    {
                        "name": "Jianhang Guo"
                    },
                    {
                        "name": "Jianlin Su"
                    },
                    {
                        "name": "Jianzhou Wang"
                    },
                    {
                        "name": "Jie Zhao"
                    },
                    {
                        "name": "Jin Zhang"
                    },
                    {
                        "name": "Jingyuan Liu"
                    },
                    {
                        "name": "Junjie Yan"
                    },
                    {
                        "name": "Junyan Wu"
                    },
                    {
                        "name": "Lidong Shi"
                    },
                    {
                        "name": "Ling Ye"
                    },
                    {
                        "name": "Longhui Yu"
                    },
                    {
                        "name": "Mengnan Dong"
                    },
                    {
                        "name": "Neo Zhang"
                    },
                    {
                        "name": "Ningchen Ma"
                    },
                    {
                        "name": "Qiwei Pan"
                    },
                    {
                        "name": "Qucheng Gong"
                    },
                    {
                        "name": "Shaowei Liu"
                    },
                    {
                        "name": "Shengling Ma"
                    },
                    {
                        "name": "Shupeng Wei"
                    },
                    {
                        "name": "Sihan Cao"
                    },
                    {
                        "name": "Siying Huang"
                    },
                    {
                        "name": "Tao Jiang"
                    },
                    {
                        "name": "Weihao Gao"
                    },
                    {
                        "name": "Weimin Xiong"
                    },
                    {
                        "name": "Weiran He"
                    },
                    {
                        "name": "Weixiao Huang"
                    },
                    {
                        "name": "Wenhao Wu"
                    },
                    {
                        "name": "Wenyang He"
                    },
                    {
                        "name": "Xianghui Wei"
                    },
                    {
                        "name": "Xianqing Jia"
                    },
                    {
                        "name": "Xingzhe Wu"
                    },
                    {
                        "name": "Xinran Xu"
                    },
                    {
                        "name": "Xinxing Zu"
                    },
                    {
                        "name": "Xinyu Zhou"
                    },
                    {
                        "name": "Xuehai Pan"
                    },
                    {
                        "name": "Y. Charles"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Yangyang Hu"
                    },
                    {
                        "name": "Yangyang Liu"
                    },
                    {
                        "name": "Yanru Chen"
                    },
                    {
                        "name": "Yejie Wang"
                    },
                    {
                        "name": "Yibo Liu"
                    },
                    {
                        "name": "Yidao Qin"
                    },
                    {
                        "name": "Yifeng Liu"
                    },
                    {
                        "name": "Ying Yang"
                    },
                    {
                        "name": "Yiping Bao"
                    },
                    {
                        "name": "Yulun Du"
                    },
                    {
                        "name": "Yuxin Wu"
                    },
                    {
                        "name": "Yuzhi Wang"
                    },
                    {
                        "name": "Zaida Zhou"
                    },
                    {
                        "name": "Zhaoji Wang"
                    },
                    {
                        "name": "Zhaowei Li"
                    },
                    {
                        "name": "Zhen Zhu"
                    },
                    {
                        "name": "Zheng Zhang"
                    },
                    {
                        "name": "Zhexu Wang"
                    },
                    {
                        "name": "Zhilin Yang"
                    },
                    {
                        "name": "Zhiqi Huang"
                    },
                    {
                        "name": "Zihao Huang"
                    },
                    {
                        "name": "Ziyao Xu"
                    },
                    {
                        "name": "Zonghan Yang"
                    }
                ],
                "author_detail": {
                    "name": "Zonghan Yang"
                },
                "author": "Zonghan Yang",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12599v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12599v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02406v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02406v2",
                "updated": "2025-01-22T02:43:21Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    2,
                    43,
                    21,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-04T23:51:43Z",
                "published_parsed": [
                    2025,
                    1,
                    4,
                    23,
                    51,
                    43,
                    5,
                    4,
                    0
                ],
                "title": "Zero-Shot Statistical Tests for LLM-Generated Text Detection using\n  Finite Sample Concentration Inequalities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Shot Statistical Tests for LLM-Generated Text Detection using\n  Finite Sample Concentration Inequalities"
                },
                "summary": "Verifying the provenance of content is crucial to the function of many\norganizations, e.g., educational institutions, social media platforms, firms,\netc. This problem is becoming increasingly difficult as text generated by Large\nLanguage Models (LLMs) becomes almost indistinguishable from human-generated\ncontent. In addition, many institutions utilize in-house LLMs and want to\nensure that external, non-sanctioned LLMs do not produce content within the\ninstitution. In this paper, we answer the following question: Given a piece of\ntext, can we identify whether it was produced by LLM $A$ or $B$ (where $B$ can\nbe a human)? We model LLM-generated text as a sequential stochastic process\nwith complete dependence on history and design zero-shot statistical tests to\ndistinguish between (i) the text generated by two different sets of LLMs $A$\n(in-house) and $B$ (non-sanctioned) and also (ii) LLM-generated and\nhuman-generated texts. We prove that the type I and type II errors for our\ntests decrease exponentially in the text length. In designing our tests, we\nderive concentration inequalities on the difference between log-perplexity and\nthe average entropy of the string under $A$. Specifically, for a given string,\nwe demonstrate that if the string is generated by $A$, the log-perplexity of\nthe string under $A$ converges to the average entropy of the string under $A$,\nexcept with an exponentially small probability in string length. We also show\nthat if $B$ generates the text, except with an exponentially small probability\nin string length, the log-perplexity of the string under $A$ converges to the\naverage cross-entropy of $B$ and $A$. Lastly, we present preliminary\nexperimental results to support our theoretical results. By enabling guaranteed\n(with high probability) finding of the origin of harmful LLM-generated text\nwith arbitrary size, we can help combat misinformation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Verifying the provenance of content is crucial to the function of many\norganizations, e.g., educational institutions, social media platforms, firms,\netc. This problem is becoming increasingly difficult as text generated by Large\nLanguage Models (LLMs) becomes almost indistinguishable from human-generated\ncontent. In addition, many institutions utilize in-house LLMs and want to\nensure that external, non-sanctioned LLMs do not produce content within the\ninstitution. In this paper, we answer the following question: Given a piece of\ntext, can we identify whether it was produced by LLM $A$ or $B$ (where $B$ can\nbe a human)? We model LLM-generated text as a sequential stochastic process\nwith complete dependence on history and design zero-shot statistical tests to\ndistinguish between (i) the text generated by two different sets of LLMs $A$\n(in-house) and $B$ (non-sanctioned) and also (ii) LLM-generated and\nhuman-generated texts. We prove that the type I and type II errors for our\ntests decrease exponentially in the text length. In designing our tests, we\nderive concentration inequalities on the difference between log-perplexity and\nthe average entropy of the string under $A$. Specifically, for a given string,\nwe demonstrate that if the string is generated by $A$, the log-perplexity of\nthe string under $A$ converges to the average entropy of the string under $A$,\nexcept with an exponentially small probability in string length. We also show\nthat if $B$ generates the text, except with an exponentially small probability\nin string length, the log-perplexity of the string under $A$ converges to the\naverage cross-entropy of $B$ and $A$. Lastly, we present preliminary\nexperimental results to support our theoretical results. By enabling guaranteed\n(with high probability) finding of the origin of harmful LLM-generated text\nwith arbitrary size, we can help combat misinformation."
                },
                "authors": [
                    {
                        "name": "Tara Radvand"
                    },
                    {
                        "name": "Mojtaba Abdolmaleki"
                    },
                    {
                        "name": "Mohamed Mostagir"
                    },
                    {
                        "name": "Ambuj Tewari"
                    }
                ],
                "author_detail": {
                    "name": "Ambuj Tewari"
                },
                "author": "Ambuj Tewari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02406v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02406v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15023v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15023v2",
                "updated": "2025-01-22T02:32:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    2,
                    32,
                    41,
                    2,
                    22,
                    0
                ],
                "published": "2024-10-19T07:41:16Z",
                "published_parsed": [
                    2024,
                    10,
                    19,
                    7,
                    41,
                    16,
                    5,
                    293,
                    0
                ],
                "title": "PaperWave: Listening to Research Papers as Conversational Podcasts\n  Scripted by LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PaperWave: Listening to Research Papers as Conversational Podcasts\n  Scripted by LLM"
                },
                "summary": "Listening to audio content, such as podcasts and audiobooks, is one way for\npeople to engage with knowledge. Listening affords people more mobility than\nreading by seeing, thereby broadening their learning opportunities. This study\nexplores the potential applications of large language models (LLMs) to adapt\ntext documents to audio content and addresses the lack of listening-friendly\nmaterials for niche content, such as research papers. LLMs can generate scripts\nof audio content in various styles tailored to specific needs, such as\nfull-content duration or speech types (monologue or dialogue). To explore this\npotential, we developed PaperWave as a prototype that transforms academic paper\nPDFs into conversational podcasts. Our two-month investigation, involving 11\nparticipants (including the authors), employed an autobiographical design, a\nfield study, and a design workshop. The findings highlight the importance of\nconsidering listener interaction with their environment when designing\ndocument-to-audio systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Listening to audio content, such as podcasts and audiobooks, is one way for\npeople to engage with knowledge. Listening affords people more mobility than\nreading by seeing, thereby broadening their learning opportunities. This study\nexplores the potential applications of large language models (LLMs) to adapt\ntext documents to audio content and addresses the lack of listening-friendly\nmaterials for niche content, such as research papers. LLMs can generate scripts\nof audio content in various styles tailored to specific needs, such as\nfull-content duration or speech types (monologue or dialogue). To explore this\npotential, we developed PaperWave as a prototype that transforms academic paper\nPDFs into conversational podcasts. Our two-month investigation, involving 11\nparticipants (including the authors), employed an autobiographical design, a\nfield study, and a design workshop. The findings highlight the importance of\nconsidering listener interaction with their environment when designing\ndocument-to-audio systems."
                },
                "authors": [
                    {
                        "name": "Yuchi Yahagi"
                    },
                    {
                        "name": "Rintaro Chujo"
                    },
                    {
                        "name": "Yuga Harada"
                    },
                    {
                        "name": "Changyo Han"
                    },
                    {
                        "name": "Kohei Sugiyama"
                    },
                    {
                        "name": "Takeshi Naemura"
                    }
                ],
                "author_detail": {
                    "name": "Takeshi Naemura"
                },
                "author": "Takeshi Naemura",
                "arxiv_doi": "10.1145/3706599.3706664",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3706599.3706664",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.15023v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15023v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13548v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13548v3",
                "updated": "2025-01-22T02:30:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    2,
                    30,
                    41,
                    2,
                    22,
                    0
                ],
                "published": "2024-12-18T06:49:46Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    6,
                    49,
                    46,
                    2,
                    353,
                    0
                ],
                "title": "TelePreview: A User-Friendly Teleoperation System with Virtual Arm\n  Assistance for Enhanced Effectiveness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TelePreview: A User-Friendly Teleoperation System with Virtual Arm\n  Assistance for Enhanced Effectiveness"
                },
                "summary": "Teleoperation provides an effective way to collect robot data, which is\ncrucial for learning from demonstrations. In this field, teleoperation faces\nseveral key challenges: user-friendliness for new users, safety assurance, and\ntransferability across different platforms. While collecting real robot\ndexterous manipulation data by teleoperation to train robots has shown\nimpressive results on diverse tasks, due to the morphological differences\nbetween human and robot hands, it is not only hard for new users to understand\nthe action mapping but also raises potential safety concerns during operation.\nTo address these limitations, we introduce TelePreview. This teleoperation\nsystem offers real-time visual feedback on robot actions based on human user\ninputs, with a total hardware cost of less than $1,000. TelePreview allows the\nuser to see a virtual robot that represents the outcome of the user's next\nmovement. By enabling flexible switching between command visualization and\nactual execution, this system helps new users learn how to demonstrate quickly\nand safely. We demonstrate that it outperforms other teleoperation systems\nacross five tasks, emphasize its ease of use, and highlight its straightforward\ndeployment across diverse robotic platforms. We release our code and a\ndeployment document on our website https://telepreview.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teleoperation provides an effective way to collect robot data, which is\ncrucial for learning from demonstrations. In this field, teleoperation faces\nseveral key challenges: user-friendliness for new users, safety assurance, and\ntransferability across different platforms. While collecting real robot\ndexterous manipulation data by teleoperation to train robots has shown\nimpressive results on diverse tasks, due to the morphological differences\nbetween human and robot hands, it is not only hard for new users to understand\nthe action mapping but also raises potential safety concerns during operation.\nTo address these limitations, we introduce TelePreview. This teleoperation\nsystem offers real-time visual feedback on robot actions based on human user\ninputs, with a total hardware cost of less than $1,000. TelePreview allows the\nuser to see a virtual robot that represents the outcome of the user's next\nmovement. By enabling flexible switching between command visualization and\nactual execution, this system helps new users learn how to demonstrate quickly\nand safely. We demonstrate that it outperforms other teleoperation systems\nacross five tasks, emphasize its ease of use, and highlight its straightforward\ndeployment across diverse robotic platforms. We release our code and a\ndeployment document on our website https://telepreview.github.io."
                },
                "authors": [
                    {
                        "name": "Jingxiang Guo"
                    },
                    {
                        "name": "Jiayu Luo"
                    },
                    {
                        "name": "Zhenyu Wei"
                    },
                    {
                        "name": "Yiwen Hou"
                    },
                    {
                        "name": "Zhixuan Xu"
                    },
                    {
                        "name": "Xiaoyi Lin"
                    },
                    {
                        "name": "Chongkai Gao"
                    },
                    {
                        "name": "Lin Shao"
                    }
                ],
                "author_detail": {
                    "name": "Lin Shao"
                },
                "author": "Lin Shao",
                "arxiv_comment": "In submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13548v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13548v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12589v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12589v1",
                "updated": "2025-01-22T02:29:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    2,
                    29,
                    37,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T02:29:37Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    2,
                    29,
                    37,
                    2,
                    22,
                    0
                ],
                "title": "D-LoRa: a Distributed Parameter Adaptation Scheme for LoRa Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "D-LoRa: a Distributed Parameter Adaptation Scheme for LoRa Network"
                },
                "summary": "The deployment of LoRa networks necessitates joint performance optimization,\nincluding packet delivery rate, energy efficiency, and throughput.\nAdditionally, multiple LoRa parameters for packet transmission must be\ndynamically configured to tailor the performance metrics prioritization across\nvarying channel environments. Because of the coupling relationship between LoRa\nparameters and metrics, existing works have opted to focus on certain\nparameters or specific metrics to circumvent the intricate coupling\nrelationship, leading to limited adaptability. Therefore, we propose D-LoRa, a\ndistributed parameter adaptation scheme, based on reinforcement learning\ntowards network performance. We decompose the joint performance optimization\nproblem into multiple independent Multi-Armed Bandit (MAB) problems with\ndifferent reward functions. We have also built a comprehensive analytical model\nfor the LoRa network that considers path loss, quasi-orthogonality of spreading\nfactor, and packet collision. Experimental results show that our scheme can\nincrease packet delivery rate by up to 28.8% and demonstrates superior\nadaptability across different performance metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of LoRa networks necessitates joint performance optimization,\nincluding packet delivery rate, energy efficiency, and throughput.\nAdditionally, multiple LoRa parameters for packet transmission must be\ndynamically configured to tailor the performance metrics prioritization across\nvarying channel environments. Because of the coupling relationship between LoRa\nparameters and metrics, existing works have opted to focus on certain\nparameters or specific metrics to circumvent the intricate coupling\nrelationship, leading to limited adaptability. Therefore, we propose D-LoRa, a\ndistributed parameter adaptation scheme, based on reinforcement learning\ntowards network performance. We decompose the joint performance optimization\nproblem into multiple independent Multi-Armed Bandit (MAB) problems with\ndifferent reward functions. We have also built a comprehensive analytical model\nfor the LoRa network that considers path loss, quasi-orthogonality of spreading\nfactor, and packet collision. Experimental results show that our scheme can\nincrease packet delivery rate by up to 28.8% and demonstrates superior\nadaptability across different performance metrics."
                },
                "authors": [
                    {
                        "name": "Ruiqi Wang"
                    },
                    {
                        "name": "Tongyu Song"
                    },
                    {
                        "name": "Jing Ren"
                    },
                    {
                        "name": "Xiong Wang"
                    },
                    {
                        "name": "Shizhong Xu"
                    },
                    {
                        "name": "Sheng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Sheng Wang"
                },
                "author": "Sheng Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12589v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12589v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12573v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12573v1",
                "updated": "2025-01-22T01:41:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    1,
                    41,
                    5,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T01:41:05Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    1,
                    41,
                    5,
                    2,
                    22,
                    0
                ],
                "title": "Leveraging LLMs to Create a Haptic Devices' Recommendation System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging LLMs to Create a Haptic Devices' Recommendation System"
                },
                "summary": "Haptic technology has seen significant growth, yet a lack of awareness of\nexisting haptic device design knowledge hinders development. This paper\naddresses these limitations by leveraging advancements in Large Language Models\n(LLMs) to develop a haptic agent, focusing specifically on Grounded Force\nFeedback (GFF) devices recommendation. Our approach involves automating the\ncreation of a structured haptic device database using information from research\npapers and product specifications. This database enables the recommendation of\nrelevant GFF devices based on user queries. To ensure precise and contextually\nrelevant recommendations, the system employs a dynamic retrieval method that\ncombines both conditional and semantic searches. Benchmarking against the\nestablished UEQ and existing haptic device searching tools, the proposed haptic\nrecommendation agent ranks in the top 10\\% across all UEQ categories with mean\ndifferences favoring the agent in nearly all subscales, and maintains no\nsignificant performance bias across different user groups, showcasing superior\nusability and user satisfaction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Haptic technology has seen significant growth, yet a lack of awareness of\nexisting haptic device design knowledge hinders development. This paper\naddresses these limitations by leveraging advancements in Large Language Models\n(LLMs) to develop a haptic agent, focusing specifically on Grounded Force\nFeedback (GFF) devices recommendation. Our approach involves automating the\ncreation of a structured haptic device database using information from research\npapers and product specifications. This database enables the recommendation of\nrelevant GFF devices based on user queries. To ensure precise and contextually\nrelevant recommendations, the system employs a dynamic retrieval method that\ncombines both conditional and semantic searches. Benchmarking against the\nestablished UEQ and existing haptic device searching tools, the proposed haptic\nrecommendation agent ranks in the top 10\\% across all UEQ categories with mean\ndifferences favoring the agent in nearly all subscales, and maintains no\nsignificant performance bias across different user groups, showcasing superior\nusability and user satisfaction."
                },
                "authors": [
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Haiwei Dong"
                    },
                    {
                        "name": "Abdulmotaleb El Saddik"
                    }
                ],
                "author_detail": {
                    "name": "Abdulmotaleb El Saddik"
                },
                "author": "Abdulmotaleb El Saddik",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12573v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12573v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09879v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09879v2",
                "updated": "2025-01-22T01:38:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    1,
                    38,
                    36,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-16T23:31:49Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    23,
                    31,
                    49,
                    3,
                    16,
                    0
                ],
                "title": "Testing Refactoring Engine via Historical Bug Report driven LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Testing Refactoring Engine via Historical Bug Report driven LLM"
                },
                "summary": "Refactoring is the process of restructuring existing code without changing\nits external behavior while improving its internal structure. Refactoring\nengines are integral components of modern Integrated Development Environments\n(IDEs) and can automate or semi-automate this process to enhance code\nreadability, reduce complexity, and improve the maintainability of software\nproducts. Similar to traditional software systems such as compilers,\nrefactoring engines may also contain bugs that can lead to unexpected\nbehaviors. In this paper, we propose a novel approach called RETESTER, a\nLLM-based framework for automated refactoring engine testing. Specifically, by\nusing input program structure templates extracted from historical bug reports\nand input program characteristics that are error-prone, we design\nchain-of-thought (CoT) prompts to perform refactoring-preserving\ntransformations. The generated variants are then tested on the latest version\nof refactoring engines using differential testing. We evaluate RETESTER on two\nmost popular modern refactoring engines (i.e., ECLIPSE, and INTELLIJ IDEA). It\nsuccessfully revealed 18 new bugs in the latest version of those refactoring\nengines. By the time we submit our paper, seven of them were confirmed by their\ndevelopers, and three were fixed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Refactoring is the process of restructuring existing code without changing\nits external behavior while improving its internal structure. Refactoring\nengines are integral components of modern Integrated Development Environments\n(IDEs) and can automate or semi-automate this process to enhance code\nreadability, reduce complexity, and improve the maintainability of software\nproducts. Similar to traditional software systems such as compilers,\nrefactoring engines may also contain bugs that can lead to unexpected\nbehaviors. In this paper, we propose a novel approach called RETESTER, a\nLLM-based framework for automated refactoring engine testing. Specifically, by\nusing input program structure templates extracted from historical bug reports\nand input program characteristics that are error-prone, we design\nchain-of-thought (CoT) prompts to perform refactoring-preserving\ntransformations. The generated variants are then tested on the latest version\nof refactoring engines using differential testing. We evaluate RETESTER on two\nmost popular modern refactoring engines (i.e., ECLIPSE, and INTELLIJ IDEA). It\nsuccessfully revealed 18 new bugs in the latest version of those refactoring\nengines. By the time we submit our paper, seven of them were confirmed by their\ndevelopers, and three were fixed."
                },
                "authors": [
                    {
                        "name": "Haibo Wang"
                    },
                    {
                        "name": "Zhuolin Xu"
                    },
                    {
                        "name": "Shin Hwei Tan"
                    }
                ],
                "author_detail": {
                    "name": "Shin Hwei Tan"
                },
                "author": "Shin Hwei Tan",
                "arxiv_comment": "Accepted at the 2nd ACM international conference on AI Foundation\n  Models and Software Engineering (FORGE 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09879v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09879v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12570v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12570v1",
                "updated": "2025-01-22T01:35:11Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    1,
                    35,
                    11,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T01:35:11Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    1,
                    35,
                    11,
                    2,
                    22,
                    0
                ],
                "title": "O1-Pruner: Length-Harmonizing Fine-Tuning for O1-Like Reasoning Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "O1-Pruner: Length-Harmonizing Fine-Tuning for O1-Like Reasoning Pruning"
                },
                "summary": "Recently, long-thought reasoning LLMs, such as OpenAI's O1, adopt extended\nreasoning processes similar to how humans ponder over complex problems. This\nreasoning paradigm significantly enhances the model's problem-solving abilities\nand has achieved promising results. However, long-thought reasoning process\nleads to a substantial increase in inference time. A pressing challenge is\nreducing the inference overhead of long-thought LLMs while ensuring accuracy.\nIn this paper, we experimentally demonstrate that long-thought reasoning models\nstruggle to effectively allocate token budgets based on problem difficulty and\nreasoning redundancies. To address this, we propose Length-Harmonizing\nFine-Tuning (O1-Pruner), aiming at minimizing reasoning overhead while\nmaintaining accuracy. This effective fine-tuning method first estimates the\nLLM's baseline performance through pre-sampling and then uses RL-style\nfine-tuning to encourage the model to generate shorter reasoning processes\nunder accuracy constraints. This allows the model to achieve efficient\nreasoning with lower redundancy while maintaining accuracy. Experiments on\nvarious mathematical reasoning benchmarks show that O1-Pruner not only\nsignificantly reduces inference overhead but also achieves higher accuracy,\nproviding a novel and promising solution to this challenge. Our code is coming\nsoon at https://github.com/StarDewXXX/O1-Pruner",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, long-thought reasoning LLMs, such as OpenAI's O1, adopt extended\nreasoning processes similar to how humans ponder over complex problems. This\nreasoning paradigm significantly enhances the model's problem-solving abilities\nand has achieved promising results. However, long-thought reasoning process\nleads to a substantial increase in inference time. A pressing challenge is\nreducing the inference overhead of long-thought LLMs while ensuring accuracy.\nIn this paper, we experimentally demonstrate that long-thought reasoning models\nstruggle to effectively allocate token budgets based on problem difficulty and\nreasoning redundancies. To address this, we propose Length-Harmonizing\nFine-Tuning (O1-Pruner), aiming at minimizing reasoning overhead while\nmaintaining accuracy. This effective fine-tuning method first estimates the\nLLM's baseline performance through pre-sampling and then uses RL-style\nfine-tuning to encourage the model to generate shorter reasoning processes\nunder accuracy constraints. This allows the model to achieve efficient\nreasoning with lower redundancy while maintaining accuracy. Experiments on\nvarious mathematical reasoning benchmarks show that O1-Pruner not only\nsignificantly reduces inference overhead but also achieves higher accuracy,\nproviding a novel and promising solution to this challenge. Our code is coming\nsoon at https://github.com/StarDewXXX/O1-Pruner"
                },
                "authors": [
                    {
                        "name": "Haotian Luo"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Haiying He"
                    },
                    {
                        "name": "Yibo Wang"
                    },
                    {
                        "name": "Shiwei Liu"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Naiqiang Tan"
                    },
                    {
                        "name": "Xiaochun Cao"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "9 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12570v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12570v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12557v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12557v1",
                "updated": "2025-01-22T00:31:51Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    0,
                    31,
                    51,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T00:31:51Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    0,
                    31,
                    51,
                    2,
                    22,
                    0
                ],
                "title": "Understanding the LLM-ification of CHI: Unpacking the Impact of LLMs at\n  CHI through a Systematic Literature Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the LLM-ification of CHI: Unpacking the Impact of LLMs at\n  CHI through a Systematic Literature Review"
                },
                "summary": "Large language models (LLMs) have been positioned to revolutionize HCI, by\nreshaping not only the interfaces, design patterns, and sociotechnical systems\nthat we study, but also the research practices we use. To-date, however, there\nhas been little understanding of LLMs' uptake in HCI. We address this gap via a\nsystematic literature review of 153 CHI papers from 2020-24 that engage with\nLLMs. We taxonomize: (1) domains where LLMs are applied; (2) roles of LLMs in\nHCI projects; (3) contribution types; and (4) acknowledged limitations and\nrisks. We find LLM work in 10 diverse domains, primarily via empirical and\nartifact contributions. Authors use LLMs in five distinct roles, including as\nresearch tools or simulated users. Still, authors often raise validity and\nreproducibility concerns, and overwhelmingly study closed models. We outline\nopportunities to improve HCI research with and on LLMs, and provide guiding\nquestions for researchers to consider the validity and appropriateness of\nLLM-related work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been positioned to revolutionize HCI, by\nreshaping not only the interfaces, design patterns, and sociotechnical systems\nthat we study, but also the research practices we use. To-date, however, there\nhas been little understanding of LLMs' uptake in HCI. We address this gap via a\nsystematic literature review of 153 CHI papers from 2020-24 that engage with\nLLMs. We taxonomize: (1) domains where LLMs are applied; (2) roles of LLMs in\nHCI projects; (3) contribution types; and (4) acknowledged limitations and\nrisks. We find LLM work in 10 diverse domains, primarily via empirical and\nartifact contributions. Authors use LLMs in five distinct roles, including as\nresearch tools or simulated users. Still, authors often raise validity and\nreproducibility concerns, and overwhelmingly study closed models. We outline\nopportunities to improve HCI research with and on LLMs, and provide guiding\nquestions for researchers to consider the validity and appropriateness of\nLLM-related work."
                },
                "authors": [
                    {
                        "name": "Rock Yuren Pang"
                    },
                    {
                        "name": "Hope Schroeder"
                    },
                    {
                        "name": "Kynnedy Simone Smith"
                    },
                    {
                        "name": "Solon Barocas"
                    },
                    {
                        "name": "Ziang Xiao"
                    },
                    {
                        "name": "Emily Tseng"
                    },
                    {
                        "name": "Danielle Bragg"
                    }
                ],
                "author_detail": {
                    "name": "Danielle Bragg"
                },
                "author": "Danielle Bragg",
                "arxiv_comment": "This is a preprint version of the paper conditionally accepted to\n  CHI'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12557v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12557v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12547v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12547v1",
                "updated": "2025-01-21T23:54:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    23,
                    54,
                    17,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T23:54:17Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    23,
                    54,
                    17,
                    1,
                    21,
                    0
                ],
                "title": "Human-like conceptual representations emerge from language prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human-like conceptual representations emerge from language prediction"
                },
                "summary": "Recent advances in large language models (LLMs) provide a new opportunity to\naddress the long-standing question of how concepts are represented and\norganized in the mind, which is central to unravelling the nature of human\ncognition. Here, we reframed the classic reverse dictionary task to simulate\nhuman concept inference in context and investigated the emergence of human-like\nconceptual representations within LLMs. We found that LLMs were able to infer\nconcepts from definitional descriptions and construct representation spaces\nthat converge towards a shared, context-independent structure. These\nrepresentations effectively predicted human behavioural judgments and aligned\nwell with neural activity patterns in the human brain, offering evidence for\nbiological plausibility. These findings demonstrate that human-like conceptual\nrepresentations and organization can naturally emerge from language prediction,\neven without real-world grounding. Our work supports the view that LLMs serve\nas valuable tools for understanding complex human cognition and paves the way\nfor better alignment between artificial and human intelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) provide a new opportunity to\naddress the long-standing question of how concepts are represented and\norganized in the mind, which is central to unravelling the nature of human\ncognition. Here, we reframed the classic reverse dictionary task to simulate\nhuman concept inference in context and investigated the emergence of human-like\nconceptual representations within LLMs. We found that LLMs were able to infer\nconcepts from definitional descriptions and construct representation spaces\nthat converge towards a shared, context-independent structure. These\nrepresentations effectively predicted human behavioural judgments and aligned\nwell with neural activity patterns in the human brain, offering evidence for\nbiological plausibility. These findings demonstrate that human-like conceptual\nrepresentations and organization can naturally emerge from language prediction,\neven without real-world grounding. Our work supports the view that LLMs serve\nas valuable tools for understanding complex human cognition and paves the way\nfor better alignment between artificial and human intelligence."
                },
                "authors": [
                    {
                        "name": "Ningyu Xu"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Qiang Luo"
                    },
                    {
                        "name": "Xipeng Qiu"
                    },
                    {
                        "name": "Xuanjing Huang"
                    },
                    {
                        "name": "Menghan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Menghan Zhang"
                },
                "author": "Menghan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12547v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12547v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12521v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12521v1",
                "updated": "2025-01-21T22:24:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    22,
                    24,
                    3,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T22:24:03Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    22,
                    24,
                    3,
                    1,
                    21,
                    0
                ],
                "title": "An Empirically-grounded tool for Automatic Prompt Linting and Repair: A\n  Case Study on Bias, Vulnerability, and Optimization in Developer Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Empirically-grounded tool for Automatic Prompt Linting and Repair: A\n  Case Study on Bias, Vulnerability, and Optimization in Developer Prompts"
                },
                "summary": "The tidal wave of advancements in Large Language Models (LLMs) has led to\ntheir swift integration into application-level logic. Many software systems now\nuse prompts to interact with these black-box models, combining natural language\nwith dynamic values interpolated at runtime, to perform tasks ranging from\nsentiment analysis to question answering. Due to the programmatic and\nstructured natural language aspects of these prompts, we refer to them as\nDeveloper Prompts. Unlike traditional software artifacts, Dev Prompts blend\nnatural language instructions with artificial languages such as programming and\nmarkup languages, thus requiring specialized tools for analysis, distinct from\nclassical software evaluation methods.\n  In response to this need, we introduce PromptDoctor, a tool explicitly\ndesigned to detect and correct issues of Dev Prompts. PromptDoctor identifies\nand addresses problems related to bias, vulnerability, and sub-optimal\nperformance in Dev Prompts, helping mitigate their possible harms. In our\nanalysis of 2,173 Dev Prompts, selected as a representative sample of 40,573\nDev Prompts, we found that 3.46% contained one or more forms of bias, 10.75%\nwere vulnerable to prompt injection attacks. Additionally, 3,310 were amenable\nto automated prompt optimization. To address these issues, we applied\nPromptDoctor to the flawed Dev Prompts we discovered. PromptDoctor de-biased\n68.29% of the biased Dev Prompts, hardened 41.81% of the vulnerable Dev\nPrompts, and improved the performance of 37.1% sub-optimal Dev Prompts.\nFinally, we developed a PromptDoctor VSCode extension, enabling developers to\neasily enhance Dev Prompts in their existing development workflows. The data\nand source code for this work are available at",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The tidal wave of advancements in Large Language Models (LLMs) has led to\ntheir swift integration into application-level logic. Many software systems now\nuse prompts to interact with these black-box models, combining natural language\nwith dynamic values interpolated at runtime, to perform tasks ranging from\nsentiment analysis to question answering. Due to the programmatic and\nstructured natural language aspects of these prompts, we refer to them as\nDeveloper Prompts. Unlike traditional software artifacts, Dev Prompts blend\nnatural language instructions with artificial languages such as programming and\nmarkup languages, thus requiring specialized tools for analysis, distinct from\nclassical software evaluation methods.\n  In response to this need, we introduce PromptDoctor, a tool explicitly\ndesigned to detect and correct issues of Dev Prompts. PromptDoctor identifies\nand addresses problems related to bias, vulnerability, and sub-optimal\nperformance in Dev Prompts, helping mitigate their possible harms. In our\nanalysis of 2,173 Dev Prompts, selected as a representative sample of 40,573\nDev Prompts, we found that 3.46% contained one or more forms of bias, 10.75%\nwere vulnerable to prompt injection attacks. Additionally, 3,310 were amenable\nto automated prompt optimization. To address these issues, we applied\nPromptDoctor to the flawed Dev Prompts we discovered. PromptDoctor de-biased\n68.29% of the biased Dev Prompts, hardened 41.81% of the vulnerable Dev\nPrompts, and improved the performance of 37.1% sub-optimal Dev Prompts.\nFinally, we developed a PromptDoctor VSCode extension, enabling developers to\neasily enhance Dev Prompts in their existing development workflows. The data\nand source code for this work are available at"
                },
                "authors": [
                    {
                        "name": "Dhia Elhaq Rzig"
                    },
                    {
                        "name": "Dhruba Jyoti Paul"
                    },
                    {
                        "name": "Kaiser Pister"
                    },
                    {
                        "name": "Jordan Henkel"
                    },
                    {
                        "name": "Foyzul Hassan"
                    }
                ],
                "author_detail": {
                    "name": "Foyzul Hassan"
                },
                "author": "Foyzul Hassan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12521v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12521v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.15524v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.15524v2",
                "updated": "2025-01-21T21:34:52Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    21,
                    34,
                    52,
                    1,
                    21,
                    0
                ],
                "published": "2023-12-24T16:32:35Z",
                "published_parsed": [
                    2023,
                    12,
                    24,
                    16,
                    32,
                    35,
                    6,
                    358,
                    0
                ],
                "title": "The Challenge of Using LLMs to Simulate Human Behavior: A Causal\n  Inference Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Challenge of Using LLMs to Simulate Human Behavior: A Causal\n  Inference Perspective"
                },
                "summary": "Large Language Models (LLMs) have shown impressive potential to simulate\nhuman behavior. We identify a fundamental challenge in using them to simulate\nexperiments: when LLM-simulated subjects are blind to the experimental design\n(as is standard practice with human subjects), variations in treatment\nsystematically affect unspecified variables that should remain constant,\nviolating the unconfoundedness assumption. Using demand estimation as a context\nand an actual experiment as a benchmark, we show this can lead to implausible\nresults. While confounding may in principle be addressed by controlling for\ncovariates, this can compromise ecological validity in the context of LLM\nsimulations: controlled covariates become artificially salient in the simulated\ndecision process, which introduces focalism. This trade-off between\nunconfoundedness and ecological validity is usually absent in traditional\nexperimental design and represents a unique challenge in LLM simulations. We\nformalize this challenge theoretically, showing it stems from ambiguous\nprompting strategies, and hence cannot be fully addressed by improving training\ndata or by fine-tuning. Alternative approaches that unblind the experimental\ndesign to the LLM show promise. Our findings suggest that effectively\nleveraging LLMs for experimental simulations requires fundamentally rethinking\nestablished experimental design practices rather than simply adapting protocols\ndeveloped for human subjects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown impressive potential to simulate\nhuman behavior. We identify a fundamental challenge in using them to simulate\nexperiments: when LLM-simulated subjects are blind to the experimental design\n(as is standard practice with human subjects), variations in treatment\nsystematically affect unspecified variables that should remain constant,\nviolating the unconfoundedness assumption. Using demand estimation as a context\nand an actual experiment as a benchmark, we show this can lead to implausible\nresults. While confounding may in principle be addressed by controlling for\ncovariates, this can compromise ecological validity in the context of LLM\nsimulations: controlled covariates become artificially salient in the simulated\ndecision process, which introduces focalism. This trade-off between\nunconfoundedness and ecological validity is usually absent in traditional\nexperimental design and represents a unique challenge in LLM simulations. We\nformalize this challenge theoretically, showing it stems from ambiguous\nprompting strategies, and hence cannot be fully addressed by improving training\ndata or by fine-tuning. Alternative approaches that unblind the experimental\ndesign to the LLM show promise. Our findings suggest that effectively\nleveraging LLMs for experimental simulations requires fundamentally rethinking\nestablished experimental design practices rather than simply adapting protocols\ndeveloped for human subjects."
                },
                "authors": [
                    {
                        "name": "George Gui"
                    },
                    {
                        "name": "Olivier Toubia"
                    }
                ],
                "author_detail": {
                    "name": "Olivier Toubia"
                },
                "author": "Olivier Toubia",
                "arxiv_doi": "10.2139/ssrn.4650172",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.2139/ssrn.4650172",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2312.15524v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.15524v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12502v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12502v1",
                "updated": "2025-01-21T21:08:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    21,
                    8,
                    40,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T21:08:40Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    21,
                    8,
                    40,
                    1,
                    21,
                    0
                ],
                "title": "Sequence Spreading-Based Semantic Communication Under High RF\n  Interference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequence Spreading-Based Semantic Communication Under High RF\n  Interference"
                },
                "summary": "In the evolving landscape of wireless communications, semantic communication\n(SemCom) has recently emerged as a 6G enabler that prioritizes the transmission\nof meaning and contextual relevance over conventional bit-centric metrics.\nHowever, the deployment of SemCom systems in industrial settings presents\nconsiderable challenges, such as high radio frequency interference (RFI), that\ncan adversely affect system performance. To address this problem, in this work,\nwe propose a novel approach based on integrating sequence spreading techniques\nwith SemCom to enhance system robustness against such adverse conditions and\nenable scalable multi-user (MU) SemCom. In addition, we propose a novel signal\nrefining network (SRN) to refine the received signal after despreading and\nequalization. The proposed network eliminates the need for computationally\nintensive end-to-end (E2E) training while improving performance metrics,\nachieving a 25% gain in BLEU score and a 12% increase in semantic similarity\ncompared to E2E training using the same bandwidth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the evolving landscape of wireless communications, semantic communication\n(SemCom) has recently emerged as a 6G enabler that prioritizes the transmission\nof meaning and contextual relevance over conventional bit-centric metrics.\nHowever, the deployment of SemCom systems in industrial settings presents\nconsiderable challenges, such as high radio frequency interference (RFI), that\ncan adversely affect system performance. To address this problem, in this work,\nwe propose a novel approach based on integrating sequence spreading techniques\nwith SemCom to enhance system robustness against such adverse conditions and\nenable scalable multi-user (MU) SemCom. In addition, we propose a novel signal\nrefining network (SRN) to refine the received signal after despreading and\nequalization. The proposed network eliminates the need for computationally\nintensive end-to-end (E2E) training while improving performance metrics,\nachieving a 25% gain in BLEU score and a 12% increase in semantic similarity\ncompared to E2E training using the same bandwidth."
                },
                "authors": [
                    {
                        "name": "Hazem Barka"
                    },
                    {
                        "name": "Georges Kaddoum"
                    },
                    {
                        "name": "Mehdi Bennis"
                    },
                    {
                        "name": "Md Sahabul Alam"
                    },
                    {
                        "name": "Minh Au"
                    }
                ],
                "author_detail": {
                    "name": "Minh Au"
                },
                "author": "Minh Au",
                "arxiv_comment": "Accepted in IEEE International Conference on Communications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12502v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12502v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12501v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12501v1",
                "updated": "2025-01-21T21:06:11Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    21,
                    6,
                    11,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T21:06:11Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    21,
                    6,
                    11,
                    1,
                    21,
                    0
                ],
                "title": "A Domain Adaptation Framework for Speech Recognition Systems with Only\n  Synthetic data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Domain Adaptation Framework for Speech Recognition Systems with Only\n  Synthetic data"
                },
                "summary": "We introduce DAS (Domain Adaptation with Synthetic data), a novel domain\nadaptation framework for pre-trained ASR model, designed to efficiently adapt\nto various language-defined domains without requiring any real data. In\nparticular, DAS first prompts large language models (LLMs) to generate\ndomain-specific texts before converting these texts to speech via\ntext-to-speech technology. The synthetic data is used to fine-tune Whisper with\nLow-Rank Adapters (LoRAs) for targeted domains such as music, weather, and\nsports. We introduce a novel one-pass decoding strategy that merges predictions\nfrom multiple LoRA adapters efficiently during the auto-regressive text\ngeneration process. Experimental results show significant improvements,\nreducing the Word Error Rate (WER) by 10% to 17% across all target domains\ncompared to the original model, with minimal performance regression in\nout-of-domain settings (e.g., -1% on Librispeech test sets). We also\ndemonstrate that DAS operates efficiently during inference, introducing an\nadditional 9% increase in Real Time Factor (RTF) compared to the original model\nwhen inferring with three LoRA adapters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce DAS (Domain Adaptation with Synthetic data), a novel domain\nadaptation framework for pre-trained ASR model, designed to efficiently adapt\nto various language-defined domains without requiring any real data. In\nparticular, DAS first prompts large language models (LLMs) to generate\ndomain-specific texts before converting these texts to speech via\ntext-to-speech technology. The synthetic data is used to fine-tune Whisper with\nLow-Rank Adapters (LoRAs) for targeted domains such as music, weather, and\nsports. We introduce a novel one-pass decoding strategy that merges predictions\nfrom multiple LoRA adapters efficiently during the auto-regressive text\ngeneration process. Experimental results show significant improvements,\nreducing the Word Error Rate (WER) by 10% to 17% across all target domains\ncompared to the original model, with minimal performance regression in\nout-of-domain settings (e.g., -1% on Librispeech test sets). We also\ndemonstrate that DAS operates efficiently during inference, introducing an\nadditional 9% increase in Real Time Factor (RTF) compared to the original model\nwhen inferring with three LoRA adapters."
                },
                "authors": [
                    {
                        "name": "Minh Tran"
                    },
                    {
                        "name": "Yutong Pang"
                    },
                    {
                        "name": "Debjyoti Paul"
                    },
                    {
                        "name": "Laxmi Pandey"
                    },
                    {
                        "name": "Kevin Jiang"
                    },
                    {
                        "name": "Jinxi Guo"
                    },
                    {
                        "name": "Ke Li"
                    },
                    {
                        "name": "Shun Zhang"
                    },
                    {
                        "name": "Xuedong Zhang"
                    },
                    {
                        "name": "Xin Lei"
                    }
                ],
                "author_detail": {
                    "name": "Xin Lei"
                },
                "author": "Xin Lei",
                "arxiv_comment": "ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12501v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12501v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06651v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06651v5",
                "updated": "2025-01-21T20:54:00Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    20,
                    54,
                    0,
                    1,
                    21,
                    0
                ],
                "published": "2024-12-09T16:50:02Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    16,
                    50,
                    2,
                    0,
                    344,
                    0
                ],
                "title": "Chatbots im Schulunterricht: Wir testen das Fobizz-Tool zur\n  automatischen Bewertung von Hausaufgaben",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chatbots im Schulunterricht: Wir testen das Fobizz-Tool zur\n  automatischen Bewertung von Hausaufgaben"
                },
                "summary": "This study examines the AI-powered grading tool \"AI Grading Assistant\" by the\nGerman company Fobizz, designed to support teachers in evaluating and providing\nfeedback on student assignments. Against the societal backdrop of an\noverburdened education system and rising expectations for artificial\nintelligence as a solution to these challenges, the investigation evaluates the\ntool's functional suitability through two test series. The results reveal\nsignificant shortcomings: The tool's numerical grades and qualitative feedback\nare often random and do not improve even when its suggestions are incorporated.\nThe highest ratings are achievable only with texts generated by ChatGPT. False\nclaims and nonsensical submissions frequently go undetected, while the\nimplementation of some grading criteria is unreliable and opaque. Since these\ndeficiencies stem from the inherent limitations of large language models\n(LLMs), fundamental improvements to this or similar tools are not immediately\nforeseeable. The study critiques the broader trend of adopting AI as a quick\nfix for systemic problems in education, concluding that Fobizz's marketing of\nthe tool as an objective and time-saving solution is misleading and\nirresponsible. Finally, the study calls for systematic evaluation and\nsubject-specific pedagogical scrutiny of the use of AI tools in educational\ncontexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study examines the AI-powered grading tool \"AI Grading Assistant\" by the\nGerman company Fobizz, designed to support teachers in evaluating and providing\nfeedback on student assignments. Against the societal backdrop of an\noverburdened education system and rising expectations for artificial\nintelligence as a solution to these challenges, the investigation evaluates the\ntool's functional suitability through two test series. The results reveal\nsignificant shortcomings: The tool's numerical grades and qualitative feedback\nare often random and do not improve even when its suggestions are incorporated.\nThe highest ratings are achievable only with texts generated by ChatGPT. False\nclaims and nonsensical submissions frequently go undetected, while the\nimplementation of some grading criteria is unreliable and opaque. Since these\ndeficiencies stem from the inherent limitations of large language models\n(LLMs), fundamental improvements to this or similar tools are not immediately\nforeseeable. The study critiques the broader trend of adopting AI as a quick\nfix for systemic problems in education, concluding that Fobizz's marketing of\nthe tool as an objective and time-saving solution is misleading and\nirresponsible. Finally, the study calls for systematic evaluation and\nsubject-specific pedagogical scrutiny of the use of AI tools in educational\ncontexts."
                },
                "authors": [
                    {
                        "name": "Rainer Muehlhoff"
                    },
                    {
                        "name": "Marte Henningsen"
                    }
                ],
                "author_detail": {
                    "name": "Marte Henningsen"
                },
                "author": "Marte Henningsen",
                "arxiv_comment": "38 pages, in German language, with an update from 2025-01-21 as\n  appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06651v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06651v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "97B10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07457v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07457v2",
                "updated": "2025-01-21T20:24:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    20,
                    24,
                    3,
                    1,
                    21,
                    0
                ],
                "published": "2024-11-12T00:48:01Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    0,
                    48,
                    1,
                    1,
                    317,
                    0
                ],
                "title": "DecoPrompt : Decoding Prompts Reduces Hallucinations when Large Language\n  Models Meet False Premises",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DecoPrompt : Decoding Prompts Reduces Hallucinations when Large Language\n  Models Meet False Premises"
                },
                "summary": "While large language models (LLMs) have demonstrated increasing power, they\nhave also called upon studies on their hallucinated outputs that deviate from\nfactually correct statements. In this paper, we focus on one important scenario\nof false premises, where LLMs are distracted by misaligned claims although the\nmodel possesses the required factual knowledge to answer original questions\naccurately. Inspired by the observation that entropy of the false-premise\nprompt is closely related to its likelihood to elicit hallucination generation,\nwe propose a new prompting algorithm, named DecoPrompt, to mitigate\nhallucination. DecoPrompt leverages LLMs to \"decode\" the false-premise prompts\nwithout really eliciting hallucination output from LLMs. We perform experiments\non two datasets, demonstrating that DecoPrompt can reduce hallucinations\neffectively on outputs from different LLMs. Moreover, DecoPrompt exhibits\ncross-model transferability, which facilitates its applications to scenarios\nsuch as LLMs of large sizes or unavailable model logits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) have demonstrated increasing power, they\nhave also called upon studies on their hallucinated outputs that deviate from\nfactually correct statements. In this paper, we focus on one important scenario\nof false premises, where LLMs are distracted by misaligned claims although the\nmodel possesses the required factual knowledge to answer original questions\naccurately. Inspired by the observation that entropy of the false-premise\nprompt is closely related to its likelihood to elicit hallucination generation,\nwe propose a new prompting algorithm, named DecoPrompt, to mitigate\nhallucination. DecoPrompt leverages LLMs to \"decode\" the false-premise prompts\nwithout really eliciting hallucination output from LLMs. We perform experiments\non two datasets, demonstrating that DecoPrompt can reduce hallucinations\neffectively on outputs from different LLMs. Moreover, DecoPrompt exhibits\ncross-model transferability, which facilitates its applications to scenarios\nsuch as LLMs of large sizes or unavailable model logits."
                },
                "authors": [
                    {
                        "name": "Nan Xu"
                    },
                    {
                        "name": "Xuezhe Ma"
                    }
                ],
                "author_detail": {
                    "name": "Xuezhe Ma"
                },
                "author": "Xuezhe Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07457v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07457v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12486v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12486v1",
                "updated": "2025-01-21T20:23:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    20,
                    23,
                    22,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T20:23:22Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    20,
                    23,
                    22,
                    1,
                    21,
                    0
                ],
                "title": "The Journey Matters: Average Parameter Count over Pre-training Unifies\n  Sparse and Dense Scaling Laws",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Journey Matters: Average Parameter Count over Pre-training Unifies\n  Sparse and Dense Scaling Laws"
                },
                "summary": "Pruning eliminates unnecessary parameters in neural networks; it offers a\npromising solution to the growing computational demands of large language\nmodels (LLMs). While many focus on post-training pruning, sparse\npre-training--which combines pruning and pre-training into a single\nphase--provides a simpler alternative. In this work, we present the first\nsystematic exploration of optimal sparse pre-training configurations for LLMs\nthrough an examination of 80 unique pruning schedules across different sparsity\nlevels and training durations. We find that initiating pruning at 25% of total\ntraining compute and concluding at 75% achieves near-optimal final evaluation\nloss. These findings provide valuable insights for efficient and effective\nsparse pre-training of LLMs. Furthermore, we propose a new scaling law that\nmodifies the Chinchilla scaling law to use the average parameter count over\npre-training. Through empirical and theoretical validation, we demonstrate that\nthis modified scaling law accurately models evaluation loss for both sparsely\nand densely pre-trained LLMs, unifying scaling laws across pre-training\nparadigms. Our findings indicate that while sparse pre-training achieves the\nsame final model quality as dense pre-training for equivalent compute budgets,\nit provides substantial benefits through reduced model size, enabling\nsignificant potential computational savings during inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pruning eliminates unnecessary parameters in neural networks; it offers a\npromising solution to the growing computational demands of large language\nmodels (LLMs). While many focus on post-training pruning, sparse\npre-training--which combines pruning and pre-training into a single\nphase--provides a simpler alternative. In this work, we present the first\nsystematic exploration of optimal sparse pre-training configurations for LLMs\nthrough an examination of 80 unique pruning schedules across different sparsity\nlevels and training durations. We find that initiating pruning at 25% of total\ntraining compute and concluding at 75% achieves near-optimal final evaluation\nloss. These findings provide valuable insights for efficient and effective\nsparse pre-training of LLMs. Furthermore, we propose a new scaling law that\nmodifies the Chinchilla scaling law to use the average parameter count over\npre-training. Through empirical and theoretical validation, we demonstrate that\nthis modified scaling law accurately models evaluation loss for both sparsely\nand densely pre-trained LLMs, unifying scaling laws across pre-training\nparadigms. Our findings indicate that while sparse pre-training achieves the\nsame final model quality as dense pre-training for equivalent compute budgets,\nit provides substantial benefits through reduced model size, enabling\nsignificant potential computational savings during inference."
                },
                "authors": [
                    {
                        "name": "Tian Jin"
                    },
                    {
                        "name": "Ahmed Imtiaz Humayun"
                    },
                    {
                        "name": "Utku Evci"
                    },
                    {
                        "name": "Suvinay Subramanian"
                    },
                    {
                        "name": "Amir Yazdanbakhsh"
                    },
                    {
                        "name": "Dan Alistarh"
                    },
                    {
                        "name": "Gintare Karolina Dziugaite"
                    }
                ],
                "author_detail": {
                    "name": "Gintare Karolina Dziugaite"
                },
                "author": "Gintare Karolina Dziugaite",
                "arxiv_comment": "17 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12486v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12486v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21548v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21548v2",
                "updated": "2025-01-21T19:45:18Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    19,
                    45,
                    18,
                    1,
                    21,
                    0
                ],
                "published": "2024-10-28T21:24:51Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    21,
                    24,
                    51,
                    0,
                    302,
                    0
                ],
                "title": "MultiTok: Variable-Length Tokenization for Efficient LLMs Adapted from\n  LZW Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MultiTok: Variable-Length Tokenization for Efficient LLMs Adapted from\n  LZW Compression"
                },
                "summary": "Large language models have drastically changed the prospects of AI by\nintroducing technologies for more complex natural language processing. However,\ncurrent methodologies to train such LLMs require extensive resources including\nbut not limited to large amounts of data, expensive machinery, and lengthy\ntraining. To solve this problem, this paper proposes a new tokenization method\ninspired by universal Lempel-Ziv-Welch data compression that compresses\nrepetitive phrases into multi-word tokens. With MultiTok as a new tokenizing\ntool, we show that language models are able to be trained notably more\nefficiently while offering a similar accuracy on more succinct and compressed\ntraining data. In fact, our results demonstrate that MultiTok achieves a\ncomparable performance to the BERT and GPT-2 standards as both a stand-alone\ntokenizer and an add-on to existing tokenizers while also providing close to\n2.5x faster training with more than 30% less training data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have drastically changed the prospects of AI by\nintroducing technologies for more complex natural language processing. However,\ncurrent methodologies to train such LLMs require extensive resources including\nbut not limited to large amounts of data, expensive machinery, and lengthy\ntraining. To solve this problem, this paper proposes a new tokenization method\ninspired by universal Lempel-Ziv-Welch data compression that compresses\nrepetitive phrases into multi-word tokens. With MultiTok as a new tokenizing\ntool, we show that language models are able to be trained notably more\nefficiently while offering a similar accuracy on more succinct and compressed\ntraining data. In fact, our results demonstrate that MultiTok achieves a\ncomparable performance to the BERT and GPT-2 standards as both a stand-alone\ntokenizer and an add-on to existing tokenizers while also providing close to\n2.5x faster training with more than 30% less training data."
                },
                "authors": [
                    {
                        "name": "Noel Elias"
                    },
                    {
                        "name": "Homa Esfahanizadeh"
                    },
                    {
                        "name": "Kaan Kale"
                    },
                    {
                        "name": "Sriram Vishwanath"
                    },
                    {
                        "name": "Muriel Medard"
                    }
                ],
                "author_detail": {
                    "name": "Muriel Medard"
                },
                "author": "Muriel Medard",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21548v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21548v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12469v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12469v1",
                "updated": "2025-01-21T19:37:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    19,
                    37,
                    48,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T19:37:48Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    19,
                    37,
                    48,
                    1,
                    21,
                    0
                ],
                "title": "An Empirical Characterization of Outages and Incidents in Public\n  Services for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Empirical Characterization of Outages and Incidents in Public\n  Services for Large Language Models"
                },
                "summary": "People and businesses increasingly rely on public LLM services, such as\nChatGPT, DALLE, and Claude. Understanding their outages, and particularly\nmeasuring their failure-recovery processes, is becoming a stringent problem.\nHowever, only limited studies exist in this emerging area. Addressing this\nproblem, in this work we conduct an empirical characterization of outages and\nfailure-recovery in public LLM services. We collect and prepare datasets for 8\ncommonly used LLM services across 3 major LLM providers, including market-leads\nOpenAI and Anthropic. We conduct a detailed analysis of failure recovery\nstatistical properties, temporal patterns, co-occurrence, and the impact range\nof outage-causing incidents. We make over 10 observations, among which: (1)\nFailures in OpenAI's ChatGPT take longer to resolve but occur less frequently\nthan those in Anthropic's Claude;(2) OpenAI and Anthropic service failures\nexhibit strong weekly and monthly periodicity; and (3) OpenAI services offer\nbetter failure-isolation than Anthropic services. Our research explains LLM\nfailure characteristics and thus enables optimization in building and using LLM\nsystems. FAIR data and code are publicly available on\nhttps://zenodo.org/records/14018219 and\nhttps://github.com/atlarge-research/llm-service-analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "People and businesses increasingly rely on public LLM services, such as\nChatGPT, DALLE, and Claude. Understanding their outages, and particularly\nmeasuring their failure-recovery processes, is becoming a stringent problem.\nHowever, only limited studies exist in this emerging area. Addressing this\nproblem, in this work we conduct an empirical characterization of outages and\nfailure-recovery in public LLM services. We collect and prepare datasets for 8\ncommonly used LLM services across 3 major LLM providers, including market-leads\nOpenAI and Anthropic. We conduct a detailed analysis of failure recovery\nstatistical properties, temporal patterns, co-occurrence, and the impact range\nof outage-causing incidents. We make over 10 observations, among which: (1)\nFailures in OpenAI's ChatGPT take longer to resolve but occur less frequently\nthan those in Anthropic's Claude;(2) OpenAI and Anthropic service failures\nexhibit strong weekly and monthly periodicity; and (3) OpenAI services offer\nbetter failure-isolation than Anthropic services. Our research explains LLM\nfailure characteristics and thus enables optimization in building and using LLM\nsystems. FAIR data and code are publicly available on\nhttps://zenodo.org/records/14018219 and\nhttps://github.com/atlarge-research/llm-service-analysis."
                },
                "authors": [
                    {
                        "name": "Xiaoyu Chu"
                    },
                    {
                        "name": "Sacheendra Talluri"
                    },
                    {
                        "name": "Qingxian Lu"
                    },
                    {
                        "name": "Alexandru Iosup"
                    }
                ],
                "author_detail": {
                    "name": "Alexandru Iosup"
                },
                "author": "Alexandru Iosup",
                "arxiv_journal_ref": "16th ACM/SPEC International Conference on Performance Engineering\n  (ICPE 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12469v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12469v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12465v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12465v1",
                "updated": "2025-01-21T19:22:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    19,
                    22,
                    45,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T19:22:45Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    19,
                    22,
                    45,
                    1,
                    21,
                    0
                ],
                "title": "Adaptive PII Mitigation Framework for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive PII Mitigation Framework for Large Language Models"
                },
                "summary": "Artificial Intelligence (AI) faces growing challenges from evolving data\nprotection laws and enforcement practices worldwide. Regulations like GDPR and\nCCPA impose strict compliance requirements on Machine Learning (ML) models,\nespecially concerning personal data use. These laws grant individuals rights\nsuch as data correction and deletion, complicating the training and deployment\nof Large Language Models (LLMs) that rely on extensive datasets. Public data\navailability does not guarantee its lawful use for ML, amplifying these\nchallenges.\n  This paper introduces an adaptive system for mitigating risk of Personally\nIdentifiable Information (PII) and Sensitive Personal Information (SPI) in\nLLMs. It dynamically aligns with diverse regulatory frameworks and integrates\nseamlessly into Governance, Risk, and Compliance (GRC) systems. The system uses\nadvanced NLP techniques, context-aware analysis, and policy-driven masking to\nensure regulatory compliance.\n  Benchmarks highlight the system's effectiveness, with an F1 score of 0.95 for\nPassport Numbers, outperforming tools like Microsoft Presidio (0.33) and Amazon\nComprehend (0.54). In human evaluations, the system achieved an average user\ntrust score of 4.6/5, with participants acknowledging its accuracy and\ntransparency. Observations demonstrate stricter anonymization under GDPR\ncompared to CCPA, which permits pseudonymization and user opt-outs. These\nresults validate the system as a scalable and robust solution for enterprise\nprivacy compliance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial Intelligence (AI) faces growing challenges from evolving data\nprotection laws and enforcement practices worldwide. Regulations like GDPR and\nCCPA impose strict compliance requirements on Machine Learning (ML) models,\nespecially concerning personal data use. These laws grant individuals rights\nsuch as data correction and deletion, complicating the training and deployment\nof Large Language Models (LLMs) that rely on extensive datasets. Public data\navailability does not guarantee its lawful use for ML, amplifying these\nchallenges.\n  This paper introduces an adaptive system for mitigating risk of Personally\nIdentifiable Information (PII) and Sensitive Personal Information (SPI) in\nLLMs. It dynamically aligns with diverse regulatory frameworks and integrates\nseamlessly into Governance, Risk, and Compliance (GRC) systems. The system uses\nadvanced NLP techniques, context-aware analysis, and policy-driven masking to\nensure regulatory compliance.\n  Benchmarks highlight the system's effectiveness, with an F1 score of 0.95 for\nPassport Numbers, outperforming tools like Microsoft Presidio (0.33) and Amazon\nComprehend (0.54). In human evaluations, the system achieved an average user\ntrust score of 4.6/5, with participants acknowledging its accuracy and\ntransparency. Observations demonstrate stricter anonymization under GDPR\ncompared to CCPA, which permits pseudonymization and user opt-outs. These\nresults validate the system as a scalable and robust solution for enterprise\nprivacy compliance."
                },
                "authors": [
                    {
                        "name": "Shubhi Asthana"
                    },
                    {
                        "name": "Ruchi Mahindru"
                    },
                    {
                        "name": "Bing Zhang"
                    },
                    {
                        "name": "Jorge Sanz"
                    }
                ],
                "author_detail": {
                    "name": "Jorge Sanz"
                },
                "author": "Jorge Sanz",
                "arxiv_comment": "This paper has been accepted at PPAI-25, the 6th AAAI Workshop on\n  Privacy-Preserving Artificial Intelligence",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12465v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12465v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12461v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12461v1",
                "updated": "2025-01-21T19:17:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    19,
                    17,
                    46,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T19:17:46Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    19,
                    17,
                    46,
                    1,
                    21,
                    0
                ],
                "title": "Empowering AIOps: Leveraging Large Language Models for IT Operations\n  ManagementOperations Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empowering AIOps: Leveraging Large Language Models for IT Operations\n  ManagementOperations Management"
                },
                "summary": "The integration of Artificial Intelligence (AI) into IT Operations Management\n(ITOM), commonly referred to as AIOps, offers substantial potential for\nautomating workflows, enhancing efficiency, and supporting informed\ndecision-making. However, implementing AI within IT operations is not without\nits challenges, including issues related to data quality, the complexity of IT\nenvironments, and skill gaps within teams. The advent of Large Language Models\n(LLMs) presents an opportunity to address some of these challenges,\nparticularly through their advanced natural language understanding\ncapabilities. These features enable organizations to process and analyze vast\namounts of unstructured data, such as system logs, incident reports, and\ntechnical documentation. This ability aligns with the motivation behind our\nresearch, where we aim to integrate traditional predictive machine learning\nmodels with generative AI technologies like LLMs. By combining these\napproaches, we propose innovative methods to tackle persistent challenges in\nAIOps and enhance the capabilities of IT operations management.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Artificial Intelligence (AI) into IT Operations Management\n(ITOM), commonly referred to as AIOps, offers substantial potential for\nautomating workflows, enhancing efficiency, and supporting informed\ndecision-making. However, implementing AI within IT operations is not without\nits challenges, including issues related to data quality, the complexity of IT\nenvironments, and skill gaps within teams. The advent of Large Language Models\n(LLMs) presents an opportunity to address some of these challenges,\nparticularly through their advanced natural language understanding\ncapabilities. These features enable organizations to process and analyze vast\namounts of unstructured data, such as system logs, incident reports, and\ntechnical documentation. This ability aligns with the motivation behind our\nresearch, where we aim to integrate traditional predictive machine learning\nmodels with generative AI technologies like LLMs. By combining these\napproaches, we propose innovative methods to tackle persistent challenges in\nAIOps and enhance the capabilities of IT operations management."
                },
                "authors": [
                    {
                        "name": "Arthur Vitui"
                    },
                    {
                        "name": "Tse-Hsun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tse-Hsun Chen"
                },
                "author": "Tse-Hsun Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12461v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12461v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12456v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12456v1",
                "updated": "2025-01-21T19:04:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    19,
                    4,
                    53,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T19:04:53Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    19,
                    4,
                    53,
                    1,
                    21,
                    0
                ],
                "title": "Deploying Privacy Guardrails for LLMs: A Comparative Analysis of\n  Real-World Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying Privacy Guardrails for LLMs: A Comparative Analysis of\n  Real-World Applications"
                },
                "summary": "The adoption of Large Language Models (LLMs) has revolutionized AI\napplications but poses significant challenges in safeguarding user privacy.\nEnsuring compliance with privacy regulations such as GDPR and CCPA while\naddressing nuanced privacy risks requires robust and scalable frameworks. This\npaper presents a detailed study of OneShield Privacy Guard, a framework\ndesigned to mitigate privacy risks in user inputs and LLM outputs across\nenterprise and open-source settings. We analyze two real-world deployments:(1)\na multilingual privacy-preserving system integrated with Data and Model\nFactory, focusing on enterprise-scale data governance; and (2) PR Insights, an\nopen-source repository emphasizing automated triaging and community-driven\nrefinements. In Deployment 1, OneShield achieved a 0.95 F1 score in detecting\nsensitive entities like dates, names, and phone numbers across 26 languages,\noutperforming state-of-the-art tool such as StarPII and Presidio by up to 12\\%.\nDeployment 2, with an average F1 score of 0.86, reduced manual effort by over\n300 hours in three months, accurately flagging 8.25\\% of 1,256 pull requests\nfor privacy risks with enhanced context sensitivity. These results demonstrate\nOneShield's adaptability and efficacy in diverse environments, offering\nactionable insights for context-aware entity recognition, automated compliance,\nand ethical AI adoption. This work advances privacy-preserving frameworks,\nsupporting user trust and compliance across operational contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The adoption of Large Language Models (LLMs) has revolutionized AI\napplications but poses significant challenges in safeguarding user privacy.\nEnsuring compliance with privacy regulations such as GDPR and CCPA while\naddressing nuanced privacy risks requires robust and scalable frameworks. This\npaper presents a detailed study of OneShield Privacy Guard, a framework\ndesigned to mitigate privacy risks in user inputs and LLM outputs across\nenterprise and open-source settings. We analyze two real-world deployments:(1)\na multilingual privacy-preserving system integrated with Data and Model\nFactory, focusing on enterprise-scale data governance; and (2) PR Insights, an\nopen-source repository emphasizing automated triaging and community-driven\nrefinements. In Deployment 1, OneShield achieved a 0.95 F1 score in detecting\nsensitive entities like dates, names, and phone numbers across 26 languages,\noutperforming state-of-the-art tool such as StarPII and Presidio by up to 12\\%.\nDeployment 2, with an average F1 score of 0.86, reduced manual effort by over\n300 hours in three months, accurately flagging 8.25\\% of 1,256 pull requests\nfor privacy risks with enhanced context sensitivity. These results demonstrate\nOneShield's adaptability and efficacy in diverse environments, offering\nactionable insights for context-aware entity recognition, automated compliance,\nand ethical AI adoption. This work advances privacy-preserving frameworks,\nsupporting user trust and compliance across operational contexts."
                },
                "authors": [
                    {
                        "name": "Shubhi Asthana"
                    },
                    {
                        "name": "Bing Zhang"
                    },
                    {
                        "name": "Ruchi Mahindru"
                    },
                    {
                        "name": "Chad DeLuca"
                    },
                    {
                        "name": "Anna Lisa Gentile"
                    },
                    {
                        "name": "Sandeep Gopisetty"
                    }
                ],
                "author_detail": {
                    "name": "Sandeep Gopisetty"
                },
                "author": "Sandeep Gopisetty",
                "arxiv_comment": "This paper has been accepted at Deployable AI workshop at AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12456v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12456v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12372v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12372v1",
                "updated": "2025-01-21T18:52:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    18,
                    52,
                    15,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T18:52:15Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    18,
                    52,
                    15,
                    1,
                    21,
                    0
                ],
                "title": "Is Long Context All You Need? Leveraging LLM's Extended Context for\n  NL2SQL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is Long Context All You Need? Leveraging LLM's Extended Context for\n  NL2SQL"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities across\na range of natural language processing tasks. In particular, improvements in\nreasoning abilities and the expansion of context windows have opened new\navenues for leveraging these powerful models. NL2SQL is challenging in that the\nnatural language question is inherently ambiguous, while the SQL generation\nrequires a precise understanding of complex data schema and semantics. One\napproach to this semantic ambiguous problem is to provide more and sufficient\ncontextual information.\n  In this work, we explore the performance and the latency trade-offs of the\nextended context window (a.k.a., long context) offered by Google's\nstate-of-the-art LLM (\\textit{gemini-1.5-pro}). We study the impact of various\ncontextual information, including column example values, question and SQL query\npairs, user-provided hints, SQL documentation, and schema. To the best of our\nknowledge, this is the first work to study how the extended context window and\nextra contextual information can help NL2SQL generation with respect to both\naccuracy and latency cost. We show that long context LLMs are robust and do not\nget lost in the extended contextual information. Additionally, our long-context\nNL2SQL pipeline based on Google's \\textit{gemini-pro-1.5} achieve a strong\nperformance with 67.41\\% on BIRD benchmark (dev) without finetuning and\nexpensive self-consistency based techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive capabilities across\na range of natural language processing tasks. In particular, improvements in\nreasoning abilities and the expansion of context windows have opened new\navenues for leveraging these powerful models. NL2SQL is challenging in that the\nnatural language question is inherently ambiguous, while the SQL generation\nrequires a precise understanding of complex data schema and semantics. One\napproach to this semantic ambiguous problem is to provide more and sufficient\ncontextual information.\n  In this work, we explore the performance and the latency trade-offs of the\nextended context window (a.k.a., long context) offered by Google's\nstate-of-the-art LLM (\\textit{gemini-1.5-pro}). We study the impact of various\ncontextual information, including column example values, question and SQL query\npairs, user-provided hints, SQL documentation, and schema. To the best of our\nknowledge, this is the first work to study how the extended context window and\nextra contextual information can help NL2SQL generation with respect to both\naccuracy and latency cost. We show that long context LLMs are robust and do not\nget lost in the extended contextual information. Additionally, our long-context\nNL2SQL pipeline based on Google's \\textit{gemini-pro-1.5} achieve a strong\nperformance with 67.41\\% on BIRD benchmark (dev) without finetuning and\nexpensive self-consistency based techniques."
                },
                "authors": [
                    {
                        "name": "Yeounoh Chung"
                    },
                    {
                        "name": "Gaurav T. Kakkar"
                    },
                    {
                        "name": "Yu Gan"
                    },
                    {
                        "name": "Brenton Milne"
                    },
                    {
                        "name": "Fatma Ozcan"
                    }
                ],
                "author_detail": {
                    "name": "Fatma Ozcan"
                },
                "author": "Fatma Ozcan",
                "arxiv_comment": "14 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12372v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12372v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14655v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14655v2",
                "updated": "2025-01-21T18:14:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    18,
                    14,
                    30,
                    1,
                    21,
                    0
                ],
                "published": "2024-10-18T17:48:27Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    17,
                    48,
                    27,
                    4,
                    292,
                    0
                ],
                "title": "Bridging the Training-Inference Gap in LLMs by Leveraging Self-Generated\n  Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging the Training-Inference Gap in LLMs by Leveraging Self-Generated\n  Tokens"
                },
                "summary": "Language models are often trained to maximize the likelihood of the next\ntoken given past tokens in the training dataset. However, during inference\ntime, they are utilized differently, generating text sequentially and\nauto-regressively by using previously generated tokens as input to predict the\nnext one. Marginal differences in predictions at each step can cascade over\nsuccessive steps, resulting in different distributions from what the models\nwere trained for and potentially leading to unpredictable behavior. This paper\nproposes two simple approaches based on model own generation to address this\ndiscrepancy between the training and inference time. Our first approach is\nBatch-Scheduled Sampling, where, during training, we stochastically choose\nbetween the ground-truth token from the dataset and the model's own generated\ntoken as input to predict the next token. This is done in an offline manner,\nmodifying the context window by interleaving ground-truth tokens with those\ngenerated by the model. Our second approach is Reference-Answer-based\nCorrection, where we explicitly incorporate a self-correction capability into\nthe model during training. This enables the model to effectively self-correct\nthe gaps between the generated sequences and the ground truth data without\nrelying on an external oracle model. By incorporating our proposed strategies\nduring training, we have observed an overall improvement in performance\ncompared to baseline methods, as demonstrated by our extensive experiments\nusing summarization, general question-answering, and math question-answering\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models are often trained to maximize the likelihood of the next\ntoken given past tokens in the training dataset. However, during inference\ntime, they are utilized differently, generating text sequentially and\nauto-regressively by using previously generated tokens as input to predict the\nnext one. Marginal differences in predictions at each step can cascade over\nsuccessive steps, resulting in different distributions from what the models\nwere trained for and potentially leading to unpredictable behavior. This paper\nproposes two simple approaches based on model own generation to address this\ndiscrepancy between the training and inference time. Our first approach is\nBatch-Scheduled Sampling, where, during training, we stochastically choose\nbetween the ground-truth token from the dataset and the model's own generated\ntoken as input to predict the next token. This is done in an offline manner,\nmodifying the context window by interleaving ground-truth tokens with those\ngenerated by the model. Our second approach is Reference-Answer-based\nCorrection, where we explicitly incorporate a self-correction capability into\nthe model during training. This enables the model to effectively self-correct\nthe gaps between the generated sequences and the ground truth data without\nrelying on an external oracle model. By incorporating our proposed strategies\nduring training, we have observed an overall improvement in performance\ncompared to baseline methods, as demonstrated by our extensive experiments\nusing summarization, general question-answering, and math question-answering\ntasks."
                },
                "authors": [
                    {
                        "name": "Zhepeng Cen"
                    },
                    {
                        "name": "Yao Liu"
                    },
                    {
                        "name": "Siliang Zeng"
                    },
                    {
                        "name": "Pratik Chaudhari"
                    },
                    {
                        "name": "Huzefa Rangwala"
                    },
                    {
                        "name": "George Karypis"
                    },
                    {
                        "name": "Rasool Fakoor"
                    }
                ],
                "author_detail": {
                    "name": "Rasool Fakoor"
                },
                "author": "Rasool Fakoor",
                "arxiv_comment": "Published in TMLR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14655v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14655v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12339v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12339v1",
                "updated": "2025-01-21T18:13:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    18,
                    13,
                    43,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T18:13:43Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    18,
                    13,
                    43,
                    1,
                    21,
                    0
                ],
                "title": "Treefix: Enabling Execution with a Tree of Prefixes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Treefix: Enabling Execution with a Tree of Prefixes"
                },
                "summary": "The ability to execute code is a prerequisite for various dynamic program\nanalyses. Learning-guided execution has been proposed as an approach to enable\nthe execution of arbitrary code snippets by letting a neural model predict\nlikely values for any missing variables. Although state-of-the-art\nlearning-guided execution approaches, such as LExecutor, can enable the\nexecution of a relative high amount of code, they are limited to predicting a\nrestricted set of possible values and do not use any feedback from previous\nexecutions to execute even more code. This paper presents Treefix, a novel\nlearning-guided execution approach that leverages LLMs to iteratively create\ncode prefixes that enable the execution of a given code snippet. The approach\naddresses the problem in a multi-step fashion, where each step uses feedback\nabout the code snippet and its execution to instruct an LLM to improve a\npreviously generated prefix. This process iteratively creates a tree of\nprefixes, a subset of which is returned to the user as prefixes that maximize\nthe number of executed lines in the code snippet. In our experiments with two\ndatasets of Python code snippets, Treefix achieves 25% and 7% more coverage\nrelative to the current state of the art in learning-guided execution, covering\na total of 84% and 82% of all lines in the code snippets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ability to execute code is a prerequisite for various dynamic program\nanalyses. Learning-guided execution has been proposed as an approach to enable\nthe execution of arbitrary code snippets by letting a neural model predict\nlikely values for any missing variables. Although state-of-the-art\nlearning-guided execution approaches, such as LExecutor, can enable the\nexecution of a relative high amount of code, they are limited to predicting a\nrestricted set of possible values and do not use any feedback from previous\nexecutions to execute even more code. This paper presents Treefix, a novel\nlearning-guided execution approach that leverages LLMs to iteratively create\ncode prefixes that enable the execution of a given code snippet. The approach\naddresses the problem in a multi-step fashion, where each step uses feedback\nabout the code snippet and its execution to instruct an LLM to improve a\npreviously generated prefix. This process iteratively creates a tree of\nprefixes, a subset of which is returned to the user as prefixes that maximize\nthe number of executed lines in the code snippet. In our experiments with two\ndatasets of Python code snippets, Treefix achieves 25% and 7% more coverage\nrelative to the current state of the art in learning-guided execution, covering\na total of 84% and 82% of all lines in the code snippets."
                },
                "authors": [
                    {
                        "name": "Beatriz Souza"
                    },
                    {
                        "name": "Michael Pradel"
                    }
                ],
                "author_detail": {
                    "name": "Michael Pradel"
                },
                "author": "Michael Pradel",
                "arxiv_comment": "Accepted in research track of the EEE/ACM International Conference on\n  Software Engineering (ICSE) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12339v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12339v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12335v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12335v1",
                "updated": "2025-01-21T18:10:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    18,
                    10,
                    3,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T18:10:03Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    18,
                    10,
                    3,
                    1,
                    21,
                    0
                ],
                "title": "Quantum Compressive Sensing Meets Quantum Noise: A Practical Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum Compressive Sensing Meets Quantum Noise: A Practical Exploration"
                },
                "summary": "Compressive sensing is a signal processing technique that enables the\nreconstruction of sparse signals from a limited number of measurements,\nleveraging the signal's inherent sparsity to facilitate efficient recovery.\nRecent works on the Quantum Compressive Sensing (QCS) architecture, a quantum\ndata-driven approach to compressive sensing where the state of the tensor\nnetwork is represented by a quantum state over a set of entangled qubits, have\nshown promise in advancing quantum data-driven methods for compressive sensing.\nHowever, the QCS framework has remained largely untested on quantum computing\nresources or in the presence of quantum noise. In this work, we present a\npractical implementation of QCS on Amazon Braket, utilizing the Quantum\nImaginary Time Evolution (QITE) projection technique to assess the framework's\ncapabilities under quantum noise. We outline the necessary modifications to the\nQCS framework for deployment on Amazon Braket, followed by results under four\ntypes of quantum noise. Finally, we discuss potential long-term directions\naimed at unlocking the full potential of quantum compressive sensing for\napplications such as signal recovery and image processing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compressive sensing is a signal processing technique that enables the\nreconstruction of sparse signals from a limited number of measurements,\nleveraging the signal's inherent sparsity to facilitate efficient recovery.\nRecent works on the Quantum Compressive Sensing (QCS) architecture, a quantum\ndata-driven approach to compressive sensing where the state of the tensor\nnetwork is represented by a quantum state over a set of entangled qubits, have\nshown promise in advancing quantum data-driven methods for compressive sensing.\nHowever, the QCS framework has remained largely untested on quantum computing\nresources or in the presence of quantum noise. In this work, we present a\npractical implementation of QCS on Amazon Braket, utilizing the Quantum\nImaginary Time Evolution (QITE) projection technique to assess the framework's\ncapabilities under quantum noise. We outline the necessary modifications to the\nQCS framework for deployment on Amazon Braket, followed by results under four\ntypes of quantum noise. Finally, we discuss potential long-term directions\naimed at unlocking the full potential of quantum compressive sensing for\napplications such as signal recovery and image processing."
                },
                "authors": [
                    {
                        "name": "Naveed Naimipour"
                    },
                    {
                        "name": "Collin Frink"
                    },
                    {
                        "name": "Harry Shaw"
                    },
                    {
                        "name": "Haleh Safavi"
                    },
                    {
                        "name": "Mojtaba Soltanalian"
                    }
                ],
                "author_detail": {
                    "name": "Mojtaba Soltanalian"
                },
                "author": "Mojtaba Soltanalian",
                "arxiv_comment": "23 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12335v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12335v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]