[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2411.19379v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19379v2",
                "updated": "2024-12-04T18:40:24Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    18,
                    40,
                    24,
                    2,
                    339,
                    0
                ],
                "published": "2024-11-28T21:10:20Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    21,
                    10,
                    20,
                    3,
                    333,
                    0
                ],
                "title": "Marconi: Prefix Caching for the Era of Hybrid LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Marconi: Prefix Caching for the Era of Hybrid LLMs"
                },
                "summary": "Hybrid models that combine the language modeling capabilities of Attention\nlayers with the efficiency of Recurrent layers (e.g., State Space Models) have\ngained traction in practically supporting long contexts in Large Language Model\nserving. Yet, the unique properties of these models complicate the usage of\ncomplementary efficiency optimizations such as prefix caching that skip\nredundant computations across requests. Most notably, their use of in-place\nstate updates for recurrent layers precludes rolling back cache entries for\npartial sequence overlaps, and instead mandates only exact-match cache hits;\nthe effect is a deluge of (large) cache entries per sequence, most of which\nyield minimal reuse opportunities. We present Marconi, the first system that\nsupports efficient prefix caching with Hybrid LLMs. Key to Marconi are its\nnovel admission and eviction policies that more judiciously assess potential\ncache entries based not only on recency, but also on (1) forecasts of their\nreuse likelihood across a taxonomy of different hit scenarios, and (2) the\ncompute savings that hits deliver relative to memory footprints. Across diverse\nworkloads and Hybrid models, Marconi achieves up to 34.4$\\times$ higher token\nhit rates (71.1% or 617 ms lower TTFT) compared to state-of-the-art prefix\ncaching systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid models that combine the language modeling capabilities of Attention\nlayers with the efficiency of Recurrent layers (e.g., State Space Models) have\ngained traction in practically supporting long contexts in Large Language Model\nserving. Yet, the unique properties of these models complicate the usage of\ncomplementary efficiency optimizations such as prefix caching that skip\nredundant computations across requests. Most notably, their use of in-place\nstate updates for recurrent layers precludes rolling back cache entries for\npartial sequence overlaps, and instead mandates only exact-match cache hits;\nthe effect is a deluge of (large) cache entries per sequence, most of which\nyield minimal reuse opportunities. We present Marconi, the first system that\nsupports efficient prefix caching with Hybrid LLMs. Key to Marconi are its\nnovel admission and eviction policies that more judiciously assess potential\ncache entries based not only on recency, but also on (1) forecasts of their\nreuse likelihood across a taxonomy of different hit scenarios, and (2) the\ncompute savings that hits deliver relative to memory footprints. Across diverse\nworkloads and Hybrid models, Marconi achieves up to 34.4$\\times$ higher token\nhit rates (71.1% or 617 ms lower TTFT) compared to state-of-the-art prefix\ncaching systems."
                },
                "authors": [
                    {
                        "name": "Rui Pan"
                    },
                    {
                        "name": "Zhuang Wang"
                    },
                    {
                        "name": "Zhen Jia"
                    },
                    {
                        "name": "Can Karakus"
                    },
                    {
                        "name": "Luca Zancato"
                    },
                    {
                        "name": "Tri Dao"
                    },
                    {
                        "name": "Yida Wang"
                    },
                    {
                        "name": "Ravi Netravali"
                    }
                ],
                "author_detail": {
                    "name": "Ravi Netravali"
                },
                "author": "Ravi Netravali",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19379v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19379v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03409v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03409v1",
                "updated": "2024-12-04T15:48:59Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    15,
                    48,
                    59,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T15:48:59Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    15,
                    48,
                    59,
                    2,
                    339,
                    0
                ],
                "title": "PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following\n  Models Need for Efficient Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following\n  Models Need for Efficient Generation"
                },
                "summary": "Recently, large vision-language models (LVLMs) have rapidly gained popularity\nfor their strong generation and reasoning capabilities given diverse multimodal\ninputs. However, these models incur significant computational and memory\noverhead during inference, which greatly hinders the efficient deployment in\npractical scenarios. The extensive key-value (KV) cache, necessitated by the\nlengthy input and output sequences, notably contributes to the high inference\ncost. Based on this, recent works have investigated ways to reduce the KV cache\nsize for higher efficiency. Although effective, they generally overlook the\ndistinct importance distributions of KV vectors across layers and maintain the\nsame cache size for each layer during the next token prediction. This results\nin the significant contextual information loss for certain layers, leading to\nnotable performance decline. To address this, we present PrefixKV. It reframes\nthe challenge of determining KV cache sizes for all layers into the task of\nsearching for the optimal global prefix configuration. With an adaptive\nlayer-wise KV retention recipe based on binary search, the maximum contextual\ninformation can thus be preserved in each layer, facilitating the generation.\nExtensive experiments demonstrate that our method achieves the state-of-the-art\nperformance compared with others. It exhibits superior inference efficiency and\ngeneration quality trade-offs, showing promising potential for practical\napplications. Code is available at \\url{https://github.com/THU-MIG/PrefixKV}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large vision-language models (LVLMs) have rapidly gained popularity\nfor their strong generation and reasoning capabilities given diverse multimodal\ninputs. However, these models incur significant computational and memory\noverhead during inference, which greatly hinders the efficient deployment in\npractical scenarios. The extensive key-value (KV) cache, necessitated by the\nlengthy input and output sequences, notably contributes to the high inference\ncost. Based on this, recent works have investigated ways to reduce the KV cache\nsize for higher efficiency. Although effective, they generally overlook the\ndistinct importance distributions of KV vectors across layers and maintain the\nsame cache size for each layer during the next token prediction. This results\nin the significant contextual information loss for certain layers, leading to\nnotable performance decline. To address this, we present PrefixKV. It reframes\nthe challenge of determining KV cache sizes for all layers into the task of\nsearching for the optimal global prefix configuration. With an adaptive\nlayer-wise KV retention recipe based on binary search, the maximum contextual\ninformation can thus be preserved in each layer, facilitating the generation.\nExtensive experiments demonstrate that our method achieves the state-of-the-art\nperformance compared with others. It exhibits superior inference efficiency and\ngeneration quality trade-offs, showing promising potential for practical\napplications. Code is available at \\url{https://github.com/THU-MIG/PrefixKV}."
                },
                "authors": [
                    {
                        "name": "Ao Wang"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Jianchao Tan"
                    },
                    {
                        "name": "Kefeng Zhang"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Zijia Lin"
                    },
                    {
                        "name": "Jungong Han"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "arxiv_comment": "12 pages, 5 figures;",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03409v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03409v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03361v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03361v1",
                "updated": "2024-12-04T14:47:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    14,
                    47,
                    42,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T14:47:42Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    14,
                    47,
                    42,
                    2,
                    339,
                    0
                ],
                "title": "Measurement of electron beam induced sample heating in SEM experiments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measurement of electron beam induced sample heating in SEM experiments"
                },
                "summary": "Scanning Electron Microscopy (SEM) experiments provide detailed insights into\nmaterial microstructures, enabling high-resolution imaging as well as\ncrystallographic analysis through advanced techniques like Electron Backscatter\nDiffraction (EBSD). However, the interaction of the high-energy electron beam\nwith the material can lead to localized heating, which may significantly impact\nspecimen integrity, especially in applications requiring prolonged beam\nexposure, for instance when mapping the crystal structure using EBSD. This\nstudy examines electron-beam-induced heating effects on a model metal sample\n(iron), directly measuring the locally deposited electron beam energy with a\nMEMS-based heating device and validating these measurements through\nsimulations, including Monte Carlo and Finite Element methods. The analysis\nfocuses on the effects of various experimental parameters such as acceleration\nvoltage (from 5 to 30 kV), beam current (from 0.17 nA to 22 nA), dwell time\n(from 1$\\mu$s to 1ms) and sample tilt (0{\\deg} to 70{\\deg}). The findings\nreveal that local sample temperatures can increase by up to 70 {\\deg}C during\nEBSD experiments, primarily affected by the choice in beam current and\nacceleration voltage, with beam current having the most significant impact.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scanning Electron Microscopy (SEM) experiments provide detailed insights into\nmaterial microstructures, enabling high-resolution imaging as well as\ncrystallographic analysis through advanced techniques like Electron Backscatter\nDiffraction (EBSD). However, the interaction of the high-energy electron beam\nwith the material can lead to localized heating, which may significantly impact\nspecimen integrity, especially in applications requiring prolonged beam\nexposure, for instance when mapping the crystal structure using EBSD. This\nstudy examines electron-beam-induced heating effects on a model metal sample\n(iron), directly measuring the locally deposited electron beam energy with a\nMEMS-based heating device and validating these measurements through\nsimulations, including Monte Carlo and Finite Element methods. The analysis\nfocuses on the effects of various experimental parameters such as acceleration\nvoltage (from 5 to 30 kV), beam current (from 0.17 nA to 22 nA), dwell time\n(from 1$\\mu$s to 1ms) and sample tilt (0{\\deg} to 70{\\deg}). The findings\nreveal that local sample temperatures can increase by up to 70 {\\deg}C during\nEBSD experiments, primarily affected by the choice in beam current and\nacceleration voltage, with beam current having the most significant impact."
                },
                "authors": [
                    {
                        "name": "Christina Koenig"
                    },
                    {
                        "name": "Alice Bastos da Silva Fanta"
                    },
                    {
                        "name": "Joerg R. Jinschek"
                    }
                ],
                "author_detail": {
                    "name": "Joerg R. Jinschek"
                },
                "author": "Joerg R. Jinschek",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03361v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03361v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03213v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03213v1",
                "updated": "2024-12-04T10:58:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    10,
                    58,
                    27,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T10:58:27Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    10,
                    58,
                    27,
                    2,
                    339,
                    0
                ],
                "title": "ClusterKV: Manipulating LLM KV Cache in Semantic Space for Recallable\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ClusterKV: Manipulating LLM KV Cache in Semantic Space for Recallable\n  Compression"
                },
                "summary": "Large Language Models (LLMs) have been widely deployed in a variety of\napplications, and the context length is rapidly increasing to handle tasks such\nas long-document QA and complex logical reasoning. However, long context poses\nsignificant challenges for inference efficiency, including high memory costs of\nkey-value (KV) cache and increased latency due to extensive memory accesses.\nRecent works have proposed compressing KV cache to approximate computation, but\nthese methods either evict tokens permanently, never recalling them for later\ninference, or recall previous tokens at the granularity of pages divided by\ntextual positions. Both approaches degrade the model accuracy and output\nquality. To achieve efficient and accurate recallable KV cache compression, we\nintroduce ClusterKV, which recalls tokens at the granularity of semantic\nclusters. We design and implement efficient algorithms and systems for\nclustering, selection, indexing and caching. Experiment results show that\nClusterKV attains negligible accuracy loss across various tasks with 32k\ncontext lengths, using only a 1k to 2k KV cache budget, and achieves up to a\n2$\\times$ speedup in latency and a 2.5$\\times$ improvement in decoding\nthroughput. Compared to SoTA recallable KV compression methods, ClusterKV\ndemonstrates higher model accuracy and output quality, while maintaining or\nexceeding inference efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been widely deployed in a variety of\napplications, and the context length is rapidly increasing to handle tasks such\nas long-document QA and complex logical reasoning. However, long context poses\nsignificant challenges for inference efficiency, including high memory costs of\nkey-value (KV) cache and increased latency due to extensive memory accesses.\nRecent works have proposed compressing KV cache to approximate computation, but\nthese methods either evict tokens permanently, never recalling them for later\ninference, or recall previous tokens at the granularity of pages divided by\ntextual positions. Both approaches degrade the model accuracy and output\nquality. To achieve efficient and accurate recallable KV cache compression, we\nintroduce ClusterKV, which recalls tokens at the granularity of semantic\nclusters. We design and implement efficient algorithms and systems for\nclustering, selection, indexing and caching. Experiment results show that\nClusterKV attains negligible accuracy loss across various tasks with 32k\ncontext lengths, using only a 1k to 2k KV cache budget, and achieves up to a\n2$\\times$ speedup in latency and a 2.5$\\times$ improvement in decoding\nthroughput. Compared to SoTA recallable KV compression methods, ClusterKV\ndemonstrates higher model accuracy and output quality, while maintaining or\nexceeding inference efficiency."
                },
                "authors": [
                    {
                        "name": "Guangda Liu"
                    },
                    {
                        "name": "Chengwei Li"
                    },
                    {
                        "name": "Jieru Zhao"
                    },
                    {
                        "name": "Chenqi Zhang"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03213v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03213v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03131v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03131v1",
                "updated": "2024-12-04T08:51:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    51,
                    23,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T08:51:23Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    51,
                    23,
                    2,
                    339,
                    0
                ],
                "title": "Unifying KV Cache Compression for Large Language Models with LeanKV",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying KV Cache Compression for Large Language Models with LeanKV"
                },
                "summary": "Large language models (LLMs) demonstrate exceptional performance but incur\nhigh serving costs due to substantial memory demands, with the key-value (KV)\ncache being a primary bottleneck. Existing KV cache compression methods,\nincluding quantization and pruning, struggle with limitations such as uniform\ntreatment of keys and values and static memory allocation across attention\nheads. To address these challenges, we introduce LeanKV, a unified KV cache\ncompression framework that enhances LLM serving efficiency without compromising\naccuracy through three innovations: (1) Hetero-KV quantization, which stores\nkeys at a higher precision than values to reflect their greater impact on\nattention computations; (2) per-head dynamic sparsity, which allocates memory\nbased on token importance per head and per request; and (3) unified KV\ncompression, integrating mixed-precision quantization and selective pruning to\nenable a smooth tradeoff between model accuracy and memory efficiency. To\nefficiently support these techniques, LeanKV introduces systems optimizations\nincluding unified paging and on-GPU parallel memory management. Implemented on\nvLLM, LeanKV compresses the KV cache by $3.0\\times$ to $5.0\\times$ without\naccuracy loss and up to $11.0\\times$ with under 5% accuracy loss, enhancing\nthroughput by $1.9\\times$ to $2.5\\times$, and up to $6.9\\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate exceptional performance but incur\nhigh serving costs due to substantial memory demands, with the key-value (KV)\ncache being a primary bottleneck. Existing KV cache compression methods,\nincluding quantization and pruning, struggle with limitations such as uniform\ntreatment of keys and values and static memory allocation across attention\nheads. To address these challenges, we introduce LeanKV, a unified KV cache\ncompression framework that enhances LLM serving efficiency without compromising\naccuracy through three innovations: (1) Hetero-KV quantization, which stores\nkeys at a higher precision than values to reflect their greater impact on\nattention computations; (2) per-head dynamic sparsity, which allocates memory\nbased on token importance per head and per request; and (3) unified KV\ncompression, integrating mixed-precision quantization and selective pruning to\nenable a smooth tradeoff between model accuracy and memory efficiency. To\nefficiently support these techniques, LeanKV introduces systems optimizations\nincluding unified paging and on-GPU parallel memory management. Implemented on\nvLLM, LeanKV compresses the KV cache by $3.0\\times$ to $5.0\\times$ without\naccuracy loss and up to $11.0\\times$ with under 5% accuracy loss, enhancing\nthroughput by $1.9\\times$ to $2.5\\times$, and up to $6.9\\times$."
                },
                "authors": [
                    {
                        "name": "Yanqi Zhang"
                    },
                    {
                        "name": "Yuwei Hu"
                    },
                    {
                        "name": "Runyuan Zhao"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03131v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03131v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2303.08066v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2303.08066v2",
                "updated": "2024-12-04T05:32:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    5,
                    32,
                    12,
                    2,
                    339,
                    0
                ],
                "published": "2023-02-06T13:46:08Z",
                "published_parsed": [
                    2023,
                    2,
                    6,
                    13,
                    46,
                    8,
                    0,
                    37,
                    0
                ],
                "title": "PASCAL: A Learning-aided Cooperative Bandwidth Control Policy for\n  Hierarchical Storage Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PASCAL: A Learning-aided Cooperative Bandwidth Control Policy for\n  Hierarchical Storage Systems"
                },
                "summary": "Nowadays, the Hierarchical Storage System (HSS) is considered as an ideal\nmodel to meet the cost-performance demand. The data migration between storing\ntiers of HSS is the way to achieve the cost-performance goal. The bandwidth\ncontrol is to limit the maximum amount of data migration. Most of previous\nresearch about HSS focus on studying the data migration policy instead of\nbandwidth control. However, the recent research about cache and networking\noptimization suggest that the bandwidth control has significant impact on the\nsystem performance. Few previous work achieves a satisfactory bandwidth control\nin HSS since it is hard to control bandwidth for so many data migration tasks\nsimultaneously. In this paper, we first give a stochastic programming model to\nformalize the bandwidth control problem in HSS. Then we propose a\nlearning-aided bandwidth control policy for HSS, named \\Pascal{}, which learns\nto control the bandwidth of different data migration task in an cooperative\nway. We implement \\Pascal{} on a commercial HSS and compare it with three\nstrong baselines over a group of workloads. Our evaluation on the physical\nsystem shows that \\Pascal{} can effectively decrease 1.95X the tail latency and\ngreatly improve throughput stability (2X $\\downarrow$ throughput jitter), and\nmeanwhile keep the throughput at a relatively high level.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nowadays, the Hierarchical Storage System (HSS) is considered as an ideal\nmodel to meet the cost-performance demand. The data migration between storing\ntiers of HSS is the way to achieve the cost-performance goal. The bandwidth\ncontrol is to limit the maximum amount of data migration. Most of previous\nresearch about HSS focus on studying the data migration policy instead of\nbandwidth control. However, the recent research about cache and networking\noptimization suggest that the bandwidth control has significant impact on the\nsystem performance. Few previous work achieves a satisfactory bandwidth control\nin HSS since it is hard to control bandwidth for so many data migration tasks\nsimultaneously. In this paper, we first give a stochastic programming model to\nformalize the bandwidth control problem in HSS. Then we propose a\nlearning-aided bandwidth control policy for HSS, named \\Pascal{}, which learns\nto control the bandwidth of different data migration task in an cooperative\nway. We implement \\Pascal{} on a commercial HSS and compare it with three\nstrong baselines over a group of workloads. Our evaluation on the physical\nsystem shows that \\Pascal{} can effectively decrease 1.95X the tail latency and\ngreatly improve throughput stability (2X $\\downarrow$ throughput jitter), and\nmeanwhile keep the throughput at a relatively high level."
                },
                "authors": [
                    {
                        "name": "Xijun Li"
                    },
                    {
                        "name": "Yunfan Zhou"
                    },
                    {
                        "name": "Ji Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ji Zhang"
                },
                "author": "Ji Zhang",
                "arxiv_comment": "for modifying part of contents",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2303.08066v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2303.08066v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03023v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03023v1",
                "updated": "2024-12-04T04:29:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    4,
                    29,
                    12,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T04:29:12Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    4,
                    29,
                    12,
                    2,
                    339,
                    0
                ],
                "title": "A Multi-Functional Web Tool for Comprehensive Threat Detection Through\n  IP Address Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Multi-Functional Web Tool for Comprehensive Threat Detection Through\n  IP Address Analysis"
                },
                "summary": "In recent years, the advances in digitalisation have also adversely\ncontributed to the significant rise in cybercrimes. Hence, building the threat\nintelligence to shield against rising cybercrimes has become a fundamental\nrequisite. Internet Protocol (IP) addresses play a crucial role in the threat\nintelligence and prevention of cyber crimes. However, we have noticed the lack\nof one-stop, free, and open-source tools that can analyse IP addresses. Hence,\nthis work introduces a comprehensive web tool for advanced IP address\ncharacterisation. Our tool offers a wide range of features, including\ngeolocation, blocklist check, VPN detection, proxy detection, bot detection,\nTor detection, port scan, and accurate domain statistics that include the\ndetails about the name servers and registrar information. In addition, our tool\ncalculates a confidence score based on a weighted sum of publicly accessible\nonline results from different reliable sources to give users a dependable\nmeasure of accuracy. Further, to improve performance, our tool also\nincorporates a local database for caching the results, to enable fast content\nretrieval with minimal external Web API calls. Our tool supports domain names\nand IPv4 addresses, making it a multi-functional and powerful IP analyser tool\nfor threat intelligence. Our tool is available at www.ipanalyzer.in",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the advances in digitalisation have also adversely\ncontributed to the significant rise in cybercrimes. Hence, building the threat\nintelligence to shield against rising cybercrimes has become a fundamental\nrequisite. Internet Protocol (IP) addresses play a crucial role in the threat\nintelligence and prevention of cyber crimes. However, we have noticed the lack\nof one-stop, free, and open-source tools that can analyse IP addresses. Hence,\nthis work introduces a comprehensive web tool for advanced IP address\ncharacterisation. Our tool offers a wide range of features, including\ngeolocation, blocklist check, VPN detection, proxy detection, bot detection,\nTor detection, port scan, and accurate domain statistics that include the\ndetails about the name servers and registrar information. In addition, our tool\ncalculates a confidence score based on a weighted sum of publicly accessible\nonline results from different reliable sources to give users a dependable\nmeasure of accuracy. Further, to improve performance, our tool also\nincorporates a local database for caching the results, to enable fast content\nretrieval with minimal external Web API calls. Our tool supports domain names\nand IPv4 addresses, making it a multi-functional and powerful IP analyser tool\nfor threat intelligence. Our tool is available at www.ipanalyzer.in"
                },
                "authors": [
                    {
                        "name": "Cebajel Tanan"
                    },
                    {
                        "name": "Sameer G. Kulkarni"
                    },
                    {
                        "name": "Tamal Das"
                    },
                    {
                        "name": "Manjesh K. Hanawal"
                    }
                ],
                "author_detail": {
                    "name": "Manjesh K. Hanawal"
                },
                "author": "Manjesh K. Hanawal",
                "arxiv_comment": "Presented at ICIE 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03023v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03023v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.12622v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.12622v2",
                "updated": "2024-12-03T22:48:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    22,
                    48,
                    9,
                    1,
                    338,
                    0
                ],
                "published": "2023-10-19T10:02:52Z",
                "published_parsed": [
                    2023,
                    10,
                    19,
                    10,
                    2,
                    52,
                    3,
                    292,
                    0
                ],
                "title": "cRVR: A Stackelberg Game Approach for Joint Privacy-Aware Video\n  Requesting and Edge Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "cRVR: A Stackelberg Game Approach for Joint Privacy-Aware Video\n  Requesting and Edge Caching"
                },
                "summary": "As users conveniently stream their favorite online videos, video request\nrecords are automatically stored by video content providers, which have a high\nchance of privacy leakage. Unfortunately, most existing privacy-enhancing\napproaches are not applicable for protecting user privacy in video requests,\nbecause they cannot be easily altered or distorted by users and must be visible\nfor content providers to stream correct videos. To preserve request privacy in\nonline video services, it is possible to request additional videos that are\nirrelevant to users' interests so that content providers cannot precisely infer\nusers' interest information. However, a naive redundant requesting approach\nwould significantly degrade the performance of edge caches and increase\nbandwidth overhead. In this paper, we are among the first to propose a\nCache-Friendly Redundant Video Requesting (cRVR) algorithm for User Devices\n(UDs) and its corresponding caching algorithm for the Edge Cache (EC), which\ncan effectively mitigate the problem of request privacy leakage with minimal\nimpact on the EC's performance. To tackle the problem, we first develop a\nStackelberg game to analyze the dedicated interaction between UDs and EC, and\nobtain their optimal strategies to maximize their respective utility. For UDs,\nthe utility function is a combination of both video playback utility and\nprivacy protection utility. We prove the existence and uniqueness of the\nequilibrium of the Stackelberg game. Extensive experiments are conducted with\nreal traces to demonstrate that cRVR can effectively protect video request\nprivacy by reducing up to 59.03\\% of privacy disclosure compared to baseline\nalgorithms. Meanwhile, the caching performance of EC is only slightly affected.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As users conveniently stream their favorite online videos, video request\nrecords are automatically stored by video content providers, which have a high\nchance of privacy leakage. Unfortunately, most existing privacy-enhancing\napproaches are not applicable for protecting user privacy in video requests,\nbecause they cannot be easily altered or distorted by users and must be visible\nfor content providers to stream correct videos. To preserve request privacy in\nonline video services, it is possible to request additional videos that are\nirrelevant to users' interests so that content providers cannot precisely infer\nusers' interest information. However, a naive redundant requesting approach\nwould significantly degrade the performance of edge caches and increase\nbandwidth overhead. In this paper, we are among the first to propose a\nCache-Friendly Redundant Video Requesting (cRVR) algorithm for User Devices\n(UDs) and its corresponding caching algorithm for the Edge Cache (EC), which\ncan effectively mitigate the problem of request privacy leakage with minimal\nimpact on the EC's performance. To tackle the problem, we first develop a\nStackelberg game to analyze the dedicated interaction between UDs and EC, and\nobtain their optimal strategies to maximize their respective utility. For UDs,\nthe utility function is a combination of both video playback utility and\nprivacy protection utility. We prove the existence and uniqueness of the\nequilibrium of the Stackelberg game. Extensive experiments are conducted with\nreal traces to demonstrate that cRVR can effectively protect video request\nprivacy by reducing up to 59.03\\% of privacy disclosure compared to baseline\nalgorithms. Meanwhile, the caching performance of EC is only slightly affected."
                },
                "authors": [
                    {
                        "name": "Xianzhi Zhang"
                    },
                    {
                        "name": "Linchang Xiao"
                    },
                    {
                        "name": "Yipeng Zhou"
                    },
                    {
                        "name": "Miao Hu"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Quan Z. Sheng"
                    }
                ],
                "author_detail": {
                    "name": "Quan Z. Sheng"
                },
                "author": "Quan Z. Sheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.12622v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.12622v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02867v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02867v1",
                "updated": "2024-12-03T22:02:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    22,
                    2,
                    42,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T22:02:42Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    22,
                    2,
                    42,
                    1,
                    338,
                    0
                ],
                "title": "GoldFish: Serverless Actors with Short-Term Memory State for the\n  Edge-Cloud Continuum",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GoldFish: Serverless Actors with Short-Term Memory State for the\n  Edge-Cloud Continuum"
                },
                "summary": "Serverless Computing is a computing paradigm that provides efficient\ninfrastructure management and elastic scalability. Serverless functions scale\nup or down based on demand, which means that functions are not directly\naddressable and rely on platform-managed invocation. Serverless stateless\nnature requires functions to leverage external services, such as object storage\nand KVS, to exchange data. Serverless actors have emerged as a solution to\nthese issues. However, the state-of-the-art serverless lifecycle and\nevent-trigger invocation force actors to leverage remote services to manage\ntheir state and exchange data, which impacts the performance and incurs\nadditional costs and dependency on third-party services.\n  To address these issues, in this paper, we introduce a novel serverless\nlifecycle model that allows short-term stateful actors, enabling actors to\nmaintain their state between executions. Additionally, we propose a novel\nserverless Invocation Model that enables serverless actors to influence the\nprocessing of future messages. We present GoldFish, a lightweight WebAssembly\nshort-term stateful serverless actor platform that provides a novel serverless\nactor lifecycle and invocation model. GoldFish leverages WebAssembly to provide\nthe actors with lightweight sandbox isolation, making them suitable for the\nEdge-Cloud Continuum, where computational resources are limited. Experimental\nresults show that GoldFish optimizes the data exchange latency by up to 92% and\nincreases the throughput by up to 10x compared to OpenFaaS and Spin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless Computing is a computing paradigm that provides efficient\ninfrastructure management and elastic scalability. Serverless functions scale\nup or down based on demand, which means that functions are not directly\naddressable and rely on platform-managed invocation. Serverless stateless\nnature requires functions to leverage external services, such as object storage\nand KVS, to exchange data. Serverless actors have emerged as a solution to\nthese issues. However, the state-of-the-art serverless lifecycle and\nevent-trigger invocation force actors to leverage remote services to manage\ntheir state and exchange data, which impacts the performance and incurs\nadditional costs and dependency on third-party services.\n  To address these issues, in this paper, we introduce a novel serverless\nlifecycle model that allows short-term stateful actors, enabling actors to\nmaintain their state between executions. Additionally, we propose a novel\nserverless Invocation Model that enables serverless actors to influence the\nprocessing of future messages. We present GoldFish, a lightweight WebAssembly\nshort-term stateful serverless actor platform that provides a novel serverless\nactor lifecycle and invocation model. GoldFish leverages WebAssembly to provide\nthe actors with lightweight sandbox isolation, making them suitable for the\nEdge-Cloud Continuum, where computational resources are limited. Experimental\nresults show that GoldFish optimizes the data exchange latency by up to 92% and\nincreases the throughput by up to 10x compared to OpenFaaS and Spin."
                },
                "authors": [
                    {
                        "name": "Cynthia Marcelino"
                    },
                    {
                        "name": "Jack Shahhoud"
                    },
                    {
                        "name": "Stefan Nastic"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Nastic"
                },
                "author": "Stefan Nastic",
                "arxiv_doi": "10.1145/3703790.3703797",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3703790.3703797",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.02867v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02867v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14th International Conference on the Internet of Things (IoT 2024),\n  November 19--22, 2024, Oulu, Finland",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05276v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05276v2",
                "updated": "2024-12-03T21:40:10Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    21,
                    40,
                    10,
                    1,
                    338,
                    0
                ],
                "published": "2024-11-08T02:21:19Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    2,
                    21,
                    19,
                    4,
                    313,
                    0
                ],
                "title": "GPT Semantic Cache: Reducing LLM Costs and Latency via Semantic\n  Embedding Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPT Semantic Cache: Reducing LLM Costs and Latency via Semantic\n  Embedding Caching"
                },
                "summary": "Large Language Models (LLMs), such as GPT, have revolutionized artificial\nintelligence by enabling nuanced understanding and generation of human-like\ntext across a wide range of applications. However, the high computational and\nfinancial costs associated with frequent API calls to these models present a\nsubstantial bottleneck, especially for applications like customer service\nchatbots that handle repetitive queries. In this paper, we introduce GPT\nSemantic Cache, a method that leverages semantic caching of query embeddings in\nin-memory storage (Redis). By storing embeddings of user queries, our approach\nefficiently identifies semantically similar questions, allowing for the\nretrieval of pre-generated responses without redundant API calls to the LLM.\nThis technique achieves a notable reduction in operational costs while\nsignificantly enhancing response times, making it a robust solution for\noptimizing LLM-powered applications. Our experiments demonstrate that GPT\nSemantic Cache reduces API calls by up to 68.8% across various query\ncategories, with cache hit rates ranging from 61.6% to 68.8%. Additionally, the\nsystem achieves high accuracy, with positive hit rates exceeding 97%,\nconfirming the reliability of cached responses. This technique not only reduces\noperational costs, but also improves response times, enhancing the efficiency\nof LLM-powered applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), such as GPT, have revolutionized artificial\nintelligence by enabling nuanced understanding and generation of human-like\ntext across a wide range of applications. However, the high computational and\nfinancial costs associated with frequent API calls to these models present a\nsubstantial bottleneck, especially for applications like customer service\nchatbots that handle repetitive queries. In this paper, we introduce GPT\nSemantic Cache, a method that leverages semantic caching of query embeddings in\nin-memory storage (Redis). By storing embeddings of user queries, our approach\nefficiently identifies semantically similar questions, allowing for the\nretrieval of pre-generated responses without redundant API calls to the LLM.\nThis technique achieves a notable reduction in operational costs while\nsignificantly enhancing response times, making it a robust solution for\noptimizing LLM-powered applications. Our experiments demonstrate that GPT\nSemantic Cache reduces API calls by up to 68.8% across various query\ncategories, with cache hit rates ranging from 61.6% to 68.8%. Additionally, the\nsystem achieves high accuracy, with positive hit rates exceeding 97%,\nconfirming the reliability of cached responses. This technique not only reduces\noperational costs, but also improves response times, enhancing the efficiency\nof LLM-powered applications."
                },
                "authors": [
                    {
                        "name": "Sajal Regmi"
                    },
                    {
                        "name": "Chetan Phakami Pun"
                    }
                ],
                "author_detail": {
                    "name": "Chetan Phakami Pun"
                },
                "author": "Chetan Phakami Pun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05276v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05276v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00876v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00876v2",
                "updated": "2024-12-03T16:12:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    16,
                    12,
                    9,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-01T16:32:31Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    16,
                    32,
                    31,
                    6,
                    336,
                    0
                ],
                "title": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava ."
                },
                "authors": [
                    {
                        "name": "Wenxuan Huang"
                    },
                    {
                        "name": "Zijie Zhai"
                    },
                    {
                        "name": "Yunhang Shen"
                    },
                    {
                        "name": "Shaoshen Cao"
                    },
                    {
                        "name": "Fei Zhao"
                    },
                    {
                        "name": "Xiangfeng Xu"
                    },
                    {
                        "name": "Zheyu Ye"
                    },
                    {
                        "name": "Shaohui Lin"
                    }
                ],
                "author_detail": {
                    "name": "Shaohui Lin"
                },
                "author": "Shaohui Lin",
                "arxiv_comment": "Code is available at https://github.com/Osilly/dynamic_llava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00876v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00876v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17897v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17897v3",
                "updated": "2024-12-03T12:36:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    12,
                    36,
                    19,
                    1,
                    338,
                    0
                ],
                "published": "2024-10-23T14:15:07Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    15,
                    7,
                    2,
                    297,
                    0
                ],
                "title": "Value Residual Learning For Alleviating Attention Concentration In\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value Residual Learning For Alleviating Attention Concentration In\n  Transformers"
                },
                "summary": "Transformers can capture long-range dependencies using self-attention,\nallowing tokens to attend to all others directly. However, stacking multiple\nattention layers leads to attention concentration. One natural way to address\nthis issue is to use cross-layer attention, allowing information from earlier\nlayers to be directly accessible to later layers. However, this approach is\ncomputationally expensive. To address this problem, we propose Transformer with\nresidual value (ResFormer) which approximates cross-layer attention through\nadding a residual connection from the values of the the first layer to all\nsubsequent layers. Based on this method, one variant is the Transformer with\nsingle layer value (SVFormer), where all layers share the same value embedding\nfrom first layer. Comprehensive empirical evidence demonstrates ResFormer\nachieves equivalent validation loss with 10.4% fewer model parameters and 13.6%\nless training data compared to Transformer, while maintaining similar memory\nusage and computational cost. Besides, SVFormer reduces KV cache size by nearly\nhalf with only a small performance penalty and can be integrated with other\nKV-efficient methods, yielding further reductions in KV cache, with performance\ninfluenced by sequence length and cumulative learning rate. Further\nvisualization results suggest that Resformer and SVFormer alleviate attention\nconcentration in deeper layers through avoiding value-state drains and enhance\nrepresentation across most layers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers can capture long-range dependencies using self-attention,\nallowing tokens to attend to all others directly. However, stacking multiple\nattention layers leads to attention concentration. One natural way to address\nthis issue is to use cross-layer attention, allowing information from earlier\nlayers to be directly accessible to later layers. However, this approach is\ncomputationally expensive. To address this problem, we propose Transformer with\nresidual value (ResFormer) which approximates cross-layer attention through\nadding a residual connection from the values of the the first layer to all\nsubsequent layers. Based on this method, one variant is the Transformer with\nsingle layer value (SVFormer), where all layers share the same value embedding\nfrom first layer. Comprehensive empirical evidence demonstrates ResFormer\nachieves equivalent validation loss with 10.4% fewer model parameters and 13.6%\nless training data compared to Transformer, while maintaining similar memory\nusage and computational cost. Besides, SVFormer reduces KV cache size by nearly\nhalf with only a small performance penalty and can be integrated with other\nKV-efficient methods, yielding further reductions in KV cache, with performance\ninfluenced by sequence length and cumulative learning rate. Further\nvisualization results suggest that Resformer and SVFormer alleviate attention\nconcentration in deeper layers through avoiding value-state drains and enhance\nrepresentation across most layers."
                },
                "authors": [
                    {
                        "name": "Zhanchao Zhou"
                    },
                    {
                        "name": "Tianyi Wu"
                    },
                    {
                        "name": "Zhiyun Jiang"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenzhong Lan"
                },
                "author": "Zhenzhong Lan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17897v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17897v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02252v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02252v1",
                "updated": "2024-12-03T08:29:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    8,
                    29,
                    27,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T08:29:27Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    8,
                    29,
                    27,
                    1,
                    338,
                    0
                ],
                "title": "Compressing KV Cache for Long-Context LLM Inference with Inter-Layer\n  Attention Similarity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compressing KV Cache for Long-Context LLM Inference with Inter-Layer\n  Attention Similarity"
                },
                "summary": "The increasing context window size in Large Language Models (LLMs), such as\nthe GPT and LLaMA series, has improved their ability to tackle complex,\nlong-text tasks, but at the cost of inference efficiency, particularly\nregarding memory and computational complexity. Existing methods, including\nselective token retention and window-based attention, improve efficiency but\nrisk discarding important tokens needed for future text generation. In this\npaper, we propose an approach that enhances LLM efficiency without token loss\nby reducing the memory and computational load of less important tokens, rather\nthan discarding them.We address two challenges: 1) investigating the\ndistribution of important tokens in the context, discovering recent tokens are\nmore important than distant tokens in context, and 2) optimizing resources for\ndistant tokens by sharing attention scores across layers. The experiments show\nthat our method saves $35\\%$ KV cache without compromising the performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing context window size in Large Language Models (LLMs), such as\nthe GPT and LLaMA series, has improved their ability to tackle complex,\nlong-text tasks, but at the cost of inference efficiency, particularly\nregarding memory and computational complexity. Existing methods, including\nselective token retention and window-based attention, improve efficiency but\nrisk discarding important tokens needed for future text generation. In this\npaper, we propose an approach that enhances LLM efficiency without token loss\nby reducing the memory and computational load of less important tokens, rather\nthan discarding them.We address two challenges: 1) investigating the\ndistribution of important tokens in the context, discovering recent tokens are\nmore important than distant tokens in context, and 2) optimizing resources for\ndistant tokens by sharing attention scores across layers. The experiments show\nthat our method saves $35\\%$ KV cache without compromising the performance."
                },
                "authors": [
                    {
                        "name": "Da Ma"
                    },
                    {
                        "name": "Lu Chen"
                    },
                    {
                        "name": "Situo Zhang"
                    },
                    {
                        "name": "Yuxun Miao"
                    },
                    {
                        "name": "Su Zhu"
                    },
                    {
                        "name": "Zhi Chen"
                    },
                    {
                        "name": "Hongshen Xu"
                    },
                    {
                        "name": "Hanqi Li"
                    },
                    {
                        "name": "Shuai Fan"
                    },
                    {
                        "name": "Lei Pan"
                    },
                    {
                        "name": "Kai Yu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Yu"
                },
                "author": "Kai Yu",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02252v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02252v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01253v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01253v2",
                "updated": "2024-12-03T04:51:10Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    4,
                    51,
                    10,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-02T08:22:56Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    8,
                    22,
                    56,
                    0,
                    337,
                    0
                ],
                "title": "Yi-Lightning Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yi-Lightning Technical Report"
                },
                "summary": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com."
                },
                "authors": [
                    {
                        "name": "01. AI"
                    },
                    {
                        "name": ":"
                    },
                    {
                        "name": "Alan Wake"
                    },
                    {
                        "name": "Albert Wang"
                    },
                    {
                        "name": "Bei Chen"
                    },
                    {
                        "name": "C. X. Lv"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Chengen Huang"
                    },
                    {
                        "name": "Chenglin Cai"
                    },
                    {
                        "name": "Chujie Zheng"
                    },
                    {
                        "name": "Daniel Cooper"
                    },
                    {
                        "name": "Ethan Dai"
                    },
                    {
                        "name": "Fan Zhou"
                    },
                    {
                        "name": "Feng Hu"
                    },
                    {
                        "name": "Heng Ji"
                    },
                    {
                        "name": "Howard Qiu"
                    },
                    {
                        "name": "Jiangcheng Zhu"
                    },
                    {
                        "name": "Jun Tian"
                    },
                    {
                        "name": "Katherine Su"
                    },
                    {
                        "name": "Lihuan Zhang"
                    },
                    {
                        "name": "Liying Li"
                    },
                    {
                        "name": "Ming Song"
                    },
                    {
                        "name": "Mou Li"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Qichen Hu"
                    },
                    {
                        "name": "Shawn Wang"
                    },
                    {
                        "name": "Shijun Zhou"
                    },
                    {
                        "name": "Shiyong Li"
                    },
                    {
                        "name": "Tianhang Zhu"
                    },
                    {
                        "name": "Wen Xie"
                    },
                    {
                        "name": "Xiang He"
                    },
                    {
                        "name": "Xiaobo Chen"
                    },
                    {
                        "name": "Xiaohui Hu"
                    },
                    {
                        "name": "Xiaoyi Ren"
                    },
                    {
                        "name": "Xinyao Niu"
                    },
                    {
                        "name": "Yanpeng Li"
                    },
                    {
                        "name": "Yongke Zhao"
                    },
                    {
                        "name": "Yongzhen Luo"
                    },
                    {
                        "name": "Yuchi Xu"
                    },
                    {
                        "name": "Yuxuan Sha"
                    },
                    {
                        "name": "Zhaodong Yan"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Zirui Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zirui Zhang"
                },
                "author": "Zirui Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01253v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01253v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02122v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02122v1",
                "updated": "2024-12-03T03:20:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    3,
                    20,
                    40,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T03:20:40Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    3,
                    20,
                    40,
                    1,
                    338,
                    0
                ],
                "title": "Improving Sequential Recommender Systems with Online and In-store User\n  Behavior",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Sequential Recommender Systems with Online and In-store User\n  Behavior"
                },
                "summary": "Online e-commerce platforms have been extending in-store shopping, which\nallows users to keep the canonical online browsing and checkout experience\nwhile exploring in-store shopping. However, the growing transition between\nonline and in-store becomes a challenge to sequential recommender systems for\nfuture online interaction prediction due to the lack of holistic modeling of\nhybrid user behaviors (online and in-store). The challenges are twofold. First,\ncombining online and in-store user behavior data into a single data schema and\nsupporting multiple stages in the model life cycle (pre-training, training,\ninference, etc.) organically needs a new data pipeline design. Second, online\nrecommender systems, which solely rely on online user behavior sequences, must\nbe redesigned to support online and in-store user data as input under the\nsequential modeling setting. To overcome the first challenge, we propose a\nhybrid, omnichannel data pipeline to compile online and in-store user behavior\ndata by caching information from diverse data sources. Later, we introduce a\nmodel-agnostic encoder module to the sequential recommender system to interpret\nthe user in-store transaction and augment the modeling capacity for better\nonline interaction prediction given the hybrid user behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online e-commerce platforms have been extending in-store shopping, which\nallows users to keep the canonical online browsing and checkout experience\nwhile exploring in-store shopping. However, the growing transition between\nonline and in-store becomes a challenge to sequential recommender systems for\nfuture online interaction prediction due to the lack of holistic modeling of\nhybrid user behaviors (online and in-store). The challenges are twofold. First,\ncombining online and in-store user behavior data into a single data schema and\nsupporting multiple stages in the model life cycle (pre-training, training,\ninference, etc.) organically needs a new data pipeline design. Second, online\nrecommender systems, which solely rely on online user behavior sequences, must\nbe redesigned to support online and in-store user data as input under the\nsequential modeling setting. To overcome the first challenge, we propose a\nhybrid, omnichannel data pipeline to compile online and in-store user behavior\ndata by caching information from diverse data sources. Later, we introduce a\nmodel-agnostic encoder module to the sequential recommender system to interpret\nthe user in-store transaction and augment the modeling capacity for better\nonline interaction prediction given the hybrid user behavior."
                },
                "authors": [
                    {
                        "name": "Luyi Ma"
                    },
                    {
                        "name": "Aashika Padmanabhan"
                    },
                    {
                        "name": "Anjana Ganesh"
                    },
                    {
                        "name": "Shengwei Tang"
                    },
                    {
                        "name": "Jiao Chen"
                    },
                    {
                        "name": "Xiaohan Li"
                    },
                    {
                        "name": "Lalitesh Morishetti"
                    },
                    {
                        "name": "Kaushiki Nag"
                    },
                    {
                        "name": "Malay Patel"
                    },
                    {
                        "name": "Jason Cho"
                    },
                    {
                        "name": "Sushant Kumar"
                    },
                    {
                        "name": "Kannan Achan"
                    }
                ],
                "author_detail": {
                    "name": "Kannan Achan"
                },
                "author": "Kannan Achan",
                "arxiv_comment": "6 pages, IEEE BigData 2024 Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02122v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02122v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01959v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01959v1",
                "updated": "2024-12-02T20:39:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    20,
                    39,
                    56,
                    0,
                    337,
                    0
                ],
                "published": "2024-12-02T20:39:56Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    20,
                    39,
                    56,
                    0,
                    337,
                    0
                ],
                "title": "Development and Application of a Decentralized Domain Name Service",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Development and Application of a Decentralized Domain Name Service"
                },
                "summary": "The current Domain Name System (DNS), as a core infrastructure of the\ninternet, exhibits several shortcomings: its centralized architecture leads to\ncensorship risks and single points of failure, making domain name resolution\nvulnerable to attacks. The lack of encryption in the resolution process exposes\nit to DNS hijacking and cache poisoning attacks. Additionally, the high\noperational costs limit participation and innovation among small to\nmedium-sized users. To address these issues, this paper proposes a\nDecentralized Domain Name Service (DDNS) based on blockchain (Phicoin) and\ndistributed storage (IPFS). By leveraging the immutability of blockchain and\nthe content verification of IPFS, the system achieves decentralized storage and\ndistribution of domain name records, eliminating the centralized dependencies\nof traditional DNS. With a block time of 15 seconds, the system supports rapid\nbroadcasting of domain name updates, significantly improving resolution\nefficiency. The DDNS aims to serve as a complement or backup to the existing\nDNS system, providing a pollution-resistant, censorship-resistant,\nhigh-performance, and low-cost domain name resolution solution, offering a new\ntechnical path for the security and stability of the internet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current Domain Name System (DNS), as a core infrastructure of the\ninternet, exhibits several shortcomings: its centralized architecture leads to\ncensorship risks and single points of failure, making domain name resolution\nvulnerable to attacks. The lack of encryption in the resolution process exposes\nit to DNS hijacking and cache poisoning attacks. Additionally, the high\noperational costs limit participation and innovation among small to\nmedium-sized users. To address these issues, this paper proposes a\nDecentralized Domain Name Service (DDNS) based on blockchain (Phicoin) and\ndistributed storage (IPFS). By leveraging the immutability of blockchain and\nthe content verification of IPFS, the system achieves decentralized storage and\ndistribution of domain name records, eliminating the centralized dependencies\nof traditional DNS. With a block time of 15 seconds, the system supports rapid\nbroadcasting of domain name updates, significantly improving resolution\nefficiency. The DDNS aims to serve as a complement or backup to the existing\nDNS system, providing a pollution-resistant, censorship-resistant,\nhigh-performance, and low-cost domain name resolution solution, offering a new\ntechnical path for the security and stability of the internet."
                },
                "authors": [
                    {
                        "name": "Guang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Guang Yang"
                },
                "author": "Guang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01959v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01959v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01827v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01827v1",
                "updated": "2024-12-02T18:59:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    18,
                    59,
                    53,
                    0,
                    337,
                    0
                ],
                "published": "2024-12-02T18:59:53Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    18,
                    59,
                    53,
                    0,
                    337,
                    0
                ],
                "title": "RandAR: Decoder-only Autoregressive Visual Generation in Random Orders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RandAR: Decoder-only Autoregressive Visual Generation in Random Orders"
                },
                "summary": "We introduce RandAR, a decoder-only visual autoregressive (AR) model capable\nof generating images in arbitrary token orders. Unlike previous decoder-only AR\nmodels that rely on a predefined generation order, RandAR removes this\ninductive bias, unlocking new capabilities in decoder-only generation. Our\nessential design enables random order by inserting a \"position instruction\ntoken\" before each image token to be predicted, representing the spatial\nlocation of the next image token. Trained on randomly permuted token sequences\n-- a more challenging task than fixed-order generation, RandAR achieves\ncomparable performance to its conventional raster-order counterpart. More\nimportantly, decoder-only transformers trained from random orders acquire new\ncapabilities. For the efficiency bottleneck of AR models, RandAR adopts\nparallel decoding with KV-Cache at inference time, enjoying 2.5x acceleration\nwithout sacrificing generation quality. Additionally, RandAR supports\ninpainting, outpainting and resolution extrapolation in a zero-shot manner. We\nhope RandAR inspires new directions for decoder-only visual generation models\nand broadens their applications across diverse scenarios. Our project page is\nat https://rand-ar.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce RandAR, a decoder-only visual autoregressive (AR) model capable\nof generating images in arbitrary token orders. Unlike previous decoder-only AR\nmodels that rely on a predefined generation order, RandAR removes this\ninductive bias, unlocking new capabilities in decoder-only generation. Our\nessential design enables random order by inserting a \"position instruction\ntoken\" before each image token to be predicted, representing the spatial\nlocation of the next image token. Trained on randomly permuted token sequences\n-- a more challenging task than fixed-order generation, RandAR achieves\ncomparable performance to its conventional raster-order counterpart. More\nimportantly, decoder-only transformers trained from random orders acquire new\ncapabilities. For the efficiency bottleneck of AR models, RandAR adopts\nparallel decoding with KV-Cache at inference time, enjoying 2.5x acceleration\nwithout sacrificing generation quality. Additionally, RandAR supports\ninpainting, outpainting and resolution extrapolation in a zero-shot manner. We\nhope RandAR inspires new directions for decoder-only visual generation models\nand broadens their applications across diverse scenarios. Our project page is\nat https://rand-ar.github.io/."
                },
                "authors": [
                    {
                        "name": "Ziqi Pang"
                    },
                    {
                        "name": "Tianyuan Zhang"
                    },
                    {
                        "name": "Fujun Luan"
                    },
                    {
                        "name": "Yunze Man"
                    },
                    {
                        "name": "Hao Tan"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "William T. Freeman"
                    },
                    {
                        "name": "Yu-Xiong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Xiong Wang"
                },
                "author": "Yu-Xiong Wang",
                "arxiv_comment": "Project page: https://rand-ar.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01827v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01827v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01659v1",
                "updated": "2024-12-02T16:10:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    16,
                    10,
                    26,
                    0,
                    337,
                    0
                ],
                "published": "2024-12-02T16:10:26Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    16,
                    10,
                    26,
                    0,
                    337,
                    0
                ],
                "title": "Local and Regional Contributions to Tropospheric Ozone Concentrations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Local and Regional Contributions to Tropospheric Ozone Concentrations"
                },
                "summary": "The Wasatch Front in Utah, USA, is currently a non-attainment area for ozone\naccording to the Environmental Protection Agency's (EPA) National Ambient Air\nQuality Standards (NAAQS). Nitrogen oxides ($\\mathrm{NO_x = NO_2 + NO}$) and\nvolatile organic compounds (VOCs), in the presence of sunlight, lead to ozone\nformation in the troposphere. When the rate of oxidant production, defined as\nthe sum of $\\mathrm{O_3}$ and $\\mathrm{NO_2}$, is faster than the rate of\n$\\mathrm{NO_x}$ production, a region is said to be $\\mathrm{NO_x}$limited, and\nozone formation will be limited by the concentration of $\\mathrm{NO_x}$ species\nin the region. The inverse of this situation makes the region VOC-limited.\nKnowing whether a region is $\\mathrm{NO_x}$-limited or VOC-limited can aid in\ngenerating effective mitigation strategies. Understanding the background or\nregional contributions to ozone in a region, whether from the transport of\nprecursors or of ozone, provides information about the lower limit for ozone\nconcentrations that a region can achieve through regulation of local\nprecursors. In this paper, measured oxidant and $\\mathrm{NO_x}$ concentrations\nare analyzed from 14 counties in the state of Utah to calculate the regional\nand local contributions to ozone for each region. This analysis is used to\ndetermine the nature of the atmosphere in each county by identifying whether\nthe region is VOC or $\\mathrm{NO_x}$-limited. Furthermore, this analysis is\nperformed for each county for the years 2012 and 2022 to assess changes in the\noxidative nature and quantify regional and local contributions to ozone over a\n10-year period. All studied counties--except for Washington County--in Utah\nwere found to be VOC-limited in 2012. This shifted in 2022, with most counties\nbeing either in a transitional state or $\\mathrm{NO_x}$-limited. Local\ncontributions to ozone increased in two major counties, Cache and Salt Lake\nCounties, but decreased in Carbon, Davis, Duchesne, Uinta, Utah, Washington,\nand Weber Counties. Generally, the regional contributions to oxidant\nconcentrations decreased across the state. A summertime spike in both regional\nand local contributions to oxidants was observed. Smoke from wildfires was\nfound to increase regional contributions to oxidants and shift the local regime\nto be more $\\mathrm{NO_x}$-limited.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Wasatch Front in Utah, USA, is currently a non-attainment area for ozone\naccording to the Environmental Protection Agency's (EPA) National Ambient Air\nQuality Standards (NAAQS). Nitrogen oxides ($\\mathrm{NO_x = NO_2 + NO}$) and\nvolatile organic compounds (VOCs), in the presence of sunlight, lead to ozone\nformation in the troposphere. When the rate of oxidant production, defined as\nthe sum of $\\mathrm{O_3}$ and $\\mathrm{NO_2}$, is faster than the rate of\n$\\mathrm{NO_x}$ production, a region is said to be $\\mathrm{NO_x}$limited, and\nozone formation will be limited by the concentration of $\\mathrm{NO_x}$ species\nin the region. The inverse of this situation makes the region VOC-limited.\nKnowing whether a region is $\\mathrm{NO_x}$-limited or VOC-limited can aid in\ngenerating effective mitigation strategies. Understanding the background or\nregional contributions to ozone in a region, whether from the transport of\nprecursors or of ozone, provides information about the lower limit for ozone\nconcentrations that a region can achieve through regulation of local\nprecursors. In this paper, measured oxidant and $\\mathrm{NO_x}$ concentrations\nare analyzed from 14 counties in the state of Utah to calculate the regional\nand local contributions to ozone for each region. This analysis is used to\ndetermine the nature of the atmosphere in each county by identifying whether\nthe region is VOC or $\\mathrm{NO_x}$-limited. Furthermore, this analysis is\nperformed for each county for the years 2012 and 2022 to assess changes in the\noxidative nature and quantify regional and local contributions to ozone over a\n10-year period. All studied counties--except for Washington County--in Utah\nwere found to be VOC-limited in 2012. This shifted in 2022, with most counties\nbeing either in a transitional state or $\\mathrm{NO_x}$-limited. Local\ncontributions to ozone increased in two major counties, Cache and Salt Lake\nCounties, but decreased in Carbon, Davis, Duchesne, Uinta, Utah, Washington,\nand Weber Counties. Generally, the regional contributions to oxidant\nconcentrations decreased across the state. A summertime spike in both regional\nand local contributions to oxidants was observed. Smoke from wildfires was\nfound to increase regional contributions to oxidants and shift the local regime\nto be more $\\mathrm{NO_x}$-limited."
                },
                "authors": [
                    {
                        "name": "Callum E. Flowerday"
                    },
                    {
                        "name": "Ryan Thalman"
                    },
                    {
                        "name": "Jaron C. Hansen"
                    }
                ],
                "author_detail": {
                    "name": "Jaron C. Hansen"
                },
                "author": "Jaron C. Hansen",
                "arxiv_doi": "10.3390/atmos14081262",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3390/atmos14081262",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.01659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Atmosphere 2023, 14, 1262",
                "arxiv_primary_category": {
                    "term": "physics.ao-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01415v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01415v1",
                "updated": "2024-12-02T11:57:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    57,
                    3,
                    0,
                    337,
                    0
                ],
                "published": "2024-12-02T11:57:03Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    57,
                    3,
                    0,
                    337,
                    0
                ],
                "title": "Excitation of quasi-monochromotic waves by a high-voltage pulse in a\n  ferrite coaxial line with the periodic structure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Excitation of quasi-monochromotic waves by a high-voltage pulse in a\n  ferrite coaxial line with the periodic structure"
                },
                "summary": "Experimental data and results of numerical simulations are presented,\nconcerning excitation of narrowband gigahertz-range wave trains in coaxial\nguiding structures that are partially filled with ferromagnetic material and\nmay involve periodically arranged metal inserts. The experiments performed\nconfirm the possibility of exciting weakly damped electromagnetic waves by\nfeeding high voltage, unilateral electromagnetic pulses of short duration into\nthe line. The coax line was of outer diameter 50.5 mm, filled with an isotropic\ndielectric (relative dielectric constant {\\epsilon} = 2.25) and a set of\nferrite rings with {\\epsilon}=16 and saturated-state {\\mu} about 4 to 5. With a\npeak voltage of the primary pulse close to 160 kV and a magnetizing field of\n17.5 kA/m, the parameters of the waves excited reached magnitudes as: frequency\n1.89 GHz to 2.1 GHz; bandwidth 16%; VHF power at the output about 20 MW.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Experimental data and results of numerical simulations are presented,\nconcerning excitation of narrowband gigahertz-range wave trains in coaxial\nguiding structures that are partially filled with ferromagnetic material and\nmay involve periodically arranged metal inserts. The experiments performed\nconfirm the possibility of exciting weakly damped electromagnetic waves by\nfeeding high voltage, unilateral electromagnetic pulses of short duration into\nthe line. The coax line was of outer diameter 50.5 mm, filled with an isotropic\ndielectric (relative dielectric constant {\\epsilon} = 2.25) and a set of\nferrite rings with {\\epsilon}=16 and saturated-state {\\mu} about 4 to 5. With a\npeak voltage of the primary pulse close to 160 kV and a magnetizing field of\n17.5 kA/m, the parameters of the waves excited reached magnitudes as: frequency\n1.89 GHz to 2.1 GHz; bandwidth 16%; VHF power at the output about 20 MW."
                },
                "authors": [
                    {
                        "name": "A. B. Batrakov"
                    },
                    {
                        "name": "S. Yu. Karelin"
                    },
                    {
                        "name": "O. M. Lebedenko"
                    },
                    {
                        "name": "V. S. Mukhin"
                    },
                    {
                        "name": "I. N. Onishchenko"
                    },
                    {
                        "name": "O. L. Rak"
                    },
                    {
                        "name": "V. G. Sinitsin"
                    },
                    {
                        "name": "M. V. Volovenko"
                    }
                ],
                "author_detail": {
                    "name": "M. V. Volovenko"
                },
                "author": "M. V. Volovenko",
                "arxiv_comment": "4 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01415v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01415v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.acc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.acc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.06892v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.06892v2",
                "updated": "2024-12-02T11:24:20Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    24,
                    20,
                    0,
                    337,
                    0
                ],
                "published": "2024-03-11T16:48:25Z",
                "published_parsed": [
                    2024,
                    3,
                    11,
                    16,
                    48,
                    25,
                    0,
                    71,
                    0
                ],
                "title": "Real-time Transformer-based Open-Vocabulary Detection with Efficient\n  Fusion Head",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time Transformer-based Open-Vocabulary Detection with Efficient\n  Fusion Head"
                },
                "summary": "End-to-end transformer-based detectors (DETRs) have shown exceptional\nperformance in both closed-set and open-vocabulary object detection (OVD) tasks\nthrough the integration of language modalities. However, their demanding\ncomputational requirements have hindered their practical application in\nreal-time object detection (OD) scenarios. In this paper, we scrutinize the\nlimitations of two leading models in the OVDEval benchmark, OmDet and\nGrounding-DINO, and introduce OmDet-Turbo. This novel transformer-based\nreal-time OVD model features an innovative Efficient Fusion Head (EFH) module\ndesigned to alleviate the bottlenecks observed in OmDet and Grounding-DINO.\nNotably, OmDet-Turbo-Base achieves a 100.2 frames per second (FPS) with\nTensorRT and language cache techniques applied. Notably, in zero-shot scenarios\non COCO and LVIS datasets, OmDet-Turbo achieves performance levels nearly on\npar with current state-of-the-art supervised models. Furthermore, it\nestablishes new state-of-the-art benchmarks on ODinW and OVDEval, boasting an\nAP of 30.1 and an NMS-AP of 26.86, respectively. The practicality of\nOmDet-Turbo in industrial applications is underscored by its exceptional\nperformance on benchmark datasets and superior inference speed, positioning it\nas a compelling choice for real-time object detection tasks. Code:\n\\url{https://github.com/om-ai-lab/OmDet}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-end transformer-based detectors (DETRs) have shown exceptional\nperformance in both closed-set and open-vocabulary object detection (OVD) tasks\nthrough the integration of language modalities. However, their demanding\ncomputational requirements have hindered their practical application in\nreal-time object detection (OD) scenarios. In this paper, we scrutinize the\nlimitations of two leading models in the OVDEval benchmark, OmDet and\nGrounding-DINO, and introduce OmDet-Turbo. This novel transformer-based\nreal-time OVD model features an innovative Efficient Fusion Head (EFH) module\ndesigned to alleviate the bottlenecks observed in OmDet and Grounding-DINO.\nNotably, OmDet-Turbo-Base achieves a 100.2 frames per second (FPS) with\nTensorRT and language cache techniques applied. Notably, in zero-shot scenarios\non COCO and LVIS datasets, OmDet-Turbo achieves performance levels nearly on\npar with current state-of-the-art supervised models. Furthermore, it\nestablishes new state-of-the-art benchmarks on ODinW and OVDEval, boasting an\nAP of 30.1 and an NMS-AP of 26.86, respectively. The practicality of\nOmDet-Turbo in industrial applications is underscored by its exceptional\nperformance on benchmark datasets and superior inference speed, positioning it\nas a compelling choice for real-time object detection tasks. Code:\n\\url{https://github.com/om-ai-lab/OmDet}"
                },
                "authors": [
                    {
                        "name": "Tiancheng Zhao"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Xuan He"
                    },
                    {
                        "name": "Lu Zhang"
                    },
                    {
                        "name": "Kyusong Lee"
                    }
                ],
                "author_detail": {
                    "name": "Kyusong Lee"
                },
                "author": "Kyusong Lee",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.06892v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.06892v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01380v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01380v1",
                "updated": "2024-12-02T11:07:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    7,
                    51,
                    0,
                    337,
                    0
                ],
                "published": "2024-12-02T11:07:51Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    7,
                    51,
                    0,
                    337,
                    0
                ],
                "title": "Efficient LLM Inference using Dynamic Input Pruning and Cache-Aware\n  Masking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Inference using Dynamic Input Pruning and Cache-Aware\n  Masking"
                },
                "summary": "While mobile devices provide ever more compute power, improvements in DRAM\nbandwidth are much slower. This is unfortunate for large language model (LLM)\ntoken generation, which is heavily memory-bound. Previous work has proposed to\nleverage natural dynamic activation sparsity in ReLU-activated LLMs to reduce\neffective DRAM bandwidth per token. However, more recent LLMs use SwiGLU\ninstead of ReLU, which result in little inherent sparsity. While SwiGLU\nactivations can be pruned based on magnitude, the resulting sparsity patterns\nare difficult to predict, rendering previous approaches ineffective. To\ncircumvent this issue, our work introduces Dynamic Input Pruning (DIP): a\npredictor-free dynamic sparsification approach, which preserves accuracy with\nminimal fine-tuning. DIP can further use lightweight LoRA adapters to regain\nsome performance lost during sparsification. Lastly, we describe a novel\ncache-aware masking strategy, which considers the cache state and activation\nmagnitude to further increase cache hit rate, improving LLM token rate on\nmobile devices. DIP outperforms other methods in terms of accuracy, memory and\nthroughput trade-offs across simulated hardware settings. On Phi-3-Medium, DIP\nachieves a 46% reduction in memory and 40% increase in throughput with $<$ 0.1\nloss in perplexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While mobile devices provide ever more compute power, improvements in DRAM\nbandwidth are much slower. This is unfortunate for large language model (LLM)\ntoken generation, which is heavily memory-bound. Previous work has proposed to\nleverage natural dynamic activation sparsity in ReLU-activated LLMs to reduce\neffective DRAM bandwidth per token. However, more recent LLMs use SwiGLU\ninstead of ReLU, which result in little inherent sparsity. While SwiGLU\nactivations can be pruned based on magnitude, the resulting sparsity patterns\nare difficult to predict, rendering previous approaches ineffective. To\ncircumvent this issue, our work introduces Dynamic Input Pruning (DIP): a\npredictor-free dynamic sparsification approach, which preserves accuracy with\nminimal fine-tuning. DIP can further use lightweight LoRA adapters to regain\nsome performance lost during sparsification. Lastly, we describe a novel\ncache-aware masking strategy, which considers the cache state and activation\nmagnitude to further increase cache hit rate, improving LLM token rate on\nmobile devices. DIP outperforms other methods in terms of accuracy, memory and\nthroughput trade-offs across simulated hardware settings. On Phi-3-Medium, DIP\nachieves a 46% reduction in memory and 40% increase in throughput with $<$ 0.1\nloss in perplexity."
                },
                "authors": [
                    {
                        "name": "Marco Federici"
                    },
                    {
                        "name": "Davide Belli"
                    },
                    {
                        "name": "Mart van Baalen"
                    },
                    {
                        "name": "Amir Jalalirad"
                    },
                    {
                        "name": "Andrii Skliar"
                    },
                    {
                        "name": "Bence Major"
                    },
                    {
                        "name": "Markus Nagel"
                    },
                    {
                        "name": "Paul Whatmough"
                    }
                ],
                "author_detail": {
                    "name": "Paul Whatmough"
                },
                "author": "Paul Whatmough",
                "arxiv_comment": "Main Text: 10 pages, 11 figures. Appendix: 3 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01380v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01380v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01195v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01195v1",
                "updated": "2024-12-02T06:57:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    6,
                    57,
                    46,
                    0,
                    337,
                    0
                ],
                "published": "2024-12-02T06:57:46Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    6,
                    57,
                    46,
                    0,
                    337,
                    0
                ],
                "title": "Memory-Efficient Training for Deep Speaker Embedding Learning in Speaker\n  Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-Efficient Training for Deep Speaker Embedding Learning in Speaker\n  Verification"
                },
                "summary": "Recent speaker verification (SV) systems have shown a trend toward adopting\ndeeper speaker embedding extractors. Although deeper and larger neural networks\ncan significantly improve performance, their substantial memory requirements\nhinder training on consumer GPUs. In this paper, we explore a memory-efficient\ntraining strategy for deep speaker embedding learning in resource-constrained\nscenarios. Firstly, we conduct a systematic analysis of GPU memory allocation\nduring SV system training. Empirical observations show that activations and\noptimizer states are the main sources of memory consumption. For activations,\nwe design two types of reversible neural networks which eliminate the need to\nstore intermediate activations during back-propagation, thereby significantly\nreducing memory usage without performance loss. For optimizer states, we\nintroduce a dynamic quantization approach that replaces the original 32-bit\nfloating-point values with a dynamic tree-based 8-bit data type. Experimental\nresults on VoxCeleb demonstrate that the reversible variants of ResNets and\nDF-ResNets can perform training without the need to cache activations in GPU\nmemory. In addition, the 8-bit versions of SGD and Adam save 75% of memory\ncosts while maintaining performance compared to their 32-bit counterparts.\nFinally, a detailed comparison of memory usage and performance indicates that\nour proposed models achieve up to 16.2x memory savings, with nearly identical\nparameters and performance compared to the vanilla systems. In contrast to the\nprevious need for multiple high-end GPUs such as the A100, we can effectively\ntrain deep speaker embedding extractors with just one or two consumer-level\n2080Ti GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent speaker verification (SV) systems have shown a trend toward adopting\ndeeper speaker embedding extractors. Although deeper and larger neural networks\ncan significantly improve performance, their substantial memory requirements\nhinder training on consumer GPUs. In this paper, we explore a memory-efficient\ntraining strategy for deep speaker embedding learning in resource-constrained\nscenarios. Firstly, we conduct a systematic analysis of GPU memory allocation\nduring SV system training. Empirical observations show that activations and\noptimizer states are the main sources of memory consumption. For activations,\nwe design two types of reversible neural networks which eliminate the need to\nstore intermediate activations during back-propagation, thereby significantly\nreducing memory usage without performance loss. For optimizer states, we\nintroduce a dynamic quantization approach that replaces the original 32-bit\nfloating-point values with a dynamic tree-based 8-bit data type. Experimental\nresults on VoxCeleb demonstrate that the reversible variants of ResNets and\nDF-ResNets can perform training without the need to cache activations in GPU\nmemory. In addition, the 8-bit versions of SGD and Adam save 75% of memory\ncosts while maintaining performance compared to their 32-bit counterparts.\nFinally, a detailed comparison of memory usage and performance indicates that\nour proposed models achieve up to 16.2x memory savings, with nearly identical\nparameters and performance compared to the vanilla systems. In contrast to the\nprevious need for multiple high-end GPUs such as the A100, we can effectively\ntrain deep speaker embedding extractors with just one or two consumer-level\n2080Ti GPUs."
                },
                "authors": [
                    {
                        "name": "Bei Liu"
                    },
                    {
                        "name": "Yanmin Qian"
                    }
                ],
                "author_detail": {
                    "name": "Yanmin Qian"
                },
                "author": "Yanmin Qian",
                "arxiv_comment": "Submitted to IEEE/ACM Transactions on Audio, Speech, and Language\n  Processing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01195v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01195v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04762v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04762v2",
                "updated": "2024-12-02T06:30:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    6,
                    30,
                    4,
                    0,
                    337,
                    0
                ],
                "published": "2024-11-07T14:59:44Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    14,
                    59,
                    44,
                    3,
                    312,
                    0
                ],
                "title": "JC5A: Service Delay Minimization for Aerial MEC-assisted Industrial\n  Cyber-Physical Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JC5A: Service Delay Minimization for Aerial MEC-assisted Industrial\n  Cyber-Physical Systems"
                },
                "summary": "In the era of the sixth generation (6G) and industrial Internet of Things\n(IIoT), an industrial cyber-physical system (ICPS) drives the proliferation of\nsensor devices and computing-intensive tasks. To address the limited resources\nof IIoT sensor devices, unmanned aerial vehicle (UAV)-assisted mobile edge\ncomputing (MEC) has emerged as a promising solution, providing flexible and\ncost-effective services in close proximity of IIoT sensor devices (ISDs).\nHowever, leveraging aerial MEC to meet the delay-sensitive and\ncomputation-intensive requirements of the ISDs could face several challenges,\nincluding the limited communication, computation and caching (3C) resources,\nstringent offloading requirements for 3C services, and constrained on-board\nenergy of UAVs. To address these issues, we first present a collaborative\naerial MEC-assisted ICPS architecture by incorporating the computing\ncapabilities of the macro base station (MBS) and UAVs. We then formulate a\nservice delay minimization optimization problem (SDMOP). Since the SDMOP is\nproved to be an NP-hard problem, we propose a joint computation offloading,\ncaching, communication resource allocation, computation resource allocation,\nand UAV trajectory control approach (JC5A). Specifically, JC5A consists of a\nblock successive upper bound minimization method of multipliers (BSUMM) for\ncomputation offloading and service caching, a convex optimization-based method\nfor communication and computation resource allocation, and a successive convex\napproximation (SCA)-based method for UAV trajectory control. Moreover, we\ntheoretically prove the convergence and polynomial complexity of JC5A.\nSimulation results demonstrate that the proposed approach can achieve superior\nsystem performance compared to the benchmark approaches and algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the era of the sixth generation (6G) and industrial Internet of Things\n(IIoT), an industrial cyber-physical system (ICPS) drives the proliferation of\nsensor devices and computing-intensive tasks. To address the limited resources\nof IIoT sensor devices, unmanned aerial vehicle (UAV)-assisted mobile edge\ncomputing (MEC) has emerged as a promising solution, providing flexible and\ncost-effective services in close proximity of IIoT sensor devices (ISDs).\nHowever, leveraging aerial MEC to meet the delay-sensitive and\ncomputation-intensive requirements of the ISDs could face several challenges,\nincluding the limited communication, computation and caching (3C) resources,\nstringent offloading requirements for 3C services, and constrained on-board\nenergy of UAVs. To address these issues, we first present a collaborative\naerial MEC-assisted ICPS architecture by incorporating the computing\ncapabilities of the macro base station (MBS) and UAVs. We then formulate a\nservice delay minimization optimization problem (SDMOP). Since the SDMOP is\nproved to be an NP-hard problem, we propose a joint computation offloading,\ncaching, communication resource allocation, computation resource allocation,\nand UAV trajectory control approach (JC5A). Specifically, JC5A consists of a\nblock successive upper bound minimization method of multipliers (BSUMM) for\ncomputation offloading and service caching, a convex optimization-based method\nfor communication and computation resource allocation, and a successive convex\napproximation (SCA)-based method for UAV trajectory control. Moreover, we\ntheoretically prove the convergence and polynomial complexity of JC5A.\nSimulation results demonstrate that the proposed approach can achieve superior\nsystem performance compared to the benchmark approaches and algorithms."
                },
                "authors": [
                    {
                        "name": "Geng Sun"
                    },
                    {
                        "name": "Jiaxu Wu"
                    },
                    {
                        "name": "Zemin Sun"
                    },
                    {
                        "name": "Long He"
                    },
                    {
                        "name": "Jiacheng Wang"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Abbas Jamalipour"
                    },
                    {
                        "name": "Shiwen Mao"
                    }
                ],
                "author_detail": {
                    "name": "Shiwen Mao"
                },
                "author": "Shiwen Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04762v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04762v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00977v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00977v1",
                "updated": "2024-12-01T21:47:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    1,
                    21,
                    47,
                    35,
                    6,
                    336,
                    0
                ],
                "published": "2024-12-01T21:47:35Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    21,
                    47,
                    35,
                    6,
                    336,
                    0
                ],
                "title": "Optimal Power Allocation in Uplink NOMA with Simultaneous Cache-Enabled\n  D2D Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Power Allocation in Uplink NOMA with Simultaneous Cache-Enabled\n  D2D Communications"
                },
                "summary": "Non-orthogonal multiple access (NOMA) is widely viewed as a potential\ncandidate for providing enhanced multiple access in future mobile networks by\neliminating the orthogonal distribution of radio resources amongst the users.\nNevertheless, the performance of NOMA can be significantly improved by\ncombining it with other sophisticated technologies such as wireless data\ncaching and device-to-device (D2D) communications. In this letter, we propose a\nnovel cellular system model which integrates uplink NOMA with cache based\ndevice-to-device (D2D) communications. The proposed system would enable a\ncellular user to upload data file to base station while simultaneously\nexchanging useful cache content with another nearby user. We maximize the\nsystem sum rate by deriving closed form solutions for optimal power allocation.\nSimulation results demonstrate the superior performance of our proposed model\nover other potential combinations of uplink NOMA and D2D communications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-orthogonal multiple access (NOMA) is widely viewed as a potential\ncandidate for providing enhanced multiple access in future mobile networks by\neliminating the orthogonal distribution of radio resources amongst the users.\nNevertheless, the performance of NOMA can be significantly improved by\ncombining it with other sophisticated technologies such as wireless data\ncaching and device-to-device (D2D) communications. In this letter, we propose a\nnovel cellular system model which integrates uplink NOMA with cache based\ndevice-to-device (D2D) communications. The proposed system would enable a\ncellular user to upload data file to base station while simultaneously\nexchanging useful cache content with another nearby user. We maximize the\nsystem sum rate by deriving closed form solutions for optimal power allocation.\nSimulation results demonstrate the superior performance of our proposed model\nover other potential combinations of uplink NOMA and D2D communications."
                },
                "authors": [
                    {
                        "name": "Aditya Powari"
                    },
                    {
                        "name": "Daniel K. C. So"
                    }
                ],
                "author_detail": {
                    "name": "Daniel K. C. So"
                },
                "author": "Daniel K. C. So",
                "arxiv_comment": "5 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00977v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00977v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00857v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00857v1",
                "updated": "2024-12-01T15:45:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    1,
                    15,
                    45,
                    26,
                    6,
                    336,
                    0
                ],
                "published": "2024-12-01T15:45:26Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    15,
                    45,
                    26,
                    6,
                    336,
                    0
                ],
                "title": "Advanced Video Inpainting Using Optical Flow-Guided Efficient Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced Video Inpainting Using Optical Flow-Guided Efficient Diffusion"
                },
                "summary": "Recently, diffusion-based methods have achieved great improvements in the\nvideo inpainting task. However, these methods still face many challenges, such\nas maintaining temporal consistency and the time-consuming issue. This paper\nproposes an advanced video inpainting framework using optical Flow-guided\nEfficient Diffusion, called FloED. Specifically, FloED employs a dual-branch\narchitecture, where a flow branch first restores corrupted flow and a\nmulti-scale flow adapter provides motion guidance to the main inpainting\nbranch. Additionally, a training-free latent interpolation method is proposed\nto accelerate the multi-step denoising process using flow warping. Further\nintroducing a flow attention cache mechanism, FLoED efficiently reduces the\ncomputational cost brought by incorporating optical flow. Comprehensive\nexperiments in both background restoration and object removal tasks demonstrate\nthat FloED outperforms state-of-the-art methods from the perspective of both\nperformance and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, diffusion-based methods have achieved great improvements in the\nvideo inpainting task. However, these methods still face many challenges, such\nas maintaining temporal consistency and the time-consuming issue. This paper\nproposes an advanced video inpainting framework using optical Flow-guided\nEfficient Diffusion, called FloED. Specifically, FloED employs a dual-branch\narchitecture, where a flow branch first restores corrupted flow and a\nmulti-scale flow adapter provides motion guidance to the main inpainting\nbranch. Additionally, a training-free latent interpolation method is proposed\nto accelerate the multi-step denoising process using flow warping. Further\nintroducing a flow attention cache mechanism, FLoED efficiently reduces the\ncomputational cost brought by incorporating optical flow. Comprehensive\nexperiments in both background restoration and object removal tasks demonstrate\nthat FloED outperforms state-of-the-art methods from the perspective of both\nperformance and efficiency."
                },
                "authors": [
                    {
                        "name": "Bohai Gu"
                    },
                    {
                        "name": "Hao Luo"
                    },
                    {
                        "name": "Song Guo"
                    },
                    {
                        "name": "Peiran Dong"
                    }
                ],
                "author_detail": {
                    "name": "Peiran Dong"
                },
                "author": "Peiran Dong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00857v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00857v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02532v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02532v3",
                "updated": "2024-11-30T21:33:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    30,
                    21,
                    33,
                    59,
                    5,
                    335,
                    0
                ],
                "published": "2024-06-04T17:53:36Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    17,
                    53,
                    36,
                    1,
                    156,
                    0
                ],
                "title": "SpecExec: Massively Parallel Speculative Decoding for Interactive LLM\n  Inference on Consumer Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecExec: Massively Parallel Speculative Decoding for Interactive LLM\n  Inference on Consumer Devices"
                },
                "summary": "As large language models gain widespread adoption, running them efficiently\nbecomes crucial. Recent works on LLM inference use speculative decoding to\nachieve extreme speedups. However, most of these works implicitly design their\nalgorithms for high-end datacenter hardware. In this work, we ask the opposite\nquestion: how fast can we run LLMs on consumer machines? Consumer GPUs can no\nlonger fit the largest available models (50B+ parameters) and must offload them\nto RAM or SSD. When running with offloaded parameters, the inference engine can\nprocess batches of hundreds or thousands of tokens at the same time as just one\ntoken, making it a natural fit for speculative decoding. We propose SpecExec\n(Speculative Execution), a simple parallel decoding method that can generate up\nto 20 tokens per target model iteration for popular LLM families. It utilizes\nthe high spikiness of the token probabilities distribution in modern LLMs and a\nhigh degree of alignment between model output probabilities. SpecExec takes the\nmost probable tokens continuation from the draft model to build a \"cache\" tree\nfor the target model, which then gets validated in a single pass. Using\nSpecExec, we demonstrate inference of 50B+ parameter LLMs on consumer GPUs with\nRAM offloading at 4-6 tokens per second with 4-bit quantization or 2-3 tokens\nper second with 16-bit weights.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models gain widespread adoption, running them efficiently\nbecomes crucial. Recent works on LLM inference use speculative decoding to\nachieve extreme speedups. However, most of these works implicitly design their\nalgorithms for high-end datacenter hardware. In this work, we ask the opposite\nquestion: how fast can we run LLMs on consumer machines? Consumer GPUs can no\nlonger fit the largest available models (50B+ parameters) and must offload them\nto RAM or SSD. When running with offloaded parameters, the inference engine can\nprocess batches of hundreds or thousands of tokens at the same time as just one\ntoken, making it a natural fit for speculative decoding. We propose SpecExec\n(Speculative Execution), a simple parallel decoding method that can generate up\nto 20 tokens per target model iteration for popular LLM families. It utilizes\nthe high spikiness of the token probabilities distribution in modern LLMs and a\nhigh degree of alignment between model output probabilities. SpecExec takes the\nmost probable tokens continuation from the draft model to build a \"cache\" tree\nfor the target model, which then gets validated in a single pass. Using\nSpecExec, we demonstrate inference of 50B+ parameter LLMs on consumer GPUs with\nRAM offloading at 4-6 tokens per second with 4-bit quantization or 2-3 tokens\nper second with 16-bit weights."
                },
                "authors": [
                    {
                        "name": "Ruslan Svirschevski"
                    },
                    {
                        "name": "Avner May"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Beidi Chen"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Max Ryabinin"
                    }
                ],
                "author_detail": {
                    "name": "Max Ryabinin"
                },
                "author": "Max Ryabinin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02532v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02532v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00209v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00209v1",
                "updated": "2024-11-29T19:14:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    19,
                    14,
                    45,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T19:14:45Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    19,
                    14,
                    45,
                    4,
                    334,
                    0
                ],
                "title": "Digital Twin in Industries: A Comprehensive Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital Twin in Industries: A Comprehensive Survey"
                },
                "summary": "Industrial networks are undergoing rapid transformation driven by the\nconvergence of emerging technologies that are revolutionizing conventional\nworkflows, enhancing operational efficiency, and fundamentally redefining the\nindustrial landscape across diverse sectors. Amidst this revolution, Digital\nTwin (DT) emerges as a transformative innovation that seamlessly integrates\nreal-world systems with their virtual counterparts, bridging the physical and\ndigital realms. In this article, we present a comprehensive survey of the\nemerging DT-enabled services and applications across industries, beginning with\nan overview of DT fundamentals and its components to a discussion of key\nenabling technologies for DT. Different from literature works, we investigate\nand analyze the capabilities of DT across a wide range of industrial services,\nincluding data sharing, data offloading, integrated sensing and communication,\ncontent caching, resource allocation, wireless networking, and metaverse. In\nparticular, we present an in-depth technical discussion of the roles of DT in\nindustrial applications across various domains, including manufacturing,\nhealthcare, transportation, energy, agriculture, space, oil and gas, as well as\nrobotics. Throughout the technical analysis, we delve into real-time data\ncommunications between physical and virtual platforms to enable industrial DT\nnetworking. Subsequently, we extensively explore and analyze a wide range of\nmajor privacy and security issues in DT-based industry. Taxonomy tables and the\nkey research findings from the survey are also given, emphasizing important\ninsights into the significance of DT in industries. Finally, we point out\nfuture research directions to spur further research in this promising area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Industrial networks are undergoing rapid transformation driven by the\nconvergence of emerging technologies that are revolutionizing conventional\nworkflows, enhancing operational efficiency, and fundamentally redefining the\nindustrial landscape across diverse sectors. Amidst this revolution, Digital\nTwin (DT) emerges as a transformative innovation that seamlessly integrates\nreal-world systems with their virtual counterparts, bridging the physical and\ndigital realms. In this article, we present a comprehensive survey of the\nemerging DT-enabled services and applications across industries, beginning with\nan overview of DT fundamentals and its components to a discussion of key\nenabling technologies for DT. Different from literature works, we investigate\nand analyze the capabilities of DT across a wide range of industrial services,\nincluding data sharing, data offloading, integrated sensing and communication,\ncontent caching, resource allocation, wireless networking, and metaverse. In\nparticular, we present an in-depth technical discussion of the roles of DT in\nindustrial applications across various domains, including manufacturing,\nhealthcare, transportation, energy, agriculture, space, oil and gas, as well as\nrobotics. Throughout the technical analysis, we delve into real-time data\ncommunications between physical and virtual platforms to enable industrial DT\nnetworking. Subsequently, we extensively explore and analyze a wide range of\nmajor privacy and security issues in DT-based industry. Taxonomy tables and the\nkey research findings from the survey are also given, emphasizing important\ninsights into the significance of DT in industries. Finally, we point out\nfuture research directions to spur further research in this promising area."
                },
                "authors": [
                    {
                        "name": "Md Bokhtiar Al Zami"
                    },
                    {
                        "name": "Shaba Shaon"
                    },
                    {
                        "name": "Vu Khanh Quy"
                    },
                    {
                        "name": "Dinh C. Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Dinh C. Nguyen"
                },
                "author": "Dinh C. Nguyen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00209v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00209v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19730v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19730v1",
                "updated": "2024-11-29T14:23:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    14,
                    23,
                    25,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T14:23:25Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    14,
                    23,
                    25,
                    4,
                    334,
                    0
                ],
                "title": "Ten Ways in which Virtual Reality Differs from Video Streaming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ten Ways in which Virtual Reality Differs from Video Streaming"
                },
                "summary": "Virtual Reality (VR) applications have a number of unique characteristics\nthat set them apart from traditional video streaming. These characteristics\nhave major implications on the design of VR rendering, adaptation, prefetching,\ncaching, and transport mechanisms. This paper contrasts VR to video streaming,\nstored 2D video streaming in particular, and discusses how to rethink system\nand network support for VR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Virtual Reality (VR) applications have a number of unique characteristics\nthat set them apart from traditional video streaming. These characteristics\nhave major implications on the design of VR rendering, adaptation, prefetching,\ncaching, and transport mechanisms. This paper contrasts VR to video streaming,\nstored 2D video streaming in particular, and discusses how to rethink system\nand network support for VR."
                },
                "authors": [
                    {
                        "name": "Gustavo de Veciana"
                    },
                    {
                        "name": "Sonia Fahmy"
                    },
                    {
                        "name": "George Kesidis"
                    },
                    {
                        "name": "Voicu Popescu"
                    }
                ],
                "author_detail": {
                    "name": "Voicu Popescu"
                },
                "author": "Voicu Popescu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19730v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19730v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01852v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01852v1",
                "updated": "2024-11-29T10:21:12Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    10,
                    21,
                    12,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T10:21:12Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    10,
                    21,
                    12,
                    4,
                    334,
                    0
                ],
                "title": "Communication efficient application of sequences of planar rotations to\n  a matrix",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Communication efficient application of sequences of planar rotations to\n  a matrix"
                },
                "summary": "We present an efficient algorithm for the application of sequences of planar\nrotations to a matrix. Applying such sequences efficiently is important in many\nnumerical linear algebra algorithms for eigenvalues. Our algorithm is novel in\nthree main ways. First, we introduce a new kernel that is optimized for\nregister reuse in a novel way. Second, we introduce a blocking and packing\nscheme that improves the cache efficiency of the algorithm. Finally, we\nthoroughly analyze the memory operations of the algorithm which leads to\nimportant theoretical insights and makes it easier to select good parameters.\nNumerical experiments show that our algorithm outperforms the state-of-the-art\nand achieves a flop rate close to the theoretical peak on modern hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an efficient algorithm for the application of sequences of planar\nrotations to a matrix. Applying such sequences efficiently is important in many\nnumerical linear algebra algorithms for eigenvalues. Our algorithm is novel in\nthree main ways. First, we introduce a new kernel that is optimized for\nregister reuse in a novel way. Second, we introduce a blocking and packing\nscheme that improves the cache efficiency of the algorithm. Finally, we\nthoroughly analyze the memory operations of the algorithm which leads to\nimportant theoretical insights and makes it easier to select good parameters.\nNumerical experiments show that our algorithm outperforms the state-of-the-art\nand achieves a flop rate close to the theoretical peak on modern hardware."
                },
                "authors": [
                    {
                        "name": "Thijs Steel"
                    },
                    {
                        "name": "Julien Langou"
                    }
                ],
                "author_detail": {
                    "name": "Julien Langou"
                },
                "author": "Julien Langou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01852v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01852v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "65F15, 65Y05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19574v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19574v1",
                "updated": "2024-11-29T09:42:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    9,
                    42,
                    38,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T09:42:38Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    9,
                    42,
                    38,
                    4,
                    334,
                    0
                ],
                "title": "KV Shifting Attention Enhances Language Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Shifting Attention Enhances Language Modeling"
                },
                "summary": "The current large language models are mainly based on decode-only structure\ntransformers, which have great in-context learning (ICL) capabilities. It is\ngenerally believed that the important foundation of its ICL capability is the\ninduction heads mechanism, which requires at least two layers attention. In\norder to more efficiently implement the ability of the model's induction, we\nrevisit the induction heads mechanism and proposed a KV shifting attention. We\ntheoretically prove that the KV shifting attention reducing the model's\nrequirements for the depth and width of the induction heads mechanism. Our\nexperimental results demonstrate that KV shifting attention is beneficial to\nlearning induction heads and language modeling, which lead to better\nperformance or faster convergence from toy models to the pre-training models\nwith more than 10 B parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current large language models are mainly based on decode-only structure\ntransformers, which have great in-context learning (ICL) capabilities. It is\ngenerally believed that the important foundation of its ICL capability is the\ninduction heads mechanism, which requires at least two layers attention. In\norder to more efficiently implement the ability of the model's induction, we\nrevisit the induction heads mechanism and proposed a KV shifting attention. We\ntheoretically prove that the KV shifting attention reducing the model's\nrequirements for the depth and width of the induction heads mechanism. Our\nexperimental results demonstrate that KV shifting attention is beneficial to\nlearning induction heads and language modeling, which lead to better\nperformance or faster convergence from toy models to the pre-training models\nwith more than 10 B parameters."
                },
                "authors": [
                    {
                        "name": "Mingyu Xu"
                    },
                    {
                        "name": "Wei Cheng"
                    },
                    {
                        "name": "Bingning Wang"
                    },
                    {
                        "name": "Weipeng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Weipeng Chen"
                },
                "author": "Weipeng Chen",
                "arxiv_comment": "22 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19574v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19574v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.07533v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.07533v3",
                "updated": "2024-11-29T08:48:01Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    8,
                    48,
                    1,
                    4,
                    334,
                    0
                ],
                "published": "2024-05-13T08:03:32Z",
                "published_parsed": [
                    2024,
                    5,
                    13,
                    8,
                    3,
                    32,
                    0,
                    134,
                    0
                ],
                "title": "DID Link: Authentication in TLS with Decentralized Identifiers and\n  Verifiable Credentials",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DID Link: Authentication in TLS with Decentralized Identifiers and\n  Verifiable Credentials"
                },
                "summary": "Authentication in TLS is predominately carried out with X.509 digital\ncertificates issued by certificate authorities (CA). The centralized nature of\ncurrent public key infrastructures, however, comes along with severe risks,\nsuch as single points of failure and susceptibility to cyber-attacks,\npotentially undermining the security and trustworthiness of the entire system.\nWith Decentralized Identifiers (DID) alongside distributed ledger technology,\nit becomes technically feasible to prove ownership of a unique identifier\nwithout requiring an attestation of the proof's public key by a centralized and\ntherefore vulnerable CA. This article presents DID Link, a novel authentication\nscheme for TLS 1.3 that empowers entities to authenticate in a TLS-compliant\nway with self-issued X.509 certificates that are equipped with ledger-anchored\nDIDs instead of CA-issued identifiers. It facilitates the exchange of\ntamper-proof and 3rd-party attested claims in the form of DID-bound Verifiable\nCredentials after the TLS handshake to complete the authentication with a full\nidentification of the communication partner. A prototypical implementation\nshows comparable TLS handshake durations of DID Link if verification material\nis cached and reasonable prolongations if it is obtained from a ledger. The\nsignificant speed improvement of the resulting TLS channel over a widely used,\nDID-based alternative transport protocol on the application layer demonstrates\nthe potential of DID Link to become a viable solution for the establishment of\nsecure and trustful end-to-end communication links with decentrally managed\ndigital identities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Authentication in TLS is predominately carried out with X.509 digital\ncertificates issued by certificate authorities (CA). The centralized nature of\ncurrent public key infrastructures, however, comes along with severe risks,\nsuch as single points of failure and susceptibility to cyber-attacks,\npotentially undermining the security and trustworthiness of the entire system.\nWith Decentralized Identifiers (DID) alongside distributed ledger technology,\nit becomes technically feasible to prove ownership of a unique identifier\nwithout requiring an attestation of the proof's public key by a centralized and\ntherefore vulnerable CA. This article presents DID Link, a novel authentication\nscheme for TLS 1.3 that empowers entities to authenticate in a TLS-compliant\nway with self-issued X.509 certificates that are equipped with ledger-anchored\nDIDs instead of CA-issued identifiers. It facilitates the exchange of\ntamper-proof and 3rd-party attested claims in the form of DID-bound Verifiable\nCredentials after the TLS handshake to complete the authentication with a full\nidentification of the communication partner. A prototypical implementation\nshows comparable TLS handshake durations of DID Link if verification material\nis cached and reasonable prolongations if it is obtained from a ledger. The\nsignificant speed improvement of the resulting TLS channel over a widely used,\nDID-based alternative transport protocol on the application layer demonstrates\nthe potential of DID Link to become a viable solution for the establishment of\nsecure and trustful end-to-end communication links with decentrally managed\ndigital identities."
                },
                "authors": [
                    {
                        "name": "Sandro Rodriguez Garzon"
                    },
                    {
                        "name": "Dennis Natusch"
                    },
                    {
                        "name": "Artur Philipp"
                    },
                    {
                        "name": "Axel Kpper"
                    },
                    {
                        "name": "Hans Joachim Einsiedler"
                    },
                    {
                        "name": "Daniela Schneider"
                    }
                ],
                "author_detail": {
                    "name": "Daniela Schneider"
                },
                "author": "Daniela Schneider",
                "arxiv_comment": "Accepted by and presented at 21st Annual International Conference on\n  Privacy, Security, and Trust (PST2024). Publication by IEEE still pending",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.07533v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.07533v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18191v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18191v2",
                "updated": "2024-11-29T08:33:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    8,
                    33,
                    49,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-27T10:14:38Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    10,
                    14,
                    38,
                    2,
                    332,
                    0
                ],
                "title": "InputSnatch: Stealing Input in LLM Services via Timing Side-Channel\n  Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InputSnatch: Stealing Input in LLM Services via Timing Side-Channel\n  Attacks"
                },
                "summary": "Large language models (LLMs) possess extensive knowledge and\nquestion-answering capabilities, having been widely deployed in\nprivacy-sensitive domains like finance and medical consultation. During LLM\ninferences, cache-sharing methods are commonly employed to enhance efficiency\nby reusing cached states or responses for the same or similar inference\nrequests. However, we identify that these cache mechanisms pose a risk of\nprivate input leakage, as the caching can result in observable variations in\nresponse times, making them a strong candidate for a timing-based attack hint.\n  In this study, we propose a novel timing-based side-channel attack to execute\ninput theft in LLMs inference. The cache-based attack faces the challenge of\nconstructing candidate inputs in a large search space to hit and steal cached\nuser queries. To address these challenges, we propose two primary components.\nThe input constructor employs machine learning techniques and LLM-based\napproaches for vocabulary correlation learning while implementing optimized\nsearch mechanisms for generalized input construction. The time analyzer\nimplements statistical time fitting with outlier elimination to identify cache\nhit patterns, continuously providing feedback to refine the constructor's\nsearch strategy. We conduct experiments across two cache mechanisms and the\nresults demonstrate that our approach consistently attains high attack success\nrates in various applications. Our work highlights the security vulnerabilities\nassociated with performance optimizations, underscoring the necessity of\nprioritizing privacy and security alongside enhancements in LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) possess extensive knowledge and\nquestion-answering capabilities, having been widely deployed in\nprivacy-sensitive domains like finance and medical consultation. During LLM\ninferences, cache-sharing methods are commonly employed to enhance efficiency\nby reusing cached states or responses for the same or similar inference\nrequests. However, we identify that these cache mechanisms pose a risk of\nprivate input leakage, as the caching can result in observable variations in\nresponse times, making them a strong candidate for a timing-based attack hint.\n  In this study, we propose a novel timing-based side-channel attack to execute\ninput theft in LLMs inference. The cache-based attack faces the challenge of\nconstructing candidate inputs in a large search space to hit and steal cached\nuser queries. To address these challenges, we propose two primary components.\nThe input constructor employs machine learning techniques and LLM-based\napproaches for vocabulary correlation learning while implementing optimized\nsearch mechanisms for generalized input construction. The time analyzer\nimplements statistical time fitting with outlier elimination to identify cache\nhit patterns, continuously providing feedback to refine the constructor's\nsearch strategy. We conduct experiments across two cache mechanisms and the\nresults demonstrate that our approach consistently attains high attack success\nrates in various applications. Our work highlights the security vulnerabilities\nassociated with performance optimizations, underscoring the necessity of\nprioritizing privacy and security alongside enhancements in LLM inference."
                },
                "authors": [
                    {
                        "name": "Xinyao Zheng"
                    },
                    {
                        "name": "Husheng Han"
                    },
                    {
                        "name": "Shangyi Shi"
                    },
                    {
                        "name": "Qiyan Fang"
                    },
                    {
                        "name": "Zidong Du"
                    },
                    {
                        "name": "Xing Hu"
                    },
                    {
                        "name": "Qi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Qi Guo"
                },
                "author": "Qi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18191v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18191v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19248v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19248v1",
                "updated": "2024-11-28T16:35:22Z",
                "updated_parsed": [
                    2024,
                    11,
                    28,
                    16,
                    35,
                    22,
                    3,
                    333,
                    0
                ],
                "published": "2024-11-28T16:35:22Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    16,
                    35,
                    22,
                    3,
                    333,
                    0
                ],
                "title": "Reflecting Intelligent Surfaces-Assisted Multiple-Antenna Coded Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reflecting Intelligent Surfaces-Assisted Multiple-Antenna Coded Caching"
                },
                "summary": "Reconfigurable intelligent surface (RIS) has been treated as a core technique\nin improving wireless propagation environments for the next generation wireless\ncommunication systems. This paper proposes a new coded caching problem,\nreferred to as Reconfigurable Intelligent Surface (RIS)-assisted\nmultiple-antenna coded caching, which is composed of a server with multiple\nantennas and some single-antenna cache-aided users. Different from the existing\nmulti-antenna coded caching problems, we introduce a passive RIS (with limited\nnumber of units) into the systems to further increase the multicast gain (i.e.,\ndegrees of freedom (DoF)) in the transmission, which is done by using\nRIS-assisted interference nulling. That is, by using RIS, we can `erase' any\npath between one transmission antenna and one receive antenna. We first propose\na new RIS-assisted interference nulling approach to search for the phase-shift\ncoefficients of RIS for the sake of interference nulling, which converges\nfaster than the state-of-the-art algorithm. After erasing some paths in each\ntime slot, the delivery can be divided into several non-overlapping groups\nincluding transmission antennas and users, where in each group the transmission\nantennas serve the contained users without suffering interference from the\ntransmissions by other groups. The division of groups for the sake of\nmaximizing the DoF could be formulated into a combinatorial optimization\nproblem. We propose a grouping algorithm which can find the optimal solution\nwith low complexity, and the corresponding coded caching scheme achieving this\nDoF.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconfigurable intelligent surface (RIS) has been treated as a core technique\nin improving wireless propagation environments for the next generation wireless\ncommunication systems. This paper proposes a new coded caching problem,\nreferred to as Reconfigurable Intelligent Surface (RIS)-assisted\nmultiple-antenna coded caching, which is composed of a server with multiple\nantennas and some single-antenna cache-aided users. Different from the existing\nmulti-antenna coded caching problems, we introduce a passive RIS (with limited\nnumber of units) into the systems to further increase the multicast gain (i.e.,\ndegrees of freedom (DoF)) in the transmission, which is done by using\nRIS-assisted interference nulling. That is, by using RIS, we can `erase' any\npath between one transmission antenna and one receive antenna. We first propose\na new RIS-assisted interference nulling approach to search for the phase-shift\ncoefficients of RIS for the sake of interference nulling, which converges\nfaster than the state-of-the-art algorithm. After erasing some paths in each\ntime slot, the delivery can be divided into several non-overlapping groups\nincluding transmission antennas and users, where in each group the transmission\nantennas serve the contained users without suffering interference from the\ntransmissions by other groups. The division of groups for the sake of\nmaximizing the DoF could be formulated into a combinatorial optimization\nproblem. We propose a grouping algorithm which can find the optimal solution\nwith low complexity, and the corresponding coded caching scheme achieving this\nDoF."
                },
                "authors": [
                    {
                        "name": "Xiaofan Niu"
                    },
                    {
                        "name": "Minquan Cheng"
                    },
                    {
                        "name": "Kai Wan"
                    },
                    {
                        "name": "Robert Caiming Qiu"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "arxiv_comment": "The short version of this paper was presented in 2024 IEEE\n  Information Theory Workshop, Nov. 24-28, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19248v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19248v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.12468v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.12468v2",
                "updated": "2024-11-28T14:42:54Z",
                "updated_parsed": [
                    2024,
                    11,
                    28,
                    14,
                    42,
                    54,
                    3,
                    333,
                    0
                ],
                "published": "2024-04-18T19:04:33Z",
                "published_parsed": [
                    2024,
                    4,
                    18,
                    19,
                    4,
                    33,
                    3,
                    109,
                    0
                ],
                "title": "Fresh Caching of Dynamic Contents using Restless Multi-armed Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fresh Caching of Dynamic Contents using Restless Multi-armed Bandits"
                },
                "summary": "We consider a dynamic content caching problem wherein the contents get\nupdated at a central server, and local copies of a subset of contents are\ncached at a local cache associated with a Base station (BS). When a content\nrequest arrives, based on whether the content is in the local cache, the BS can\ndecide whether to fetch the content from the central server or serve the cached\nversion from the local cache. Fetching a content incurs a fixed fetching cost,\nand serving the cached version incurs an ageing cost proportional to the\nage-of-version (AoV) of the content. The BS has only partial information\nregarding AoVs of the contents. We formulate an optimal content fetching and\ncaching problem to minimize the average cost subject to cache capacity\nconstraints. The problem suffers from the curse of dimensionality and is\nprovably hard to solve. We formulate this problem as a continuous time restless\nmulti-armed bandit process (RMAB), where a single content problem of the\ncorresponding RMAB is a partially observable Markov decision process. We\nreformulate the single content problem as a semi-Markov decision process, prove\nindexability, and provide a Whittle index based solution to this problem.\nFinally, we compare the performance with recent work and show that our proposed\npolicy is optimal via simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a dynamic content caching problem wherein the contents get\nupdated at a central server, and local copies of a subset of contents are\ncached at a local cache associated with a Base station (BS). When a content\nrequest arrives, based on whether the content is in the local cache, the BS can\ndecide whether to fetch the content from the central server or serve the cached\nversion from the local cache. Fetching a content incurs a fixed fetching cost,\nand serving the cached version incurs an ageing cost proportional to the\nage-of-version (AoV) of the content. The BS has only partial information\nregarding AoVs of the contents. We formulate an optimal content fetching and\ncaching problem to minimize the average cost subject to cache capacity\nconstraints. The problem suffers from the curse of dimensionality and is\nprovably hard to solve. We formulate this problem as a continuous time restless\nmulti-armed bandit process (RMAB), where a single content problem of the\ncorresponding RMAB is a partially observable Markov decision process. We\nreformulate the single content problem as a semi-Markov decision process, prove\nindexability, and provide a Whittle index based solution to this problem.\nFinally, we compare the performance with recent work and show that our proposed\npolicy is optimal via simulations."
                },
                "authors": [
                    {
                        "name": "Ankita Koley"
                    },
                    {
                        "name": "Chandramani Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandramani Singh"
                },
                "author": "Chandramani Singh",
                "arxiv_comment": "14 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.12468v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.12468v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19108v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19108v1",
                "updated": "2024-11-28T12:50:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    28,
                    12,
                    50,
                    5,
                    3,
                    333,
                    0
                ],
                "published": "2024-11-28T12:50:05Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    12,
                    50,
                    5,
                    3,
                    333,
                    0
                ],
                "title": "Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model"
                },
                "summary": "As a fundamental backbone for video generation, diffusion models are\nchallenged by low inference speed due to the sequential nature of denoising.\nPrevious methods speed up the models by caching and reusing model outputs at\nuniformly selected timesteps. However, such a strategy neglects the fact that\ndifferences among model outputs are not uniform across timesteps, which hinders\nselecting the appropriate model outputs to cache, leading to a poor balance\nbetween inference efficiency and visual quality. In this study, we introduce\nTimestep Embedding Aware Cache (TeaCache), a training-free caching approach\nthat estimates and leverages the fluctuating differences among model outputs\nacross timesteps. Rather than directly using the time-consuming model outputs,\nTeaCache focuses on model inputs, which have a strong correlation with the\nmodeloutputs while incurring negligible computational cost. TeaCache first\nmodulates the noisy inputs using the timestep embeddings to ensure their\ndifferences better approximating those of model outputs. TeaCache then\nintroduces a rescaling strategy to refine the estimated differences and\nutilizes them to indicate output caching. Experiments show that TeaCache\nachieves up to 4.41x acceleration over Open-Sora-Plan with negligible (-0.07%\nVbench score) degradation of visual quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a fundamental backbone for video generation, diffusion models are\nchallenged by low inference speed due to the sequential nature of denoising.\nPrevious methods speed up the models by caching and reusing model outputs at\nuniformly selected timesteps. However, such a strategy neglects the fact that\ndifferences among model outputs are not uniform across timesteps, which hinders\nselecting the appropriate model outputs to cache, leading to a poor balance\nbetween inference efficiency and visual quality. In this study, we introduce\nTimestep Embedding Aware Cache (TeaCache), a training-free caching approach\nthat estimates and leverages the fluctuating differences among model outputs\nacross timesteps. Rather than directly using the time-consuming model outputs,\nTeaCache focuses on model inputs, which have a strong correlation with the\nmodeloutputs while incurring negligible computational cost. TeaCache first\nmodulates the noisy inputs using the timestep embeddings to ensure their\ndifferences better approximating those of model outputs. TeaCache then\nintroduces a rescaling strategy to refine the estimated differences and\nutilizes them to indicate output caching. Experiments show that TeaCache\nachieves up to 4.41x acceleration over Open-Sora-Plan with negligible (-0.07%\nVbench score) degradation of visual quality."
                },
                "authors": [
                    {
                        "name": "Feng Liu"
                    },
                    {
                        "name": "Shiwei Zhang"
                    },
                    {
                        "name": "Xiaofeng Wang"
                    },
                    {
                        "name": "Yujie Wei"
                    },
                    {
                        "name": "Haonan Qiu"
                    },
                    {
                        "name": "Yuzhong Zhao"
                    },
                    {
                        "name": "Yingya Zhang"
                    },
                    {
                        "name": "Qixiang Ye"
                    },
                    {
                        "name": "Fang Wan"
                    }
                ],
                "author_detail": {
                    "name": "Fang Wan"
                },
                "author": "Fang Wan",
                "arxiv_comment": "Project: https://liewfeng.github.io/TeaCache",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19108v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19108v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18077v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18077v2",
                "updated": "2024-11-28T02:01:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    28,
                    2,
                    1,
                    50,
                    3,
                    333,
                    0
                ],
                "published": "2024-11-27T06:10:49Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    6,
                    10,
                    49,
                    2,
                    332,
                    0
                ],
                "title": "MiniKV: Pushing the Limits of LLM Inference via 2-Bit\n  Layer-Discriminative KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MiniKV: Pushing the Limits of LLM Inference via 2-Bit\n  Layer-Discriminative KV Cache"
                },
                "summary": "How to efficiently serve LLMs in practice has become exceptionally\nchallenging due to their prohibitive memory and computation requirements. In\nthis study, we investigate optimizing the KV cache, whose memory footprint\nposes a critical bottleneck in LLM inference, especially when dealing with long\ncontext tasks. To tackle the challenge, we introduce MiniKV, a KV cache\noptimization method that simultaneously preserves long context task accuracy\nwhile significantly reducing KV cache size via a novel 2-bit\nlayer-discriminative KV cache. More importantly, we develop specialized CUDA\nkernels to make MiniKV compatible with FlashAttention. Experiments on a wide\nrange of long context tasks show that MiniKV effectively achieves 86% KV cache\ncompression ratio while recovering over 98.5% of accuracy, outperforming\nstate-of-the-art methods while achieving excellent measured system performance\nimprovements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to efficiently serve LLMs in practice has become exceptionally\nchallenging due to their prohibitive memory and computation requirements. In\nthis study, we investigate optimizing the KV cache, whose memory footprint\nposes a critical bottleneck in LLM inference, especially when dealing with long\ncontext tasks. To tackle the challenge, we introduce MiniKV, a KV cache\noptimization method that simultaneously preserves long context task accuracy\nwhile significantly reducing KV cache size via a novel 2-bit\nlayer-discriminative KV cache. More importantly, we develop specialized CUDA\nkernels to make MiniKV compatible with FlashAttention. Experiments on a wide\nrange of long context tasks show that MiniKV effectively achieves 86% KV cache\ncompression ratio while recovering over 98.5% of accuracy, outperforming\nstate-of-the-art methods while achieving excellent measured system performance\nimprovements."
                },
                "authors": [
                    {
                        "name": "Akshat Sharma"
                    },
                    {
                        "name": "Hangliang Ding"
                    },
                    {
                        "name": "Jianping Li"
                    },
                    {
                        "name": "Neel Dani"
                    },
                    {
                        "name": "Minjia Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Minjia Zhang"
                },
                "author": "Minjia Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18077v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18077v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00099v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00099v1",
                "updated": "2024-11-27T18:59:48Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    59,
                    48,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T18:59:48Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    59,
                    48,
                    2,
                    332,
                    0
                ],
                "title": "Mixture of Cache-Conditional Experts for Efficient Mobile Device\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Cache-Conditional Experts for Efficient Mobile Device\n  Inference"
                },
                "summary": "Mixture of Experts (MoE) LLMs have recently gained attention for their\nability to enhance performance by selectively engaging specialized subnetworks\nor \"experts\" for each input. However, deploying MoEs on memory-constrained\ndevices remains challenging, particularly when generating tokens sequentially\nwith a batch size of one, as opposed to typical high-throughput settings\ninvolving long sequences or large batches. In this work, we optimize MoE on\nmemory-constrained devices where only a subset of expert weights fit in DRAM.\nWe introduce a novel cache-aware routing strategy that leverages expert reuse\nduring token generation to improve cache locality. We evaluate our approach on\nlanguage modeling, MMLU, and GSM8K benchmarks and present on-device results\ndemonstrating 2$\\times$ speedups on mobile devices, offering a flexible,\ntraining-free solution to extend MoE's applicability across real-world\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Experts (MoE) LLMs have recently gained attention for their\nability to enhance performance by selectively engaging specialized subnetworks\nor \"experts\" for each input. However, deploying MoEs on memory-constrained\ndevices remains challenging, particularly when generating tokens sequentially\nwith a batch size of one, as opposed to typical high-throughput settings\ninvolving long sequences or large batches. In this work, we optimize MoE on\nmemory-constrained devices where only a subset of expert weights fit in DRAM.\nWe introduce a novel cache-aware routing strategy that leverages expert reuse\nduring token generation to improve cache locality. We evaluate our approach on\nlanguage modeling, MMLU, and GSM8K benchmarks and present on-device results\ndemonstrating 2$\\times$ speedups on mobile devices, offering a flexible,\ntraining-free solution to extend MoE's applicability across real-world\napplications."
                },
                "authors": [
                    {
                        "name": "Andrii Skliar"
                    },
                    {
                        "name": "Ties van Rozendaal"
                    },
                    {
                        "name": "Romain Lepert"
                    },
                    {
                        "name": "Todor Boinovski"
                    },
                    {
                        "name": "Mart van Baalen"
                    },
                    {
                        "name": "Markus Nagel"
                    },
                    {
                        "name": "Paul Whatmough"
                    },
                    {
                        "name": "Babak Ehteshami Bejnordi"
                    }
                ],
                "author_detail": {
                    "name": "Babak Ehteshami Bejnordi"
                },
                "author": "Babak Ehteshami Bejnordi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00099v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00099v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18566v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18566v1",
                "updated": "2024-11-27T18:09:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    9,
                    29,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T18:09:29Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    9,
                    29,
                    2,
                    332,
                    0
                ],
                "title": "OASIS-UROS: Open Acquisition System for IEPE Sensors -- Upgraded,\n  Refined, and Overhauled Software",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OASIS-UROS: Open Acquisition System for IEPE Sensors -- Upgraded,\n  Refined, and Overhauled Software"
                },
                "summary": "OASIS-UROS continues the previously published Open Acquisition System for\nIEPE Sensors (OASIS). While still building on the ESP32 microcontroller, this\nversion improves the overall performance by switching to an SD card caching\nsystem and upgrading the analog-digital converter to an AD7606C-18, which has a\nhigher resolution, provides eight channels, oversampling, and\nsoftware-adjustable voltage ranges. Also improved is the IEPE front-end and\npower supply, as well as the firmware of the acquisition system, which can now\nachieve a sample rate of up to 36 kHz while sampling all eight channels. This\npaper documents the hardware and software of OASIS-UROS and provides all\nmaterials required to reproduce the open acquisition system. Lastly, the system\nwas validated against commercial hardware and software in an experimental modal\nanalysis context. This showed that the system performs close to the commercial\none in some aspects with respect to the utilized test case. While OASIS-UROS\ncannot match the full performance of the commercial system, the developed\nsystem can be a viable alternative for students, people in academia, or smaller\ncompanies that have a constrained budget or require complete insight as well as\nadaptability of the hardware and software.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OASIS-UROS continues the previously published Open Acquisition System for\nIEPE Sensors (OASIS). While still building on the ESP32 microcontroller, this\nversion improves the overall performance by switching to an SD card caching\nsystem and upgrading the analog-digital converter to an AD7606C-18, which has a\nhigher resolution, provides eight channels, oversampling, and\nsoftware-adjustable voltage ranges. Also improved is the IEPE front-end and\npower supply, as well as the firmware of the acquisition system, which can now\nachieve a sample rate of up to 36 kHz while sampling all eight channels. This\npaper documents the hardware and software of OASIS-UROS and provides all\nmaterials required to reproduce the open acquisition system. Lastly, the system\nwas validated against commercial hardware and software in an experimental modal\nanalysis context. This showed that the system performs close to the commercial\none in some aspects with respect to the utilized test case. While OASIS-UROS\ncannot match the full performance of the commercial system, the developed\nsystem can be a viable alternative for students, people in academia, or smaller\ncompanies that have a constrained budget or require complete insight as well as\nadaptability of the hardware and software."
                },
                "authors": [
                    {
                        "name": "Oliver Maximilian Zobel"
                    },
                    {
                        "name": "Johannes Maierhofer"
                    },
                    {
                        "name": "Andreas Kstler"
                    },
                    {
                        "name": "Daniel J. Rixen"
                    }
                ],
                "author_detail": {
                    "name": "Daniel J. Rixen"
                },
                "author": "Daniel J. Rixen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18566v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18566v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.08895v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.08895v4",
                "updated": "2024-11-27T18:05:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    5,
                    57,
                    2,
                    332,
                    0
                ],
                "published": "2024-01-17T00:36:58Z",
                "published_parsed": [
                    2024,
                    1,
                    17,
                    0,
                    36,
                    58,
                    2,
                    17,
                    0
                ],
                "title": "cedar: Optimized and Unified Machine Learning Input Data Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "cedar: Optimized and Unified Machine Learning Input Data Pipelines"
                },
                "summary": "The input data pipeline is an essential component of each machine learning\n(ML) training job. It is responsible for reading massive amounts of training\ndata, processing batches of samples using complex transformations, and loading\nthem onto training nodes at low latency and high throughput. Performant input\ndata systems are becoming increasingly critical, driven by skyrocketing data\nvolumes and training throughput demands. Unfortunately, current input data\nsystems cannot fully leverage key performance optimizations, resulting in\nhugely inefficient infrastructures that require significant resources - or\nworse - underutilize expensive accelerators.\n  To address these demands, we present cedar, an optimized and unified\nprogramming framework for ML input data pipelines. cedar allows users to define\ninput data pipelines using composable operators that support arbitrary ML\nframeworks and libraries. cedar introduces an extensible optimizer that\nsystematically applies a complex combination of optimizations (e.g.,\noffloading, caching, prefetching, fusion, and reordering). It orchestrates\nprocessing across a customizable set of local and distributed compute resources\nin order to improve processing performance and efficiency, all without user\ninput. Across eight pipelines, cedar improves performance by up to 1.87x to\n10.65x compared to state-of-the-art input data systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The input data pipeline is an essential component of each machine learning\n(ML) training job. It is responsible for reading massive amounts of training\ndata, processing batches of samples using complex transformations, and loading\nthem onto training nodes at low latency and high throughput. Performant input\ndata systems are becoming increasingly critical, driven by skyrocketing data\nvolumes and training throughput demands. Unfortunately, current input data\nsystems cannot fully leverage key performance optimizations, resulting in\nhugely inefficient infrastructures that require significant resources - or\nworse - underutilize expensive accelerators.\n  To address these demands, we present cedar, an optimized and unified\nprogramming framework for ML input data pipelines. cedar allows users to define\ninput data pipelines using composable operators that support arbitrary ML\nframeworks and libraries. cedar introduces an extensible optimizer that\nsystematically applies a complex combination of optimizations (e.g.,\noffloading, caching, prefetching, fusion, and reordering). It orchestrates\nprocessing across a customizable set of local and distributed compute resources\nin order to improve processing performance and efficiency, all without user\ninput. Across eight pipelines, cedar improves performance by up to 1.87x to\n10.65x compared to state-of-the-art input data systems."
                },
                "authors": [
                    {
                        "name": "Mark Zhao"
                    },
                    {
                        "name": "Emanuel Adamiak"
                    },
                    {
                        "name": "Christos Kozyrakis"
                    }
                ],
                "author_detail": {
                    "name": "Christos Kozyrakis"
                },
                "author": "Christos Kozyrakis",
                "arxiv_comment": "Published in PVLDB Volume 18, Issue 2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.08895v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.08895v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18424v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18424v1",
                "updated": "2024-11-27T15:07:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    15,
                    7,
                    28,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T15:07:28Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    15,
                    7,
                    28,
                    2,
                    332,
                    0
                ],
                "title": "FastSwitch: Optimizing Context Switching Efficiency in Fairness-aware\n  Large Language Model Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastSwitch: Optimizing Context Switching Efficiency in Fairness-aware\n  Large Language Model Serving"
                },
                "summary": "Serving numerous users and requests concurrently requires good fairness in\nLarge Language Models (LLMs) serving system. This ensures that, at the same\ncost, the system can meet the Service Level Objectives (SLOs) of more users ,\nsuch as time to first token (TTFT) and time between tokens (TBT), rather than\nallowing a few users to experience performance far exceeding the SLOs. To\nachieve better fairness, the preemption-based scheduling policy dynamically\nadjusts the priority of each request to maintain balance during runtime.\nHowever, existing systems tend to overly prioritize throughput, overlooking the\noverhead caused by preemption-induced context switching, which is crucial for\nmaintaining fairness through priority adjustments. In this work, we identify\nthree main challenges that result in this overhead. 1) Inadequate I/O\nutilization. 2) GPU idleness. 3) Unnecessary I/O transmission during multi-turn\nconversations. Our key insight is that the block-based KV cache memory policy\nin existing systems, while achieving near-zero memory waste, leads to\ndiscontinuity and insufficient granularity in the KV cache memory. To respond,\nwe introduce FastSwitch, a fairness-aware serving system that not only aligns\nwith existing KV cache memory allocation policy but also mitigates context\nswitching overhead. Our evaluation shows that FastSwitch outperforms the\nstate-of-the-art LLM serving system vLLM with speedups of 1.4-11.2x across\ndifferent tail TTFT and TBT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving numerous users and requests concurrently requires good fairness in\nLarge Language Models (LLMs) serving system. This ensures that, at the same\ncost, the system can meet the Service Level Objectives (SLOs) of more users ,\nsuch as time to first token (TTFT) and time between tokens (TBT), rather than\nallowing a few users to experience performance far exceeding the SLOs. To\nachieve better fairness, the preemption-based scheduling policy dynamically\nadjusts the priority of each request to maintain balance during runtime.\nHowever, existing systems tend to overly prioritize throughput, overlooking the\noverhead caused by preemption-induced context switching, which is crucial for\nmaintaining fairness through priority adjustments. In this work, we identify\nthree main challenges that result in this overhead. 1) Inadequate I/O\nutilization. 2) GPU idleness. 3) Unnecessary I/O transmission during multi-turn\nconversations. Our key insight is that the block-based KV cache memory policy\nin existing systems, while achieving near-zero memory waste, leads to\ndiscontinuity and insufficient granularity in the KV cache memory. To respond,\nwe introduce FastSwitch, a fairness-aware serving system that not only aligns\nwith existing KV cache memory allocation policy but also mitigates context\nswitching overhead. Our evaluation shows that FastSwitch outperforms the\nstate-of-the-art LLM serving system vLLM with speedups of 1.4-11.2x across\ndifferent tail TTFT and TBT."
                },
                "authors": [
                    {
                        "name": "Ao Shen"
                    },
                    {
                        "name": "Zhiyao Li"
                    },
                    {
                        "name": "Mingyu Gao"
                    }
                ],
                "author_detail": {
                    "name": "Mingyu Gao"
                },
                "author": "Mingyu Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18424v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18424v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17616v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17616v2",
                "updated": "2024-11-27T14:43:46Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    14,
                    43,
                    46,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-26T17:28:10Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    28,
                    10,
                    1,
                    331,
                    0
                ],
                "title": "Accelerating Vision Diffusion Transformers with Skip Branches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Vision Diffusion Transformers with Skip Branches"
                },
                "summary": "Diffusion Transformers (DiT), an emerging image and video generation model\narchitecture, has demonstrated great potential because of its high generation\nquality and scalability properties. Despite the impressive performance, its\npractical deployment is constrained by computational complexity and redundancy\nin the sequential denoising process. While feature caching across timesteps has\nproven effective in accelerating diffusion models, its application to DiT is\nlimited by fundamental architectural differences from U-Net-based approaches.\nThrough empirical analysis of DiT feature dynamics, we identify that\nsignificant feature variation between DiT blocks presents a key challenge for\nfeature reusability. To address this, we convert standard DiT into Skip-DiT\nwith skip branches to enhance feature smoothness. Further, we introduce\nSkip-Cache which utilizes the skip branches to cache DiT features across\ntimesteps at the inference time. We validated effectiveness of our proposal on\ndifferent DiT backbones for video and image generation, showcasing skip\nbranches to help preserve generation quality and achieve higher speedup.\nExperimental results indicate that Skip-DiT achieves a 1.5x speedup almost for\nfree and a 2.2x speedup with only a minor reduction in quantitative metrics.\nCode is available at https://github.com/OpenSparseLLMs/Skip-DiT.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT), an emerging image and video generation model\narchitecture, has demonstrated great potential because of its high generation\nquality and scalability properties. Despite the impressive performance, its\npractical deployment is constrained by computational complexity and redundancy\nin the sequential denoising process. While feature caching across timesteps has\nproven effective in accelerating diffusion models, its application to DiT is\nlimited by fundamental architectural differences from U-Net-based approaches.\nThrough empirical analysis of DiT feature dynamics, we identify that\nsignificant feature variation between DiT blocks presents a key challenge for\nfeature reusability. To address this, we convert standard DiT into Skip-DiT\nwith skip branches to enhance feature smoothness. Further, we introduce\nSkip-Cache which utilizes the skip branches to cache DiT features across\ntimesteps at the inference time. We validated effectiveness of our proposal on\ndifferent DiT backbones for video and image generation, showcasing skip\nbranches to help preserve generation quality and achieve higher speedup.\nExperimental results indicate that Skip-DiT achieves a 1.5x speedup almost for\nfree and a 2.2x speedup with only a minor reduction in quantitative metrics.\nCode is available at https://github.com/OpenSparseLLMs/Skip-DiT.git."
                },
                "authors": [
                    {
                        "name": "Guanjie Chen"
                    },
                    {
                        "name": "Xinyu Zhao"
                    },
                    {
                        "name": "Yucheng Zhou"
                    },
                    {
                        "name": "Tianlong Chen"
                    },
                    {
                        "name": "Yu Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Yu Cheng"
                },
                "author": "Yu Cheng",
                "arxiv_comment": "17 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17616v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17616v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17459v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17459v2",
                "updated": "2024-11-27T08:21:47Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    8,
                    21,
                    47,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-26T14:23:53Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    14,
                    23,
                    53,
                    1,
                    331,
                    0
                ],
                "title": "WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent\n  Video Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent\n  Video Diffusion Model"
                },
                "summary": "Video Variational Autoencoder (VAE) encodes videos into a low-dimensional\nlatent space, becoming a key component of most Latent Video Diffusion Models\n(LVDMs) to reduce model training costs. However, as the resolution and duration\nof generated videos increase, the encoding cost of Video VAEs becomes a\nlimiting bottleneck in training LVDMs. Moreover, the block-wise inference\nmethod adopted by most LVDMs can lead to discontinuities of latent space when\nprocessing long-duration videos. The key to addressing the computational\nbottleneck lies in decomposing videos into distinct components and efficiently\nencoding the critical information. Wavelet transform can decompose videos into\nmultiple frequency-domain components and improve the efficiency significantly,\nwe thus propose Wavelet Flow VAE (WF-VAE), an autoencoder that leverages\nmulti-level wavelet transform to facilitate low-frequency energy flow into\nlatent representation. Furthermore, we introduce a method called Causal Cache,\nwhich maintains the integrity of latent space during block-wise inference.\nCompared to state-of-the-art video VAEs, WF-VAE demonstrates superior\nperformance in both PSNR and LPIPS metrics, achieving 2x higher throughput and\n4x lower memory consumption while maintaining competitive reconstruction\nquality. Our code and models are available at\nhttps://github.com/PKU-YuanGroup/WF-VAE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Variational Autoencoder (VAE) encodes videos into a low-dimensional\nlatent space, becoming a key component of most Latent Video Diffusion Models\n(LVDMs) to reduce model training costs. However, as the resolution and duration\nof generated videos increase, the encoding cost of Video VAEs becomes a\nlimiting bottleneck in training LVDMs. Moreover, the block-wise inference\nmethod adopted by most LVDMs can lead to discontinuities of latent space when\nprocessing long-duration videos. The key to addressing the computational\nbottleneck lies in decomposing videos into distinct components and efficiently\nencoding the critical information. Wavelet transform can decompose videos into\nmultiple frequency-domain components and improve the efficiency significantly,\nwe thus propose Wavelet Flow VAE (WF-VAE), an autoencoder that leverages\nmulti-level wavelet transform to facilitate low-frequency energy flow into\nlatent representation. Furthermore, we introduce a method called Causal Cache,\nwhich maintains the integrity of latent space during block-wise inference.\nCompared to state-of-the-art video VAEs, WF-VAE demonstrates superior\nperformance in both PSNR and LPIPS metrics, achieving 2x higher throughput and\n4x lower memory consumption while maintaining competitive reconstruction\nquality. Our code and models are available at\nhttps://github.com/PKU-YuanGroup/WF-VAE."
                },
                "authors": [
                    {
                        "name": "Zongjian Li"
                    },
                    {
                        "name": "Bin Lin"
                    },
                    {
                        "name": "Yang Ye"
                    },
                    {
                        "name": "Liuhan Chen"
                    },
                    {
                        "name": "Xinhua Cheng"
                    },
                    {
                        "name": "Shenghai Yuan"
                    },
                    {
                        "name": "Li Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Li Yuan"
                },
                "author": "Li Yuan",
                "arxiv_comment": "8 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17459v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17459v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15785v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15785v2",
                "updated": "2024-11-27T03:07:20Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    3,
                    7,
                    20,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-24T11:30:00Z",
                "published_parsed": [
                    2024,
                    11,
                    24,
                    11,
                    30,
                    0,
                    6,
                    329,
                    0
                ],
                "title": "A Method for Building Large Language Models with Predefined KV Cache\n  Capacity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Method for Building Large Language Models with Predefined KV Cache\n  Capacity"
                },
                "summary": "This paper introduces a novel approach, the Bounded-Cache Transformer (BCT),\nfor building large language models with a predefined Key-Value (KV) cache\ncapacity. The BCT addresses the excessive memory consumption issue in\ntraditional KV caches by implementing a bounded-length KV cache, which is\nparticularly suitable for the attention layers in Transformer decode-only\narchitectures. By dynamically updating the key-value vector sequences, the BCT\nachieves efficient inference within limited cache capacity, significantly\nreducing memory usage while maintaining model performance and system\nthroughput. Experimental results demonstrate that the BCT significantly reduces\nmemory usage while maintaining the model's inference quality, offering a new\nsolution for efficient inference in large language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel approach, the Bounded-Cache Transformer (BCT),\nfor building large language models with a predefined Key-Value (KV) cache\ncapacity. The BCT addresses the excessive memory consumption issue in\ntraditional KV caches by implementing a bounded-length KV cache, which is\nparticularly suitable for the attention layers in Transformer decode-only\narchitectures. By dynamically updating the key-value vector sequences, the BCT\nachieves efficient inference within limited cache capacity, significantly\nreducing memory usage while maintaining model performance and system\nthroughput. Experimental results demonstrate that the BCT significantly reduces\nmemory usage while maintaining the model's inference quality, offering a new\nsolution for efficient inference in large language models."
                },
                "authors": [
                    {
                        "name": "Zhonghua Yi"
                    },
                    {
                        "name": "Ge Niu"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Wei Tang"
                    },
                    {
                        "name": "Liqiu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Liqiu Zhang"
                },
                "author": "Liqiu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15785v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15785v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17685v1",
                "updated": "2024-11-26T18:52:06Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    52,
                    6,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T18:52:06Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    52,
                    6,
                    1,
                    331,
                    0
                ],
                "title": "Attamba: Attending To Multi-Token States",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attamba: Attending To Multi-Token States"
                },
                "summary": "When predicting the next token in a sequence, vanilla transformers compute\nattention over all previous tokens, resulting in quadratic scaling of compute\nwith sequence length. State-space models compress the entire sequence of tokens\ninto a fixed-dimensional representation to improve efficiency, while other\narchitectures achieve sub-quadratic complexity via low-rank projections or\nsparse attention patterns over the sequence. In this paper, we introduce\nAttamba, a novel architecture that uses state-space models to compress chunks\nof tokens and applies attention on these compressed key-value representations.\nWe find that replacing key and value projections in a transformer with SSMs can\nimprove model quality and enable flexible token chunking, resulting in 24%\nimproved perplexity with transformer of similar KV-Cache and attention\nfootprint, and ~4 times smaller KV-Cache and Attention FLOPs for 5% perplexity\ntrade-off. Attamba can perform attention on chunked-sequences of variable\nlength, enabling a smooth transition between quadratic and linear scaling,\noffering adaptable efficiency gains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When predicting the next token in a sequence, vanilla transformers compute\nattention over all previous tokens, resulting in quadratic scaling of compute\nwith sequence length. State-space models compress the entire sequence of tokens\ninto a fixed-dimensional representation to improve efficiency, while other\narchitectures achieve sub-quadratic complexity via low-rank projections or\nsparse attention patterns over the sequence. In this paper, we introduce\nAttamba, a novel architecture that uses state-space models to compress chunks\nof tokens and applies attention on these compressed key-value representations.\nWe find that replacing key and value projections in a transformer with SSMs can\nimprove model quality and enable flexible token chunking, resulting in 24%\nimproved perplexity with transformer of similar KV-Cache and attention\nfootprint, and ~4 times smaller KV-Cache and Attention FLOPs for 5% perplexity\ntrade-off. Attamba can perform attention on chunked-sequences of variable\nlength, enabling a smooth transition between quadratic and linear scaling,\noffering adaptable efficiency gains."
                },
                "authors": [
                    {
                        "name": "Yash Akhauri"
                    },
                    {
                        "name": "Safeen Huda"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed S. Abdelfattah"
                },
                "author": "Mohamed S. Abdelfattah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17800v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17800v1",
                "updated": "2024-11-26T18:42:42Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    42,
                    42,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T18:42:42Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    42,
                    42,
                    1,
                    331,
                    0
                ],
                "title": "STAR: Synthesis of Tailored Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STAR: Synthesis of Tailored Architectures"
                },
                "summary": "Iterative improvement of model architectures is fundamental to deep learning:\nTransformers first enabled scaling, and recent advances in model hybridization\nhave pushed the quality-efficiency frontier. However, optimizing architectures\nremains challenging and expensive. Current automated or manual approaches fall\nshort, largely due to limited progress in the design of search spaces and due\nto the simplicity of resulting patterns and heuristics. In this work, we\npropose a new approach for the synthesis of tailored architectures (STAR). Our\napproach combines a novel search space based on the theory of linear\ninput-varying systems, supporting a hierarchical numerical encoding into\narchitecture genomes. STAR genomes are automatically refined and recombined\nwith gradient-free, evolutionary algorithms to optimize for multiple model\nquality and efficiency metrics. Using STAR, we optimize large populations of\nnew architectures, leveraging diverse computational units and interconnection\npatterns, improving over highly-optimized Transformers and striped hybrid\nmodels on the frontier of quality, parameter size, and inference cache for\nautoregressive language modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Iterative improvement of model architectures is fundamental to deep learning:\nTransformers first enabled scaling, and recent advances in model hybridization\nhave pushed the quality-efficiency frontier. However, optimizing architectures\nremains challenging and expensive. Current automated or manual approaches fall\nshort, largely due to limited progress in the design of search spaces and due\nto the simplicity of resulting patterns and heuristics. In this work, we\npropose a new approach for the synthesis of tailored architectures (STAR). Our\napproach combines a novel search space based on the theory of linear\ninput-varying systems, supporting a hierarchical numerical encoding into\narchitecture genomes. STAR genomes are automatically refined and recombined\nwith gradient-free, evolutionary algorithms to optimize for multiple model\nquality and efficiency metrics. Using STAR, we optimize large populations of\nnew architectures, leveraging diverse computational units and interconnection\npatterns, improving over highly-optimized Transformers and striped hybrid\nmodels on the frontier of quality, parameter size, and inference cache for\nautoregressive language modeling."
                },
                "authors": [
                    {
                        "name": "Armin W. Thomas"
                    },
                    {
                        "name": "Rom Parnichkun"
                    },
                    {
                        "name": "Alexander Amini"
                    },
                    {
                        "name": "Stefano Massaroli"
                    },
                    {
                        "name": "Michael Poli"
                    }
                ],
                "author_detail": {
                    "name": "Michael Poli"
                },
                "author": "Michael Poli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17800v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17800v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.15651v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.15651v3",
                "updated": "2024-11-26T17:28:06Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    28,
                    6,
                    1,
                    331,
                    0
                ],
                "published": "2024-03-22T23:47:19Z",
                "published_parsed": [
                    2024,
                    3,
                    22,
                    23,
                    47,
                    19,
                    4,
                    82,
                    0
                ],
                "title": "GaNI: Global and Near Field Illumination Aware Neural Inverse Rendering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GaNI: Global and Near Field Illumination Aware Neural Inverse Rendering"
                },
                "summary": "In this paper, we present GaNI, a Global and Near-field Illumination-aware\nneural inverse rendering technique that can reconstruct geometry, albedo, and\nroughness parameters from images of a scene captured with co-located light and\ncamera. Existing inverse rendering techniques with co-located light-camera\nfocus on single objects only, without modeling global illumination and\nnear-field lighting more prominent in scenes with multiple objects. We\nintroduce a system that solves this problem in two stages; we first reconstruct\nthe geometry powered by neural volumetric rendering NeuS, followed by inverse\nneural radiosity that uses the previously predicted geometry to estimate albedo\nand roughness. However, such a naive combination fails and we propose multiple\ntechnical contributions that enable this two-stage approach. We observe that\nNeuS fails to handle near-field illumination and strong specular reflections\nfrom the flashlight in a scene. We propose to implicitly model the effects of\nnear-field illumination and introduce a surface angle loss function to handle\nspecular reflections. Similarly, we observe that invNeRad assumes constant\nillumination throughout the capture and cannot handle moving flashlights during\ncapture. We propose a light position-aware radiance cache network and\nadditional smoothness priors on roughness to reconstruct reflectance.\nExperimental evaluation on synthetic and real data shows that our method\noutperforms the existing co-located light-camera-based inverse rendering\ntechniques. Our approach produces significantly better reflectance and slightly\nbetter geometry than capture strategies that do not require a dark room.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present GaNI, a Global and Near-field Illumination-aware\nneural inverse rendering technique that can reconstruct geometry, albedo, and\nroughness parameters from images of a scene captured with co-located light and\ncamera. Existing inverse rendering techniques with co-located light-camera\nfocus on single objects only, without modeling global illumination and\nnear-field lighting more prominent in scenes with multiple objects. We\nintroduce a system that solves this problem in two stages; we first reconstruct\nthe geometry powered by neural volumetric rendering NeuS, followed by inverse\nneural radiosity that uses the previously predicted geometry to estimate albedo\nand roughness. However, such a naive combination fails and we propose multiple\ntechnical contributions that enable this two-stage approach. We observe that\nNeuS fails to handle near-field illumination and strong specular reflections\nfrom the flashlight in a scene. We propose to implicitly model the effects of\nnear-field illumination and introduce a surface angle loss function to handle\nspecular reflections. Similarly, we observe that invNeRad assumes constant\nillumination throughout the capture and cannot handle moving flashlights during\ncapture. We propose a light position-aware radiance cache network and\nadditional smoothness priors on roughness to reconstruct reflectance.\nExperimental evaluation on synthetic and real data shows that our method\noutperforms the existing co-located light-camera-based inverse rendering\ntechniques. Our approach produces significantly better reflectance and slightly\nbetter geometry than capture strategies that do not require a dark room."
                },
                "authors": [
                    {
                        "name": "Jiaye Wu"
                    },
                    {
                        "name": "Saeed Hadadan"
                    },
                    {
                        "name": "Geng Lin"
                    },
                    {
                        "name": "Matthias Zwicker"
                    },
                    {
                        "name": "David Jacobs"
                    },
                    {
                        "name": "Roni Sengupta"
                    }
                ],
                "author_detail": {
                    "name": "Roni Sengupta"
                },
                "author": "Roni Sengupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.15651v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.15651v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17559v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17559v1",
                "updated": "2024-11-26T16:21:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    16,
                    21,
                    10,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T16:21:10Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    16,
                    21,
                    10,
                    1,
                    331,
                    0
                ],
                "title": "Degrees of Freedom of Cache-Aided Interference Channels Assisted by\n  Active Intelligent Reflecting Surfaces",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Degrees of Freedom of Cache-Aided Interference Channels Assisted by\n  Active Intelligent Reflecting Surfaces"
                },
                "summary": "This paper studies cache-aided wireless networks in the presence of active\nintelligent reflecting surfaces (IRS) from an information-theoretic\nperspective. Specifically, we explore interference management in a cache-aided\nwireless network assisted by an active IRS, to enhance the achievable degrees\nof freedom (DoF). To this end, we jointly design the content placement,\ndelivery phase, and phase shifts of the IRS and propose a one-shot achievable\nscheme. Our scheme exploits transmitters' cooperation, cache contents (as side\ninformation), interference alignment, and IRS capabilities, adapting to the\nnetwork's parameters. We derive the achievable one-shot sum-DoF for different\nsizes of cache memories, network configurations, and numbers of IRS elements.\nOur results highlight the potential of deploying an IRS in cache-aided wireless\ncommunication systems, underscoring the enhancement of achievable DoF for\nvarious parameter regimes, particularly when the sizes of the caches\n(especially at the transmitters) are inadequate. Notably, we show that access\nto an IRS with a sufficient number of elements enables the achievement of the\nmaximum possible DoF for various parameter regimes of interest.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper studies cache-aided wireless networks in the presence of active\nintelligent reflecting surfaces (IRS) from an information-theoretic\nperspective. Specifically, we explore interference management in a cache-aided\nwireless network assisted by an active IRS, to enhance the achievable degrees\nof freedom (DoF). To this end, we jointly design the content placement,\ndelivery phase, and phase shifts of the IRS and propose a one-shot achievable\nscheme. Our scheme exploits transmitters' cooperation, cache contents (as side\ninformation), interference alignment, and IRS capabilities, adapting to the\nnetwork's parameters. We derive the achievable one-shot sum-DoF for different\nsizes of cache memories, network configurations, and numbers of IRS elements.\nOur results highlight the potential of deploying an IRS in cache-aided wireless\ncommunication systems, underscoring the enhancement of achievable DoF for\nvarious parameter regimes, particularly when the sizes of the caches\n(especially at the transmitters) are inadequate. Notably, we show that access\nto an IRS with a sufficient number of elements enables the achievement of the\nmaximum possible DoF for various parameter regimes of interest."
                },
                "authors": [
                    {
                        "name": "Abolfazl Changizi"
                    },
                    {
                        "name": "Ali H. Abdollahi Bafghi"
                    },
                    {
                        "name": "Masoumeh Nasiri-Kenari"
                    }
                ],
                "author_detail": {
                    "name": "Masoumeh Nasiri-Kenari"
                },
                "author": "Masoumeh Nasiri-Kenari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17559v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17559v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17786v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17786v1",
                "updated": "2024-11-26T15:03:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    15,
                    3,
                    14,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T15:03:14Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    15,
                    3,
                    14,
                    1,
                    331,
                    0
                ],
                "title": "DreamCache: Finetuning-Free Lightweight Personalized Image Generation\n  via Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DreamCache: Finetuning-Free Lightweight Personalized Image Generation\n  via Feature Caching"
                },
                "summary": "Personalized image generation requires text-to-image generative models that\ncapture the core features of a reference subject to allow for controlled\ngeneration across different contexts. Existing methods face challenges due to\ncomplex training requirements, high inference costs, limited flexibility, or a\ncombination of these issues. In this paper, we introduce DreamCache, a scalable\napproach for efficient and high-quality personalized image generation. By\ncaching a small number of reference image features from a subset of layers and\na single timestep of the pretrained diffusion denoiser, DreamCache enables\ndynamic modulation of the generated image features through lightweight, trained\nconditioning adapters. DreamCache achieves state-of-the-art image and text\nalignment, utilizing an order of magnitude fewer extra parameters, and is both\nmore computationally effective and versatile than existing models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized image generation requires text-to-image generative models that\ncapture the core features of a reference subject to allow for controlled\ngeneration across different contexts. Existing methods face challenges due to\ncomplex training requirements, high inference costs, limited flexibility, or a\ncombination of these issues. In this paper, we introduce DreamCache, a scalable\napproach for efficient and high-quality personalized image generation. By\ncaching a small number of reference image features from a subset of layers and\na single timestep of the pretrained diffusion denoiser, DreamCache enables\ndynamic modulation of the generated image features through lightweight, trained\nconditioning adapters. DreamCache achieves state-of-the-art image and text\nalignment, utilizing an order of magnitude fewer extra parameters, and is both\nmore computationally effective and versatile than existing models."
                },
                "authors": [
                    {
                        "name": "Emanuele Aiello"
                    },
                    {
                        "name": "Umberto Michieli"
                    },
                    {
                        "name": "Diego Valsesia"
                    },
                    {
                        "name": "Mete Ozay"
                    },
                    {
                        "name": "Enrico Magli"
                    }
                ],
                "author_detail": {
                    "name": "Enrico Magli"
                },
                "author": "Enrico Magli",
                "arxiv_comment": "16 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17786v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17786v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17116v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17116v1",
                "updated": "2024-11-26T05:10:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    5,
                    10,
                    4,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T05:10:04Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    5,
                    10,
                    4,
                    1,
                    331,
                    0
                ],
                "title": "Star Attention: Efficient LLM Inference over Long Sequences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Star Attention: Efficient LLM Inference over Long Sequences"
                },
                "summary": "Inference with Transformer-based Large Language Models (LLMs) on long\nsequences is both costly and slow due to the quadratic complexity of the\nself-attention mechanism. We introduce Star Attention, a two-phase block-sparse\napproximation that improves computational efficiency by sharding attention\nacross multiple hosts while minimizing communication overhead. In the first\nphase, the context is processed using blockwise-local attention across hosts,\nin parallel. In the second phase, query and response tokens attend to all prior\ncached tokens through sequence-global attention. Star Attention integrates\nseamlessly with most Transformer-based LLMs trained with global attention,\nreducing memory requirements and inference time by up to 11x while preserving\n95-100% of accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference with Transformer-based Large Language Models (LLMs) on long\nsequences is both costly and slow due to the quadratic complexity of the\nself-attention mechanism. We introduce Star Attention, a two-phase block-sparse\napproximation that improves computational efficiency by sharding attention\nacross multiple hosts while minimizing communication overhead. In the first\nphase, the context is processed using blockwise-local attention across hosts,\nin parallel. In the second phase, query and response tokens attend to all prior\ncached tokens through sequence-global attention. Star Attention integrates\nseamlessly with most Transformer-based LLMs trained with global attention,\nreducing memory requirements and inference time by up to 11x while preserving\n95-100% of accuracy."
                },
                "authors": [
                    {
                        "name": "Shantanu Acharya"
                    },
                    {
                        "name": "Fei Jia"
                    },
                    {
                        "name": "Boris Ginsburg"
                    }
                ],
                "author_detail": {
                    "name": "Boris Ginsburg"
                },
                "author": "Boris Ginsburg",
                "arxiv_comment": "Code: https://github.com/NVIDIA/Star-Attention",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17116v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17116v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17089v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17089v1",
                "updated": "2024-11-26T04:03:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    4,
                    3,
                    14,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T04:03:14Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    4,
                    3,
                    14,
                    1,
                    331,
                    0
                ],
                "title": "Efficient LLM Inference with I/O-Aware Partial KV Cache Recomputation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Inference with I/O-Aware Partial KV Cache Recomputation"
                },
                "summary": "Inference for Large Language Models (LLMs) is computationally demanding. To\nreduce the cost of auto-regressive decoding, Key-Value (KV) caching is used to\nstore intermediate activations, enabling GPUs to perform only the incremental\ncomputation required for each new token. This approach significantly lowers the\ncomputational overhead for token generation. However, the memory required for\nKV caching grows rapidly, often exceeding the capacity of GPU memory. A\ncost-effective alternative is to offload KV cache to CPU memory, which\nalleviates GPU memory pressure but shifts the bottleneck to the limited\nbandwidth of the PCIe connection between the CPU and GPU. Existing methods\nattempt to address these issues by overlapping GPU computation with I/O or\nemploying CPU-GPU heterogeneous execution, but they are hindered by excessive\ndata movement and dependence on CPU capabilities. In this paper, we introduce\nan efficient CPU-GPU I/O-aware LLM inference method that avoids transferring\nthe entire KV cache from CPU to GPU by recomputing partial KV cache from\nactivations while concurrently transferring the remaining KV cache via PCIe\nbus. This approach overlaps GPU recomputation with data transfer to minimize\nidle GPU time and maximize inference performance. Our method is fully automated\nby integrating a profiler module that utilizes input characteristics and system\nhardware information, a scheduler module to optimize the distribution of\ncomputation and communication workloads, and a runtime module to efficiently\nexecute the derived execution plan. Experimental results show that our method\nachieves up to 35.8% lower latency and 46.2% higher throughput during decoding\ncompared to state-of-the-art approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference for Large Language Models (LLMs) is computationally demanding. To\nreduce the cost of auto-regressive decoding, Key-Value (KV) caching is used to\nstore intermediate activations, enabling GPUs to perform only the incremental\ncomputation required for each new token. This approach significantly lowers the\ncomputational overhead for token generation. However, the memory required for\nKV caching grows rapidly, often exceeding the capacity of GPU memory. A\ncost-effective alternative is to offload KV cache to CPU memory, which\nalleviates GPU memory pressure but shifts the bottleneck to the limited\nbandwidth of the PCIe connection between the CPU and GPU. Existing methods\nattempt to address these issues by overlapping GPU computation with I/O or\nemploying CPU-GPU heterogeneous execution, but they are hindered by excessive\ndata movement and dependence on CPU capabilities. In this paper, we introduce\nan efficient CPU-GPU I/O-aware LLM inference method that avoids transferring\nthe entire KV cache from CPU to GPU by recomputing partial KV cache from\nactivations while concurrently transferring the remaining KV cache via PCIe\nbus. This approach overlaps GPU recomputation with data transfer to minimize\nidle GPU time and maximize inference performance. Our method is fully automated\nby integrating a profiler module that utilizes input characteristics and system\nhardware information, a scheduler module to optimize the distribution of\ncomputation and communication workloads, and a runtime module to efficiently\nexecute the derived execution plan. Experimental results show that our method\nachieves up to 35.8% lower latency and 46.2% higher throughput during decoding\ncompared to state-of-the-art approaches."
                },
                "authors": [
                    {
                        "name": "Chaoyi Jiang"
                    },
                    {
                        "name": "Lei Gao"
                    },
                    {
                        "name": "Hossein Entezari Zarch"
                    },
                    {
                        "name": "Murali Annavaram"
                    }
                ],
                "author_detail": {
                    "name": "Murali Annavaram"
                },
                "author": "Murali Annavaram",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17089v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17089v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16375v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16375v1",
                "updated": "2024-11-25T13:33:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    13,
                    33,
                    41,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T13:33:41Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    13,
                    33,
                    41,
                    0,
                    330,
                    0
                ],
                "title": "Ca2-VDM: Efficient Autoregressive Video Diffusion Model with Causal\n  Generation and Cache Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ca2-VDM: Efficient Autoregressive Video Diffusion Model with Causal\n  Generation and Cache Sharing"
                },
                "summary": "With the advance of diffusion models, today's video generation has achieved\nimpressive quality. To extend the generation length and facilitate real-world\napplications, a majority of video diffusion models (VDMs) generate videos in an\nautoregressive manner, i.e., generating subsequent clips conditioned on the\nlast frame(s) of the previous clip. However, existing autoregressive VDMs are\nhighly inefficient and redundant: The model must re-compute all the conditional\nframes that are overlapped between adjacent clips. This issue is exacerbated\nwhen the conditional frames are extended autoregressively to provide the model\nwith long-term context. In such cases, the computational demands increase\nsignificantly (i.e., with a quadratic complexity w.r.t. the autoregression\nstep). In this paper, we propose Ca2-VDM, an efficient autoregressive VDM with\nCausal generation and Cache sharing. For causal generation, it introduces\nunidirectional feature computation, which ensures that the cache of conditional\nframes can be precomputed in previous autoregression steps and reused in every\nsubsequent step, eliminating redundant computations. For cache sharing, it\nshares the cache across all denoising steps to avoid the huge cache storage\ncost. Extensive experiments demonstrated that our Ca2-VDM achieves\nstate-of-the-art quantitative and qualitative video generation results and\nsignificantly improves the generation speed. Code is available at\nhttps://github.com/Dawn-LX/CausalCache-VDM",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advance of diffusion models, today's video generation has achieved\nimpressive quality. To extend the generation length and facilitate real-world\napplications, a majority of video diffusion models (VDMs) generate videos in an\nautoregressive manner, i.e., generating subsequent clips conditioned on the\nlast frame(s) of the previous clip. However, existing autoregressive VDMs are\nhighly inefficient and redundant: The model must re-compute all the conditional\nframes that are overlapped between adjacent clips. This issue is exacerbated\nwhen the conditional frames are extended autoregressively to provide the model\nwith long-term context. In such cases, the computational demands increase\nsignificantly (i.e., with a quadratic complexity w.r.t. the autoregression\nstep). In this paper, we propose Ca2-VDM, an efficient autoregressive VDM with\nCausal generation and Cache sharing. For causal generation, it introduces\nunidirectional feature computation, which ensures that the cache of conditional\nframes can be precomputed in previous autoregression steps and reused in every\nsubsequent step, eliminating redundant computations. For cache sharing, it\nshares the cache across all denoising steps to avoid the huge cache storage\ncost. Extensive experiments demonstrated that our Ca2-VDM achieves\nstate-of-the-art quantitative and qualitative video generation results and\nsignificantly improves the generation speed. Code is available at\nhttps://github.com/Dawn-LX/CausalCache-VDM"
                },
                "authors": [
                    {
                        "name": "Kaifeng Gao"
                    },
                    {
                        "name": "Jiaxin Shi"
                    },
                    {
                        "name": "Hanwang Zhang"
                    },
                    {
                        "name": "Chunping Wang"
                    },
                    {
                        "name": "Jun Xiao"
                    },
                    {
                        "name": "Long Chen"
                    }
                ],
                "author_detail": {
                    "name": "Long Chen"
                },
                "author": "Long Chen",
                "arxiv_comment": "Technical Report. Code is available at\n  https://github.com/Dawn-LX/CausalCache-VDM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16375v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16375v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19315v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19315v2",
                "updated": "2024-11-25T12:14:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    12,
                    14,
                    33,
                    0,
                    330,
                    0
                ],
                "published": "2024-09-28T11:00:11Z",
                "published_parsed": [
                    2024,
                    9,
                    28,
                    11,
                    0,
                    11,
                    5,
                    272,
                    0
                ],
                "title": "Analog In-Memory Computing Attention Mechanism for Fast and\n  Energy-Efficient Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analog In-Memory Computing Attention Mechanism for Fast and\n  Energy-Efficient Large Language Models"
                },
                "summary": "Transformer networks, driven by self-attention, are central to Large Language\nModels. In generative Transformers, self-attention uses cache memory to store\ntoken projections, avoiding recomputation at each time step. However,\nGPU-stored projections must be loaded into SRAM for each new generation step,\ncausing latency and energy bottlenecks.\n  We present a custom self-attention in-memory computing architecture based on\nemerging charge-based memories called gain cells, which can be efficiently\nwritten to store new tokens during sequence generation and enable parallel\nanalog dot-product computation required for self-attention. However, the analog\ngain cell circuits introduce non-idealities and constraints preventing the\ndirect mapping of pre-trained models. To circumvent this problem, we design an\ninitialization algorithm achieving text processing performance comparable to\nGPT-2 without training from scratch. Our architecture respectively reduces\nattention latency and energy consumption by up to two and five orders of\nmagnitude compared to GPUs, marking a significant step toward ultra-fast,\nlow-power generative Transformers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer networks, driven by self-attention, are central to Large Language\nModels. In generative Transformers, self-attention uses cache memory to store\ntoken projections, avoiding recomputation at each time step. However,\nGPU-stored projections must be loaded into SRAM for each new generation step,\ncausing latency and energy bottlenecks.\n  We present a custom self-attention in-memory computing architecture based on\nemerging charge-based memories called gain cells, which can be efficiently\nwritten to store new tokens during sequence generation and enable parallel\nanalog dot-product computation required for self-attention. However, the analog\ngain cell circuits introduce non-idealities and constraints preventing the\ndirect mapping of pre-trained models. To circumvent this problem, we design an\ninitialization algorithm achieving text processing performance comparable to\nGPT-2 without training from scratch. Our architecture respectively reduces\nattention latency and energy consumption by up to two and five orders of\nmagnitude compared to GPUs, marking a significant step toward ultra-fast,\nlow-power generative Transformers."
                },
                "authors": [
                    {
                        "name": "Nathan Leroux"
                    },
                    {
                        "name": "Paul-Philipp Manea"
                    },
                    {
                        "name": "Chirag Sudarshan"
                    },
                    {
                        "name": "Jan Finkbeiner"
                    },
                    {
                        "name": "Sebastian Siegel"
                    },
                    {
                        "name": "John Paul Strachan"
                    },
                    {
                        "name": "Emre Neftci"
                    }
                ],
                "author_detail": {
                    "name": "Emre Neftci"
                },
                "author": "Emre Neftci",
                "arxiv_comment": "25 pages, 6 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19315v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19315v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11469v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11469v2",
                "updated": "2024-11-24T21:57:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    24,
                    21,
                    57,
                    29,
                    6,
                    329,
                    0
                ],
                "published": "2024-11-18T11:12:57Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    11,
                    12,
                    57,
                    0,
                    323,
                    0
                ],
                "title": "Deegen: A JIT-Capable VM Generator for Dynamic Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deegen: A JIT-Capable VM Generator for Dynamic Languages"
                },
                "summary": "Building a high-performance JIT-capable VM for a dynamic language has\ntraditionally required a tremendous amount of time, money, and expertise. We\npresent Deegen, a meta-compiler that allows users to generate a\nhigh-performance JIT-capable VM for their own language at an engineering cost\nsimilar to writing a simple interpreter. Deegen takes in the execution\nsemantics of the bytecodes implemented as C++ functions, and automatically\ngenerates a two-tier VM execution engine with a state-of-the-art interpreter, a\nstate-of-the-art baseline JIT, and the tier-switching logic that connects them\ninto a self-adaptive system.\n  We are the first to demonstrate the automatic generation of a JIT compiler,\nand the automatic generation of an interpreter that outperforms the state of\nthe art. Our performance comes from a long list of optimizations supported by\nDeegen, including bytecode specialization and quickening, register pinning, tag\nregister optimization, call inline caching, generic inline caching, JIT\npolymorphic IC, JIT IC inline slab, type-check removal and strength reduction,\ntype-based slow-path extraction and outlining, JIT hot-cold code splitting, and\nJIT OSR-entry. These optimizations are either employed automatically, or guided\nby the language implementer through intuitive APIs. As a result, the\ndisassembly of the Deegen-generated interpreter, baseline JIT, and the\ngenerated JIT code rivals the assembly code hand-written by experts in\nstate-of-the-art VMs.\n  We implement LuaJIT Remake (LJR), a standard-compliant Lua 5.1 VM, using\nDeegen. Across 44 benchmarks, LJR's interpreter is on average 179% faster than\nthe official PUC Lua interpreter, and 31% faster than LuaJIT's interpreter.\nLJR's baseline JIT has negligible startup delay, and its execution performance\nis on average 360% faster than PUC Lua and only 33% slower (but faster on 13/44\nbenchmarks) than LuaJIT's optimizing JIT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building a high-performance JIT-capable VM for a dynamic language has\ntraditionally required a tremendous amount of time, money, and expertise. We\npresent Deegen, a meta-compiler that allows users to generate a\nhigh-performance JIT-capable VM for their own language at an engineering cost\nsimilar to writing a simple interpreter. Deegen takes in the execution\nsemantics of the bytecodes implemented as C++ functions, and automatically\ngenerates a two-tier VM execution engine with a state-of-the-art interpreter, a\nstate-of-the-art baseline JIT, and the tier-switching logic that connects them\ninto a self-adaptive system.\n  We are the first to demonstrate the automatic generation of a JIT compiler,\nand the automatic generation of an interpreter that outperforms the state of\nthe art. Our performance comes from a long list of optimizations supported by\nDeegen, including bytecode specialization and quickening, register pinning, tag\nregister optimization, call inline caching, generic inline caching, JIT\npolymorphic IC, JIT IC inline slab, type-check removal and strength reduction,\ntype-based slow-path extraction and outlining, JIT hot-cold code splitting, and\nJIT OSR-entry. These optimizations are either employed automatically, or guided\nby the language implementer through intuitive APIs. As a result, the\ndisassembly of the Deegen-generated interpreter, baseline JIT, and the\ngenerated JIT code rivals the assembly code hand-written by experts in\nstate-of-the-art VMs.\n  We implement LuaJIT Remake (LJR), a standard-compliant Lua 5.1 VM, using\nDeegen. Across 44 benchmarks, LJR's interpreter is on average 179% faster than\nthe official PUC Lua interpreter, and 31% faster than LuaJIT's interpreter.\nLJR's baseline JIT has negligible startup delay, and its execution performance\nis on average 360% faster than PUC Lua and only 33% slower (but faster on 13/44\nbenchmarks) than LuaJIT's optimizing JIT."
                },
                "authors": [
                    {
                        "name": "Haoran Xu"
                    },
                    {
                        "name": "Fredrik Kjolstad"
                    }
                ],
                "author_detail": {
                    "name": "Fredrik Kjolstad"
                },
                "author": "Fredrik Kjolstad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11469v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11469v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17741v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17741v1",
                "updated": "2024-11-24T16:20:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    24,
                    16,
                    20,
                    57,
                    6,
                    329,
                    0
                ],
                "published": "2024-11-24T16:20:57Z",
                "published_parsed": [
                    2024,
                    11,
                    24,
                    16,
                    20,
                    57,
                    6,
                    329,
                    0
                ],
                "title": "Chameleon: Adaptive Caching and Scheduling for Many-Adapter LLM\n  Inference Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chameleon: Adaptive Caching and Scheduling for Many-Adapter LLM\n  Inference Environments"
                },
                "summary": "The widespread adoption of LLMs has driven an exponential rise in their\ndeployment, imposing substantial demands on inference clusters. These clusters\nmust handle numerous concurrent queries for different LLM downstream tasks. To\nhandle multi-task settings with vast LLM parameter counts, methods like\nLow-Rank Adaptation (LoRA) enable task-specific fine-tuning while sharing most\nof the base LLM model across tasks. Hence, they allow concurrent task serving\nwith minimal memory requirements. However, existing LLM serving systems face\ninefficiencies: they overlook workload heterogeneity, impose high link\nbandwidth from frequent adapter loading, and suffer from head-of-line blocking\nin their schedulers. To address these challenges, we present Chameleon, a novel\nLLM serving system optimized for many adapter environments, that relies on two\ncore ideas: adapter caching and adapter-aware scheduling. First, Chameleon\ncaches popular adapters in GPU memory, minimizing the adapter loading times.\nImportantly, it uses the otherwise idle GPU memory, avoiding extra memory\ncosts. Second, Chameleon uses a non-preemptive multi-queue scheduling to\nefficiently account for workload heterogeneity. In this way, Chameleon\nsimultaneously prevents head of line blocking and starvation. We implement\nChameleon on top of a state-of-the-art LLM serving platform and evaluate it\nwith real-world production traces and open-source LLMs. Under high loads,\nChameleon reduces P99 and P50 TTFT latency by 80.7% and 48.1%, respectively,\nwhile improving throughput by 1.5x compared to state-of-the-art baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread adoption of LLMs has driven an exponential rise in their\ndeployment, imposing substantial demands on inference clusters. These clusters\nmust handle numerous concurrent queries for different LLM downstream tasks. To\nhandle multi-task settings with vast LLM parameter counts, methods like\nLow-Rank Adaptation (LoRA) enable task-specific fine-tuning while sharing most\nof the base LLM model across tasks. Hence, they allow concurrent task serving\nwith minimal memory requirements. However, existing LLM serving systems face\ninefficiencies: they overlook workload heterogeneity, impose high link\nbandwidth from frequent adapter loading, and suffer from head-of-line blocking\nin their schedulers. To address these challenges, we present Chameleon, a novel\nLLM serving system optimized for many adapter environments, that relies on two\ncore ideas: adapter caching and adapter-aware scheduling. First, Chameleon\ncaches popular adapters in GPU memory, minimizing the adapter loading times.\nImportantly, it uses the otherwise idle GPU memory, avoiding extra memory\ncosts. Second, Chameleon uses a non-preemptive multi-queue scheduling to\nefficiently account for workload heterogeneity. In this way, Chameleon\nsimultaneously prevents head of line blocking and starvation. We implement\nChameleon on top of a state-of-the-art LLM serving platform and evaluate it\nwith real-world production traces and open-source LLMs. Under high loads,\nChameleon reduces P99 and P50 TTFT latency by 80.7% and 48.1%, respectively,\nwhile improving throughput by 1.5x compared to state-of-the-art baselines."
                },
                "authors": [
                    {
                        "name": "Nikoleta Iliakopoulou"
                    },
                    {
                        "name": "Jovan Stojkovic"
                    },
                    {
                        "name": "Chloe Alverti"
                    },
                    {
                        "name": "Tianyin Xu"
                    },
                    {
                        "name": "Hubertus Franke"
                    },
                    {
                        "name": "Josep Torrellas"
                    }
                ],
                "author_detail": {
                    "name": "Josep Torrellas"
                },
                "author": "Josep Torrellas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17741v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17741v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.0; D.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15735v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15735v1",
                "updated": "2024-11-24T06:43:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    24,
                    6,
                    43,
                    38,
                    6,
                    329,
                    0
                ],
                "published": "2024-11-24T06:43:38Z",
                "published_parsed": [
                    2024,
                    11,
                    24,
                    6,
                    43,
                    38,
                    6,
                    329,
                    0
                ],
                "title": "Test-time Alignment-Enhanced Adapter for Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time Alignment-Enhanced Adapter for Vision-Language Models"
                },
                "summary": "Test-time adaptation with pre-trained vision-language models (VLMs) has\nattracted increasing attention for tackling the issue of distribution shift\nduring the test phase. While prior methods have shown effectiveness in\naddressing distribution shift by adjusting classification logits, they are not\noptimal due to keeping text features unchanged. To address this issue, we\nintroduce a new approach called Test-time Alignment-Enhanced Adapter (TAEA),\nwhich trains an adapter with test samples to adjust text features during the\ntest phase. We can enhance the text-to-image alignment prediction by utilizing\nan adapter to adapt text features. Furthermore, we also propose to adopt the\nnegative cache from TDA as enhancement module, which further improves the\nperformance of TAEA. Our approach outperforms the state-of-the-art TTA method\nof pre-trained VLMs by an average of 0.75% on the out-of-distribution benchmark\nand 2.5% on the cross-domain benchmark, with an acceptable training time. Code\nwill be available at https://github.com/BaoshunWq/clip-TAEA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time adaptation with pre-trained vision-language models (VLMs) has\nattracted increasing attention for tackling the issue of distribution shift\nduring the test phase. While prior methods have shown effectiveness in\naddressing distribution shift by adjusting classification logits, they are not\noptimal due to keeping text features unchanged. To address this issue, we\nintroduce a new approach called Test-time Alignment-Enhanced Adapter (TAEA),\nwhich trains an adapter with test samples to adjust text features during the\ntest phase. We can enhance the text-to-image alignment prediction by utilizing\nan adapter to adapt text features. Furthermore, we also propose to adopt the\nnegative cache from TDA as enhancement module, which further improves the\nperformance of TAEA. Our approach outperforms the state-of-the-art TTA method\nof pre-trained VLMs by an average of 0.75% on the out-of-distribution benchmark\nand 2.5% on the cross-domain benchmark, with an acceptable training time. Code\nwill be available at https://github.com/BaoshunWq/clip-TAEA."
                },
                "authors": [
                    {
                        "name": "Baoshun Tong"
                    },
                    {
                        "name": "Kaiyu Song"
                    },
                    {
                        "name": "Hanjiang Lai"
                    }
                ],
                "author_detail": {
                    "name": "Hanjiang Lai"
                },
                "author": "Hanjiang Lai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15735v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15735v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09688v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09688v2",
                "updated": "2024-11-23T22:11:42Z",
                "updated_parsed": [
                    2024,
                    11,
                    23,
                    22,
                    11,
                    42,
                    5,
                    328,
                    0
                ],
                "published": "2024-11-14T18:54:19Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    18,
                    54,
                    19,
                    3,
                    319,
                    0
                ],
                "title": "Squeezed Attention: Accelerating Long Context Length LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Squeezed Attention: Accelerating Long Context Length LLM Inference"
                },
                "summary": "Emerging Large Language Model (LLM) applications require long input prompts\nto perform complex downstream tasks like document analysis and code generation.\nFor these long context length applications, the length of the input prompt\nposes a significant challenge in terms of inference efficiency since the\ninference costs increase linearly with sequence length. However, for many of\nthese applications, much of the context in the prompt is fixed across different\nuser inputs, thereby providing the opportunity to perform offline optimizations\nto process user inputs quickly, as they are received. In this work, we propose\nSqueezed Attention as a mechanism to accelerate LLM applications where a large\nportion of the input prompt is fixed. We first leverage K-means clustering\noffline to group the keys for the fixed context based on semantic similarity\nand represent each cluster with a single centroid value. During inference, we\ncompare query tokens from the user input with the centroids to predict which of\nthe keys from the fixed context are semantically relevant and need to be loaded\nduring inference. We then compute exact attention using only these important\nkeys from the fixed context, thereby reducing bandwidth and computational\ncosts. We also extend our method to use a hierarchical centroid lookup to\nidentify important keys, which can reduce the complexity of attention from\nlinear to logarithmic with respect to the context length. We implement\noptimized Triton kernels for centroid comparison and sparse FlashAttention with\nimportant keys, achieving more than 4x speedups during both the prefill and\ngeneration phases for long-context inference. Furthermore, we have extensively\nevaluated our method on various long-context benchmarks including LongBench,\nwhere it achieves a 3x reduction in KV cache budget without accuracy loss and\nup to an 8x reduction with <0.5 point accuracy gap for various models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging Large Language Model (LLM) applications require long input prompts\nto perform complex downstream tasks like document analysis and code generation.\nFor these long context length applications, the length of the input prompt\nposes a significant challenge in terms of inference efficiency since the\ninference costs increase linearly with sequence length. However, for many of\nthese applications, much of the context in the prompt is fixed across different\nuser inputs, thereby providing the opportunity to perform offline optimizations\nto process user inputs quickly, as they are received. In this work, we propose\nSqueezed Attention as a mechanism to accelerate LLM applications where a large\nportion of the input prompt is fixed. We first leverage K-means clustering\noffline to group the keys for the fixed context based on semantic similarity\nand represent each cluster with a single centroid value. During inference, we\ncompare query tokens from the user input with the centroids to predict which of\nthe keys from the fixed context are semantically relevant and need to be loaded\nduring inference. We then compute exact attention using only these important\nkeys from the fixed context, thereby reducing bandwidth and computational\ncosts. We also extend our method to use a hierarchical centroid lookup to\nidentify important keys, which can reduce the complexity of attention from\nlinear to logarithmic with respect to the context length. We implement\noptimized Triton kernels for centroid comparison and sparse FlashAttention with\nimportant keys, achieving more than 4x speedups during both the prefill and\ngeneration phases for long-context inference. Furthermore, we have extensively\nevaluated our method on various long-context benchmarks including LongBench,\nwhere it achieves a 3x reduction in KV cache budget without accuracy loss and\nup to an 8x reduction with <0.5 point accuracy gap for various models."
                },
                "authors": [
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Sehoon Kim"
                    },
                    {
                        "name": "Hiva Mohammadzadeh"
                    },
                    {
                        "name": "Monishwaran Maheswaran"
                    },
                    {
                        "name": "June Paik"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09688v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09688v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.05396v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.05396v3",
                "updated": "2024-11-23T10:42:11Z",
                "updated_parsed": [
                    2024,
                    11,
                    23,
                    10,
                    42,
                    11,
                    5,
                    328,
                    0
                ],
                "published": "2024-02-08T04:16:35Z",
                "published_parsed": [
                    2024,
                    2,
                    8,
                    4,
                    16,
                    35,
                    3,
                    39,
                    0
                ],
                "title": "TASER: Temporal Adaptive Sampling for Fast and Accurate Dynamic Graph\n  Representation Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TASER: Temporal Adaptive Sampling for Fast and Accurate Dynamic Graph\n  Representation Learning"
                },
                "summary": "Recently, Temporal Graph Neural Networks (TGNNs) have demonstrated\nstate-of-the-art performance in various high-impact applications, including\nfraud detection and content recommendation. Despite the success of TGNNs, they\nare prone to the prevalent noise found in real-world dynamic graphs like\ntime-deprecated links and skewed interaction distribution. The noise causes two\ncritical issues that significantly compromise the accuracy of TGNNs: (1) models\nare supervised by inferior interactions, and (2) noisy input induces high\nvariance in the aggregated messages. However, current TGNN denoising techniques\ndo not consider the diverse and dynamic noise pattern of each node. In\naddition, they also suffer from the excessive mini-batch generation overheads\ncaused by traversing more neighbors. We believe the remedy for fast and\naccurate TGNNs lies in temporal adaptive sampling. In this work, we propose\nTASER, the first adaptive sampling method for TGNNs optimized for accuracy,\nefficiency, and scalability. TASER adapts its mini-batch selection based on\ntraining dynamics and temporal neighbor selection based on the contextual,\nstructural, and temporal properties of past interactions. To alleviate the\nbottleneck in mini-batch generation, TASER implements a pure GPU-based temporal\nneighbor finder and a dedicated GPU feature cache. We evaluate the performance\nof TASER using two state-of-the-art backbone TGNNs. On five popular datasets,\nTASER outperforms the corresponding baselines by an average of 2.3% in Mean\nReciprocal Rank (MRR) while achieving an average of 5.1x speedup in training\ntime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Temporal Graph Neural Networks (TGNNs) have demonstrated\nstate-of-the-art performance in various high-impact applications, including\nfraud detection and content recommendation. Despite the success of TGNNs, they\nare prone to the prevalent noise found in real-world dynamic graphs like\ntime-deprecated links and skewed interaction distribution. The noise causes two\ncritical issues that significantly compromise the accuracy of TGNNs: (1) models\nare supervised by inferior interactions, and (2) noisy input induces high\nvariance in the aggregated messages. However, current TGNN denoising techniques\ndo not consider the diverse and dynamic noise pattern of each node. In\naddition, they also suffer from the excessive mini-batch generation overheads\ncaused by traversing more neighbors. We believe the remedy for fast and\naccurate TGNNs lies in temporal adaptive sampling. In this work, we propose\nTASER, the first adaptive sampling method for TGNNs optimized for accuracy,\nefficiency, and scalability. TASER adapts its mini-batch selection based on\ntraining dynamics and temporal neighbor selection based on the contextual,\nstructural, and temporal properties of past interactions. To alleviate the\nbottleneck in mini-batch generation, TASER implements a pure GPU-based temporal\nneighbor finder and a dedicated GPU feature cache. We evaluate the performance\nof TASER using two state-of-the-art backbone TGNNs. On five popular datasets,\nTASER outperforms the corresponding baselines by an average of 2.3% in Mean\nReciprocal Rank (MRR) while achieving an average of 5.1x speedup in training\ntime."
                },
                "authors": [
                    {
                        "name": "Gangda Deng"
                    },
                    {
                        "name": "Hongkuan Zhou"
                    },
                    {
                        "name": "Hanqing Zeng"
                    },
                    {
                        "name": "Yinglong Xia"
                    },
                    {
                        "name": "Christopher Leung"
                    },
                    {
                        "name": "Jianbo Li"
                    },
                    {
                        "name": "Rajgopal Kannan"
                    },
                    {
                        "name": "Viktor Prasanna"
                    }
                ],
                "author_detail": {
                    "name": "Viktor Prasanna"
                },
                "author": "Viktor Prasanna",
                "arxiv_comment": "IPDPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.05396v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.05396v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02109v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02109v2",
                "updated": "2024-11-23T01:44:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    23,
                    1,
                    44,
                    0,
                    5,
                    328,
                    0
                ],
                "published": "2024-07-02T09:51:56Z",
                "published_parsed": [
                    2024,
                    7,
                    2,
                    9,
                    51,
                    56,
                    1,
                    184,
                    0
                ],
                "title": "HRSAM: Efficient Interactive Segmentation in High-Resolution Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HRSAM: Efficient Interactive Segmentation in High-Resolution Images"
                },
                "summary": "The Segment Anything Model (SAM) has advanced interactive segmentation but is\nlimited by the high computational cost on high-resolution images. This requires\ndownsampling to meet GPU constraints, sacrificing the fine-grained details\nneeded for high-precision interactive segmentation. To address SAM's\nlimitations, we focus on visual length extrapolation and propose a lightweight\nmodel named HRSAM. The extrapolation enables HRSAM trained on low resolutions\nto generalize to high resolutions. We begin by finding the link between the\nextrapolation and attention scores, which leads us to base HRSAM on Swin\nattention. We then introduce the Flexible Local Attention (FLA) framework,\nusing CUDA-optimized Efficient Memory Attention to accelerate HRSAM. Within\nFLA, we implement Flash Swin attention, achieving over a 35% speedup compared\nto traditional Swin attention, and propose a KV-only padding mechanism to\nenhance extrapolation. We also develop the Cycle-scan module that uses State\nSpace models to efficiently expand HRSAM's receptive field. We further develop\nthe HRSAM++ within FLA by adding an anchor map, providing multi-scale data\naugmentation for the extrapolation and a larger receptive field at slight\ncomputational cost. Experiments show that, under standard training, HRSAMs\nsurpass the previous SOTA with only 38% of the latency. With SAM-distillation,\nthe extrapolation enables HRSAMs to outperform the teacher model at lower\nlatency. Further finetuning achieves performance significantly exceeding the\nprevious SOTA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Segment Anything Model (SAM) has advanced interactive segmentation but is\nlimited by the high computational cost on high-resolution images. This requires\ndownsampling to meet GPU constraints, sacrificing the fine-grained details\nneeded for high-precision interactive segmentation. To address SAM's\nlimitations, we focus on visual length extrapolation and propose a lightweight\nmodel named HRSAM. The extrapolation enables HRSAM trained on low resolutions\nto generalize to high resolutions. We begin by finding the link between the\nextrapolation and attention scores, which leads us to base HRSAM on Swin\nattention. We then introduce the Flexible Local Attention (FLA) framework,\nusing CUDA-optimized Efficient Memory Attention to accelerate HRSAM. Within\nFLA, we implement Flash Swin attention, achieving over a 35% speedup compared\nto traditional Swin attention, and propose a KV-only padding mechanism to\nenhance extrapolation. We also develop the Cycle-scan module that uses State\nSpace models to efficiently expand HRSAM's receptive field. We further develop\nthe HRSAM++ within FLA by adding an anchor map, providing multi-scale data\naugmentation for the extrapolation and a larger receptive field at slight\ncomputational cost. Experiments show that, under standard training, HRSAMs\nsurpass the previous SOTA with only 38% of the latency. With SAM-distillation,\nthe extrapolation enables HRSAMs to outperform the teacher model at lower\nlatency. Further finetuning achieves performance significantly exceeding the\nprevious SOTA."
                },
                "authors": [
                    {
                        "name": "You Huang"
                    },
                    {
                        "name": "Wenbin Lai"
                    },
                    {
                        "name": "Jiayi Ji"
                    },
                    {
                        "name": "Liujuan Cao"
                    },
                    {
                        "name": "Shengchuan Zhang"
                    },
                    {
                        "name": "Rongrong Ji"
                    }
                ],
                "author_detail": {
                    "name": "Rongrong Ji"
                },
                "author": "Rongrong Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02109v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02109v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15322v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15322v1",
                "updated": "2024-11-22T19:30:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    19,
                    30,
                    40,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T19:30:40Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    19,
                    30,
                    40,
                    4,
                    327,
                    0
                ],
                "title": "Deep Learning-Based Automatic Delineation of Liver Domes in kV Triggered\n  Images for Online Breath-hold Reproducibility Verification of Liver\n  Stereotactic Body Radiation Therapy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning-Based Automatic Delineation of Liver Domes in kV Triggered\n  Images for Online Breath-hold Reproducibility Verification of Liver\n  Stereotactic Body Radiation Therapy"
                },
                "summary": "Stereotactic Body Radiation Therapy (SBRT) can be a precise, minimally\ninvasive treatment method for liver cancer and liver metastases. However, the\neffectiveness of SBRT relies on the accurate delivery of the dose to the tumor\nwhile sparing healthy tissue. Challenges persist in ensuring breath-hold\nreproducibility, with current methods often requiring manual verification of\nliver dome positions from kV-triggered images. To address this, we propose a\nproof-of-principle study of a deep learning-based pipeline to automatically\ndelineate the liver dome from kV-planar images. From 24 patients who received\nSBRT for liver cancer or metastasis inside liver, 711 KV-triggered images\nacquired for online breath-hold verification were included in the current\nstudy. We developed a pipeline comprising a trained U-Net for automatic liver\ndome region segmentation from the triggered images followed by extraction of\nthe liver dome via thresholding, edge detection, and morphological operations.\nThe performance and generalizability of the pipeline was evaluated using 2-fold\ncross validation. The training of the U-Net model for liver region segmentation\ntook under 30 minutes and the automatic delineation of a liver dome for any\ntriggered image took less than one second. The RMSE and rate of detection for\nFold1 with 366 images was (6.4 +/- 1.6) mm and 91.7%, respectively. For Fold2\nwith 345 images, the RMSE and rate of detection was (7.7 +/- 2.3) mm and 76.3%\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stereotactic Body Radiation Therapy (SBRT) can be a precise, minimally\ninvasive treatment method for liver cancer and liver metastases. However, the\neffectiveness of SBRT relies on the accurate delivery of the dose to the tumor\nwhile sparing healthy tissue. Challenges persist in ensuring breath-hold\nreproducibility, with current methods often requiring manual verification of\nliver dome positions from kV-triggered images. To address this, we propose a\nproof-of-principle study of a deep learning-based pipeline to automatically\ndelineate the liver dome from kV-planar images. From 24 patients who received\nSBRT for liver cancer or metastasis inside liver, 711 KV-triggered images\nacquired for online breath-hold verification were included in the current\nstudy. We developed a pipeline comprising a trained U-Net for automatic liver\ndome region segmentation from the triggered images followed by extraction of\nthe liver dome via thresholding, edge detection, and morphological operations.\nThe performance and generalizability of the pipeline was evaluated using 2-fold\ncross validation. The training of the U-Net model for liver region segmentation\ntook under 30 minutes and the automatic delineation of a liver dome for any\ntriggered image took less than one second. The RMSE and rate of detection for\nFold1 with 366 images was (6.4 +/- 1.6) mm and 91.7%, respectively. For Fold2\nwith 345 images, the RMSE and rate of detection was (7.7 +/- 2.3) mm and 76.3%\nrespectively."
                },
                "authors": [
                    {
                        "name": "Sugandima Weragoda"
                    },
                    {
                        "name": "Ping Xia"
                    },
                    {
                        "name": "Kevin Stephans"
                    },
                    {
                        "name": "Neil Woody"
                    },
                    {
                        "name": "Michael Martens"
                    },
                    {
                        "name": "Robert Brown"
                    },
                    {
                        "name": "Bingqi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Bingqi Guo"
                },
                "author": "Bingqi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15322v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15322v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15102v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15102v1",
                "updated": "2024-11-22T18:06:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    18,
                    6,
                    14,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T18:06:14Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    18,
                    6,
                    14,
                    4,
                    327,
                    0
                ],
                "title": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution"
                },
                "summary": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods."
                },
                "authors": [
                    {
                        "name": "Fengyuan Liu"
                    },
                    {
                        "name": "Nikhil Kandpal"
                    },
                    {
                        "name": "Colin Raffel"
                    }
                ],
                "author_detail": {
                    "name": "Colin Raffel"
                },
                "author": "Colin Raffel",
                "arxiv_comment": "29 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15102v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15102v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15024v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15024v1",
                "updated": "2024-11-22T15:55:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    55,
                    19,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T15:55:19Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    55,
                    19,
                    4,
                    327,
                    0
                ],
                "title": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models"
                },
                "summary": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training."
                },
                "authors": [
                    {
                        "name": "Keda Tao"
                    },
                    {
                        "name": "Can Qin"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "arxiv_comment": "12 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15024v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15024v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.04032v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.04032v5",
                "updated": "2024-11-21T05:55:43Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    5,
                    55,
                    43,
                    3,
                    326,
                    0
                ],
                "published": "2024-02-06T14:26:22Z",
                "published_parsed": [
                    2024,
                    2,
                    6,
                    14,
                    26,
                    22,
                    1,
                    37,
                    0
                ],
                "title": "ProactivePIM: Accelerating Weight-Sharing Embedding Layer with PIM for\n  Scalable Recommendation System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProactivePIM: Accelerating Weight-Sharing Embedding Layer with PIM for\n  Scalable Recommendation System"
                },
                "summary": "The model size growth of personalized recommendation systems poses new\nchallenges for inference. Weight-sharing algorithms have been proposed for size\nreduction, but they increase memory access. Recent advancements in\nprocessing-in-memory (PIM) enhanced the model throughput by exploiting memory\nparallelism, but such algorithms introduce massive CPU-PIM communication into\nprior PIM systems. We propose ProactivePIM, a PIM system for weight-sharing\nrecommendation system acceleration. ProactivePIM integrates a cache within the\nPIM with a prefetching scheme to leverage a unique locality of the algorithm\nand eliminate communication overhead through a subtable mapping strategy.\nProactivePIM achieves a 4.8x speedup compared to prior works.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The model size growth of personalized recommendation systems poses new\nchallenges for inference. Weight-sharing algorithms have been proposed for size\nreduction, but they increase memory access. Recent advancements in\nprocessing-in-memory (PIM) enhanced the model throughput by exploiting memory\nparallelism, but such algorithms introduce massive CPU-PIM communication into\nprior PIM systems. We propose ProactivePIM, a PIM system for weight-sharing\nrecommendation system acceleration. ProactivePIM integrates a cache within the\nPIM with a prefetching scheme to leverage a unique locality of the algorithm\nand eliminate communication overhead through a subtable mapping strategy.\nProactivePIM achieves a 4.8x speedup compared to prior works."
                },
                "authors": [
                    {
                        "name": "Youngsuk Kim"
                    },
                    {
                        "name": "Junghwan Lim"
                    },
                    {
                        "name": "Hyuk-Jae Lee"
                    },
                    {
                        "name": "Chae Eun Rhee"
                    }
                ],
                "author_detail": {
                    "name": "Chae Eun Rhee"
                },
                "author": "Chae Eun Rhee",
                "arxiv_comment": "8 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.04032v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.04032v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13854v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13854v1",
                "updated": "2024-11-21T05:26:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    5,
                    26,
                    57,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T05:26:57Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    5,
                    26,
                    57,
                    3,
                    326,
                    0
                ],
                "title": "Static Reuse Profile Estimation for Array Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Static Reuse Profile Estimation for Array Applications"
                },
                "summary": "Reuse distance analysis is a widely recognized method for application\ncharacterization that illustrates cache locality. Although there are various\ntechniques to calculate the reuse profile from dynamic memory traces, it is\nboth time and space-consuming due to the requirement to collect dynamic memory\ntraces at runtime. In contrast, static analysis reuse profile estimation is a\npromisingly faster approach since it is calculated at compile time without\nrunning the program or collecting memory traces. This work presents a static\nanalysis technique to estimate the reuse profile of loop-based programs. For an\ninput program, we generate a basic block-level control flow graph and the\nexecution count by analyzing the LLVM IR of the program. We present the memory\naccesses of the application kernel in a compact bracketed format and use a\nrecursive algorithm to predict the reuse distance histogram. We deploy a\nseparate predictor that unrolls the loop(s) for smaller bounds and generates a\ntemporary reuse distance profile for those small cases. Using these smaller\nprofiles, the reuse profile is extrapolated for the actual loop bound(s). We\nuse this reuse profile to predict the cache hit rate. Results show that our\nmodel can predict cache hit rates with an average accuracy of 95% relative to\nthe dynamic reuse profile methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reuse distance analysis is a widely recognized method for application\ncharacterization that illustrates cache locality. Although there are various\ntechniques to calculate the reuse profile from dynamic memory traces, it is\nboth time and space-consuming due to the requirement to collect dynamic memory\ntraces at runtime. In contrast, static analysis reuse profile estimation is a\npromisingly faster approach since it is calculated at compile time without\nrunning the program or collecting memory traces. This work presents a static\nanalysis technique to estimate the reuse profile of loop-based programs. For an\ninput program, we generate a basic block-level control flow graph and the\nexecution count by analyzing the LLVM IR of the program. We present the memory\naccesses of the application kernel in a compact bracketed format and use a\nrecursive algorithm to predict the reuse distance histogram. We deploy a\nseparate predictor that unrolls the loop(s) for smaller bounds and generates a\ntemporary reuse distance profile for those small cases. Using these smaller\nprofiles, the reuse profile is extrapolated for the actual loop bound(s). We\nuse this reuse profile to predict the cache hit rate. Results show that our\nmodel can predict cache hit rates with an average accuracy of 95% relative to\nthe dynamic reuse profile methods."
                },
                "authors": [
                    {
                        "name": "Abdur Razzak"
                    },
                    {
                        "name": "Atanu Barai"
                    },
                    {
                        "name": "Nandakishore Santhi"
                    },
                    {
                        "name": "Abdel-Hameed A. Badawy"
                    }
                ],
                "author_detail": {
                    "name": "Abdel-Hameed A. Badawy"
                },
                "author": "Abdel-Hameed A. Badawy",
                "arxiv_comment": "Accepted in The International Symposium on Memory Systems (MEMSYS\n  24), September 30 to October 03, 2024, Washington, DC, USA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13854v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13854v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.02243v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.02243v3",
                "updated": "2024-11-21T04:12:53Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    4,
                    12,
                    53,
                    3,
                    326,
                    0
                ],
                "published": "2023-06-04T03:06:37Z",
                "published_parsed": [
                    2023,
                    6,
                    4,
                    3,
                    6,
                    37,
                    6,
                    155,
                    0
                ],
                "title": "Retrieval-Enhanced Visual Prompt Learning for Few-shot Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Enhanced Visual Prompt Learning for Few-shot Classification"
                },
                "summary": "The Contrastive Language-Image Pretraining (CLIP) model has been widely used\nin various downstream vision tasks. The few-shot learning paradigm has been\nwidely adopted to augment its capacity for these tasks. However, current\nparadigms may struggle with fine-grained classification, such as satellite\nimage recognition, due to widening domain gaps. To address this limitation, we\npropose retrieval-enhanced visual prompt learning (RePrompt), which introduces\nretrieval mechanisms to cache and reuse the knowledge of downstream tasks.\nRePrompt constructs a retrieval database from either training examples or\nexternal data if available, and uses a retrieval mechanism to enhance multiple\nstages of a simple prompt learning baseline, thus narrowing the domain gap.\nDuring inference, our enhanced model can reference similar samples brought by\nretrieval to make more accurate predictions. A detailed analysis reveals that\nretrieval helps to improve the distribution of late features, thus, improving\ngeneralization for downstream tasks. Reprompt attains state-of-the-art\nperformance on a wide range of vision datasets, including 11 image datasets, 3\nvideo datasets, 1 multi-view dataset, and 4 domain generalization benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Contrastive Language-Image Pretraining (CLIP) model has been widely used\nin various downstream vision tasks. The few-shot learning paradigm has been\nwidely adopted to augment its capacity for these tasks. However, current\nparadigms may struggle with fine-grained classification, such as satellite\nimage recognition, due to widening domain gaps. To address this limitation, we\npropose retrieval-enhanced visual prompt learning (RePrompt), which introduces\nretrieval mechanisms to cache and reuse the knowledge of downstream tasks.\nRePrompt constructs a retrieval database from either training examples or\nexternal data if available, and uses a retrieval mechanism to enhance multiple\nstages of a simple prompt learning baseline, thus narrowing the domain gap.\nDuring inference, our enhanced model can reference similar samples brought by\nretrieval to make more accurate predictions. A detailed analysis reveals that\nretrieval helps to improve the distribution of late features, thus, improving\ngeneralization for downstream tasks. Reprompt attains state-of-the-art\nperformance on a wide range of vision datasets, including 11 image datasets, 3\nvideo datasets, 1 multi-view dataset, and 4 domain generalization benchmarks."
                },
                "authors": [
                    {
                        "name": "Jintao Rong"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Linlin Ou"
                    },
                    {
                        "name": "Tianxiao Chen"
                    },
                    {
                        "name": "Xinyi Yu"
                    },
                    {
                        "name": "Yifan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yifan Liu"
                },
                "author": "Yifan Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.02243v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.02243v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13820v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13820v1",
                "updated": "2024-11-21T03:52:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    3,
                    52,
                    41,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T03:52:41Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    3,
                    52,
                    41,
                    3,
                    326,
                    0
                ],
                "title": "InstCache: A Predictive Cache for LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InstCache: A Predictive Cache for LLM Serving"
                },
                "summary": "Large language models are revolutionizing every aspect of human life.\nHowever, the unprecedented power comes at the cost of significant computing\nintensity, suggesting long latency and large energy footprint. Key-Value Cache\nand Semantic Cache have been proposed as a solution to the above problem, but\nboth suffer from limited scalability due to significant memory cost for each\ntoken or instruction embeddings. Motivated by the observations that most\ninstructions are short, repetitive and predictable by LLMs, we propose to\npredict user-instructions by an instruction-aligned LLM and store them in a\npredictive cache, so-called InstCache. We introduce an instruction\npre-population algorithm based on the negative log likelihood of instructions,\ndetermining the cache size with regard to the hit rate. The proposed InstCache\nis efficiently implemented as a hash table with minimal lookup latency for\ndeployment. Experimental results show that InstCache can achieve up to 51.34%\nhit rate on LMSys dataset, which corresponds to a 2x speedup, at a memory cost\nof only 4.5GB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models are revolutionizing every aspect of human life.\nHowever, the unprecedented power comes at the cost of significant computing\nintensity, suggesting long latency and large energy footprint. Key-Value Cache\nand Semantic Cache have been proposed as a solution to the above problem, but\nboth suffer from limited scalability due to significant memory cost for each\ntoken or instruction embeddings. Motivated by the observations that most\ninstructions are short, repetitive and predictable by LLMs, we propose to\npredict user-instructions by an instruction-aligned LLM and store them in a\npredictive cache, so-called InstCache. We introduce an instruction\npre-population algorithm based on the negative log likelihood of instructions,\ndetermining the cache size with regard to the hit rate. The proposed InstCache\nis efficiently implemented as a hash table with minimal lookup latency for\ndeployment. Experimental results show that InstCache can achieve up to 51.34%\nhit rate on LMSys dataset, which corresponds to a 2x speedup, at a memory cost\nof only 4.5GB."
                },
                "authors": [
                    {
                        "name": "Longwei Zou"
                    },
                    {
                        "name": "Tingfeng Liu"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Jiangang Kong"
                    },
                    {
                        "name": "Yangdong Deng"
                    }
                ],
                "author_detail": {
                    "name": "Yangdong Deng"
                },
                "author": "Yangdong Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13820v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13820v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22649v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22649v2",
                "updated": "2024-11-21T03:34:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    3,
                    34,
                    44,
                    3,
                    326,
                    0
                ],
                "published": "2024-10-30T02:36:55Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    2,
                    36,
                    55,
                    2,
                    304,
                    0
                ],
                "title": "WaveRoRA: Wavelet Rotary Route Attention for Multivariate Time Series\n  Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WaveRoRA: Wavelet Rotary Route Attention for Multivariate Time Series\n  Forecasting"
                },
                "summary": "In recent years, Transformer-based models (Transformers) have achieved\nsignificant success in multivariate time series forecasting (MTSF). However,\nprevious works focus on extracting features either from the time domain or the\nfrequency domain, which inadequately captures the trends and periodic\ncharacteristics. To address this issue, we propose a wavelet learning framework\nto model complex temporal dependencies of the time series data. The wavelet\ndomain integrates both time and frequency information, allowing for the\nanalysis of local characteristics of signals at different scales. Additionally,\nthe Softmax self-attention mechanism used by Transformers has quadratic\ncomplexity, which leads to excessive computational costs when capturing\nlong-term dependencies. Therefore, we propose a novel attention mechanism:\nRotary Route Attention (RoRA). Unlike Softmax attention, RoRA utilizes rotary\nposition embeddings to inject relative positional information to sequence\ntokens and introduces a small number of routing tokens $r$ to aggregate\ninformation from the $KV$ matrices and redistribute it to the $Q$ matrix,\noffering linear complexity. We further propose WaveRoRA, which leverages RoRA\nto capture inter-series dependencies in the wavelet domain. We conduct\nextensive experiments on eight real-world datasets. The results indicate that\nWaveRoRA outperforms existing state-of-the-art models while maintaining lower\ncomputational costs. Our code is available at\nhttps://github.com/Leopold2333/WaveRoRA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Transformer-based models (Transformers) have achieved\nsignificant success in multivariate time series forecasting (MTSF). However,\nprevious works focus on extracting features either from the time domain or the\nfrequency domain, which inadequately captures the trends and periodic\ncharacteristics. To address this issue, we propose a wavelet learning framework\nto model complex temporal dependencies of the time series data. The wavelet\ndomain integrates both time and frequency information, allowing for the\nanalysis of local characteristics of signals at different scales. Additionally,\nthe Softmax self-attention mechanism used by Transformers has quadratic\ncomplexity, which leads to excessive computational costs when capturing\nlong-term dependencies. Therefore, we propose a novel attention mechanism:\nRotary Route Attention (RoRA). Unlike Softmax attention, RoRA utilizes rotary\nposition embeddings to inject relative positional information to sequence\ntokens and introduces a small number of routing tokens $r$ to aggregate\ninformation from the $KV$ matrices and redistribute it to the $Q$ matrix,\noffering linear complexity. We further propose WaveRoRA, which leverages RoRA\nto capture inter-series dependencies in the wavelet domain. We conduct\nextensive experiments on eight real-world datasets. The results indicate that\nWaveRoRA outperforms existing state-of-the-art models while maintaining lower\ncomputational costs. Our code is available at\nhttps://github.com/Leopold2333/WaveRoRA."
                },
                "authors": [
                    {
                        "name": "Aobo Liang"
                    },
                    {
                        "name": "Yan Sun"
                    },
                    {
                        "name": "Nadra Guizani"
                    }
                ],
                "author_detail": {
                    "name": "Nadra Guizani"
                },
                "author": "Nadra Guizani",
                "arxiv_comment": "Model architecture changed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22649v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22649v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13786v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13786v1",
                "updated": "2024-11-21T02:15:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    2,
                    15,
                    52,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T02:15:52Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    2,
                    15,
                    52,
                    3,
                    326,
                    0
                ],
                "title": "Adaptable Embeddings Network (AEN)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptable Embeddings Network (AEN)"
                },
                "summary": "Modern day Language Models see extensive use in text classification, yet this\ncomes at significant computational cost. Compute-effective classification\nmodels are needed for low-resource environments, most notably on edge devices.\nWe introduce Adaptable Embeddings Networks (AEN), a novel dual-encoder\narchitecture using Kernel Density Estimation (KDE). This architecture allows\nfor runtime adaptation of classification criteria without retraining and is\nnon-autoregressive. Through thorough synthetic data experimentation, we\ndemonstrate our model outputs comparable and in certain cases superior results\nto that of autoregressive models an order of magnitude larger than AEN's size.\nThe architecture's ability to preprocess and cache condition embeddings makes\nit ideal for edge computing applications and real-time monitoring systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern day Language Models see extensive use in text classification, yet this\ncomes at significant computational cost. Compute-effective classification\nmodels are needed for low-resource environments, most notably on edge devices.\nWe introduce Adaptable Embeddings Networks (AEN), a novel dual-encoder\narchitecture using Kernel Density Estimation (KDE). This architecture allows\nfor runtime adaptation of classification criteria without retraining and is\nnon-autoregressive. Through thorough synthetic data experimentation, we\ndemonstrate our model outputs comparable and in certain cases superior results\nto that of autoregressive models an order of magnitude larger than AEN's size.\nThe architecture's ability to preprocess and cache condition embeddings makes\nit ideal for edge computing applications and real-time monitoring systems."
                },
                "authors": [
                    {
                        "name": "Stan Loosmore"
                    },
                    {
                        "name": "Alexander Titus"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Titus"
                },
                "author": "Alexander Titus",
                "arxiv_comment": "20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13786v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13786v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13676v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13676v1",
                "updated": "2024-11-20T19:51:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    19,
                    51,
                    25,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T19:51:25Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    19,
                    51,
                    25,
                    2,
                    325,
                    0
                ],
                "title": "Hymba: A Hybrid-head Architecture for Small Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hymba: A Hybrid-head Architecture for Small Language Models"
                },
                "summary": "We propose Hymba, a family of small language models featuring a hybrid-head\nparallel architecture that integrates transformer attention mechanisms with\nstate space models (SSMs) for enhanced efficiency. Attention heads provide\nhigh-resolution recall, while SSM heads enable efficient context summarization.\nAdditionally, we introduce learnable meta tokens that are prepended to prompts,\nstoring critical information and alleviating the \"forced-to-attend\" burden\nassociated with attention mechanisms. This model is further optimized by\nincorporating cross-layer key-value (KV) sharing and partial sliding window\nattention, resulting in a compact cache size. During development, we conducted\na controlled study comparing various architectures under identical settings and\nobserved significant advantages of our proposed architecture. Notably, Hymba\nachieves state-of-the-art results for small LMs: Our Hymba-1.5B-Base model\nsurpasses all sub-2B public models in performance and even outperforms\nLlama-3.2-3B with 1.32% higher average accuracy, an 11.67x cache size\nreduction, and 3.49x throughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Hymba, a family of small language models featuring a hybrid-head\nparallel architecture that integrates transformer attention mechanisms with\nstate space models (SSMs) for enhanced efficiency. Attention heads provide\nhigh-resolution recall, while SSM heads enable efficient context summarization.\nAdditionally, we introduce learnable meta tokens that are prepended to prompts,\nstoring critical information and alleviating the \"forced-to-attend\" burden\nassociated with attention mechanisms. This model is further optimized by\nincorporating cross-layer key-value (KV) sharing and partial sliding window\nattention, resulting in a compact cache size. During development, we conducted\na controlled study comparing various architectures under identical settings and\nobserved significant advantages of our proposed architecture. Notably, Hymba\nachieves state-of-the-art results for small LMs: Our Hymba-1.5B-Base model\nsurpasses all sub-2B public models in performance and even outperforms\nLlama-3.2-3B with 1.32% higher average accuracy, an 11.67x cache size\nreduction, and 3.49x throughput."
                },
                "authors": [
                    {
                        "name": "Xin Dong"
                    },
                    {
                        "name": "Yonggan Fu"
                    },
                    {
                        "name": "Shizhe Diao"
                    },
                    {
                        "name": "Wonmin Byeon"
                    },
                    {
                        "name": "Zijia Chen"
                    },
                    {
                        "name": "Ameya Sunil Mahabaleshwarkar"
                    },
                    {
                        "name": "Shih-Yang Liu"
                    },
                    {
                        "name": "Matthijs Van Keirsbilck"
                    },
                    {
                        "name": "Min-Hung Chen"
                    },
                    {
                        "name": "Yoshi Suhara"
                    },
                    {
                        "name": "Yingyan Lin"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    }
                ],
                "author_detail": {
                    "name": "Pavlo Molchanov"
                },
                "author": "Pavlo Molchanov",
                "arxiv_comment": "20 pages, models are available on huggingface",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13676v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13676v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17720v1",
                "updated": "2024-11-20T19:44:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    19,
                    44,
                    26,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T19:44:26Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    19,
                    44,
                    26,
                    2,
                    325,
                    0
                ],
                "title": "MAS-Attention: Memory-Aware Stream Processing for Attention Acceleration\n  on Resource-Constrained Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAS-Attention: Memory-Aware Stream Processing for Attention Acceleration\n  on Resource-Constrained Edge Devices"
                },
                "summary": "The advent of foundation models have revolutionized various fields, enabling\nunprecedented task accuracy and flexibility in computational linguistics,\ncomputer vision and other domains. Attention mechanism has become an essential\ncomponent of foundation models, due to their superb capability of capturing\ncorrelations in a sequence. However, attention results in quadratic complexity\nin memory and compute as the context length grows. Although many fusion-based\nexact attention acceleration algorithms have been developed for\ndatacenter-grade GPUs and accelerators leveraging multi-core parallelism and\ndata locality, yet it remains a significant challenge to accelerate attention\non resource-constrained edge neural accelerators with limited compute units and\nstringent on-chip caches. In this paper, we propose a scheme for exact\nattention inference acceleration on memory-constrained edge accelerators, by\nparallelizing the utilization of heterogeneous compute units, i.e., vector\nprocessing units and matrix processing units. Our method involves scheduling\nworkloads onto these different compute units in a multi-tiered tiling scheme to\nprocess tiled vector workloads and matrix workloads in attention as two\nstreams, respecting the workload dependencies. We search for tiling factors to\nmaximize the parallelization of both compute units while considering I/O\noverhead, and propose a proactive cache overwrite strategy to avoid undesirable\ncache spills in reality. Extensive results based on open-sourced simulation\nframeworks show up to 2.75x speedup and 54% reduction in energy consumption as\ncompared to the state-of-the-art attention fusion method (FLAT) in the edge\ncomputing scenario. Further experiments on a real-world edge neural processing\nunit demonstrate speedup of up to 1.76x for attention as compared to FLAT,\nwithout affecting model output accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of foundation models have revolutionized various fields, enabling\nunprecedented task accuracy and flexibility in computational linguistics,\ncomputer vision and other domains. Attention mechanism has become an essential\ncomponent of foundation models, due to their superb capability of capturing\ncorrelations in a sequence. However, attention results in quadratic complexity\nin memory and compute as the context length grows. Although many fusion-based\nexact attention acceleration algorithms have been developed for\ndatacenter-grade GPUs and accelerators leveraging multi-core parallelism and\ndata locality, yet it remains a significant challenge to accelerate attention\non resource-constrained edge neural accelerators with limited compute units and\nstringent on-chip caches. In this paper, we propose a scheme for exact\nattention inference acceleration on memory-constrained edge accelerators, by\nparallelizing the utilization of heterogeneous compute units, i.e., vector\nprocessing units and matrix processing units. Our method involves scheduling\nworkloads onto these different compute units in a multi-tiered tiling scheme to\nprocess tiled vector workloads and matrix workloads in attention as two\nstreams, respecting the workload dependencies. We search for tiling factors to\nmaximize the parallelization of both compute units while considering I/O\noverhead, and propose a proactive cache overwrite strategy to avoid undesirable\ncache spills in reality. Extensive results based on open-sourced simulation\nframeworks show up to 2.75x speedup and 54% reduction in energy consumption as\ncompared to the state-of-the-art attention fusion method (FLAT) in the edge\ncomputing scenario. Further experiments on a real-world edge neural processing\nunit demonstrate speedup of up to 1.76x for attention as compared to FLAT,\nwithout affecting model output accuracy."
                },
                "authors": [
                    {
                        "name": "Mohammadali Shakerdargah"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Chao Gao"
                    },
                    {
                        "name": "Di Niu"
                    }
                ],
                "author_detail": {
                    "name": "Di Niu"
                },
                "author": "Di Niu",
                "arxiv_comment": "10 pages, 6 figures, under review for MLSys 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.1.4; I.2.7; I.5.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13532v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13532v1",
                "updated": "2024-11-20T18:31:39Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    18,
                    31,
                    39,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T18:31:39Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    18,
                    31,
                    39,
                    2,
                    325,
                    0
                ],
                "title": "A Distributed-memory Tridiagonal Solver Based on a Specialised Data\n  Structure Optimised for CPU and GPU Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Distributed-memory Tridiagonal Solver Based on a Specialised Data\n  Structure Optimised for CPU and GPU Architectures"
                },
                "summary": "Various numerical methods used for solving partial differential equations\n(PDE) result in tridiagonal systems. Solving tridiagonal systems on\ndistributed-memory environments is not straightforward, and often requires\nsignificant amount of communication. In this article, we present a novel\ndistributed-memory tridiagonal solver algorithm, DistD2-TDS, based on a\nspecialised data structure. DistD2-TDS algorithm takes advantage of the\ndiagonal dominance in tridiagonal systems to reduce the communications in\ndistributed-memory environments. The underlying data structure plays a crucial\nrole for the performance of the algorithm. First, the data structure improves\ndata localities and makes it possible to minimise data movements via cache\nblocking and kernel fusion strategies. Second, data continuity enables a\ncontiguous data access pattern and results in efficient utilisation of the\navailable memory bandwidth. Finally, the data layout supports vectorisation on\nCPUs and thread level parallelisation on GPUs for improved performance. In\norder to demonstrate the robustness of the algorithm, we implemented and\nbenchmarked the algorithm on CPUs and GPUs. We investigated the single rank\nperformance and compared against existing algorithms. Furthermore, we analysed\nthe strong scaling of the implementation up to 384 NVIDIA H100 GPUs and up to\n8192 AMD EPYC 7742 CPUs. Finally, we demonstrated a practical use case of the\nalgorithm by using compact finite difference schemes to solve a 3D non-linear\nPDE. The results demonstrate that DistD2 algorithm can sustain around 66% of\nthe theoretical peak bandwidth at scale on CPU and GPU based supercomputers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Various numerical methods used for solving partial differential equations\n(PDE) result in tridiagonal systems. Solving tridiagonal systems on\ndistributed-memory environments is not straightforward, and often requires\nsignificant amount of communication. In this article, we present a novel\ndistributed-memory tridiagonal solver algorithm, DistD2-TDS, based on a\nspecialised data structure. DistD2-TDS algorithm takes advantage of the\ndiagonal dominance in tridiagonal systems to reduce the communications in\ndistributed-memory environments. The underlying data structure plays a crucial\nrole for the performance of the algorithm. First, the data structure improves\ndata localities and makes it possible to minimise data movements via cache\nblocking and kernel fusion strategies. Second, data continuity enables a\ncontiguous data access pattern and results in efficient utilisation of the\navailable memory bandwidth. Finally, the data layout supports vectorisation on\nCPUs and thread level parallelisation on GPUs for improved performance. In\norder to demonstrate the robustness of the algorithm, we implemented and\nbenchmarked the algorithm on CPUs and GPUs. We investigated the single rank\nperformance and compared against existing algorithms. Furthermore, we analysed\nthe strong scaling of the implementation up to 384 NVIDIA H100 GPUs and up to\n8192 AMD EPYC 7742 CPUs. Finally, we demonstrated a practical use case of the\nalgorithm by using compact finite difference schemes to solve a 3D non-linear\nPDE. The results demonstrate that DistD2 algorithm can sustain around 66% of\nthe theoretical peak bandwidth at scale on CPU and GPU based supercomputers."
                },
                "authors": [
                    {
                        "name": "Semih Akkurt"
                    },
                    {
                        "name": "Sbastien Lemaire"
                    },
                    {
                        "name": "Paul Bartholomew"
                    },
                    {
                        "name": "Sylvain Laizet"
                    }
                ],
                "author_detail": {
                    "name": "Sylvain Laizet"
                },
                "author": "Sylvain Laizet",
                "arxiv_comment": "42 pages, 13 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13532v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13532v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13373v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13373v1",
                "updated": "2024-11-20T14:52:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    14,
                    52,
                    36,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T14:52:36Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    14,
                    52,
                    36,
                    2,
                    325,
                    0
                ],
                "title": "Upgrade of the Diagnostic Neutral Beam Injector for the RFX-mod2\n  experiment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Upgrade of the Diagnostic Neutral Beam Injector for the RFX-mod2\n  experiment"
                },
                "summary": "Diagnostic Neutral Beam Injectors (DNBI), through the combined use of Charge\nExchange Recombination Spectroscopy (CHERS) and Motional Stark effect\ndiagnostics (MSE), are a well-known tool to access important information about\nmagnetically confined plasmas, such as radial profiles of ion temperature, ion\nflow, impurity content and intensity and direction of the magnetic field. For\nthis purpose, a DNBI was installed and operated in the RFX-mod experiment,\nwhich was designed to confine plasma mainly through the Reversed Field Pinch\nconfiguration. The DNBI, designed and built by the Budker Institute of Plasma\nPhysics, was based on a source of positive hydrogen ions, accelerated to 50 keV\nand for an equivalent neutral beam current of about 5 A at the source. The beam\ncould be modulated and the maximum overall duration was 50 ms. With the upgrade\nof RFX-mod to the present RFX-mod2 machine, the DNBI is being renovated to\nsolve several plant faults and improve the overall reliability of the system.\nThe 50 kV power supply is being improved, as well as the power supplies in the\nhigh voltage deck and its insulation transformer. The control system,\noriginally based on CAMAC technology, was redesigned to be fully replaced. This\ncontribution reviews the technical criticalities emerged in the DNBI check-up\nand the new solutions adopted to make the DNBI operative and more reliable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diagnostic Neutral Beam Injectors (DNBI), through the combined use of Charge\nExchange Recombination Spectroscopy (CHERS) and Motional Stark effect\ndiagnostics (MSE), are a well-known tool to access important information about\nmagnetically confined plasmas, such as radial profiles of ion temperature, ion\nflow, impurity content and intensity and direction of the magnetic field. For\nthis purpose, a DNBI was installed and operated in the RFX-mod experiment,\nwhich was designed to confine plasma mainly through the Reversed Field Pinch\nconfiguration. The DNBI, designed and built by the Budker Institute of Plasma\nPhysics, was based on a source of positive hydrogen ions, accelerated to 50 keV\nand for an equivalent neutral beam current of about 5 A at the source. The beam\ncould be modulated and the maximum overall duration was 50 ms. With the upgrade\nof RFX-mod to the present RFX-mod2 machine, the DNBI is being renovated to\nsolve several plant faults and improve the overall reliability of the system.\nThe 50 kV power supply is being improved, as well as the power supplies in the\nhigh voltage deck and its insulation transformer. The control system,\noriginally based on CAMAC technology, was redesigned to be fully replaced. This\ncontribution reviews the technical criticalities emerged in the DNBI check-up\nand the new solutions adopted to make the DNBI operative and more reliable."
                },
                "authors": [
                    {
                        "name": "Marco Barbisan"
                    },
                    {
                        "name": "Marco Boldrin"
                    },
                    {
                        "name": "Luca Cinnirella"
                    },
                    {
                        "name": "Bruno Laterza"
                    },
                    {
                        "name": "Alberto Maistrello"
                    },
                    {
                        "name": "Lionello Marrelli"
                    },
                    {
                        "name": "Federico Molon"
                    },
                    {
                        "name": "Simone Peruzzo"
                    },
                    {
                        "name": "Cesare Taliercio"
                    },
                    {
                        "name": "Marco Valisa"
                    },
                    {
                        "name": "Enrico Zampiva"
                    }
                ],
                "author_detail": {
                    "name": "Enrico Zampiva"
                },
                "author": "Enrico Zampiva",
                "arxiv_comment": "6 pages (excl. highlights), 8 figures. Contribution to the 33rd\n  Symposium on Fusion Technology (SOFT), 22-27 September 2024. This is a\n  preprint for the \"Fusion Engineering and Design\" journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13373v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13373v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18003v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18003v4",
                "updated": "2024-11-20T02:04:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    2,
                    4,
                    10,
                    2,
                    325,
                    0
                ],
                "published": "2024-07-25T12:56:22Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    12,
                    56,
                    22,
                    3,
                    207,
                    0
                ],
                "title": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption"
                },
                "summary": "Large Language Models (LLMs), epitomized by ChatGPT's release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture's struggle with handling long texts. KV Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV Cache and elaborate on various\nmethods currently used to optimize the KV Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field. Links to the papers\nmentioned in this review can be found in our Github Repo\nhttps://github.com/zcli-charlie/Awesome-KV-Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), epitomized by ChatGPT's release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture's struggle with handling long texts. KV Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV Cache and elaborate on various\nmethods currently used to optimize the KV Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field. Links to the papers\nmentioned in this review can be found in our Github Repo\nhttps://github.com/zcli-charlie/Awesome-KV-Cache."
                },
                "authors": [
                    {
                        "name": "Luohe Shi"
                    },
                    {
                        "name": "Hongyi Zhang"
                    },
                    {
                        "name": "Yao Yao"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "Published on the First Conference on Language Modeling (COLM 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18003v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18003v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17918v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17918v3",
                "updated": "2024-11-19T18:24:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    18,
                    24,
                    3,
                    1,
                    324,
                    0
                ],
                "published": "2024-06-25T20:00:32Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    20,
                    0,
                    32,
                    1,
                    177,
                    0
                ],
                "title": "GraphSnapShot: Graph Machine Learning Acceleration with Fast Storage and\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphSnapShot: Graph Machine Learning Acceleration with Fast Storage and\n  Retrieval"
                },
                "summary": "In our recent research, we have developed a framework called GraphSnapShot,\nwhich has been proven an useful tool for graph learning acceleration.\nGraphSnapShot is a framework for fast cache, storage, retrieval and computation\nfor graph learning. It can quickly store and update the local topology of graph\nstructure and allows us to track patterns in the structure of graph networks,\njust like take snapshots of the graphs. In experiments, GraphSnapShot shows\nefficiency, it can achieve up to 30% training acceleration and 73% memory\nreduction for lossless graph ML training compared to current baselines such as\ndgl.This technique is particular useful for large dynamic graph learning tasks\nsuch as social media analysis and recommendation systems to process complex\nrelationships between entities.\n  The code for GraphSnapShot is publicly available at\nhttps://github.com/NoakLiu/GraphSnapShot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In our recent research, we have developed a framework called GraphSnapShot,\nwhich has been proven an useful tool for graph learning acceleration.\nGraphSnapShot is a framework for fast cache, storage, retrieval and computation\nfor graph learning. It can quickly store and update the local topology of graph\nstructure and allows us to track patterns in the structure of graph networks,\njust like take snapshots of the graphs. In experiments, GraphSnapShot shows\nefficiency, it can achieve up to 30% training acceleration and 73% memory\nreduction for lossless graph ML training compared to current baselines such as\ndgl.This technique is particular useful for large dynamic graph learning tasks\nsuch as social media analysis and recommendation systems to process complex\nrelationships between entities.\n  The code for GraphSnapShot is publicly available at\nhttps://github.com/NoakLiu/GraphSnapShot."
                },
                "authors": [
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Roger Waleffe"
                    },
                    {
                        "name": "Meng Jiang"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17918v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17918v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12430v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12430v1",
                "updated": "2024-11-19T11:40:56Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    11,
                    40,
                    56,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T11:40:56Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    11,
                    40,
                    56,
                    1,
                    324,
                    0
                ],
                "title": "An Eulerian approach to regularized JKO scheme with low-rank tensor\n  decompositions for Bayesian inversion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Eulerian approach to regularized JKO scheme with low-rank tensor\n  decompositions for Bayesian inversion"
                },
                "summary": "The possibility of using the Eulerian discretization for the problem of\nmodelling high-dimensional distributions and sampling, is studied. The problem\nis posed as a minimization problem over the space of probability measures with\nrespect to the Wasserstein distance and solved with entropy-regularized JKO\nscheme. Each proximal step can be formulated as a fixed-point equation and\nsolved with accelerated methods, such as Anderson's. The usage of low-rank\nTensor Train format allows to overcome the \\emph{curse of dimensionality}, i.e.\nthe exponential growth of degrees of freedom with dimension, inherent to\nEulerian approaches. The resulting method requires only pointwise computations\nof the unnormalized posterior and is, in particular, gradient-free. Fixed\nEulerian grid allows to employ a caching strategy, significally reducing the\nexpensive evaluations of the posterior. When the Eulerian model of the target\ndistribution is fitted, the passage back to the Lagrangian perspective can also\nbe made, allowing to approximately sample from it. We test our method both for\nsynthetic target distributions and particular Bayesian inverse problems and\nreport comparable or better performance than the baseline Metropolis-Hastings\nMCMC with same amount of resources. Finally, the fitted model can be modified\nto facilitate the solution of certain associated problems, which we demonstrate\nby fitting an importance distribution for a particular quantity of interest.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The possibility of using the Eulerian discretization for the problem of\nmodelling high-dimensional distributions and sampling, is studied. The problem\nis posed as a minimization problem over the space of probability measures with\nrespect to the Wasserstein distance and solved with entropy-regularized JKO\nscheme. Each proximal step can be formulated as a fixed-point equation and\nsolved with accelerated methods, such as Anderson's. The usage of low-rank\nTensor Train format allows to overcome the \\emph{curse of dimensionality}, i.e.\nthe exponential growth of degrees of freedom with dimension, inherent to\nEulerian approaches. The resulting method requires only pointwise computations\nof the unnormalized posterior and is, in particular, gradient-free. Fixed\nEulerian grid allows to employ a caching strategy, significally reducing the\nexpensive evaluations of the posterior. When the Eulerian model of the target\ndistribution is fitted, the passage back to the Lagrangian perspective can also\nbe made, allowing to approximately sample from it. We test our method both for\nsynthetic target distributions and particular Bayesian inverse problems and\nreport comparable or better performance than the baseline Metropolis-Hastings\nMCMC with same amount of resources. Finally, the fitted model can be modified\nto facilitate the solution of certain associated problems, which we demonstrate\nby fitting an importance distribution for a particular quantity of interest."
                },
                "authors": [
                    {
                        "name": "Vitalii Aksenov"
                    },
                    {
                        "name": "Martin Eigel"
                    }
                ],
                "author_detail": {
                    "name": "Martin Eigel"
                },
                "author": "Martin Eigel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12430v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12430v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "46E27, 49Q22, 62F15, 68W25",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12161v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12161v1",
                "updated": "2024-11-19T01:55:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    1,
                    55,
                    26,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T01:55:26Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    1,
                    55,
                    26,
                    1,
                    324,
                    0
                ],
                "title": "Adaptive Cache Management for Complex Storage Systems Using\n  CNN-LSTM-Based Spatiotemporal Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Cache Management for Complex Storage Systems Using\n  CNN-LSTM-Based Spatiotemporal Prediction"
                },
                "summary": "This paper proposes an intelligent cache management strategy based on\nCNN-LSTM to improve the performance and cache hit rate of storage systems.\nThrough comparative experiments with traditional algorithms (such as LRU and\nLFU) and other deep learning models (such as RNN, GRU-RNN and LSTM), the\nresults show that the CNN-LSTM model has significant advantages in cache demand\nprediction. The MSE and MAE values of this model are significantly reduced,\nproving its effectiveness under complex data access patterns. This study not\nonly verifies the potential of deep learning technology in storage system\noptimization, but also provides direction and reference for further optimizing\nand improving cache management strategies. This intelligent cache management\nstrategy performs well in complex storage environments. By combining the\nspatial feature extraction capabilities of convolutional neural networks and\nthe time series modeling capabilities of long short-term memory networks, the\nCNN-LSTM model can more accurately predict cache needs, thereby Dynamically\noptimize cache allocation to improve system response speed and resource\nutilization. This research provides theoretical support and practical reference\nfor cache optimization under large-scale data access modes, and is of great\nsignificance to improving the performance of future storage systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes an intelligent cache management strategy based on\nCNN-LSTM to improve the performance and cache hit rate of storage systems.\nThrough comparative experiments with traditional algorithms (such as LRU and\nLFU) and other deep learning models (such as RNN, GRU-RNN and LSTM), the\nresults show that the CNN-LSTM model has significant advantages in cache demand\nprediction. The MSE and MAE values of this model are significantly reduced,\nproving its effectiveness under complex data access patterns. This study not\nonly verifies the potential of deep learning technology in storage system\noptimization, but also provides direction and reference for further optimizing\nand improving cache management strategies. This intelligent cache management\nstrategy performs well in complex storage environments. By combining the\nspatial feature extraction capabilities of convolutional neural networks and\nthe time series modeling capabilities of long short-term memory networks, the\nCNN-LSTM model can more accurately predict cache needs, thereby Dynamically\noptimize cache allocation to improve system response speed and resource\nutilization. This research provides theoretical support and practical reference\nfor cache optimization under large-scale data access modes, and is of great\nsignificance to improving the performance of future storage systems."
                },
                "authors": [
                    {
                        "name": "Xiaoye Wang"
                    },
                    {
                        "name": "Xuan Li"
                    },
                    {
                        "name": "Linji Wang"
                    },
                    {
                        "name": "Tingyi Ruan"
                    },
                    {
                        "name": "Pochun Li"
                    }
                ],
                "author_detail": {
                    "name": "Pochun Li"
                },
                "author": "Pochun Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12161v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12161v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11843v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11843v1",
                "updated": "2024-11-18T18:59:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    59,
                    15,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T18:59:15Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    59,
                    15,
                    0,
                    323,
                    0
                ],
                "title": "Bi-Mamba: Towards Accurate 1-Bit State Space Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bi-Mamba: Towards Accurate 1-Bit State Space Models"
                },
                "summary": "The typical selective state-space model (SSM) of Mamba addresses several\nlimitations of Transformers, such as quadratic computational complexity with\nsequence length and significant inference-time memory requirements due to the\nkey-value cache. However, the growing size of Mamba models continues to pose\ntraining and deployment challenges and raises environmental concerns due to\nconsiderable energy consumption. In this work, we introduce Bi-Mamba, a\nscalable and powerful 1-bit Mamba architecture designed for more efficient\nlarge language models with multiple sizes across 780M, 1.3B, and 2.7B. Bi-Mamba\nmodels are trained from scratch on data volume as regular LLM pertaining using\nan autoregressive distillation loss. Extensive experimental results on language\nmodeling demonstrate that Bi-Mamba achieves performance comparable to its\nfull-precision counterparts (e.g., FP16 or BF16) and much better accuracy than\npost-training-binarization (PTB) Mamba baselines, while significantly reducing\nmemory footprint and energy consumption compared to the original Mamba model.\nOur study pioneers a new linear computational complexity LLM framework under\nlow-bit representation and facilitates the future design of specialized\nhardware tailored for efficient 1-bit Mamba-based LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The typical selective state-space model (SSM) of Mamba addresses several\nlimitations of Transformers, such as quadratic computational complexity with\nsequence length and significant inference-time memory requirements due to the\nkey-value cache. However, the growing size of Mamba models continues to pose\ntraining and deployment challenges and raises environmental concerns due to\nconsiderable energy consumption. In this work, we introduce Bi-Mamba, a\nscalable and powerful 1-bit Mamba architecture designed for more efficient\nlarge language models with multiple sizes across 780M, 1.3B, and 2.7B. Bi-Mamba\nmodels are trained from scratch on data volume as regular LLM pertaining using\nan autoregressive distillation loss. Extensive experimental results on language\nmodeling demonstrate that Bi-Mamba achieves performance comparable to its\nfull-precision counterparts (e.g., FP16 or BF16) and much better accuracy than\npost-training-binarization (PTB) Mamba baselines, while significantly reducing\nmemory footprint and energy consumption compared to the original Mamba model.\nOur study pioneers a new linear computational complexity LLM framework under\nlow-bit representation and facilitates the future design of specialized\nhardware tailored for efficient 1-bit Mamba-based LLMs."
                },
                "authors": [
                    {
                        "name": "Shengkun Tang"
                    },
                    {
                        "name": "Liqun Ma"
                    },
                    {
                        "name": "Haonan Li"
                    },
                    {
                        "name": "Mingjie Sun"
                    },
                    {
                        "name": "Zhiqiang Shen"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Shen"
                },
                "author": "Zhiqiang Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11843v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11843v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11739v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11739v1",
                "updated": "2024-11-18T17:08:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    17,
                    8,
                    35,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T17:08:35Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    17,
                    8,
                    35,
                    0,
                    323,
                    0
                ],
                "title": "QARM: Quantitative Alignment Multi-Modal Recommendation at Kuaishou",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QARM: Quantitative Alignment Multi-Modal Recommendation at Kuaishou"
                },
                "summary": "In recent years, with the significant evolution of multi-modal large models,\nmany recommender researchers realized the potential of multi-modal information\nfor user interest modeling. In industry, a wide-used modeling architecture is a\ncascading paradigm: (1) first pre-training a multi-modal model to provide\nomnipotent representations for downstream services; (2) The downstream\nrecommendation model takes the multi-modal representation as additional input\nto fit real user-item behaviours. Although such paradigm achieves remarkable\nimprovements, however, there still exist two problems that limit model\nperformance: (1) Representation Unmatching: The pre-trained multi-modal model\nis always supervised by the classic NLP/CV tasks, while the recommendation\nmodels are supervised by real user-item interaction. As a result, the two\nfundamentally different tasks' goals were relatively separate, and there was a\nlack of consistent objective on their representations; (2) Representation\nUnlearning: The generated multi-modal representations are always stored in\ncache store and serve as extra fixed input of recommendation model, thus could\nnot be updated by recommendation model gradient, further unfriendly for\ndownstream training. Inspired by the two difficulties challenges in downstream\ntasks usage, we introduce a quantitative multi-modal framework to customize the\nspecialized and trainable multi-modal information for different downstream\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, with the significant evolution of multi-modal large models,\nmany recommender researchers realized the potential of multi-modal information\nfor user interest modeling. In industry, a wide-used modeling architecture is a\ncascading paradigm: (1) first pre-training a multi-modal model to provide\nomnipotent representations for downstream services; (2) The downstream\nrecommendation model takes the multi-modal representation as additional input\nto fit real user-item behaviours. Although such paradigm achieves remarkable\nimprovements, however, there still exist two problems that limit model\nperformance: (1) Representation Unmatching: The pre-trained multi-modal model\nis always supervised by the classic NLP/CV tasks, while the recommendation\nmodels are supervised by real user-item interaction. As a result, the two\nfundamentally different tasks' goals were relatively separate, and there was a\nlack of consistent objective on their representations; (2) Representation\nUnlearning: The generated multi-modal representations are always stored in\ncache store and serve as extra fixed input of recommendation model, thus could\nnot be updated by recommendation model gradient, further unfriendly for\ndownstream training. Inspired by the two difficulties challenges in downstream\ntasks usage, we introduce a quantitative multi-modal framework to customize the\nspecialized and trainable multi-modal information for different downstream\nmodels."
                },
                "authors": [
                    {
                        "name": "Xinchen Luo"
                    },
                    {
                        "name": "Jiangxia Cao"
                    },
                    {
                        "name": "Tianyu Sun"
                    },
                    {
                        "name": "Jinkai Yu"
                    },
                    {
                        "name": "Rui Huang"
                    },
                    {
                        "name": "Wei Yuan"
                    },
                    {
                        "name": "Hezheng Lin"
                    },
                    {
                        "name": "Yichen Zheng"
                    },
                    {
                        "name": "Shiyao Wang"
                    },
                    {
                        "name": "Qigen Hu"
                    },
                    {
                        "name": "Changqing Qiu"
                    },
                    {
                        "name": "Jiaqi Zhang"
                    },
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Zhiheng Yan"
                    },
                    {
                        "name": "Jingming Zhang"
                    },
                    {
                        "name": "Simin Zhang"
                    },
                    {
                        "name": "Mingxing Wen"
                    },
                    {
                        "name": "Zhaojie Liu"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Guorui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guorui Zhou"
                },
                "author": "Guorui Zhou",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11739v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11739v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "N/A",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11300v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11300v1",
                "updated": "2024-11-18T05:50:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    5,
                    50,
                    58,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T05:50:58Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    5,
                    50,
                    58,
                    0,
                    323,
                    0
                ],
                "title": "Accelerating spherical K-means clustering for large-scale sparse\n  document data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating spherical K-means clustering for large-scale sparse\n  document data"
                },
                "summary": "This paper presents an accelerated spherical K-means clustering algorithm for\nlarge-scale and high-dimensional sparse document data sets. We design an\nalgorithm working in an architecture-friendly manner (AFM), which is a\nprocedure of suppressing performance-degradation factors such as the numbers of\ninstructions, branch mispredictions, and cache misses in CPUs of a modern\ncomputer system. For the AFM operation, we leverage unique universal\ncharacteristics (UCs) of a data-object and a cluster's mean set, which are\nskewed distributions on data relationships such as Zipf's law and a\nfeature-value concentration phenomenon. The UCs indicate that the most part of\nthe number of multiplications for similarity calculations is executed regarding\nterms with high document frequencies (df) and the most part of a similarity\nbetween an object- and a mean-feature vector is obtained by the multiplications\nregarding a few high mean-feature values. Our proposed algorithm applies an\ninverted-index data structure to a mean set, extracts the specific region with\nhigh-df terms and high mean-feature values in the mean-inverted index by newly\nintroduced two structural parameters, and exploits the index divided into three\nparts for efficient pruning. The algorithm determines the two structural\nparameters by minimizing the approximate number of multiplications related to\nthat of instructions, reduces the branch mispredictions by sharing the index\nstructure including the two parameters with all the objects, and suppressing\nthe cache misses by keeping in the caches the frequently used data in the\nforegoing specific region, resulting in working in the AFM. We experimentally\ndemonstrate that our algorithm efficiently achieves superior speed performance\nin large-scale documents compared with algorithms using the state-of-the-art\ntechniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an accelerated spherical K-means clustering algorithm for\nlarge-scale and high-dimensional sparse document data sets. We design an\nalgorithm working in an architecture-friendly manner (AFM), which is a\nprocedure of suppressing performance-degradation factors such as the numbers of\ninstructions, branch mispredictions, and cache misses in CPUs of a modern\ncomputer system. For the AFM operation, we leverage unique universal\ncharacteristics (UCs) of a data-object and a cluster's mean set, which are\nskewed distributions on data relationships such as Zipf's law and a\nfeature-value concentration phenomenon. The UCs indicate that the most part of\nthe number of multiplications for similarity calculations is executed regarding\nterms with high document frequencies (df) and the most part of a similarity\nbetween an object- and a mean-feature vector is obtained by the multiplications\nregarding a few high mean-feature values. Our proposed algorithm applies an\ninverted-index data structure to a mean set, extracts the specific region with\nhigh-df terms and high mean-feature values in the mean-inverted index by newly\nintroduced two structural parameters, and exploits the index divided into three\nparts for efficient pruning. The algorithm determines the two structural\nparameters by minimizing the approximate number of multiplications related to\nthat of instructions, reduces the branch mispredictions by sharing the index\nstructure including the two parameters with all the objects, and suppressing\nthe cache misses by keeping in the caches the frequently used data in the\nforegoing specific region, resulting in working in the AFM. We experimentally\ndemonstrate that our algorithm efficiently achieves superior speed performance\nin large-scale documents compared with algorithms using the state-of-the-art\ntechniques."
                },
                "authors": [
                    {
                        "name": "Kazuo Aoyama"
                    },
                    {
                        "name": "Kazumi Saito"
                    }
                ],
                "author_detail": {
                    "name": "Kazumi Saito"
                },
                "author": "Kazumi Saito",
                "arxiv_comment": "28 pages, 23 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11300v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13588v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13588v1",
                "updated": "2024-11-18T02:49:23Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    2,
                    49,
                    23,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T02:49:23Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    2,
                    49,
                    23,
                    0,
                    323,
                    0
                ],
                "title": "Unveiling Redundancy in Diffusion Transformers (DiTs): A Systematic\n  Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling Redundancy in Diffusion Transformers (DiTs): A Systematic\n  Study"
                },
                "summary": "The increased model capacity of Diffusion Transformers (DiTs) and the demand\nfor generating higher resolutions of images and videos have led to a\nsignificant rise in inference latency, impacting real-time performance\nadversely. While prior research has highlighted the presence of high similarity\nin activation values between adjacent diffusion steps (referred to as\nredundancy) and proposed various caching mechanisms to mitigate computational\noverhead, the exploration of redundancy in existing literature remains limited,\nwith findings often not generalizable across different DiT models. This study\naims to address this gap by conducting a comprehensive investigation into\nredundancy across a broad spectrum of mainstream DiT models. Our experimental\nanalysis reveals substantial variations in the distribution of redundancy\nacross diffusion steps among different DiT models. Interestingly, within a\nsingle model, the redundancy distribution remains stable regardless of\nvariations in input prompts, step counts, or scheduling strategies. Given the\nlack of a consistent pattern across diverse models, caching strategies designed\nfor a specific group of models may not easily transfer to others. To overcome\nthis challenge, we introduce a tool for analyzing the redundancy of individual\nmodels, enabling subsequent research to develop tailored caching strategies for\nspecific model architectures. The project is publicly available at\nhttps://github.com/xdit-project/DiTCacheAnalysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increased model capacity of Diffusion Transformers (DiTs) and the demand\nfor generating higher resolutions of images and videos have led to a\nsignificant rise in inference latency, impacting real-time performance\nadversely. While prior research has highlighted the presence of high similarity\nin activation values between adjacent diffusion steps (referred to as\nredundancy) and proposed various caching mechanisms to mitigate computational\noverhead, the exploration of redundancy in existing literature remains limited,\nwith findings often not generalizable across different DiT models. This study\naims to address this gap by conducting a comprehensive investigation into\nredundancy across a broad spectrum of mainstream DiT models. Our experimental\nanalysis reveals substantial variations in the distribution of redundancy\nacross diffusion steps among different DiT models. Interestingly, within a\nsingle model, the redundancy distribution remains stable regardless of\nvariations in input prompts, step counts, or scheduling strategies. Given the\nlack of a consistent pattern across diverse models, caching strategies designed\nfor a specific group of models may not easily transfer to others. To overcome\nthis challenge, we introduce a tool for analyzing the redundancy of individual\nmodels, enabling subsequent research to develop tailored caching strategies for\nspecific model architectures. The project is publicly available at\nhttps://github.com/xdit-project/DiTCacheAnalysis."
                },
                "authors": [
                    {
                        "name": "Xibo Sun"
                    },
                    {
                        "name": "Jiarui Fang"
                    },
                    {
                        "name": "Aoyu Li"
                    },
                    {
                        "name": "Jinzhe Pan"
                    }
                ],
                "author_detail": {
                    "name": "Jinzhe Pan"
                },
                "author": "Jinzhe Pan",
                "arxiv_comment": "9 pages including reference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13588v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13588v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06392v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06392v2",
                "updated": "2024-11-18T02:10:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    2,
                    10,
                    28,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-10T08:31:18Z",
                "published_parsed": [
                    2024,
                    11,
                    10,
                    8,
                    31,
                    18,
                    6,
                    315,
                    0
                ],
                "title": "LSMGraph: A High-Performance Dynamic Graph Storage System with\n  Multi-Level CSR",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LSMGraph: A High-Performance Dynamic Graph Storage System with\n  Multi-Level CSR"
                },
                "summary": "The growing volume of graph data may exhaust the main memory. It is crucial\nto design a disk-based graph storage system to ingest updates and analyze\ngraphs efficiently. However, existing dynamic graph storage systems suffer from\nread or write amplification and face the challenge of optimizing both read and\nwrite performance simultaneously. To address this challenge, we propose\nLSMGraph, a novel dynamic graph storage system that combines the write-friendly\nLSM-tree and the read-friendly CSR. It leverages the multi-level structure of\nLSM-trees to optimize write performance while utilizing the compact CSR\nstructures embedded in the LSM-trees to boost read performance. LSMGraph uses a\nnew memory structure, MemGraph, to efficiently cache graph updates and uses a\nmulti-level index to speed up reads within the multi-level structure.\nFurthermore, LSMGraph incorporates a vertex-grained version control mechanism\nto mitigate the impact of LSM-tree compaction on read performance and ensure\nthe correctness of concurrent read and write operations. Our evaluation shows\nthat LSMGraph significantly outperforms state-of-the-art (graph) storage\nsystems on both graph update and graph analytical workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing volume of graph data may exhaust the main memory. It is crucial\nto design a disk-based graph storage system to ingest updates and analyze\ngraphs efficiently. However, existing dynamic graph storage systems suffer from\nread or write amplification and face the challenge of optimizing both read and\nwrite performance simultaneously. To address this challenge, we propose\nLSMGraph, a novel dynamic graph storage system that combines the write-friendly\nLSM-tree and the read-friendly CSR. It leverages the multi-level structure of\nLSM-trees to optimize write performance while utilizing the compact CSR\nstructures embedded in the LSM-trees to boost read performance. LSMGraph uses a\nnew memory structure, MemGraph, to efficiently cache graph updates and uses a\nmulti-level index to speed up reads within the multi-level structure.\nFurthermore, LSMGraph incorporates a vertex-grained version control mechanism\nto mitigate the impact of LSM-tree compaction on read performance and ensure\nthe correctness of concurrent read and write operations. Our evaluation shows\nthat LSMGraph significantly outperforms state-of-the-art (graph) storage\nsystems on both graph update and graph analytical workloads."
                },
                "authors": [
                    {
                        "name": "Song Yu"
                    },
                    {
                        "name": "Shufeng Gong"
                    },
                    {
                        "name": "Qian Tao"
                    },
                    {
                        "name": "Sijie Shen"
                    },
                    {
                        "name": "Yanfeng Zhang"
                    },
                    {
                        "name": "Wenyuan Yu"
                    },
                    {
                        "name": "Pengxi Liu"
                    },
                    {
                        "name": "Zhixin Zhang"
                    },
                    {
                        "name": "Hongfu Li"
                    },
                    {
                        "name": "Xiaojian Luo"
                    },
                    {
                        "name": "Ge Yu"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06392v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06392v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11091v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11091v1",
                "updated": "2024-11-17T14:47:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    17,
                    14,
                    47,
                    15,
                    6,
                    322,
                    0
                ],
                "published": "2024-11-17T14:47:15Z",
                "published_parsed": [
                    2024,
                    11,
                    17,
                    14,
                    47,
                    15,
                    6,
                    322,
                    0
                ],
                "title": "KV-Tandem -- a Modular Approach to Building High-Speed LSM Storage\n  Engines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Tandem -- a Modular Approach to Building High-Speed LSM Storage\n  Engines"
                },
                "summary": "We present~\\emph{KV-Tandem}, a modular architecture for building LSM-based\nstorage engines on top of simple, non-ordered persistent key-value stores\n(KVSs). KV-Tandem enables advanced functionalities such as range queries and\nsnapshot reads, while maintaining the native KVS performance for random reads\nand writes. Its modular design offers better performance trade-offs compared to\nprevious KV-separation solutions, which struggle to decompose the monolithic\nLSM structure. Central to KV-Tandem is~\\emph{LSM bypass} -- a novel algorithm\nthat offers a fast path to basic operations while ensuring the correctness of\nadvanced APIs.\n  We implement KV-Tandem in \\emph{XDP-Rocks}, a RocksDB-compatible storage\nengine that leverages the XDP KVS and incorporates practical design\noptimizations for real-world deployment. Through extensive microbenchmark and\nsystem-level comparisons, we demonstrate that XDP-Rocks achieves 3x to 4x\nperformance improvements over RocksDB across various workloads. XDP-Rocks is\nalready deployed in production, delivering significant operator cost savings\nconsistent with these performance gains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present~\\emph{KV-Tandem}, a modular architecture for building LSM-based\nstorage engines on top of simple, non-ordered persistent key-value stores\n(KVSs). KV-Tandem enables advanced functionalities such as range queries and\nsnapshot reads, while maintaining the native KVS performance for random reads\nand writes. Its modular design offers better performance trade-offs compared to\nprevious KV-separation solutions, which struggle to decompose the monolithic\nLSM structure. Central to KV-Tandem is~\\emph{LSM bypass} -- a novel algorithm\nthat offers a fast path to basic operations while ensuring the correctness of\nadvanced APIs.\n  We implement KV-Tandem in \\emph{XDP-Rocks}, a RocksDB-compatible storage\nengine that leverages the XDP KVS and incorporates practical design\noptimizations for real-world deployment. Through extensive microbenchmark and\nsystem-level comparisons, we demonstrate that XDP-Rocks achieves 3x to 4x\nperformance improvements over RocksDB across various workloads. XDP-Rocks is\nalready deployed in production, delivering significant operator cost savings\nconsistent with these performance gains."
                },
                "authors": [
                    {
                        "name": "Edward Bortnikov"
                    },
                    {
                        "name": "Michael Azran"
                    },
                    {
                        "name": "Asa Bornstein"
                    },
                    {
                        "name": "Shmuel Dashevsky"
                    },
                    {
                        "name": "Dennis Huang"
                    },
                    {
                        "name": "Omer Kepten"
                    },
                    {
                        "name": "Michael Pan"
                    },
                    {
                        "name": "Gali Sheffi"
                    },
                    {
                        "name": "Moshe Twitto"
                    },
                    {
                        "name": "Tamar Weiss Orzech"
                    },
                    {
                        "name": "Idit Keidar"
                    },
                    {
                        "name": "Guy Gueta"
                    },
                    {
                        "name": "Roey Maor"
                    },
                    {
                        "name": "Niv Dayan"
                    }
                ],
                "author_detail": {
                    "name": "Niv Dayan"
                },
                "author": "Niv Dayan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11091v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11091v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07635v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07635v3",
                "updated": "2024-11-17T12:56:16Z",
                "updated_parsed": [
                    2024,
                    11,
                    17,
                    12,
                    56,
                    16,
                    6,
                    322,
                    0
                ],
                "published": "2024-11-12T08:30:59Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    8,
                    30,
                    59,
                    1,
                    317,
                    0
                ],
                "title": "Breaking the Low-Rank Dilemma of Linear Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking the Low-Rank Dilemma of Linear Attention"
                },
                "summary": "The Softmax attention mechanism in Transformer models is notoriously\ncomputationally expensive, particularly due to its quadratic complexity, posing\nsignificant challenges in vision applications. In contrast, linear attention\nprovides a far more efficient solution by reducing the complexity to linear\nlevels. However, compared to Softmax attention, linear attention often\nexperiences significant performance degradation. Our experiments indicate that\nthis performance drop is due to the low-rank nature of linear attention's\nfeature map, which hinders its ability to adequately model complex spatial\ninformation. In this paper, to break the low-rank dilemma of linear attention,\nwe conduct rank analysis from two perspectives: the KV buffer and the output\nfeatures. Consequently, we introduce Rank-Augmented Linear Attention (RALA),\nwhich rivals the performance of Softmax attention while maintaining linear\ncomplexity and high efficiency. Based on RALA, we construct the Rank-Augmented\nVision Linear Transformer (RAVLT). Extensive experiments demonstrate that RAVLT\nachieves excellent performance across various vision tasks. Specifically,\nwithout using any additional labels, data, or supervision during training,\nRAVLT achieves an 84.4% Top-1 accuracy on ImageNet-1k with only 26M parameters\nand 4.6G FLOPs. This result significantly surpasses previous linear attention\nmechanisms, fully illustrating the potential of RALA. Code will be available at\nhttps://github.com/qhfan/RALA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Softmax attention mechanism in Transformer models is notoriously\ncomputationally expensive, particularly due to its quadratic complexity, posing\nsignificant challenges in vision applications. In contrast, linear attention\nprovides a far more efficient solution by reducing the complexity to linear\nlevels. However, compared to Softmax attention, linear attention often\nexperiences significant performance degradation. Our experiments indicate that\nthis performance drop is due to the low-rank nature of linear attention's\nfeature map, which hinders its ability to adequately model complex spatial\ninformation. In this paper, to break the low-rank dilemma of linear attention,\nwe conduct rank analysis from two perspectives: the KV buffer and the output\nfeatures. Consequently, we introduce Rank-Augmented Linear Attention (RALA),\nwhich rivals the performance of Softmax attention while maintaining linear\ncomplexity and high efficiency. Based on RALA, we construct the Rank-Augmented\nVision Linear Transformer (RAVLT). Extensive experiments demonstrate that RAVLT\nachieves excellent performance across various vision tasks. Specifically,\nwithout using any additional labels, data, or supervision during training,\nRAVLT achieves an 84.4% Top-1 accuracy on ImageNet-1k with only 26M parameters\nand 4.6G FLOPs. This result significantly surpasses previous linear attention\nmechanisms, fully illustrating the potential of RALA. Code will be available at\nhttps://github.com/qhfan/RALA."
                },
                "authors": [
                    {
                        "name": "Qihang Fan"
                    },
                    {
                        "name": "Huaibo Huang"
                    },
                    {
                        "name": "Ran He"
                    }
                ],
                "author_detail": {
                    "name": "Ran He"
                },
                "author": "Ran He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07635v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07635v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10883v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10883v1",
                "updated": "2024-11-16T20:40:08Z",
                "updated_parsed": [
                    2024,
                    11,
                    16,
                    20,
                    40,
                    8,
                    5,
                    321,
                    0
                ],
                "published": "2024-11-16T20:40:08Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    20,
                    40,
                    8,
                    5,
                    321,
                    0
                ],
                "title": "I Know What You Sync: Covert and Side Channel Attacks on File Systems\n  via syncfs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I Know What You Sync: Covert and Side Channel Attacks on File Systems\n  via syncfs"
                },
                "summary": "Operating Systems enforce logical isolation using abstractions such as\nprocesses, containers, and isolation technologies to protect a system from\nmalicious or buggy code. In this paper, we show new types of side channels\nthrough the file system that break this logical isolation. The file system\nplays a critical role in the operating system, managing all I/O activities\nbetween the application layer and the physical storage device. We observe that\nthe file system implementation is shared, leading to timing leakage when using\ncommon I/O system calls. Specifically, we found that modern operating systems\ntake advantage of any flush operation (which saves cached blocks in memory to\nthe SSD or disk) to flush all of the I/O buffers, even those used by other\nisolation domains. Thus, by measuring the delay of syncfs, the attacker can\ninfer the I/O behavior of victim programs. We then demonstrate a syncfs covert\nchannel attack on multiple file systems, including both Linux native file\nsystems and the Windows file system, achieving a maximum bandwidth of 5 Kbps\nwith an error rate of 0.15% on Linux and 7.6 Kbps with an error rate of 1.9% on\nWindows. In addition, we construct three side-channel attacks targeting both\nLinux and Android devices. On Linux devices, we implement a website\nfingerprinting attack and a video fingerprinting attack by tracking the write\npatterns of temporary buffering files. On Android devices, we design an\napplication fingerprinting attack that leaks application write patterns during\nboot-up. The attacks achieve over 90% F1 score, precision, and recall. Finally,\nwe demonstrate that these attacks can be exploited across containers\nimplementing a container detection technique and a cross-container covert\nchannel attack.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Operating Systems enforce logical isolation using abstractions such as\nprocesses, containers, and isolation technologies to protect a system from\nmalicious or buggy code. In this paper, we show new types of side channels\nthrough the file system that break this logical isolation. The file system\nplays a critical role in the operating system, managing all I/O activities\nbetween the application layer and the physical storage device. We observe that\nthe file system implementation is shared, leading to timing leakage when using\ncommon I/O system calls. Specifically, we found that modern operating systems\ntake advantage of any flush operation (which saves cached blocks in memory to\nthe SSD or disk) to flush all of the I/O buffers, even those used by other\nisolation domains. Thus, by measuring the delay of syncfs, the attacker can\ninfer the I/O behavior of victim programs. We then demonstrate a syncfs covert\nchannel attack on multiple file systems, including both Linux native file\nsystems and the Windows file system, achieving a maximum bandwidth of 5 Kbps\nwith an error rate of 0.15% on Linux and 7.6 Kbps with an error rate of 1.9% on\nWindows. In addition, we construct three side-channel attacks targeting both\nLinux and Android devices. On Linux devices, we implement a website\nfingerprinting attack and a video fingerprinting attack by tracking the write\npatterns of temporary buffering files. On Android devices, we design an\napplication fingerprinting attack that leaks application write patterns during\nboot-up. The attacks achieve over 90% F1 score, precision, and recall. Finally,\nwe demonstrate that these attacks can be exploited across containers\nimplementing a container detection technique and a cross-container covert\nchannel attack."
                },
                "authors": [
                    {
                        "name": "Cheng Gu"
                    },
                    {
                        "name": "Yicheng Zhang"
                    },
                    {
                        "name": "Nael Abu-Ghazaleh"
                    }
                ],
                "author_detail": {
                    "name": "Nael Abu-Ghazaleh"
                },
                "author": "Nael Abu-Ghazaleh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10883v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10883v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.13112v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.13112v3",
                "updated": "2024-11-16T20:39:46Z",
                "updated_parsed": [
                    2024,
                    11,
                    16,
                    20,
                    39,
                    46,
                    5,
                    321,
                    0
                ],
                "published": "2024-03-19T19:27:23Z",
                "published_parsed": [
                    2024,
                    3,
                    19,
                    19,
                    27,
                    23,
                    1,
                    79,
                    0
                ],
                "title": "Efficient Encoder-Decoder Transformer Decoding for Decomposable Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Encoder-Decoder Transformer Decoding for Decomposable Tasks"
                },
                "summary": "Transformer-based NLP models are powerful but have high computational costs\nthat limit deployment. Finetuned encoder-decoder models are popular in\nspecialized domains and can outperform larger more generalized decoder-only\nmodels, such as GPT-4. We introduce a new configuration for encoder-decoder\nmodels that improves efficiency on structured output and decomposable tasks\nwhere multiple outputs are required for a single shared input. Our method,\nprompt-in-decoder (PiD), encodes the input once and decodes the output in\nparallel, boosting both training and inference efficiency by avoiding duplicate\ninput encoding and increasing the operational intensity (ratio of numbers of\narithmetic operation to memory access) of decoding process by sharing the input\nkey-value cache. We achieve computation reduction that roughly scales with the\nnumber of subtasks, gaining up to 4.6x speed-up over state-of-the-art models\nfor dialogue state tracking, summarization, and question-answering tasks, with\ncomparable or better performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based NLP models are powerful but have high computational costs\nthat limit deployment. Finetuned encoder-decoder models are popular in\nspecialized domains and can outperform larger more generalized decoder-only\nmodels, such as GPT-4. We introduce a new configuration for encoder-decoder\nmodels that improves efficiency on structured output and decomposable tasks\nwhere multiple outputs are required for a single shared input. Our method,\nprompt-in-decoder (PiD), encodes the input once and decodes the output in\nparallel, boosting both training and inference efficiency by avoiding duplicate\ninput encoding and increasing the operational intensity (ratio of numbers of\narithmetic operation to memory access) of decoding process by sharing the input\nkey-value cache. We achieve computation reduction that roughly scales with the\nnumber of subtasks, gaining up to 4.6x speed-up over state-of-the-art models\nfor dialogue state tracking, summarization, and question-answering tasks, with\ncomparable or better performance."
                },
                "authors": [
                    {
                        "name": "Bo-Ru Lu"
                    },
                    {
                        "name": "Nikita Haduong"
                    },
                    {
                        "name": "Chien-Yu Lin"
                    },
                    {
                        "name": "Hao Cheng"
                    },
                    {
                        "name": "Noah A. Smith"
                    },
                    {
                        "name": "Mari Ostendorf"
                    }
                ],
                "author_detail": {
                    "name": "Mari Ostendorf"
                },
                "author": "Mari Ostendorf",
                "arxiv_comment": "18 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.13112v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.13112v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10803v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10803v1",
                "updated": "2024-11-16T13:45:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    16,
                    13,
                    45,
                    33,
                    5,
                    321,
                    0
                ],
                "published": "2024-11-16T13:45:33Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    13,
                    45,
                    33,
                    5,
                    321,
                    0
                ],
                "title": "Multi-Stage Vision Token Dropping: Towards Efficient Multimodal Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Stage Vision Token Dropping: Towards Efficient Multimodal Large\n  Language Model"
                },
                "summary": "The vision tokens in multimodal large language models usually exhibit\nsignificant spatial and temporal redundancy and take up most of the input\ntokens, which harms their inference efficiency. To solve this problem, some\nrecent works were introduced to drop the unimportant tokens during inference\nwhere the importance of each token is decided only by the information in either\nthe vision encoding stage or the prefilling stage. In this paper, we propose\nMulti-stage Token Dropping (MustDrop) to measure the importance of each token\nfrom the whole lifecycle, including the vision encoding stage, prefilling\nstage, and decoding stage. Concretely, in the visual encoding stage, MustDrop\nmerges spatially adjacent tokens with high similarity, and establishes a key\ntoken set to retain the most vision-critical tokens, preventing them from being\ndiscarded in later stages. In the prefilling stage, MustDrop further compresses\nvision tokens by the guidance of text semantics, with a dual-attention\nfiltering strategy. In the decoding stage, an output-aware cache policy is\nproposed to further reduce the size of the KV cache. By leveraging tailored\nstrategies in the multi-stage process, MustDrop can more precisely recognize\nthe important and redundant tokens, thus achieving an optimal balance between\nperformance and efficiency. For instance, MustDrop reduces about 88.5\\% FLOPs\non LLaVA with a compression ratio of 92.2\\% while maintaining comparable\naccuracy. Our codes are available at\n\\url{https://github.com/liuting20/MustDrop}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The vision tokens in multimodal large language models usually exhibit\nsignificant spatial and temporal redundancy and take up most of the input\ntokens, which harms their inference efficiency. To solve this problem, some\nrecent works were introduced to drop the unimportant tokens during inference\nwhere the importance of each token is decided only by the information in either\nthe vision encoding stage or the prefilling stage. In this paper, we propose\nMulti-stage Token Dropping (MustDrop) to measure the importance of each token\nfrom the whole lifecycle, including the vision encoding stage, prefilling\nstage, and decoding stage. Concretely, in the visual encoding stage, MustDrop\nmerges spatially adjacent tokens with high similarity, and establishes a key\ntoken set to retain the most vision-critical tokens, preventing them from being\ndiscarded in later stages. In the prefilling stage, MustDrop further compresses\nvision tokens by the guidance of text semantics, with a dual-attention\nfiltering strategy. In the decoding stage, an output-aware cache policy is\nproposed to further reduce the size of the KV cache. By leveraging tailored\nstrategies in the multi-stage process, MustDrop can more precisely recognize\nthe important and redundant tokens, thus achieving an optimal balance between\nperformance and efficiency. For instance, MustDrop reduces about 88.5\\% FLOPs\non LLaVA with a compression ratio of 92.2\\% while maintaining comparable\naccuracy. Our codes are available at\n\\url{https://github.com/liuting20/MustDrop}."
                },
                "authors": [
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Liangtao Shi"
                    },
                    {
                        "name": "Richang Hong"
                    },
                    {
                        "name": "Yue Hu"
                    },
                    {
                        "name": "Quanjun Yin"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "8 pages, 4figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10803v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10803v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.01733v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.01733v2",
                "updated": "2024-11-16T07:43:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    16,
                    7,
                    43,
                    28,
                    5,
                    321,
                    0
                ],
                "published": "2024-06-03T18:49:57Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    18,
                    49,
                    57,
                    0,
                    155,
                    0
                ],
                "title": "Learning-to-Cache: Accelerating Diffusion Transformer via Layer Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning-to-Cache: Accelerating Diffusion Transformer via Layer Caching"
                },
                "summary": "Diffusion Transformers have recently demonstrated unprecedented generative\ncapabilities for various tasks. The encouraging results, however, come with the\ncost of slow inference, since each denoising step requires inference on a\ntransformer model with a large scale of parameters. In this study, we make an\ninteresting and somehow surprising observation: the computation of a large\nproportion of layers in the diffusion transformer, through introducing a\ncaching mechanism, can be readily removed even without updating the model\nparameters. In the case of U-ViT-H/2, for example, we may remove up to 93.68%\nof the computation in the cache steps (46.84% for all steps), with less than\n0.01 drop in FID. To achieve this, we introduce a novel scheme, named\nLearning-to-Cache (L2C), that learns to conduct caching in a dynamic manner for\ndiffusion transformers. Specifically, by leveraging the identical structure of\nlayers in transformers and the sequential nature of diffusion, we explore\nredundant computations between timesteps by treating each layer as the\nfundamental unit for caching. To address the challenge of the exponential\nsearch space in deep models for identifying layers to cache and remove, we\npropose a novel differentiable optimization objective. An input-invariant yet\ntimestep-variant router is then optimized, which can finally produce a static\ncomputation graph. Experimental results show that L2C largely outperforms\nsamplers such as DDIM and DPM-Solver, alongside prior cache-based methods at\nthe same inference speed. Code is available at\nhttps://github.com/horseee/learning-to-cache",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers have recently demonstrated unprecedented generative\ncapabilities for various tasks. The encouraging results, however, come with the\ncost of slow inference, since each denoising step requires inference on a\ntransformer model with a large scale of parameters. In this study, we make an\ninteresting and somehow surprising observation: the computation of a large\nproportion of layers in the diffusion transformer, through introducing a\ncaching mechanism, can be readily removed even without updating the model\nparameters. In the case of U-ViT-H/2, for example, we may remove up to 93.68%\nof the computation in the cache steps (46.84% for all steps), with less than\n0.01 drop in FID. To achieve this, we introduce a novel scheme, named\nLearning-to-Cache (L2C), that learns to conduct caching in a dynamic manner for\ndiffusion transformers. Specifically, by leveraging the identical structure of\nlayers in transformers and the sequential nature of diffusion, we explore\nredundant computations between timesteps by treating each layer as the\nfundamental unit for caching. To address the challenge of the exponential\nsearch space in deep models for identifying layers to cache and remove, we\npropose a novel differentiable optimization objective. An input-invariant yet\ntimestep-variant router is then optimized, which can finally produce a static\ncomputation graph. Experimental results show that L2C largely outperforms\nsamplers such as DDIM and DPM-Solver, alongside prior cache-based methods at\nthe same inference speed. Code is available at\nhttps://github.com/horseee/learning-to-cache"
                },
                "authors": [
                    {
                        "name": "Xinyin Ma"
                    },
                    {
                        "name": "Gongfan Fang"
                    },
                    {
                        "name": "Michael Bi Mi"
                    },
                    {
                        "name": "Xinchao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinchao Wang"
                },
                "author": "Xinchao Wang",
                "arxiv_comment": "Accepted at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.01733v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.01733v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10659v1",
                "updated": "2024-11-16T01:39:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "published": "2024-11-16T01:39:44Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "title": "Spineless Traversal for Layout Invalidation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spineless Traversal for Layout Invalidation"
                },
                "summary": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations."
                },
                "authors": [
                    {
                        "name": "Marisa Kirisame"
                    },
                    {
                        "name": "Tiezhi Wang"
                    },
                    {
                        "name": "Pavel Panchekha"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Panchekha"
                },
                "author": "Pavel Panchekha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.16219v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.16219v4",
                "updated": "2024-11-15T22:37:48Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    22,
                    37,
                    48,
                    4,
                    320,
                    0
                ],
                "published": "2024-04-24T21:35:12Z",
                "published_parsed": [
                    2024,
                    4,
                    24,
                    21,
                    35,
                    12,
                    2,
                    115,
                    0
                ],
                "title": "Can Increasing the Hit Ratio Hurt Cache Throughput? (Long Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Increasing the Hit Ratio Hurt Cache Throughput? (Long Version)"
                },
                "summary": "Software caches are an intrinsic component of almost every computer system.\nConsequently, caching algorithms, particularly eviction policies, are the topic\nof many papers. Almost all these prior papers evaluate the caching algorithm\nbased on its hit ratio, namely the fraction of requests that are found in the\ncache, as opposed to disk. The hit ratio is viewed as a proxy for traditional\nperformance metrics like system throughput or response time. Intuitively it\nmakes sense that higher hit ratio should lead to higher throughput (and lower\nresponse time), since more requests are found in the cache (low access time) as\nopposed to the disk (high access time).\n  This paper challenges this intuition. We show that increasing the hit ratio\ncan actually hurt the throughput (and response time) for many caching\nalgorithms. Our investigation follows a three-pronged approach involving (i)\nqueueing modeling and analysis, (ii) implementation and measurement, and (iii)\nsimulation to validate the accuracy of the queueing model. We also show that\nthe phenomenon of throughput decreasing at higher hit ratios is likely to be\nmore pronounced in future systems, where the trend is towards faster disks and\nhigher numbers of cores per CPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software caches are an intrinsic component of almost every computer system.\nConsequently, caching algorithms, particularly eviction policies, are the topic\nof many papers. Almost all these prior papers evaluate the caching algorithm\nbased on its hit ratio, namely the fraction of requests that are found in the\ncache, as opposed to disk. The hit ratio is viewed as a proxy for traditional\nperformance metrics like system throughput or response time. Intuitively it\nmakes sense that higher hit ratio should lead to higher throughput (and lower\nresponse time), since more requests are found in the cache (low access time) as\nopposed to the disk (high access time).\n  This paper challenges this intuition. We show that increasing the hit ratio\ncan actually hurt the throughput (and response time) for many caching\nalgorithms. Our investigation follows a three-pronged approach involving (i)\nqueueing modeling and analysis, (ii) implementation and measurement, and (iii)\nsimulation to validate the accuracy of the queueing model. We also show that\nthe phenomenon of throughput decreasing at higher hit ratios is likely to be\nmore pronounced in future systems, where the trend is towards faster disks and\nhigher numbers of cores per CPU."
                },
                "authors": [
                    {
                        "name": "Ziyue Qiu"
                    },
                    {
                        "name": "Juncheng Yang"
                    },
                    {
                        "name": "Mor Harchol-Balter"
                    }
                ],
                "author_detail": {
                    "name": "Mor Harchol-Balter"
                },
                "author": "Mor Harchol-Balter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.16219v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.16219v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13853v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13853v2",
                "updated": "2024-11-15T22:30:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    22,
                    30,
                    38,
                    4,
                    320,
                    0
                ],
                "published": "2024-07-18T18:47:52Z",
                "published_parsed": [
                    2024,
                    7,
                    18,
                    18,
                    47,
                    52,
                    3,
                    200,
                    0
                ],
                "title": "Forecasting GPU Performance for Deep Learning Training and Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasting GPU Performance for Deep Learning Training and Inference"
                },
                "summary": "Deep learning kernels exhibit predictable memory accesses and compute\npatterns, making GPUs' parallel architecture well-suited for their execution.\nSoftware and runtime systems for GPUs are optimized to better utilize the\nstream multiprocessors, on-chip cache, and off-chip high-bandwidth memory. As\ndeep learning models and GPUs evolve, access to newer GPUs is often limited,\nraising questions about the performance of new model architectures on existing\nGPUs, existing models on new GPUs, and new model architectures on new GPUs. To\naddress these questions, we introduce NeuSight, a framework to predict the\nperformance of various deep learning models, for both training and inference,\non unseen GPUs without requiring actual execution. The framework leverages both\nGPU hardware behavior and software library optimizations to estimate end-to-end\nperformance. Previous work uses regression models that capture linear trends or\nmultilayer perceptrons to predict the overall latency of deep learning kernels\non GPUs. These approaches suffer from higher error percentages when forecasting\nperformance on unseen models and new GPUs. Instead, NeuSight decomposes the\nprediction problem into smaller problems, bounding the prediction through\nfundamental performance laws. NeuSight decomposes a single deep learning kernel\nprediction into smaller working sets called tiles, which are executed\nindependently on the GPU. Tile-granularity predictions are determined using a\nmachine learning approach and aggregated to estimate end-to-end latency.\nNeuSight outperforms prior work across various deep learning workloads and the\nlatest GPUs. It reduces the percentage error from 198% and 19.7% to 3.8% in\npredicting the latency of GPT3 model for training and inference on H100,\ncompared to state-of-the-art prior works, where both GPT3 and H100 were not\nused to train the framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning kernels exhibit predictable memory accesses and compute\npatterns, making GPUs' parallel architecture well-suited for their execution.\nSoftware and runtime systems for GPUs are optimized to better utilize the\nstream multiprocessors, on-chip cache, and off-chip high-bandwidth memory. As\ndeep learning models and GPUs evolve, access to newer GPUs is often limited,\nraising questions about the performance of new model architectures on existing\nGPUs, existing models on new GPUs, and new model architectures on new GPUs. To\naddress these questions, we introduce NeuSight, a framework to predict the\nperformance of various deep learning models, for both training and inference,\non unseen GPUs without requiring actual execution. The framework leverages both\nGPU hardware behavior and software library optimizations to estimate end-to-end\nperformance. Previous work uses regression models that capture linear trends or\nmultilayer perceptrons to predict the overall latency of deep learning kernels\non GPUs. These approaches suffer from higher error percentages when forecasting\nperformance on unseen models and new GPUs. Instead, NeuSight decomposes the\nprediction problem into smaller problems, bounding the prediction through\nfundamental performance laws. NeuSight decomposes a single deep learning kernel\nprediction into smaller working sets called tiles, which are executed\nindependently on the GPU. Tile-granularity predictions are determined using a\nmachine learning approach and aggregated to estimate end-to-end latency.\nNeuSight outperforms prior work across various deep learning workloads and the\nlatest GPUs. It reduces the percentage error from 198% and 19.7% to 3.8% in\npredicting the latency of GPT3 model for training and inference on H100,\ncompared to state-of-the-art prior works, where both GPT3 and H100 were not\nused to train the framework."
                },
                "authors": [
                    {
                        "name": "Seonho Lee"
                    },
                    {
                        "name": "Amar Phanishayee"
                    },
                    {
                        "name": "Divya Mahajan"
                    }
                ],
                "author_detail": {
                    "name": "Divya Mahajan"
                },
                "author": "Divya Mahajan",
                "arxiv_comment": "Accepted at the 30th ACM International Conference on Architectural\n  Support for Programming Languages and Operating Systems (ASPLOS), 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13853v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13853v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10510v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10510v1",
                "updated": "2024-11-15T16:24:02Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    16,
                    24,
                    2,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T16:24:02Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    16,
                    24,
                    2,
                    4,
                    320,
                    0
                ],
                "title": "SmoothCache: A Universal Inference Acceleration Technique for Diffusion\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SmoothCache: A Universal Inference Acceleration Technique for Diffusion\n  Transformers"
                },
                "summary": "Diffusion Transformers (DiT) have emerged as powerful generative models for\nvarious tasks, including image, video, and speech synthesis. However, their\ninference process remains computationally expensive due to the repeated\nevaluation of resource-intensive attention and feed-forward modules. To address\nthis, we introduce SmoothCache, a model-agnostic inference acceleration\ntechnique for DiT architectures. SmoothCache leverages the observed high\nsimilarity between layer outputs across adjacent diffusion timesteps. By\nanalyzing layer-wise representation errors from a small calibration set,\nSmoothCache adaptively caches and reuses key features during inference. Our\nexperiments demonstrate that SmoothCache achieves 8% to 71% speed up while\nmaintaining or even improving generation quality across diverse modalities. We\nshowcase its effectiveness on DiT-XL for image generation, Open-Sora for\ntext-to-video, and Stable Audio Open for text-to-audio, highlighting its\npotential to enable real-time applications and broaden the accessibility of\npowerful DiT models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) have emerged as powerful generative models for\nvarious tasks, including image, video, and speech synthesis. However, their\ninference process remains computationally expensive due to the repeated\nevaluation of resource-intensive attention and feed-forward modules. To address\nthis, we introduce SmoothCache, a model-agnostic inference acceleration\ntechnique for DiT architectures. SmoothCache leverages the observed high\nsimilarity between layer outputs across adjacent diffusion timesteps. By\nanalyzing layer-wise representation errors from a small calibration set,\nSmoothCache adaptively caches and reuses key features during inference. Our\nexperiments demonstrate that SmoothCache achieves 8% to 71% speed up while\nmaintaining or even improving generation quality across diverse modalities. We\nshowcase its effectiveness on DiT-XL for image generation, Open-Sora for\ntext-to-video, and Stable Audio Open for text-to-audio, highlighting its\npotential to enable real-time applications and broaden the accessibility of\npowerful DiT models."
                },
                "authors": [
                    {
                        "name": "Joseph Liu"
                    },
                    {
                        "name": "Joshua Geddes"
                    },
                    {
                        "name": "Ziyu Guo"
                    },
                    {
                        "name": "Haomiao Jiang"
                    },
                    {
                        "name": "Mahesh Kumar Nandwana"
                    }
                ],
                "author_detail": {
                    "name": "Mahesh Kumar Nandwana"
                },
                "author": "Mahesh Kumar Nandwana",
                "arxiv_comment": "Code can be found at https://github.com/Roblox/SmoothCache",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10510v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10510v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03174v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03174v2",
                "updated": "2024-11-15T07:25:54Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    7,
                    25,
                    54,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-05T15:22:11Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    15,
                    22,
                    11,
                    1,
                    310,
                    0
                ],
                "title": "ZipCache: A DRAM/SSD Cache with Built-in Transparent Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZipCache: A DRAM/SSD Cache with Built-in Transparent Compression"
                },
                "summary": "As a core component in modern data centers, key-value cache provides\nhigh-throughput and low-latency services for high-speed data processing. The\neffectiveness of a key-value cache relies on its ability of accommodating the\nneeded data. However, expanding the cache capacity is often more difficult than\ncommonly expected because of many practical constraints, such as server costs,\ncooling issues, rack space, and even human resource expenses. A potential\nsolution is compression, which virtually extends the cache capacity by\ncondensing data in cache. In practice, this seemingly simple idea has not\ngained much traction in key-value cache system design, due to several critical\nissues: the compression-unfriendly index structure, severe read/write\namplification, wasteful decompression operations, and heavy computing cost.\nThis paper presents a hybrid DRAM-SSD cache design to realize a systematic\nintegration of data compression in key-value cache. By treating compression as\nan essential component, we have redesigned the indexing structure, data\nmanagement, and leveraged the emerging computational SSD hardware for\ncollaborative optimizations. We have developed a prototype, called ZipCache.\nOur experimental results show that ZipCache can achieve up to 72.4% higher\nthroughput and 42.4% lower latency, while reducing the write amplification by\nup to 26.2 times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a core component in modern data centers, key-value cache provides\nhigh-throughput and low-latency services for high-speed data processing. The\neffectiveness of a key-value cache relies on its ability of accommodating the\nneeded data. However, expanding the cache capacity is often more difficult than\ncommonly expected because of many practical constraints, such as server costs,\ncooling issues, rack space, and even human resource expenses. A potential\nsolution is compression, which virtually extends the cache capacity by\ncondensing data in cache. In practice, this seemingly simple idea has not\ngained much traction in key-value cache system design, due to several critical\nissues: the compression-unfriendly index structure, severe read/write\namplification, wasteful decompression operations, and heavy computing cost.\nThis paper presents a hybrid DRAM-SSD cache design to realize a systematic\nintegration of data compression in key-value cache. By treating compression as\nan essential component, we have redesigned the indexing structure, data\nmanagement, and leveraged the emerging computational SSD hardware for\ncollaborative optimizations. We have developed a prototype, called ZipCache.\nOur experimental results show that ZipCache can achieve up to 72.4% higher\nthroughput and 42.4% lower latency, while reducing the write amplification by\nup to 26.2 times."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Alex Zhong"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03174v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03174v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09859v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09859v1",
                "updated": "2024-11-15T00:37:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    0,
                    37,
                    31,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T00:37:31Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    0,
                    37,
                    31,
                    4,
                    320,
                    0
                ],
                "title": "Skew-Symmetric Matrix Decompositions on Shared-Memory Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skew-Symmetric Matrix Decompositions on Shared-Memory Architectures"
                },
                "summary": "The factorization of skew-symmetric matrices is a critically understudied\narea of dense linear algebra (DLA), particularly in comparison to that of\nsymmetric matrices. While some algorithms can be adapted from the symmetric\ncase, the cost of algorithms can be reduced by exploiting skew-symmetry. A\nmotivating example is the factorization $X=LTL^T$ of a skew-symmetric matrix\n$X$, which is used in practical applications as a means of determining the\ndeterminant of $X$ as the square of the (cheaply-computed) Pfaffian of the\nskew-symmetric tridiagonal matrix $T$, for example in fields such as quantum\nelectronic structure and machine learning. Such applications also often require\npivoting in order to improve numerical stability. In this work we explore a\ncombination of known literature algorithms and new algorithms recently derived\nusing formal methods. High-performance parallel CPU implementations are\ncreated, leveraging the concept of fusion at multiple levels in order to reduce\nmemory traffic overhead, as well as the BLIS framework which provides\nhigh-performance GEMM kernels, hierarchical parallelism, and cache blocking. We\nfind that operation fusion and improved use of available bandwidth via\nparallelization of bandwidth-bound (level-2 BLAS) operations are essential for\nobtaining high performance, while a concise C++ implementation provides a clear\nand close connection to the formal derivation process without sacrificing\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The factorization of skew-symmetric matrices is a critically understudied\narea of dense linear algebra (DLA), particularly in comparison to that of\nsymmetric matrices. While some algorithms can be adapted from the symmetric\ncase, the cost of algorithms can be reduced by exploiting skew-symmetry. A\nmotivating example is the factorization $X=LTL^T$ of a skew-symmetric matrix\n$X$, which is used in practical applications as a means of determining the\ndeterminant of $X$ as the square of the (cheaply-computed) Pfaffian of the\nskew-symmetric tridiagonal matrix $T$, for example in fields such as quantum\nelectronic structure and machine learning. Such applications also often require\npivoting in order to improve numerical stability. In this work we explore a\ncombination of known literature algorithms and new algorithms recently derived\nusing formal methods. High-performance parallel CPU implementations are\ncreated, leveraging the concept of fusion at multiple levels in order to reduce\nmemory traffic overhead, as well as the BLIS framework which provides\nhigh-performance GEMM kernels, hierarchical parallelism, and cache blocking. We\nfind that operation fusion and improved use of available bandwidth via\nparallelization of bandwidth-bound (level-2 BLAS) operations are essential for\nobtaining high performance, while a concise C++ implementation provides a clear\nand close connection to the formal derivation process without sacrificing\nperformance."
                },
                "authors": [
                    {
                        "name": "Ishna Satyarth"
                    },
                    {
                        "name": "Chao Yin"
                    },
                    {
                        "name": "RuQing G. Xu"
                    },
                    {
                        "name": "Devin A. Matthews"
                    }
                ],
                "author_detail": {
                    "name": "Devin A. Matthews"
                },
                "author": "Devin A. Matthews",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09859v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09859v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09812v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09812v1",
                "updated": "2024-11-14T21:01:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    21,
                    1,
                    29,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T21:01:29Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    21,
                    1,
                    29,
                    3,
                    319,
                    0
                ],
                "title": "Edge Caching Optimization with PPO and Transfer Learning for Dynamic\n  Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge Caching Optimization with PPO and Transfer Learning for Dynamic\n  Environments"
                },
                "summary": "This paper addresses the challenge of edge caching in dynamic environments,\nwhere rising traffic loads strain backhaul links and core networks. We propose\na Proximal Policy Optimization (PPO)-based caching strategy that fully\nincorporates key file attributes such as size, lifetime, importance, and\npopularity, while also considering random file request arrivals, reflecting\nmore realistic edge caching scenarios. In dynamic environments, changes such as\nshifts in content popularity and variations in request rates frequently occur,\nmaking previously learned policies less effective as they were optimized for\nearlier conditions. Without adaptation, caching efficiency and response times\ncan degrade. While learning a new policy from scratch in a new environment is\nan option, it is highly inefficient and computationally expensive. Thus,\nadapting an existing policy to these changes is critical. To address this, we\ndevelop a mechanism that detects changes in content popularity and request\nrates, ensuring timely adjustments to the caching strategy. We also propose a\ntransfer learning-based PPO algorithm that accelerates convergence in new\nenvironments by leveraging prior knowledge. Simulation results demonstrate the\nsignificant effectiveness of our approach, outperforming a recent Deep\nReinforcement Learning (DRL)-based method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses the challenge of edge caching in dynamic environments,\nwhere rising traffic loads strain backhaul links and core networks. We propose\na Proximal Policy Optimization (PPO)-based caching strategy that fully\nincorporates key file attributes such as size, lifetime, importance, and\npopularity, while also considering random file request arrivals, reflecting\nmore realistic edge caching scenarios. In dynamic environments, changes such as\nshifts in content popularity and variations in request rates frequently occur,\nmaking previously learned policies less effective as they were optimized for\nearlier conditions. Without adaptation, caching efficiency and response times\ncan degrade. While learning a new policy from scratch in a new environment is\nan option, it is highly inefficient and computationally expensive. Thus,\nadapting an existing policy to these changes is critical. To address this, we\ndevelop a mechanism that detects changes in content popularity and request\nrates, ensuring timely adjustments to the caching strategy. We also propose a\ntransfer learning-based PPO algorithm that accelerates convergence in new\nenvironments by leveraging prior knowledge. Simulation results demonstrate the\nsignificant effectiveness of our approach, outperforming a recent Deep\nReinforcement Learning (DRL)-based method."
                },
                "authors": [
                    {
                        "name": "Farnaz Niknia"
                    },
                    {
                        "name": "Ping Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ping Wang"
                },
                "author": "Ping Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09812v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09812v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09546v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09546v1",
                "updated": "2024-11-14T16:01:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    16,
                    1,
                    5,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T16:01:05Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    16,
                    1,
                    5,
                    3,
                    319,
                    0
                ],
                "title": "Architectural Exploration of Application-Specific Resonant SRAM\n  Compute-in-Memory (rCiM)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Architectural Exploration of Application-Specific Resonant SRAM\n  Compute-in-Memory (rCiM)"
                },
                "summary": "While general-purpose computing follows Von Neumann's architecture, the data\nmovement between memory and processor elements dictates the processor's\nperformance. The evolving compute-in-memory (CiM) paradigm tackles this issue\nby facilitating simultaneous processing and storage within static random-access\nmemory (SRAM) elements. Numerous design decisions taken at different levels of\nhierarchy affect the figure of merits (FoMs) of SRAM, such as power,\nperformance, area, and yield. The absence of a rapid assessment mechanism for\nthe impact of changes at different hierarchy levels on global FoMs poses a\nchallenge to accurately evaluating innovative SRAM designs. This paper presents\nan automation tool designed to optimize the energy and latency of SRAM designs\nincorporating diverse implementation strategies for executing logic operations\nwithin the SRAM. The tool structure allows easy comparison across different\narray topologies and various design strategies to result in energy-efficient\nimplementations. Our study involves a comprehensive comparison of over 6900+\ndistinct design implementation strategies for EPFL combinational benchmark\ncircuits on the energy-recycling resonant compute-in-memory (rCiM) architecture\ndesigned using TSMC 28 nm technology. When provided with a combinational\ncircuit, the tool aims to generate an energy-efficient implementation strategy\ntailored to the specified input memory and latency constraints. The tool\nreduces 80.9% of energy consumption on average across all benchmarks while\nusing the six-topology implementation compared to baseline implementation of\nsingle-macro topology by considering the parallel processing capability of rCiM\ncache size ranging from 4KB to 192KB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While general-purpose computing follows Von Neumann's architecture, the data\nmovement between memory and processor elements dictates the processor's\nperformance. The evolving compute-in-memory (CiM) paradigm tackles this issue\nby facilitating simultaneous processing and storage within static random-access\nmemory (SRAM) elements. Numerous design decisions taken at different levels of\nhierarchy affect the figure of merits (FoMs) of SRAM, such as power,\nperformance, area, and yield. The absence of a rapid assessment mechanism for\nthe impact of changes at different hierarchy levels on global FoMs poses a\nchallenge to accurately evaluating innovative SRAM designs. This paper presents\nan automation tool designed to optimize the energy and latency of SRAM designs\nincorporating diverse implementation strategies for executing logic operations\nwithin the SRAM. The tool structure allows easy comparison across different\narray topologies and various design strategies to result in energy-efficient\nimplementations. Our study involves a comprehensive comparison of over 6900+\ndistinct design implementation strategies for EPFL combinational benchmark\ncircuits on the energy-recycling resonant compute-in-memory (rCiM) architecture\ndesigned using TSMC 28 nm technology. When provided with a combinational\ncircuit, the tool aims to generate an energy-efficient implementation strategy\ntailored to the specified input memory and latency constraints. The tool\nreduces 80.9% of energy consumption on average across all benchmarks while\nusing the six-topology implementation compared to baseline implementation of\nsingle-macro topology by considering the parallel processing capability of rCiM\ncache size ranging from 4KB to 192KB."
                },
                "authors": [
                    {
                        "name": "Dhandeep Challagundla"
                    },
                    {
                        "name": "Ignatius Bezzam"
                    },
                    {
                        "name": "Riadul Islam"
                    }
                ],
                "author_detail": {
                    "name": "Riadul Islam"
                },
                "author": "Riadul Islam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09546v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09546v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09473v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09473v1",
                "updated": "2024-11-14T14:28:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    14,
                    28,
                    31,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T14:28:31Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    14,
                    28,
                    31,
                    3,
                    319,
                    0
                ],
                "title": "Enhancing Scalability and Performance in Influence Maximization with\n  Optimized Parallel Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Scalability and Performance in Influence Maximization with\n  Optimized Parallel Processing"
                },
                "summary": "Influence Maximization (IM) is vital in viral marketing and biological\nnetwork analysis for identifying key influencers. Given its NP-hard nature,\napproximate solutions are employed. This paper addresses scalability challenges\nin scale-out shared memory system by focusing on the state-of-the-art Influence\nMaximization via Martingales (IMM) benchmark. To enhance the work efficiency of\nthe current IMM implementation, we propose EFFICIENTIMM with key strategies,\nincluding new parallelization scheme, NUMA-aware memory usage, dynamic load\nbalancing and fine-grained adaptive data structures. Benchmarking on a 128-core\nCPU system with 8 NUMA nodes, EFFICIENTIMM demonstrated significant performance\nimprovements, achieving an average 5.9x speedup over Ripples across 8 diverse\nSNAP datasets, when compared to the best execution times of the original\nRipples framework. Additionally, on the Youtube graph, EFFICIENTIMM\ndemonstrates a better memory access pattern with 357.4x reduction in L1+L2\ncache misses as compared to Ripples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Influence Maximization (IM) is vital in viral marketing and biological\nnetwork analysis for identifying key influencers. Given its NP-hard nature,\napproximate solutions are employed. This paper addresses scalability challenges\nin scale-out shared memory system by focusing on the state-of-the-art Influence\nMaximization via Martingales (IMM) benchmark. To enhance the work efficiency of\nthe current IMM implementation, we propose EFFICIENTIMM with key strategies,\nincluding new parallelization scheme, NUMA-aware memory usage, dynamic load\nbalancing and fine-grained adaptive data structures. Benchmarking on a 128-core\nCPU system with 8 NUMA nodes, EFFICIENTIMM demonstrated significant performance\nimprovements, achieving an average 5.9x speedup over Ripples across 8 diverse\nSNAP datasets, when compared to the best execution times of the original\nRipples framework. Additionally, on the Youtube graph, EFFICIENTIMM\ndemonstrates a better memory access pattern with 357.4x reduction in L1+L2\ncache misses as compared to Ripples."
                },
                "authors": [
                    {
                        "name": "Hanjiang Wu"
                    },
                    {
                        "name": "Huan Xu"
                    },
                    {
                        "name": "Joongun Park"
                    },
                    {
                        "name": "Jesmin Jahan Tithi"
                    },
                    {
                        "name": "Fabio Checconi"
                    },
                    {
                        "name": "Jordi Wolfson-Pou"
                    },
                    {
                        "name": "Fabrizio Petrini"
                    },
                    {
                        "name": "Tushar Krishna"
                    }
                ],
                "author_detail": {
                    "name": "Tushar Krishna"
                },
                "author": "Tushar Krishna",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09473v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09473v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09425v1",
                "updated": "2024-11-14T13:22:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    13,
                    22,
                    41,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T13:22:41Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    13,
                    22,
                    41,
                    3,
                    319,
                    0
                ],
                "title": "MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity"
                },
                "summary": "Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully."
                },
                "authors": [
                    {
                        "name": "Xiao Lv"
                    },
                    {
                        "name": "Jiangxia Cao"
                    },
                    {
                        "name": "Shijie Guan"
                    },
                    {
                        "name": "Xiaoyou Zhou"
                    },
                    {
                        "name": "Zhiguang Qi"
                    },
                    {
                        "name": "Yaqiang Zang"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Ben Wang"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Guorui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guorui Zhou"
                },
                "author": "Guorui Zhou",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "N/A",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09317v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09317v1",
                "updated": "2024-11-14T09:50:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    9,
                    50,
                    41,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T09:50:41Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    9,
                    50,
                    41,
                    3,
                    319,
                    0
                ],
                "title": "Pie: Pooling CPU Memory for LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pie: Pooling CPU Memory for LLM Inference"
                },
                "summary": "The rapid growth of LLMs has revolutionized natural language processing and\nAI analysis, but their increasing size and memory demands present significant\nchallenges. A common solution is to spill over to CPU memory; however,\ntraditional GPU-CPU memory swapping often results in higher latency and lower\nthroughput.\n  This paper introduces Pie, an LLM inference framework that addresses these\nchallenges with performance-transparent swapping and adaptive expansion. By\nleveraging predictable memory access patterns and the high bandwidth of modern\nhardware like the NVIDIA GH200 Grace Hopper Superchip, Pie enables concurrent\ndata swapping without affecting foreground computation, expanding effective\nmemory without added latency. Adaptive expansion dynamically adjusts CPU memory\nallocation based on real-time information, optimizing memory usage and\nperformance under varying conditions.\n  Pie maintains low computation latency, high throughput, and high elasticity.\nOur experimental evaluation demonstrates that Pie achieves optimal swapping\npolicy during cache warmup and effectively balances increased memory capacity\nwith negligible impact on computation. With its extended capacity, Pie\noutperforms vLLM by up to 1.9X in throughput and 2X in latency. Additionally,\nPie can reduce GPU memory usage by up to 1.67X while maintaining the same\nperformance. Compared to FlexGen, an offline profiling-based swapping solution,\nPie achieves magnitudes lower latency and 9.4X higher throughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of LLMs has revolutionized natural language processing and\nAI analysis, but their increasing size and memory demands present significant\nchallenges. A common solution is to spill over to CPU memory; however,\ntraditional GPU-CPU memory swapping often results in higher latency and lower\nthroughput.\n  This paper introduces Pie, an LLM inference framework that addresses these\nchallenges with performance-transparent swapping and adaptive expansion. By\nleveraging predictable memory access patterns and the high bandwidth of modern\nhardware like the NVIDIA GH200 Grace Hopper Superchip, Pie enables concurrent\ndata swapping without affecting foreground computation, expanding effective\nmemory without added latency. Adaptive expansion dynamically adjusts CPU memory\nallocation based on real-time information, optimizing memory usage and\nperformance under varying conditions.\n  Pie maintains low computation latency, high throughput, and high elasticity.\nOur experimental evaluation demonstrates that Pie achieves optimal swapping\npolicy during cache warmup and effectively balances increased memory capacity\nwith negligible impact on computation. With its extended capacity, Pie\noutperforms vLLM by up to 1.9X in throughput and 2X in latency. Additionally,\nPie can reduce GPU memory usage by up to 1.67X while maintaining the same\nperformance. Compared to FlexGen, an offline profiling-based swapping solution,\nPie achieves magnitudes lower latency and 9.4X higher throughput."
                },
                "authors": [
                    {
                        "name": "Yi Xu"
                    },
                    {
                        "name": "Ziming Mao"
                    },
                    {
                        "name": "Xiangxi Mo"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09317v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09317v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09275v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09275v1",
                "updated": "2024-11-14T08:25:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    8,
                    25,
                    31,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T08:25:31Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    8,
                    25,
                    31,
                    3,
                    319,
                    0
                ],
                "title": "Pkd-tree: Parallel $k$d-tree with Batch Updates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pkd-tree: Parallel $k$d-tree with Batch Updates"
                },
                "summary": "The $k$d-tree is one of the most widely used data structures to manage\nmulti-dimensional data. Due to the ever-growing data volume, it is imperative\nto consider parallelism in $k$d-trees. However, we observed challenges in\nexisting parallel kd-tree implementations, for both constructions and updates.\n  The goal of this paper is to develop efficient in-memory $k$d-trees by\nsupporting high parallelism and cache-efficiency. We propose the Pkd-tree\n(Parallel $k$d-tree), a parallel $k$d-tree that is efficient both in theory and\nin practice. The Pkd-tree supports parallel tree construction, batch update\n(insertion and deletion), and various queries including k-nearest neighbor\nsearch, range query, and range count. We proved that our algorithms have strong\ntheoretical bounds in work (sequential time complexity), span (parallelism),\nand cache complexity. Our key techniques include 1) an efficient construction\nalgorithm that optimizes work, span, and cache complexity simultaneously, and\n2) reconstruction-based update algorithms that guarantee the tree to be\nweight-balanced. With the new algorithmic insights and careful engineering\neffort, we achieved a highly optimized implementation of the Pkd-tree.\n  We tested Pkd-tree with various synthetic and real-world datasets, including\nboth uniform and highly skewed data. We compare the Pkd-tree with\nstate-of-the-art parallel $k$d-tree implementations. In all tests, with better\nor competitive query performance, Pkd-tree is much faster in construction and\nupdates consistently than all baselines. We released our code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The $k$d-tree is one of the most widely used data structures to manage\nmulti-dimensional data. Due to the ever-growing data volume, it is imperative\nto consider parallelism in $k$d-trees. However, we observed challenges in\nexisting parallel kd-tree implementations, for both constructions and updates.\n  The goal of this paper is to develop efficient in-memory $k$d-trees by\nsupporting high parallelism and cache-efficiency. We propose the Pkd-tree\n(Parallel $k$d-tree), a parallel $k$d-tree that is efficient both in theory and\nin practice. The Pkd-tree supports parallel tree construction, batch update\n(insertion and deletion), and various queries including k-nearest neighbor\nsearch, range query, and range count. We proved that our algorithms have strong\ntheoretical bounds in work (sequential time complexity), span (parallelism),\nand cache complexity. Our key techniques include 1) an efficient construction\nalgorithm that optimizes work, span, and cache complexity simultaneously, and\n2) reconstruction-based update algorithms that guarantee the tree to be\nweight-balanced. With the new algorithmic insights and careful engineering\neffort, we achieved a highly optimized implementation of the Pkd-tree.\n  We tested Pkd-tree with various synthetic and real-world datasets, including\nboth uniform and highly skewed data. We compare the Pkd-tree with\nstate-of-the-art parallel $k$d-tree implementations. In all tests, with better\nor competitive query performance, Pkd-tree is much faster in construction and\nupdates consistently than all baselines. We released our code."
                },
                "authors": [
                    {
                        "name": "Ziyang Men"
                    },
                    {
                        "name": "Zheqi Shen"
                    },
                    {
                        "name": "Yan Gu"
                    },
                    {
                        "name": "Yihan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yihan Sun"
                },
                "author": "Yihan Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09275v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09275v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19258v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19258v3",
                "updated": "2024-11-14T01:56:11Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    1,
                    56,
                    11,
                    3,
                    319,
                    0
                ],
                "published": "2024-10-25T02:22:00Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    2,
                    22,
                    0,
                    4,
                    299,
                    0
                ],
                "title": "Not All Heads Matter: A Head-Level KV Cache Compression Method with\n  Integrated Retrieval and Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Heads Matter: A Head-Level KV Cache Compression Method with\n  Integrated Retrieval and Reasoning"
                },
                "summary": "Key-Value (KV) caching is a common technique to enhance the computational\nefficiency of Large Language Models (LLMs), but its memory overhead grows\nrapidly with input length. Prior work has shown that not all tokens are equally\nimportant for text generation, proposing layer-level KV cache compression to\nselectively retain key information. Recognizing the distinct roles of attention\nheads in generation, we propose HeadKV, a head-level KV cache compression\nmethod, and HeadKV-R2, which leverages a novel contextual reasoning ability\nestimation for compression. Our approach operates at the level of individual\nheads, estimating their importance for contextual QA tasks that require both\nretrieval and reasoning capabilities. Extensive experiments across diverse\nbenchmarks (LongBench, LooGLE), model architectures (e.g., Llama-3-8B-Instruct,\nMistral-7B-Instruct), and long-context abilities tests demonstrate that our\nhead-level KV cache compression significantly outperforms strong baselines,\nparticularly in low-resource settings (KV size = 64 & 128). Notably, our method\nretains just 1.5% of the KV cache while achieving 97% of the performance of the\nfull KV cache on the contextual question answering benchmark.Codes are\navailable at https://github.com/FYYFU/HeadKV",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) caching is a common technique to enhance the computational\nefficiency of Large Language Models (LLMs), but its memory overhead grows\nrapidly with input length. Prior work has shown that not all tokens are equally\nimportant for text generation, proposing layer-level KV cache compression to\nselectively retain key information. Recognizing the distinct roles of attention\nheads in generation, we propose HeadKV, a head-level KV cache compression\nmethod, and HeadKV-R2, which leverages a novel contextual reasoning ability\nestimation for compression. Our approach operates at the level of individual\nheads, estimating their importance for contextual QA tasks that require both\nretrieval and reasoning capabilities. Extensive experiments across diverse\nbenchmarks (LongBench, LooGLE), model architectures (e.g., Llama-3-8B-Instruct,\nMistral-7B-Instruct), and long-context abilities tests demonstrate that our\nhead-level KV cache compression significantly outperforms strong baselines,\nparticularly in low-resource settings (KV size = 64 & 128). Notably, our method\nretains just 1.5% of the KV cache while achieving 97% of the performance of the\nfull KV cache on the contextual question answering benchmark.Codes are\navailable at https://github.com/FYYFU/HeadKV"
                },
                "authors": [
                    {
                        "name": "Yu Fu"
                    },
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Abedelkadir Asi"
                    },
                    {
                        "name": "Wayne Xiong"
                    },
                    {
                        "name": "Yue Dong"
                    },
                    {
                        "name": "Wen Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Wen Xiao"
                },
                "author": "Wen Xiao",
                "arxiv_comment": "18pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19258v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19258v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08672v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08672v1",
                "updated": "2024-11-13T15:07:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    13,
                    15,
                    7,
                    15,
                    2,
                    318,
                    0
                ],
                "published": "2024-11-13T15:07:15Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    15,
                    7,
                    15,
                    2,
                    318,
                    0
                ],
                "title": "Joint Model Caching and Resource Allocation in Generative AI-Enabled\n  Wireless Edge Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Model Caching and Resource Allocation in Generative AI-Enabled\n  Wireless Edge Networks"
                },
                "summary": "With the rapid advancement of artificial intelligence (AI), generative AI\n(GenAI) has emerged as a transformative tool, enabling customized and\npersonalized AI-generated content (AIGC) services. However, GenAI models with\nbillions of parameters require substantial memory capacity and computational\npower for deployment and execution, presenting significant challenges to\nresource-limited edge networks. In this paper, we address the joint model\ncaching and resource allocation problem in GenAI-enabled wireless edge\nnetworks. Our objective is to balance the trade-off between delivering\nhigh-quality AIGC and minimizing the delay in AIGC service provisioning. To\ntackle this problem, we employ a deep deterministic policy gradient\n(DDPG)-based reinforcement learning approach, capable of efficiently\ndetermining optimal model caching and resource allocation decisions for AIGC\nservices in response to user mobility and time-varying channel conditions.\nNumerical results demonstrate that DDPG achieves a higher model hit ratio and\nprovides superior-quality, lower-latency AIGC services compared to other\nbenchmark solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid advancement of artificial intelligence (AI), generative AI\n(GenAI) has emerged as a transformative tool, enabling customized and\npersonalized AI-generated content (AIGC) services. However, GenAI models with\nbillions of parameters require substantial memory capacity and computational\npower for deployment and execution, presenting significant challenges to\nresource-limited edge networks. In this paper, we address the joint model\ncaching and resource allocation problem in GenAI-enabled wireless edge\nnetworks. Our objective is to balance the trade-off between delivering\nhigh-quality AIGC and minimizing the delay in AIGC service provisioning. To\ntackle this problem, we employ a deep deterministic policy gradient\n(DDPG)-based reinforcement learning approach, capable of efficiently\ndetermining optimal model caching and resource allocation decisions for AIGC\nservices in response to user mobility and time-varying channel conditions.\nNumerical results demonstrate that DDPG achieves a higher model hit ratio and\nprovides superior-quality, lower-latency AIGC services compared to other\nbenchmark solutions."
                },
                "authors": [
                    {
                        "name": "Zhang Liu"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Lianfen Huang"
                    },
                    {
                        "name": "Zhibin Gao"
                    },
                    {
                        "name": "Dusit Niyato"
                    }
                ],
                "author_detail": {
                    "name": "Dusit Niyato"
                },
                "author": "Dusit Niyato",
                "arxiv_comment": "conference paper with 6 pages and 5 figures. arXiv admin note: text\n  overlap with arXiv:2411.01458",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08672v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08672v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2412.03570v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03570v1",
                "updated": "2024-12-04T18:59:24Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    18,
                    59,
                    24,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T18:59:24Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    18,
                    59,
                    24,
                    2,
                    339,
                    0
                ],
                "title": "Sparse-view Pose Estimation and Reconstruction via Analysis by\n  Generative Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse-view Pose Estimation and Reconstruction via Analysis by\n  Generative Synthesis"
                },
                "summary": "Inferring the 3D structure underlying a set of multi-view images typically\nrequires solving two co-dependent tasks -- accurate 3D reconstruction requires\nprecise camera poses, and predicting camera poses relies on (implicitly or\nexplicitly) modeling the underlying 3D. The classical framework of analysis by\nsynthesis casts this inference as a joint optimization seeking to explain the\nobserved pixels, and recent instantiations learn expressive 3D representations\n(e.g., Neural Fields) with gradient-descent-based pose refinement of initial\npose estimates. However, given a sparse set of observed views, the observations\nmay not provide sufficient direct evidence to obtain complete and accurate 3D.\nMoreover, large errors in pose estimation may not be easily corrected and can\nfurther degrade the inferred 3D. To allow robust 3D reconstruction and pose\nestimation in this challenging setup, we propose SparseAGS, a method that\nadapts this analysis-by-synthesis approach by: a) including\nnovel-view-synthesis-based generative priors in conjunction with photometric\nobjectives to improve the quality of the inferred 3D, and b) explicitly\nreasoning about outliers and using a discrete search with a continuous\noptimization-based strategy to correct them. We validate our framework across\nreal-world and synthetic datasets in combination with several off-the-shelf\npose estimation systems as initialization. We find that it significantly\nimproves the base systems' pose accuracy while yielding high-quality 3D\nreconstructions that outperform the results from current multi-view\nreconstruction baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring the 3D structure underlying a set of multi-view images typically\nrequires solving two co-dependent tasks -- accurate 3D reconstruction requires\nprecise camera poses, and predicting camera poses relies on (implicitly or\nexplicitly) modeling the underlying 3D. The classical framework of analysis by\nsynthesis casts this inference as a joint optimization seeking to explain the\nobserved pixels, and recent instantiations learn expressive 3D representations\n(e.g., Neural Fields) with gradient-descent-based pose refinement of initial\npose estimates. However, given a sparse set of observed views, the observations\nmay not provide sufficient direct evidence to obtain complete and accurate 3D.\nMoreover, large errors in pose estimation may not be easily corrected and can\nfurther degrade the inferred 3D. To allow robust 3D reconstruction and pose\nestimation in this challenging setup, we propose SparseAGS, a method that\nadapts this analysis-by-synthesis approach by: a) including\nnovel-view-synthesis-based generative priors in conjunction with photometric\nobjectives to improve the quality of the inferred 3D, and b) explicitly\nreasoning about outliers and using a discrete search with a continuous\noptimization-based strategy to correct them. We validate our framework across\nreal-world and synthetic datasets in combination with several off-the-shelf\npose estimation systems as initialization. We find that it significantly\nimproves the base systems' pose accuracy while yielding high-quality 3D\nreconstructions that outperform the results from current multi-view\nreconstruction baselines."
                },
                "authors": [
                    {
                        "name": "Qitao Zhao"
                    },
                    {
                        "name": "Shubham Tulsiani"
                    }
                ],
                "author_detail": {
                    "name": "Shubham Tulsiani"
                },
                "author": "Shubham Tulsiani",
                "arxiv_comment": "NeurIPS 2024. Project website: https://qitaozhao.github.io/SparseAGS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03570v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03570v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03563v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03563v1",
                "updated": "2024-12-04T18:56:37Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    18,
                    56,
                    37,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T18:56:37Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    18,
                    56,
                    37,
                    2,
                    339,
                    0
                ],
                "title": "From Individual to Society: A Survey on Social Simulation Driven by\n  Large Language Model-based Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Individual to Society: A Survey on Social Simulation Driven by\n  Large Language Model-based Agents"
                },
                "summary": "Traditional sociological research often relies on human participation, which,\nthough effective, is expensive, challenging to scale, and with ethical\nconcerns. Recent advancements in large language models (LLMs) highlight their\npotential to simulate human behavior, enabling the replication of individual\nresponses and facilitating studies on many interdisciplinary studies. In this\npaper, we conduct a comprehensive survey of this field, illustrating the recent\nprogress in simulation driven by LLM-empowered agents. We categorize the\nsimulations into three types: (1) Individual Simulation, which mimics specific\nindividuals or demographic groups; (2) Scenario Simulation, where multiple\nagents collaborate to achieve goals within specific contexts; and (3) Society\nSimulation, which models interactions within agent societies to reflect the\ncomplexity and variety of real-world dynamics. These simulations follow a\nprogression, ranging from detailed individual modeling to large-scale societal\nphenomena. We provide a detailed discussion of each simulation type, including\nthe architecture or key components of the simulation, the classification of\nobjectives or scenarios and the evaluation method. Afterward, we summarize\ncommonly used datasets and benchmarks. Finally, we discuss the trends across\nthese three types of simulation. A repository for the related sources is at\n{\\url{https://github.com/FudanDISC/SocialAgent}}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional sociological research often relies on human participation, which,\nthough effective, is expensive, challenging to scale, and with ethical\nconcerns. Recent advancements in large language models (LLMs) highlight their\npotential to simulate human behavior, enabling the replication of individual\nresponses and facilitating studies on many interdisciplinary studies. In this\npaper, we conduct a comprehensive survey of this field, illustrating the recent\nprogress in simulation driven by LLM-empowered agents. We categorize the\nsimulations into three types: (1) Individual Simulation, which mimics specific\nindividuals or demographic groups; (2) Scenario Simulation, where multiple\nagents collaborate to achieve goals within specific contexts; and (3) Society\nSimulation, which models interactions within agent societies to reflect the\ncomplexity and variety of real-world dynamics. These simulations follow a\nprogression, ranging from detailed individual modeling to large-scale societal\nphenomena. We provide a detailed discussion of each simulation type, including\nthe architecture or key components of the simulation, the classification of\nobjectives or scenarios and the evaluation method. Afterward, we summarize\ncommonly used datasets and benchmarks. Finally, we discuss the trends across\nthese three types of simulation. A repository for the related sources is at\n{\\url{https://github.com/FudanDISC/SocialAgent}}."
                },
                "authors": [
                    {
                        "name": "Xinyi Mou"
                    },
                    {
                        "name": "Xuanwen Ding"
                    },
                    {
                        "name": "Qi He"
                    },
                    {
                        "name": "Liang Wang"
                    },
                    {
                        "name": "Jingcong Liang"
                    },
                    {
                        "name": "Xinnong Zhang"
                    },
                    {
                        "name": "Libo Sun"
                    },
                    {
                        "name": "Jiayu Lin"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Xuanjing Huang"
                    },
                    {
                        "name": "Zhongyu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Zhongyu Wei"
                },
                "author": "Zhongyu Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03563v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03563v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03553v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03553v1",
                "updated": "2024-12-04T18:50:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    18,
                    50,
                    12,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T18:50:12Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    18,
                    50,
                    12,
                    2,
                    339,
                    0
                ],
                "title": "BinSparX: Sparsified Binary Neural Networks for Reduced Hardware\n  Non-Idealities in Xbar Arrays",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BinSparX: Sparsified Binary Neural Networks for Reduced Hardware\n  Non-Idealities in Xbar Arrays"
                },
                "summary": "Compute-in-memory (CiM)-based binary neural network (CiM-BNN) accelerators\nmarry the benefits of CiM and ultra-low precision quantization, making them\nhighly suitable for edge computing. However, CiM-enabled crossbar (Xbar) arrays\nare plagued with hardware non-idealities like parasitic resistances and device\nnon-linearities that impair inference accuracy, especially in scaled\ntechnologies. In this work, we first analyze the impact of Xbar non-idealities\non the inference accuracy of various CiM-BNNs, establishing that the unique\nproperties of CiM-BNNs make them more prone to hardware non-idealities compared\nto higher precision deep neural networks (DNNs). To address this issue, we\npropose BinSparX, a training-free technique that mitigates non-idealities in\nCiM-BNNs. BinSparX utilizes the distinct attributes of BNNs to reduce the\naverage current generated during the CiM operations in Xbar arrays. This is\nachieved by statically and dynamically sparsifying the BNN weights and\nactivations, respectively (which, in the context of BNNs, is defined as\nreducing the number of +1 weights and activations). This minimizes the IR drops\nacross the parasitic resistances, drastically mitigating their impact on\ninference accuracy. To evaluate our technique, we conduct experiments on\nResNet-18 and VGG-small CiM-BNNs designed at the 7nm technology node using\n8T-SRAM and 1T-1ReRAM. Our results show that BinSparX is highly effective in\nalleviating the impact of non-idealities, recouping the inference accuracy to\nnear-ideal (software) levels in some cases and providing accuracy boost of up\nto 77.25%. These benefits are accompanied by energy reduction, albeit at the\ncost of mild latency/area increase.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute-in-memory (CiM)-based binary neural network (CiM-BNN) accelerators\nmarry the benefits of CiM and ultra-low precision quantization, making them\nhighly suitable for edge computing. However, CiM-enabled crossbar (Xbar) arrays\nare plagued with hardware non-idealities like parasitic resistances and device\nnon-linearities that impair inference accuracy, especially in scaled\ntechnologies. In this work, we first analyze the impact of Xbar non-idealities\non the inference accuracy of various CiM-BNNs, establishing that the unique\nproperties of CiM-BNNs make them more prone to hardware non-idealities compared\nto higher precision deep neural networks (DNNs). To address this issue, we\npropose BinSparX, a training-free technique that mitigates non-idealities in\nCiM-BNNs. BinSparX utilizes the distinct attributes of BNNs to reduce the\naverage current generated during the CiM operations in Xbar arrays. This is\nachieved by statically and dynamically sparsifying the BNN weights and\nactivations, respectively (which, in the context of BNNs, is defined as\nreducing the number of +1 weights and activations). This minimizes the IR drops\nacross the parasitic resistances, drastically mitigating their impact on\ninference accuracy. To evaluate our technique, we conduct experiments on\nResNet-18 and VGG-small CiM-BNNs designed at the 7nm technology node using\n8T-SRAM and 1T-1ReRAM. Our results show that BinSparX is highly effective in\nalleviating the impact of non-idealities, recouping the inference accuracy to\nnear-ideal (software) levels in some cases and providing accuracy boost of up\nto 77.25%. These benefits are accompanied by energy reduction, albeit at the\ncost of mild latency/area increase."
                },
                "authors": [
                    {
                        "name": "Akul Malhotra"
                    },
                    {
                        "name": "Sumeet Kumar Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Sumeet Kumar Gupta"
                },
                "author": "Sumeet Kumar Gupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03553v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03553v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.15957v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.15957v2",
                "updated": "2024-12-04T18:48:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    18,
                    48,
                    43,
                    2,
                    339,
                    0
                ],
                "published": "2024-02-25T02:36:03Z",
                "published_parsed": [
                    2024,
                    2,
                    25,
                    2,
                    36,
                    3,
                    6,
                    56,
                    0
                ],
                "title": "DynaMITE-RL: A Dynamic Model for Improved Temporal Meta-Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DynaMITE-RL: A Dynamic Model for Improved Temporal Meta-Reinforcement\n  Learning"
                },
                "summary": "We introduce DynaMITE-RL, a meta-reinforcement learning (meta-RL) approach to\napproximate inference in environments where the latent state evolves at varying\nrates. We model episode sessions - parts of the episode where the latent state\nis fixed - and propose three key modifications to existing meta-RL methods:\nconsistency of latent information within sessions, session masking, and prior\nlatent conditioning. We demonstrate the importance of these modifications in\nvarious domains, ranging from discrete Gridworld environments to\ncontinuous-control and simulated robot assistive tasks, demonstrating that\nDynaMITE-RL significantly outperforms state-of-the-art baselines in sample\nefficiency and inference returns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce DynaMITE-RL, a meta-reinforcement learning (meta-RL) approach to\napproximate inference in environments where the latent state evolves at varying\nrates. We model episode sessions - parts of the episode where the latent state\nis fixed - and propose three key modifications to existing meta-RL methods:\nconsistency of latent information within sessions, session masking, and prior\nlatent conditioning. We demonstrate the importance of these modifications in\nvarious domains, ranging from discrete Gridworld environments to\ncontinuous-control and simulated robot assistive tasks, demonstrating that\nDynaMITE-RL significantly outperforms state-of-the-art baselines in sample\nefficiency and inference returns."
                },
                "authors": [
                    {
                        "name": "Anthony Liang"
                    },
                    {
                        "name": "Guy Tennenholtz"
                    },
                    {
                        "name": "Chih-wei Hsu"
                    },
                    {
                        "name": "Yinlam Chow"
                    },
                    {
                        "name": "Erdem Byk"
                    },
                    {
                        "name": "Craig Boutilier"
                    }
                ],
                "author_detail": {
                    "name": "Craig Boutilier"
                },
                "author": "Craig Boutilier",
                "arxiv_journal_ref": "Neural Information Processing Systems (NeurIPS) 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.15957v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.15957v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03547v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03547v1",
                "updated": "2024-12-04T18:45:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    18,
                    45,
                    30,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T18:45:30Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    18,
                    45,
                    30,
                    2,
                    339,
                    0
                ],
                "title": "Learning from galactic rotation curves: a neural network approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning from galactic rotation curves: a neural network approach"
                },
                "summary": "For a galaxy, given its observed rotation curve, can one directly infer\nparameters of the dark matter density profile (such as dark matter particle\nmass $m$, scaling parameter $s$, core-to-envelope transition radius $r_t$ and\nNFW scale radius $r_s$), along with Baryonic parameters (such as the stellar\nmass-to-light ratio $\\Upsilon_*$)? In this work, using simulated rotation\ncurves, we train neural networks, which can then be fed observed rotation\ncurves of dark matter dominated dwarf galaxies from the SPARC catalog, to infer\nparameter values and their uncertainties. Since observed rotation curves have\nerrors, we also explore the very important effect of noise in the training data\non the inference. We employ two different methods to quantify uncertainties in\nthe estimated parameters, and compare the results with those obtained using\nBayesian methods. We find that the trained neural networks can extract\nparameters that describe observations well for the galaxies we studied.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For a galaxy, given its observed rotation curve, can one directly infer\nparameters of the dark matter density profile (such as dark matter particle\nmass $m$, scaling parameter $s$, core-to-envelope transition radius $r_t$ and\nNFW scale radius $r_s$), along with Baryonic parameters (such as the stellar\nmass-to-light ratio $\\Upsilon_*$)? In this work, using simulated rotation\ncurves, we train neural networks, which can then be fed observed rotation\ncurves of dark matter dominated dwarf galaxies from the SPARC catalog, to infer\nparameter values and their uncertainties. Since observed rotation curves have\nerrors, we also explore the very important effect of noise in the training data\non the inference. We employ two different methods to quantify uncertainties in\nthe estimated parameters, and compare the results with those obtained using\nBayesian methods. We find that the trained neural networks can extract\nparameters that describe observations well for the galaxies we studied."
                },
                "authors": [
                    {
                        "name": "Bihag Dave"
                    },
                    {
                        "name": "Gaurav Goswami"
                    }
                ],
                "author_detail": {
                    "name": "Gaurav Goswami"
                },
                "author": "Gaurav Goswami",
                "arxiv_comment": "18 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03547v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03547v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03546v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03546v1",
                "updated": "2024-12-04T18:45:01Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    18,
                    45,
                    1,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T18:45:01Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    18,
                    45,
                    1,
                    2,
                    339,
                    0
                ],
                "title": "Revisiting the impact of neutrino mass hierarchies on neutrino mass\n  constraints in light of recent DESI data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting the impact of neutrino mass hierarchies on neutrino mass\n  constraints in light of recent DESI data"
                },
                "summary": "Recent results from DESI combined with cosmic microwave background data give\nthe tightest constraints on the sum of neutrino masses to date. However, these\nanalyses approximate the neutrino mass hierarchy by three degenerate-mass (DM)\nneutrinos, instead of the normal (NH) and inverted hierarchies (IH) informed by\nterrestrial neutrino oscillation experiments. Given the stringency of the upper\nlimits from DESI data, we test explicitly whether the inferred neutrino\nconstraints are robust to the choice of neutrino mass ordering using both\nBayesian and frequentist methods. For Planck data alone, we find that the DM\nhierarchy presents a good approximation to the physically motivated hierarchies\nwhile showing a strong dependence on the assumed lower bound of the prior,\nconfirming previous studies. For the combined Planck and DESI baryon acoustic\noscillation data, we find that assuming NH ($M_\\mathrm{tot} <\n0.13\\,\\mathrm{eV}$) or IH ($M_\\mathrm{tot} < 0.16\\,\\mathrm{eV}$) loosens the\nBayesian upper limits compared to the DM approximation ($M_\\mathrm{tot} <\n0.086\\,\\mathrm{eV}$). The frequentist analysis shows that the different\nneutrino models fit the data equally well and the loosening of the constraints\ncan thus be attributed to the lower bounds induced by NH and IH. Overall, we\nfind that the DM hierarchy presents a good approximation to the physically\nmotivated hierarchies also for Planck+DESI data as long as the corresponding\nlower neutrino mass bounds are imposed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent results from DESI combined with cosmic microwave background data give\nthe tightest constraints on the sum of neutrino masses to date. However, these\nanalyses approximate the neutrino mass hierarchy by three degenerate-mass (DM)\nneutrinos, instead of the normal (NH) and inverted hierarchies (IH) informed by\nterrestrial neutrino oscillation experiments. Given the stringency of the upper\nlimits from DESI data, we test explicitly whether the inferred neutrino\nconstraints are robust to the choice of neutrino mass ordering using both\nBayesian and frequentist methods. For Planck data alone, we find that the DM\nhierarchy presents a good approximation to the physically motivated hierarchies\nwhile showing a strong dependence on the assumed lower bound of the prior,\nconfirming previous studies. For the combined Planck and DESI baryon acoustic\noscillation data, we find that assuming NH ($M_\\mathrm{tot} <\n0.13\\,\\mathrm{eV}$) or IH ($M_\\mathrm{tot} < 0.16\\,\\mathrm{eV}$) loosens the\nBayesian upper limits compared to the DM approximation ($M_\\mathrm{tot} <\n0.086\\,\\mathrm{eV}$). The frequentist analysis shows that the different\nneutrino models fit the data equally well and the loosening of the constraints\ncan thus be attributed to the lower bounds induced by NH and IH. Overall, we\nfind that the DM hierarchy presents a good approximation to the physically\nmotivated hierarchies also for Planck+DESI data as long as the corresponding\nlower neutrino mass bounds are imposed."
                },
                "authors": [
                    {
                        "name": "Laura Herold"
                    },
                    {
                        "name": "Marc Kamionkowski"
                    }
                ],
                "author_detail": {
                    "name": "Marc Kamionkowski"
                },
                "author": "Marc Kamionkowski",
                "arxiv_comment": "11 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03546v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03546v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19379v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19379v2",
                "updated": "2024-12-04T18:40:24Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    18,
                    40,
                    24,
                    2,
                    339,
                    0
                ],
                "published": "2024-11-28T21:10:20Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    21,
                    10,
                    20,
                    3,
                    333,
                    0
                ],
                "title": "Marconi: Prefix Caching for the Era of Hybrid LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Marconi: Prefix Caching for the Era of Hybrid LLMs"
                },
                "summary": "Hybrid models that combine the language modeling capabilities of Attention\nlayers with the efficiency of Recurrent layers (e.g., State Space Models) have\ngained traction in practically supporting long contexts in Large Language Model\nserving. Yet, the unique properties of these models complicate the usage of\ncomplementary efficiency optimizations such as prefix caching that skip\nredundant computations across requests. Most notably, their use of in-place\nstate updates for recurrent layers precludes rolling back cache entries for\npartial sequence overlaps, and instead mandates only exact-match cache hits;\nthe effect is a deluge of (large) cache entries per sequence, most of which\nyield minimal reuse opportunities. We present Marconi, the first system that\nsupports efficient prefix caching with Hybrid LLMs. Key to Marconi are its\nnovel admission and eviction policies that more judiciously assess potential\ncache entries based not only on recency, but also on (1) forecasts of their\nreuse likelihood across a taxonomy of different hit scenarios, and (2) the\ncompute savings that hits deliver relative to memory footprints. Across diverse\nworkloads and Hybrid models, Marconi achieves up to 34.4$\\times$ higher token\nhit rates (71.1% or 617 ms lower TTFT) compared to state-of-the-art prefix\ncaching systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid models that combine the language modeling capabilities of Attention\nlayers with the efficiency of Recurrent layers (e.g., State Space Models) have\ngained traction in practically supporting long contexts in Large Language Model\nserving. Yet, the unique properties of these models complicate the usage of\ncomplementary efficiency optimizations such as prefix caching that skip\nredundant computations across requests. Most notably, their use of in-place\nstate updates for recurrent layers precludes rolling back cache entries for\npartial sequence overlaps, and instead mandates only exact-match cache hits;\nthe effect is a deluge of (large) cache entries per sequence, most of which\nyield minimal reuse opportunities. We present Marconi, the first system that\nsupports efficient prefix caching with Hybrid LLMs. Key to Marconi are its\nnovel admission and eviction policies that more judiciously assess potential\ncache entries based not only on recency, but also on (1) forecasts of their\nreuse likelihood across a taxonomy of different hit scenarios, and (2) the\ncompute savings that hits deliver relative to memory footprints. Across diverse\nworkloads and Hybrid models, Marconi achieves up to 34.4$\\times$ higher token\nhit rates (71.1% or 617 ms lower TTFT) compared to state-of-the-art prefix\ncaching systems."
                },
                "authors": [
                    {
                        "name": "Rui Pan"
                    },
                    {
                        "name": "Zhuang Wang"
                    },
                    {
                        "name": "Zhen Jia"
                    },
                    {
                        "name": "Can Karakus"
                    },
                    {
                        "name": "Luca Zancato"
                    },
                    {
                        "name": "Tri Dao"
                    },
                    {
                        "name": "Yida Wang"
                    },
                    {
                        "name": "Ravi Netravali"
                    }
                ],
                "author_detail": {
                    "name": "Ravi Netravali"
                },
                "author": "Ravi Netravali",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19379v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19379v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03537v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03537v1",
                "updated": "2024-12-04T18:32:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    18,
                    32,
                    42,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T18:32:42Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    18,
                    32,
                    42,
                    2,
                    339,
                    0
                ],
                "title": "Evaluating Gender Bias Transfer between Pre-trained and Prompt-Adapted\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Gender Bias Transfer between Pre-trained and Prompt-Adapted\n  Language Models"
                },
                "summary": "Large language models (LLMs) are increasingly being adapted to achieve\ntask-specificity for deployment in real-world decision systems. Several\nprevious works have investigated the bias transfer hypothesis (BTH) by studying\nthe effect of the fine-tuning adaptation strategy on model fairness to find\nthat fairness in pre-trained masked language models have limited effect on the\nfairness of models when adapted using fine-tuning. In this work, we expand the\nstudy of BTH to causal models under prompt adaptations, as prompting is an\naccessible, and compute-efficient way to deploy models in real-world systems.\nIn contrast to previous works, we establish that intrinsic biases in\npre-trained Mistral, Falcon and Llama models are strongly correlated (rho >=\n0.94) with biases when the same models are zero- and few-shot prompted, using a\npronoun co-reference resolution task. Further, we find that bias transfer\nremains strongly correlated even when LLMs are specifically prompted to exhibit\nfair or biased behavior (rho >= 0.92), and few-shot length and stereotypical\ncomposition are varied (rho >= 0.97). Our findings highlight the importance of\nensuring fairness in pre-trained LLMs, especially when they are later used to\nperform downstream tasks via prompt adaptation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly being adapted to achieve\ntask-specificity for deployment in real-world decision systems. Several\nprevious works have investigated the bias transfer hypothesis (BTH) by studying\nthe effect of the fine-tuning adaptation strategy on model fairness to find\nthat fairness in pre-trained masked language models have limited effect on the\nfairness of models when adapted using fine-tuning. In this work, we expand the\nstudy of BTH to causal models under prompt adaptations, as prompting is an\naccessible, and compute-efficient way to deploy models in real-world systems.\nIn contrast to previous works, we establish that intrinsic biases in\npre-trained Mistral, Falcon and Llama models are strongly correlated (rho >=\n0.94) with biases when the same models are zero- and few-shot prompted, using a\npronoun co-reference resolution task. Further, we find that bias transfer\nremains strongly correlated even when LLMs are specifically prompted to exhibit\nfair or biased behavior (rho >= 0.92), and few-shot length and stereotypical\ncomposition are varied (rho >= 0.97). Our findings highlight the importance of\nensuring fairness in pre-trained LLMs, especially when they are later used to\nperform downstream tasks via prompt adaptation."
                },
                "authors": [
                    {
                        "name": "Natalie Mackraz"
                    },
                    {
                        "name": "Nivedha Sivakumar"
                    },
                    {
                        "name": "Samira Khorshidi"
                    },
                    {
                        "name": "Krishna Patel"
                    },
                    {
                        "name": "Barry-John Theobald"
                    },
                    {
                        "name": "Luca Zappella"
                    },
                    {
                        "name": "Nicholas Apostoloff"
                    }
                ],
                "author_detail": {
                    "name": "Nicholas Apostoloff"
                },
                "author": "Nicholas Apostoloff",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03537v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03537v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03531v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03531v1",
                "updated": "2024-12-04T18:26:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    18,
                    26,
                    13,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T18:26:13Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    18,
                    26,
                    13,
                    2,
                    339,
                    0
                ],
                "title": "A Review on Scientific Knowledge Extraction using Large Language Models\n  in Biomedical Sciences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Review on Scientific Knowledge Extraction using Large Language Models\n  in Biomedical Sciences"
                },
                "summary": "The rapid advancement of large language models (LLMs) has opened new\nboundaries in the extraction and synthesis of medical knowledge, particularly\nwithin evidence synthesis. This paper reviews the state-of-the-art applications\nof LLMs in the biomedical domain, exploring their effectiveness in automating\ncomplex tasks such as evidence synthesis and data extraction from a biomedical\ncorpus of documents. While LLMs demonstrate remarkable potential, significant\nchallenges remain, including issues related to hallucinations, contextual\nunderstanding, and the ability to generalize across diverse medical tasks. We\nhighlight critical gaps in the current research literature, particularly the\nneed for unified benchmarks to standardize evaluations and ensure reliability\nin real-world applications. In addition, we propose directions for future\nresearch, emphasizing the integration of state-of-the-art techniques such as\nretrieval-augmented generation (RAG) to enhance LLM performance in evidence\nsynthesis. By addressing these challenges and utilizing the strengths of LLMs,\nwe aim to improve access to medical literature and facilitate meaningful\ndiscoveries in healthcare.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) has opened new\nboundaries in the extraction and synthesis of medical knowledge, particularly\nwithin evidence synthesis. This paper reviews the state-of-the-art applications\nof LLMs in the biomedical domain, exploring their effectiveness in automating\ncomplex tasks such as evidence synthesis and data extraction from a biomedical\ncorpus of documents. While LLMs demonstrate remarkable potential, significant\nchallenges remain, including issues related to hallucinations, contextual\nunderstanding, and the ability to generalize across diverse medical tasks. We\nhighlight critical gaps in the current research literature, particularly the\nneed for unified benchmarks to standardize evaluations and ensure reliability\nin real-world applications. In addition, we propose directions for future\nresearch, emphasizing the integration of state-of-the-art techniques such as\nretrieval-augmented generation (RAG) to enhance LLM performance in evidence\nsynthesis. By addressing these challenges and utilizing the strengths of LLMs,\nwe aim to improve access to medical literature and facilitate meaningful\ndiscoveries in healthcare."
                },
                "authors": [
                    {
                        "name": "Gabriel Lino Garcia"
                    },
                    {
                        "name": "Joo Renato Ribeiro Manesco"
                    },
                    {
                        "name": "Pedro Henrique Paiola"
                    },
                    {
                        "name": "Lucas Miranda"
                    },
                    {
                        "name": "Maria Paola de Salvo"
                    },
                    {
                        "name": "Joo Paulo Papa"
                    }
                ],
                "author_detail": {
                    "name": "Joo Paulo Papa"
                },
                "author": "Joo Paulo Papa",
                "arxiv_comment": "9 pages, 1 table, 1 figure, conference paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03531v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03531v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.12712v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.12712v3",
                "updated": "2024-12-04T18:18:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    18,
                    18,
                    47,
                    2,
                    339,
                    0
                ],
                "published": "2024-03-19T13:19:41Z",
                "published_parsed": [
                    2024,
                    3,
                    19,
                    13,
                    19,
                    41,
                    1,
                    79,
                    0
                ],
                "title": "Instance-Warp: Saliency Guided Image Warping for Unsupervised Domain\n  Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instance-Warp: Saliency Guided Image Warping for Unsupervised Domain\n  Adaptation"
                },
                "summary": "Driving is challenging in conditions like night, rain, and snow. Lack of good\nlabeled datasets has hampered progress in scene understanding under such\nconditions. Unsupervised Domain Adaptation (UDA) using large labeled clear-day\ndatasets is a promising research direction in such cases. However, many UDA\nmethods are trained with dominant scene backgrounds (e.g., roads, sky,\nsidewalks) that appear dramatically different across domains. As a result, they\nstruggle to learn effective features of smaller and often sparse foreground\nobjects (e.g., people, vehicles, signs).\n  In this work, we improve UDA training by applying in-place image warping to\nfocus on salient objects. We design instance-level saliency guidance to\nadaptively oversample object regions and undersample background areas, which\nreduces adverse effects from background context and enhances backbone feature\nlearning. Our approach improves adaptation across geographies, lighting, and\nweather conditions, and is agnostic to the task (segmentation, detection),\ndomain adaptation algorithm, saliency guidance, and underlying model\narchitecture. Result highlights include +6.1 mAP50 for BDD100K Clear\n$\\rightarrow$ DENSE Foggy, +3.7 mAP50 for BDD100K Day $\\rightarrow$ Night, +3.0\nmAP50 for BDD100K Clear $\\rightarrow$ Rainy, and +6.3 mIoU for Cityscapes\n$\\rightarrow$ ACDC. Besides, Our method adds minimal training memory and no\nadditional inference latency. Code is available at\nhttps://github.com/ShenZheng2000/Instance-Warp",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Driving is challenging in conditions like night, rain, and snow. Lack of good\nlabeled datasets has hampered progress in scene understanding under such\nconditions. Unsupervised Domain Adaptation (UDA) using large labeled clear-day\ndatasets is a promising research direction in such cases. However, many UDA\nmethods are trained with dominant scene backgrounds (e.g., roads, sky,\nsidewalks) that appear dramatically different across domains. As a result, they\nstruggle to learn effective features of smaller and often sparse foreground\nobjects (e.g., people, vehicles, signs).\n  In this work, we improve UDA training by applying in-place image warping to\nfocus on salient objects. We design instance-level saliency guidance to\nadaptively oversample object regions and undersample background areas, which\nreduces adverse effects from background context and enhances backbone feature\nlearning. Our approach improves adaptation across geographies, lighting, and\nweather conditions, and is agnostic to the task (segmentation, detection),\ndomain adaptation algorithm, saliency guidance, and underlying model\narchitecture. Result highlights include +6.1 mAP50 for BDD100K Clear\n$\\rightarrow$ DENSE Foggy, +3.7 mAP50 for BDD100K Day $\\rightarrow$ Night, +3.0\nmAP50 for BDD100K Clear $\\rightarrow$ Rainy, and +6.3 mIoU for Cityscapes\n$\\rightarrow$ ACDC. Besides, Our method adds minimal training memory and no\nadditional inference latency. Code is available at\nhttps://github.com/ShenZheng2000/Instance-Warp"
                },
                "authors": [
                    {
                        "name": "Shen Zheng"
                    },
                    {
                        "name": "Anurag Ghosh"
                    },
                    {
                        "name": "Srinivasa G. Narasimhan"
                    }
                ],
                "author_detail": {
                    "name": "Srinivasa G. Narasimhan"
                },
                "author": "Srinivasa G. Narasimhan",
                "arxiv_comment": "WACV 2025 Accepted Paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.12712v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.12712v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03528v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03528v1",
                "updated": "2024-12-04T18:17:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    18,
                    17,
                    9,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T18:17:09Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    18,
                    17,
                    9,
                    2,
                    339,
                    0
                ],
                "title": "The R.O.A.D. to clinical trial emulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The R.O.A.D. to clinical trial emulation"
                },
                "summary": "Observational studies provide the only evidence on the effectiveness of\ninterventions when randomized controlled trials (RCTs) are impractical due to\ncost, ethical concerns, or time constraints. While many methodologies aim to\ndraw causal inferences from observational data, there is a growing trend to\nmodel observational study designs after RCTs, a strategy known as \"target trial\nemulation.\" Despite its potential, causal inference through target trial\nemulation cannot fully address the confounding bias in real-world data due to\nthe lack of randomization. In this work, we present a novel framework for\ntarget trial emulation that aims to overcome several key limitations, including\nconfounding bias. The framework proceeds as follows: First, we apply the\neligibility criteria of a specific trial to an observational cohort. We then\n\"correct\" this cohort by extracting a subset that matches both the distribution\nof covariates and the baseline prognosis of the control group in the target\nRCT. Next, we address unmeasured confounding by adjusting the prognosis\nestimates of the treated group to align with those observed in the trial.\nFollowing trial emulation, we go a step further by leveraging the emulated\ncohort to train optimal decision trees, to identify subgroups of patients with\nheterogeneity in treatment effects (HTE). The absence of confounding is\nverified using two external models, and the validity of the treatment\nrecommendations is independently confirmed by the team responsible for the\noriginal trial we emulate. To our knowledge, this is the first framework to\nsuccessfully address both observed and unobserved confounding, a challenge that\nhas historically limited the use of randomized trial emulation and causal\ninference. Additionally, our framework holds promise in advancing precision\nmedicine by identifying patient subgroups that benefit most from specific\ntreatments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Observational studies provide the only evidence on the effectiveness of\ninterventions when randomized controlled trials (RCTs) are impractical due to\ncost, ethical concerns, or time constraints. While many methodologies aim to\ndraw causal inferences from observational data, there is a growing trend to\nmodel observational study designs after RCTs, a strategy known as \"target trial\nemulation.\" Despite its potential, causal inference through target trial\nemulation cannot fully address the confounding bias in real-world data due to\nthe lack of randomization. In this work, we present a novel framework for\ntarget trial emulation that aims to overcome several key limitations, including\nconfounding bias. The framework proceeds as follows: First, we apply the\neligibility criteria of a specific trial to an observational cohort. We then\n\"correct\" this cohort by extracting a subset that matches both the distribution\nof covariates and the baseline prognosis of the control group in the target\nRCT. Next, we address unmeasured confounding by adjusting the prognosis\nestimates of the treated group to align with those observed in the trial.\nFollowing trial emulation, we go a step further by leveraging the emulated\ncohort to train optimal decision trees, to identify subgroups of patients with\nheterogeneity in treatment effects (HTE). The absence of confounding is\nverified using two external models, and the validity of the treatment\nrecommendations is independently confirmed by the team responsible for the\noriginal trial we emulate. To our knowledge, this is the first framework to\nsuccessfully address both observed and unobserved confounding, a challenge that\nhas historically limited the use of randomized trial emulation and causal\ninference. Additionally, our framework holds promise in advancing precision\nmedicine by identifying patient subgroups that benefit most from specific\ntreatments."
                },
                "authors": [
                    {
                        "name": "Dimitris Bertsimas"
                    },
                    {
                        "name": "Angelos G. Koulouras"
                    },
                    {
                        "name": "Hiroshi Nagata"
                    },
                    {
                        "name": "Carol Gao"
                    },
                    {
                        "name": "Junki Mizusawa"
                    },
                    {
                        "name": "Yukihide Kanemitsu"
                    },
                    {
                        "name": "Georgios Antonios Margonis"
                    }
                ],
                "author_detail": {
                    "name": "Georgios Antonios Margonis"
                },
                "author": "Georgios Antonios Margonis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03528v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03528v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.07360v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.07360v3",
                "updated": "2024-12-04T17:58:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    17,
                    58,
                    35,
                    2,
                    339,
                    0
                ],
                "published": "2023-12-12T15:30:24Z",
                "published_parsed": [
                    2023,
                    12,
                    12,
                    15,
                    30,
                    24,
                    1,
                    346,
                    0
                ],
                "title": "Boosting Latent Diffusion with Flow Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Latent Diffusion with Flow Matching"
                },
                "summary": "Visual synthesis has recently seen significant leaps in performance, largely\ndue to breakthroughs in generative models. Diffusion models have been a key\nenabler, as they excel in image diversity. However, this comes at the cost of\nslow training and synthesis, which is only partially alleviated by latent\ndiffusion. To this end, flow matching is an appealing approach due to its\ncomplementary characteristics of faster training and inference but less diverse\nsynthesis. We demonstrate that introducing flow matching between a frozen\ndiffusion model and a convolutional decoder enables high-resolution image\nsynthesis at reduced computational cost and model size. A small diffusion model\ncan then effectively provide the necessary visual diversity, while flow\nmatching efficiently enhances resolution and detail by mapping the small to a\nhigh-dimensional latent space. These latents are then projected to\nhigh-resolution images by the subsequent convolutional decoder of the latent\ndiffusion approach. Combining the diversity of diffusion models, the efficiency\nof flow matching, and the effectiveness of convolutional decoders,\nstate-of-the-art high-resolution image synthesis is achieved at $1024^2$ pixels\nwith minimal computational cost. Further scaling up our method we can reach\nresolutions up to $2048^2$ pixels. Importantly, our approach is orthogonal to\nrecent approximation and speed-up strategies for the underlying model, making\nit easily integrable into the various diffusion model frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual synthesis has recently seen significant leaps in performance, largely\ndue to breakthroughs in generative models. Diffusion models have been a key\nenabler, as they excel in image diversity. However, this comes at the cost of\nslow training and synthesis, which is only partially alleviated by latent\ndiffusion. To this end, flow matching is an appealing approach due to its\ncomplementary characteristics of faster training and inference but less diverse\nsynthesis. We demonstrate that introducing flow matching between a frozen\ndiffusion model and a convolutional decoder enables high-resolution image\nsynthesis at reduced computational cost and model size. A small diffusion model\ncan then effectively provide the necessary visual diversity, while flow\nmatching efficiently enhances resolution and detail by mapping the small to a\nhigh-dimensional latent space. These latents are then projected to\nhigh-resolution images by the subsequent convolutional decoder of the latent\ndiffusion approach. Combining the diversity of diffusion models, the efficiency\nof flow matching, and the effectiveness of convolutional decoders,\nstate-of-the-art high-resolution image synthesis is achieved at $1024^2$ pixels\nwith minimal computational cost. Further scaling up our method we can reach\nresolutions up to $2048^2$ pixels. Importantly, our approach is orthogonal to\nrecent approximation and speed-up strategies for the underlying model, making\nit easily integrable into the various diffusion model frameworks."
                },
                "authors": [
                    {
                        "name": "Johannes Schusterbauer"
                    },
                    {
                        "name": "Ming Gui"
                    },
                    {
                        "name": "Pingchuan Ma"
                    },
                    {
                        "name": "Nick Stracke"
                    },
                    {
                        "name": "Stefan A. Baumann"
                    },
                    {
                        "name": "Vincent Tao Hu"
                    },
                    {
                        "name": "Bjrn Ommer"
                    }
                ],
                "author_detail": {
                    "name": "Bjrn Ommer"
                },
                "author": "Bjrn Ommer",
                "arxiv_doi": "10.1007/978-3-031-73030-6_19",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-73030-6_19",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2312.07360v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.07360v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "ECCV 2024 (Oral), Project Page:\n  https://compvis.github.io/fm-boosting/",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03517v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03517v1",
                "updated": "2024-12-04T17:58:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    17,
                    58,
                    3,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T17:58:03Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    17,
                    58,
                    3,
                    2,
                    339,
                    0
                ],
                "title": "NVComposer: Boosting Generative Novel View Synthesis with Multiple\n  Sparse and Unposed Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NVComposer: Boosting Generative Novel View Synthesis with Multiple\n  Sparse and Unposed Images"
                },
                "summary": "Recent advancements in generative models have significantly improved novel\nview synthesis (NVS) from multi-view data. However, existing methods depend on\nexternal multi-view alignment processes, such as explicit pose estimation or\npre-reconstruction, which limits their flexibility and accessibility,\nespecially when alignment is unstable due to insufficient overlap or occlusions\nbetween views. In this paper, we propose NVComposer, a novel approach that\neliminates the need for explicit external alignment. NVComposer enables the\ngenerative model to implicitly infer spatial and geometric relationships\nbetween multiple conditional views by introducing two key components: 1) an\nimage-pose dual-stream diffusion model that simultaneously generates target\nnovel views and condition camera poses, and 2) a geometry-aware feature\nalignment module that distills geometric priors from dense stereo models during\ntraining. Extensive experiments demonstrate that NVComposer achieves\nstate-of-the-art performance in generative multi-view NVS tasks, removing the\nreliance on external alignment and thus improving model accessibility. Our\napproach shows substantial improvements in synthesis quality as the number of\nunposed input views increases, highlighting its potential for more flexible and\naccessible generative NVS systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in generative models have significantly improved novel\nview synthesis (NVS) from multi-view data. However, existing methods depend on\nexternal multi-view alignment processes, such as explicit pose estimation or\npre-reconstruction, which limits their flexibility and accessibility,\nespecially when alignment is unstable due to insufficient overlap or occlusions\nbetween views. In this paper, we propose NVComposer, a novel approach that\neliminates the need for explicit external alignment. NVComposer enables the\ngenerative model to implicitly infer spatial and geometric relationships\nbetween multiple conditional views by introducing two key components: 1) an\nimage-pose dual-stream diffusion model that simultaneously generates target\nnovel views and condition camera poses, and 2) a geometry-aware feature\nalignment module that distills geometric priors from dense stereo models during\ntraining. Extensive experiments demonstrate that NVComposer achieves\nstate-of-the-art performance in generative multi-view NVS tasks, removing the\nreliance on external alignment and thus improving model accessibility. Our\napproach shows substantial improvements in synthesis quality as the number of\nunposed input views increases, highlighting its potential for more flexible and\naccessible generative NVS systems."
                },
                "authors": [
                    {
                        "name": "Lingen Li"
                    },
                    {
                        "name": "Zhaoyang Zhang"
                    },
                    {
                        "name": "Yaowei Li"
                    },
                    {
                        "name": "Jiale Xu"
                    },
                    {
                        "name": "Xiaoyu Li"
                    },
                    {
                        "name": "Wenbo Hu"
                    },
                    {
                        "name": "Weihao Cheng"
                    },
                    {
                        "name": "Jinwei Gu"
                    },
                    {
                        "name": "Tianfan Xue"
                    },
                    {
                        "name": "Ying Shan"
                    }
                ],
                "author_detail": {
                    "name": "Ying Shan"
                },
                "author": "Ying Shan",
                "arxiv_comment": "Project webpage: https://lg-li.github.io/project/nvcomposer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03517v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03517v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03516v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03516v1",
                "updated": "2024-12-04T17:57:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    17,
                    57,
                    39,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T17:57:39Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    17,
                    57,
                    39,
                    2,
                    339,
                    0
                ],
                "title": "You're (Not) My Type -- Can LLMs Generate Feedback of Specific Types for\n  Introductory Programming Tasks?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "You're (Not) My Type -- Can LLMs Generate Feedback of Specific Types for\n  Introductory Programming Tasks?"
                },
                "summary": "Background: Feedback as one of the most influential factors for learning has\nbeen subject to a great body of research. It plays a key role in the\ndevelopment of educational technology systems and is traditionally rooted in\ndeterministic feedback defined by experts and their experience. However, with\nthe rise of generative AI and especially Large Language Models (LLMs), we\nexpect feedback as part of learning systems to transform, especially for the\ncontext of programming. In the past, it was challenging to automate feedback\nfor learners of programming. LLMs may create new possibilities to provide\nricher, and more individual feedback than ever before.\n  Objectives: This paper aims to generate specific types of feedback for\nintroductory programming tasks using LLMs. We revisit existing feedback\ntaxonomies to capture the specifics of the generated feedback, such as\nrandomness, uncertainty, and degrees of variation.\n  Methods: We iteratively designed prompts for the generation of specific\nfeedback types (as part of existing feedback taxonomies) in response to\nauthentic student programs. We then evaluated the generated output and\ndetermined to what extent it reflected certain feedback types.\n  Results and Conclusion: The present work provides a better understanding of\ndifferent feedback dimensions and characteristics. The results have\nimplications for future feedback research with regard to, for example, feedback\neffects and learners' informational needs. It further provides a basis for the\ndevelopment of new tools and learning systems for novice programmers including\nfeedback generated by AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background: Feedback as one of the most influential factors for learning has\nbeen subject to a great body of research. It plays a key role in the\ndevelopment of educational technology systems and is traditionally rooted in\ndeterministic feedback defined by experts and their experience. However, with\nthe rise of generative AI and especially Large Language Models (LLMs), we\nexpect feedback as part of learning systems to transform, especially for the\ncontext of programming. In the past, it was challenging to automate feedback\nfor learners of programming. LLMs may create new possibilities to provide\nricher, and more individual feedback than ever before.\n  Objectives: This paper aims to generate specific types of feedback for\nintroductory programming tasks using LLMs. We revisit existing feedback\ntaxonomies to capture the specifics of the generated feedback, such as\nrandomness, uncertainty, and degrees of variation.\n  Methods: We iteratively designed prompts for the generation of specific\nfeedback types (as part of existing feedback taxonomies) in response to\nauthentic student programs. We then evaluated the generated output and\ndetermined to what extent it reflected certain feedback types.\n  Results and Conclusion: The present work provides a better understanding of\ndifferent feedback dimensions and characteristics. The results have\nimplications for future feedback research with regard to, for example, feedback\neffects and learners' informational needs. It further provides a basis for the\ndevelopment of new tools and learning systems for novice programmers including\nfeedback generated by AI."
                },
                "authors": [
                    {
                        "name": "Dominic Lohr"
                    },
                    {
                        "name": "Hieke Keuning"
                    },
                    {
                        "name": "Natalie Kiesler"
                    }
                ],
                "author_detail": {
                    "name": "Natalie Kiesler"
                },
                "author": "Natalie Kiesler",
                "arxiv_comment": "Accepted at Journal of Computer Assisted Learning (2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03516v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03516v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.16860v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.16860v2",
                "updated": "2024-12-04T17:57:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    17,
                    57,
                    32,
                    2,
                    339,
                    0
                ],
                "published": "2024-06-24T17:59:42Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    17,
                    59,
                    42,
                    0,
                    176,
                    0
                ],
                "title": "Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs"
                },
                "summary": "We introduce Cambrian-1, a family of multimodal LLMs (MLLMs) designed with a\nvision-centric approach. While stronger language models can enhance multimodal\ncapabilities, the design choices for vision components are often insufficiently\nexplored and disconnected from visual representation learning research. This\ngap hinders accurate sensory grounding in real-world scenarios. Our study uses\nLLMs and visual instruction tuning as an interface to evaluate various visual\nrepresentations, offering new insights into different models and architectures\n-- self-supervised, strongly supervised, or combinations thereof -- based on\nexperiments with over 20 vision encoders. We critically examine existing MLLM\nbenchmarks, address the difficulties involved in consolidating and interpreting\nresults from various tasks, and introduce a new vision-centric benchmark,\nCV-Bench. To further improve visual grounding, we propose the Spatial Vision\nAggregator (SVA), a dynamic and spatially-aware connector that integrates\nhigh-resolution vision features with LLMs while reducing the number of tokens.\nAdditionally, we discuss the curation of high-quality visual instruction-tuning\ndata from publicly available sources, emphasizing the importance of data source\nbalancing and distribution ratio. Collectively, Cambrian-1 not only achieves\nstate-of-the-art performance but also serves as a comprehensive, open cookbook\nfor instruction-tuned MLLMs. We provide model weights, code, supporting tools,\ndatasets, and detailed instruction-tuning and evaluation recipes. We hope our\nrelease will inspire and accelerate advancements in multimodal systems and\nvisual representation learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Cambrian-1, a family of multimodal LLMs (MLLMs) designed with a\nvision-centric approach. While stronger language models can enhance multimodal\ncapabilities, the design choices for vision components are often insufficiently\nexplored and disconnected from visual representation learning research. This\ngap hinders accurate sensory grounding in real-world scenarios. Our study uses\nLLMs and visual instruction tuning as an interface to evaluate various visual\nrepresentations, offering new insights into different models and architectures\n-- self-supervised, strongly supervised, or combinations thereof -- based on\nexperiments with over 20 vision encoders. We critically examine existing MLLM\nbenchmarks, address the difficulties involved in consolidating and interpreting\nresults from various tasks, and introduce a new vision-centric benchmark,\nCV-Bench. To further improve visual grounding, we propose the Spatial Vision\nAggregator (SVA), a dynamic and spatially-aware connector that integrates\nhigh-resolution vision features with LLMs while reducing the number of tokens.\nAdditionally, we discuss the curation of high-quality visual instruction-tuning\ndata from publicly available sources, emphasizing the importance of data source\nbalancing and distribution ratio. Collectively, Cambrian-1 not only achieves\nstate-of-the-art performance but also serves as a comprehensive, open cookbook\nfor instruction-tuned MLLMs. We provide model weights, code, supporting tools,\ndatasets, and detailed instruction-tuning and evaluation recipes. We hope our\nrelease will inspire and accelerate advancements in multimodal systems and\nvisual representation learning."
                },
                "authors": [
                    {
                        "name": "Shengbang Tong"
                    },
                    {
                        "name": "Ellis Brown"
                    },
                    {
                        "name": "Penghao Wu"
                    },
                    {
                        "name": "Sanghyun Woo"
                    },
                    {
                        "name": "Manoj Middepogu"
                    },
                    {
                        "name": "Sai Charitha Akula"
                    },
                    {
                        "name": "Jihan Yang"
                    },
                    {
                        "name": "Shusheng Yang"
                    },
                    {
                        "name": "Adithya Iyer"
                    },
                    {
                        "name": "Xichen Pan"
                    },
                    {
                        "name": "Ziteng Wang"
                    },
                    {
                        "name": "Rob Fergus"
                    },
                    {
                        "name": "Yann LeCun"
                    },
                    {
                        "name": "Saining Xie"
                    }
                ],
                "author_detail": {
                    "name": "Saining Xie"
                },
                "author": "Saining Xie",
                "arxiv_comment": "NeurIPS 2024 (Oral). Website at https://cambrian-mllm.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.16860v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.16860v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03503v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03503v1",
                "updated": "2024-12-04T17:45:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    17,
                    45,
                    21,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T17:45:21Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    17,
                    45,
                    21,
                    2,
                    339,
                    0
                ],
                "title": "Reducing nuisance prior sensitivity via non-linear reparameterization,\n  with application to EFT analyses of large-scale structure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reducing nuisance prior sensitivity via non-linear reparameterization,\n  with application to EFT analyses of large-scale structure"
                },
                "summary": "Many physical models contain nuisance parameters that quantify unknown and\nirrelevant properties of an experiment. Typically, these cannot be measured\nexcept by fitting the models to the data from the experiment, requiring\nsimultaneous measurement of interesting parameters that are our target of\ninference and nuisance terms that are not directly of interest. A recent\nexample of this is fitting Effective Field Theory (EFT) models to large-scale\nstructure (LSS) data to make cosmological inferences. These models have a large\nnumber of nuisance parameters that are typically correlated with cosmological\nparameters in the posterior, leading to strong dependence on the nuisance\nparameter priors. We introduce a reparametrization method that leverages\nGeneralized Additive Models (GAMs) to decorrelate nuisance parameters from the\nparameters of interest in the likelihood, even in the presence of non-linear\nrelationships. This reparametrization forms a natural basis within which to\ndefine priors that are independent between nuisance and target parameters: the\nseparation means that the marginal posterior for cosmological parameters does\nnot depend on simple priors placed on nuisance terms. In application to EFT\nmodels using LSS data, we demonstrate that the proposed approach leads to\nrobust cosmological inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many physical models contain nuisance parameters that quantify unknown and\nirrelevant properties of an experiment. Typically, these cannot be measured\nexcept by fitting the models to the data from the experiment, requiring\nsimultaneous measurement of interesting parameters that are our target of\ninference and nuisance terms that are not directly of interest. A recent\nexample of this is fitting Effective Field Theory (EFT) models to large-scale\nstructure (LSS) data to make cosmological inferences. These models have a large\nnumber of nuisance parameters that are typically correlated with cosmological\nparameters in the posterior, leading to strong dependence on the nuisance\nparameter priors. We introduce a reparametrization method that leverages\nGeneralized Additive Models (GAMs) to decorrelate nuisance parameters from the\nparameters of interest in the likelihood, even in the presence of non-linear\nrelationships. This reparametrization forms a natural basis within which to\ndefine priors that are independent between nuisance and target parameters: the\nseparation means that the marginal posterior for cosmological parameters does\nnot depend on simple priors placed on nuisance terms. In application to EFT\nmodels using LSS data, we demonstrate that the proposed approach leads to\nrobust cosmological inference."
                },
                "authors": [
                    {
                        "name": "S. Paradiso"
                    },
                    {
                        "name": "M. Bonici"
                    },
                    {
                        "name": "M. Chen"
                    },
                    {
                        "name": "W. J. Percival"
                    },
                    {
                        "name": "G. D'Amico"
                    },
                    {
                        "name": "H. Zhang"
                    },
                    {
                        "name": "G. McGee"
                    }
                ],
                "author_detail": {
                    "name": "G. McGee"
                },
                "author": "G. McGee",
                "arxiv_comment": "24 pages, 9 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03503v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03503v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11376v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11376v2",
                "updated": "2024-12-04T17:45:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    17,
                    45,
                    14,
                    2,
                    339,
                    0
                ],
                "published": "2024-09-17T17:23:44Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    17,
                    23,
                    44,
                    1,
                    261,
                    0
                ],
                "title": "Towards Time Series Reasoning with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Time Series Reasoning with LLMs"
                },
                "summary": "Multi-modal large language models (MLLMs) have enabled numerous advances in\nunderstanding and reasoning in domains like vision, but we have not yet seen\nthis broad success for time-series. Although prior works on time-series MLLMs\nhave shown promising performance in time-series forecasting, very few works\nshow how an LLM could be used for time-series reasoning in natural language. We\npropose a novel multi-modal time-series LLM approach that learns generalizable\ninformation across various domains with powerful zero-shot performance. First,\nwe train a lightweight time-series encoder on top of an LLM to directly extract\ntime-series information. Then, we fine-tune our model with chain-of-thought\naugmented time-series tasks to encourage the model to generate reasoning paths.\nWe show that our model learns a latent representation that reflects specific\ntime-series features (e.g. slope, frequency), as well as outperforming GPT-4o\non a set of zero-shot reasoning tasks on a variety of domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-modal large language models (MLLMs) have enabled numerous advances in\nunderstanding and reasoning in domains like vision, but we have not yet seen\nthis broad success for time-series. Although prior works on time-series MLLMs\nhave shown promising performance in time-series forecasting, very few works\nshow how an LLM could be used for time-series reasoning in natural language. We\npropose a novel multi-modal time-series LLM approach that learns generalizable\ninformation across various domains with powerful zero-shot performance. First,\nwe train a lightweight time-series encoder on top of an LLM to directly extract\ntime-series information. Then, we fine-tune our model with chain-of-thought\naugmented time-series tasks to encourage the model to generate reasoning paths.\nWe show that our model learns a latent representation that reflects specific\ntime-series features (e.g. slope, frequency), as well as outperforming GPT-4o\non a set of zero-shot reasoning tasks on a variety of domains."
                },
                "authors": [
                    {
                        "name": "Winnie Chow"
                    },
                    {
                        "name": "Lauren Gardiner"
                    },
                    {
                        "name": "Haraldur T. Hallgrmsson"
                    },
                    {
                        "name": "Maxwell A. Xu"
                    },
                    {
                        "name": "Shirley You Ren"
                    }
                ],
                "author_detail": {
                    "name": "Shirley You Ren"
                },
                "author": "Shirley You Ren",
                "arxiv_comment": "Oral Presentation at 2024 NeurIPS Workshop on Time Series in the Age\n  of Large Models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11376v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11376v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02060v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02060v2",
                "updated": "2024-12-04T17:43:10Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    17,
                    43,
                    10,
                    2,
                    339,
                    0
                ],
                "published": "2024-08-04T15:20:23Z",
                "published_parsed": [
                    2024,
                    8,
                    4,
                    15,
                    20,
                    23,
                    6,
                    217,
                    0
                ],
                "title": "Winners with Confidence: Discrete Argmin Inference with an Application\n  to Model Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Winners with Confidence: Discrete Argmin Inference with an Application\n  to Model Selection"
                },
                "summary": "We study the problem of finding the index of the minimum value of a vector\nfrom noisy observations. This problem is relevant in population/policy\ncomparison, discrete maximum likelihood, and model selection. We develop an\nasymptotically normal test statistic, even in high-dimensional settings and\nwith potentially many ties in the population mean vector, by integrating\nconcepts and tools from cross-validation and differential privacy. The key\ntechnical ingredient is a central limit theorem for globally dependent data. We\nalso propose practical ways to select the tuning parameter that adapts to the\nsignal landscape. Numerical experiments and data examples demonstrate the\nability of the proposed method to achieve a favorable bias-variance trade-off\nin practical scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the problem of finding the index of the minimum value of a vector\nfrom noisy observations. This problem is relevant in population/policy\ncomparison, discrete maximum likelihood, and model selection. We develop an\nasymptotically normal test statistic, even in high-dimensional settings and\nwith potentially many ties in the population mean vector, by integrating\nconcepts and tools from cross-validation and differential privacy. The key\ntechnical ingredient is a central limit theorem for globally dependent data. We\nalso propose practical ways to select the tuning parameter that adapts to the\nsignal landscape. Numerical experiments and data examples demonstrate the\nability of the proposed method to achieve a favorable bias-variance trade-off\nin practical scenarios."
                },
                "authors": [
                    {
                        "name": "Tianyu Zhang"
                    },
                    {
                        "name": "Hao Lee"
                    },
                    {
                        "name": "Jing Lei"
                    }
                ],
                "author_detail": {
                    "name": "Jing Lei"
                },
                "author": "Jing Lei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02060v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02060v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23036v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23036v2",
                "updated": "2024-12-04T17:27:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    17,
                    27,
                    53,
                    2,
                    339,
                    0
                ],
                "published": "2024-10-30T14:03:37Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    3,
                    37,
                    2,
                    304,
                    0
                ],
                "title": "Evidence for a Sharp CO Snowline Transition in a Protoplanetary Disk and\n  Implications for Millimeter-wave Observations of CO Isotopologues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evidence for a Sharp CO Snowline Transition in a Protoplanetary Disk and\n  Implications for Millimeter-wave Observations of CO Isotopologues"
                },
                "summary": "Observations of CO isotopologue emission from protoplanetary disks at\nmillimeter wavelengths are a powerful tool for probing the CO snowline, an\nimportant marker for disk chemistry, and also for estimating total disk gas\nmass, a key quantity for planet formation. We use simple models to demonstrate\nthat the vertical thickness of an isothermal layer around the disk midplane has\nimportant effects on the CO column density radial profile, with a thick layer\nproducing a sharp CO snowline transition. We simulate ngVLA and ALMA images to\nshow that this sharp change in CO column density can be detected in the\nderivative of the radial profile of emission from optically thin CO\nisotopologue lines. We apply this method to archival ALMA observations of the\ndisk around the Herbig Ae star HD 163296 in the C$^{17}$O and C$^{18}$O J=1-0\nand J=2-1 lines to identify a sharp CO snowline transition near 80 au (0.8\narcsec at 101 pc), and show the CO column density decreases by more than a\nfactor of 20. This finding is consistent with previous inferences from the\nsteep rise of N$_2$H$^+$ emission, which marks the location where CO depletes.\nWe also demonstrate that the disk's thermal structure introduces a significant\nsystematic uncertainty to estimates of total disk gas mass derived from these\nlines. The substantial improvement in sensitivity envisioned for the ngVLA over\nALMA for observations of ground-state lines of CO isotopologues has the\npotential to extend this approach to a much larger population of disks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Observations of CO isotopologue emission from protoplanetary disks at\nmillimeter wavelengths are a powerful tool for probing the CO snowline, an\nimportant marker for disk chemistry, and also for estimating total disk gas\nmass, a key quantity for planet formation. We use simple models to demonstrate\nthat the vertical thickness of an isothermal layer around the disk midplane has\nimportant effects on the CO column density radial profile, with a thick layer\nproducing a sharp CO snowline transition. We simulate ngVLA and ALMA images to\nshow that this sharp change in CO column density can be detected in the\nderivative of the radial profile of emission from optically thin CO\nisotopologue lines. We apply this method to archival ALMA observations of the\ndisk around the Herbig Ae star HD 163296 in the C$^{17}$O and C$^{18}$O J=1-0\nand J=2-1 lines to identify a sharp CO snowline transition near 80 au (0.8\narcsec at 101 pc), and show the CO column density decreases by more than a\nfactor of 20. This finding is consistent with previous inferences from the\nsteep rise of N$_2$H$^+$ emission, which marks the location where CO depletes.\nWe also demonstrate that the disk's thermal structure introduces a significant\nsystematic uncertainty to estimates of total disk gas mass derived from these\nlines. The substantial improvement in sensitivity envisioned for the ngVLA over\nALMA for observations of ground-state lines of CO isotopologues has the\npotential to extend this approach to a much larger population of disks."
                },
                "authors": [
                    {
                        "name": "Chunhua Qi"
                    },
                    {
                        "name": "David J. Wilner"
                    }
                ],
                "author_detail": {
                    "name": "David J. Wilner"
                },
                "author": "David J. Wilner",
                "arxiv_doi": "10.3847/1538-4357/ad8d55",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/1538-4357/ad8d55",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.23036v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23036v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "15 pages, 8 figures, ApJ published",
                "arxiv_journal_ref": "The Astrophysical Journal, 977 (2024) 60",
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.18945v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.18945v3",
                "updated": "2024-12-04T17:17:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    17,
                    17,
                    51,
                    2,
                    339,
                    0
                ],
                "published": "2023-05-30T11:24:27Z",
                "published_parsed": [
                    2023,
                    5,
                    30,
                    11,
                    24,
                    27,
                    1,
                    150,
                    0
                ],
                "title": "String Diagrams for $$-calculi and Functional Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "String Diagrams for $$-calculi and Functional Computation"
                },
                "summary": "This tutorial gives an advanced introduction to string diagrams and graph\nlanguages for higher-order computation. The subject matter develops in a\nprincipled way, starting from the two dimensional syntax of key categorical\nconcepts such as functors, adjunctions, and strictification, and leading up to\nCartesian Closed Categories, the core mathematical model of the lambda calculus\nand of functional programming languages. This methodology inverts the usual\napproach of proceeding from syntax to a categorical interpretation, by\nrationally reconstructing a syntax from the categorical model. The result is a\ngraph syntax -- more precisely, a hierarchical hypergraph syntax -- which in\nmany ways is shown to be an improvement over the conventional linear term\nsyntax. The rest of the tutorial focuses on applications of interest to\nprogramming languages: operational semantics, general frameworks for type\ninference, and complex whole-program transformations such as closure conversion\nand automatic differentiation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This tutorial gives an advanced introduction to string diagrams and graph\nlanguages for higher-order computation. The subject matter develops in a\nprincipled way, starting from the two dimensional syntax of key categorical\nconcepts such as functors, adjunctions, and strictification, and leading up to\nCartesian Closed Categories, the core mathematical model of the lambda calculus\nand of functional programming languages. This methodology inverts the usual\napproach of proceeding from syntax to a categorical interpretation, by\nrationally reconstructing a syntax from the categorical model. The result is a\ngraph syntax -- more precisely, a hierarchical hypergraph syntax -- which in\nmany ways is shown to be an improvement over the conventional linear term\nsyntax. The rest of the tutorial focuses on applications of interest to\nprogramming languages: operational semantics, general frameworks for type\ninference, and complex whole-program transformations such as closure conversion\nand automatic differentiation."
                },
                "authors": [
                    {
                        "name": "Dan Ghica"
                    },
                    {
                        "name": "Fabio Zanasi"
                    }
                ],
                "author_detail": {
                    "name": "Fabio Zanasi"
                },
                "author": "Fabio Zanasi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.18945v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.18945v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.CT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02862v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02862v3",
                "updated": "2024-12-04T17:15:01Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    17,
                    15,
                    1,
                    2,
                    339,
                    0
                ],
                "published": "2024-09-04T16:41:56Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    16,
                    41,
                    56,
                    2,
                    248,
                    0
                ],
                "title": "Four-dimensional phase space tomography from one-dimensional\n  measurements of a hadron beam",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Four-dimensional phase space tomography from one-dimensional\n  measurements of a hadron beam"
                },
                "summary": "In this paper, we use one-dimensional measurements to infer the\nfour-dimensional phase space density of an accumulated proton beam in the\nSpallation Neutron Source (SNS) accelerator. The reconstruction was performed\nby maximizing the distribution's entropy subject to the measurement\nconstraints, and thus represents the most conservative inference from the data.\nThe reconstructed distribution reproduces the measured profiles down to the\nnoise level, and simulations indicate that the problem is reasonably\nwell-constrained. Similar measurements could serve as benchmarks for beam\ndynamics simulations in the SNS or hadron accelerators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we use one-dimensional measurements to infer the\nfour-dimensional phase space density of an accumulated proton beam in the\nSpallation Neutron Source (SNS) accelerator. The reconstruction was performed\nby maximizing the distribution's entropy subject to the measurement\nconstraints, and thus represents the most conservative inference from the data.\nThe reconstructed distribution reproduces the measured profiles down to the\nnoise level, and simulations indicate that the problem is reasonably\nwell-constrained. Similar measurements could serve as benchmarks for beam\ndynamics simulations in the SNS or hadron accelerators."
                },
                "authors": [
                    {
                        "name": "Austin Hoover"
                    }
                ],
                "author_detail": {
                    "name": "Austin Hoover"
                },
                "author": "Austin Hoover",
                "arxiv_comment": "9 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02862v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02862v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.acc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.acc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13928v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13928v2",
                "updated": "2024-12-04T17:03:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    17,
                    3,
                    13,
                    2,
                    339,
                    0
                ],
                "published": "2024-10-17T17:56:01Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    17,
                    56,
                    1,
                    3,
                    291,
                    0
                ],
                "title": "Automatically Interpreting Millions of Features in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatically Interpreting Millions of Features in Large Language Models"
                },
                "summary": "While the activations of neurons in deep neural networks usually do not have\na simple human-understandable interpretation, sparse autoencoders (SAEs) can be\nused to transform these activations into a higher-dimensional latent space\nwhich may be more easily interpretable. However, these SAEs can have millions\nof distinct latent features, making it infeasible for humans to manually\ninterpret each one. In this work, we build an open-source automated pipeline to\ngenerate and evaluate natural language explanations for SAE features using\nLLMs. We test our framework on SAEs of varying sizes, activation functions, and\nlosses, trained on two different open-weight LLMs. We introduce five new\ntechniques to score the quality of explanations that are cheaper to run than\nthe previous state of the art. One of these techniques, intervention scoring,\nevaluates the interpretability of the effects of intervening on a feature,\nwhich we find explains features that are not recalled by existing methods. We\npropose guidelines for generating better explanations that remain valid for a\nbroader set of activating contexts, and discuss pitfalls with existing scoring\ntechniques. We use our explanations to measure the semantic similarity of\nindependently trained SAEs, and find that SAEs trained on nearby layers of the\nresidual stream are highly similar. Our large-scale analysis confirms that SAE\nlatents are indeed much more interpretable than neurons, even when neurons are\nsparsified using top-$k$ postprocessing. Our code is available at\nhttps://github.com/EleutherAI/sae-auto-interp, and our explanations are\navailable at\nhttps://huggingface.co/datasets/EleutherAI/auto_interp_explanations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While the activations of neurons in deep neural networks usually do not have\na simple human-understandable interpretation, sparse autoencoders (SAEs) can be\nused to transform these activations into a higher-dimensional latent space\nwhich may be more easily interpretable. However, these SAEs can have millions\nof distinct latent features, making it infeasible for humans to manually\ninterpret each one. In this work, we build an open-source automated pipeline to\ngenerate and evaluate natural language explanations for SAE features using\nLLMs. We test our framework on SAEs of varying sizes, activation functions, and\nlosses, trained on two different open-weight LLMs. We introduce five new\ntechniques to score the quality of explanations that are cheaper to run than\nthe previous state of the art. One of these techniques, intervention scoring,\nevaluates the interpretability of the effects of intervening on a feature,\nwhich we find explains features that are not recalled by existing methods. We\npropose guidelines for generating better explanations that remain valid for a\nbroader set of activating contexts, and discuss pitfalls with existing scoring\ntechniques. We use our explanations to measure the semantic similarity of\nindependently trained SAEs, and find that SAEs trained on nearby layers of the\nresidual stream are highly similar. Our large-scale analysis confirms that SAE\nlatents are indeed much more interpretable than neurons, even when neurons are\nsparsified using top-$k$ postprocessing. Our code is available at\nhttps://github.com/EleutherAI/sae-auto-interp, and our explanations are\navailable at\nhttps://huggingface.co/datasets/EleutherAI/auto_interp_explanations."
                },
                "authors": [
                    {
                        "name": "Gonalo Paulo"
                    },
                    {
                        "name": "Alex Mallen"
                    },
                    {
                        "name": "Caden Juang"
                    },
                    {
                        "name": "Nora Belrose"
                    }
                ],
                "author_detail": {
                    "name": "Nora Belrose"
                },
                "author": "Nora Belrose",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13928v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13928v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03467v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03467v1",
                "updated": "2024-12-04T16:56:20Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    16,
                    56,
                    20,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T16:56:20Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    16,
                    56,
                    20,
                    2,
                    339,
                    0
                ],
                "title": "Training-Free Mitigation of Language Reasoning Degradation After\n  Multimodal Instruction Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Mitigation of Language Reasoning Degradation After\n  Multimodal Instruction Tuning"
                },
                "summary": "Multimodal models typically combine a powerful large language model (LLM)\nwith a vision encoder and are then trained on multimodal data via instruction\ntuning. While this process adapts LLMs to multimodal settings, it remains\nunclear whether this adaptation compromises their original language reasoning\ncapabilities. In this work, we explore the effects of multimodal instruction\ntuning on language reasoning performance. We focus on LLaVA, a leading\nmultimodal framework that integrates LLMs such as Vicuna or Mistral with the\nCLIP vision encoder. We compare the performance of the original LLMs with their\nmultimodal-adapted counterparts across eight language reasoning tasks. Our\nexperiments yield several key insights. First, the impact of multimodal\nlearning varies between Vicuna and Mistral: we observe a degradation in\nlanguage reasoning for Mistral but improvements for Vicuna across most tasks.\nSecond, while multimodal instruction learning consistently degrades performance\non mathematical reasoning tasks (e.g., GSM8K), it enhances performance on\ncommonsense reasoning tasks (e.g., CommonsenseQA). Finally, we demonstrate that\na training-free model merging technique can effectively mitigate the language\nreasoning degradation observed in multimodal-adapted Mistral and even improve\nperformance on visual tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal models typically combine a powerful large language model (LLM)\nwith a vision encoder and are then trained on multimodal data via instruction\ntuning. While this process adapts LLMs to multimodal settings, it remains\nunclear whether this adaptation compromises their original language reasoning\ncapabilities. In this work, we explore the effects of multimodal instruction\ntuning on language reasoning performance. We focus on LLaVA, a leading\nmultimodal framework that integrates LLMs such as Vicuna or Mistral with the\nCLIP vision encoder. We compare the performance of the original LLMs with their\nmultimodal-adapted counterparts across eight language reasoning tasks. Our\nexperiments yield several key insights. First, the impact of multimodal\nlearning varies between Vicuna and Mistral: we observe a degradation in\nlanguage reasoning for Mistral but improvements for Vicuna across most tasks.\nSecond, while multimodal instruction learning consistently degrades performance\non mathematical reasoning tasks (e.g., GSM8K), it enhances performance on\ncommonsense reasoning tasks (e.g., CommonsenseQA). Finally, we demonstrate that\na training-free model merging technique can effectively mitigate the language\nreasoning degradation observed in multimodal-adapted Mistral and even improve\nperformance on visual tasks."
                },
                "authors": [
                    {
                        "name": "Neale Ratzlaff"
                    },
                    {
                        "name": "Man Luo"
                    },
                    {
                        "name": "Xin Su"
                    },
                    {
                        "name": "Vasudev Lal"
                    },
                    {
                        "name": "Phillip Howard"
                    }
                ],
                "author_detail": {
                    "name": "Phillip Howard"
                },
                "author": "Phillip Howard",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03467v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03467v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08181v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08181v2",
                "updated": "2024-12-04T16:55:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    16,
                    55,
                    18,
                    2,
                    339,
                    0
                ],
                "published": "2024-11-12T20:57:12Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    20,
                    57,
                    12,
                    1,
                    317,
                    0
                ],
                "title": "Challenges in Guardrailing Large Language Models for Science",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Challenges in Guardrailing Large Language Models for Science"
                },
                "summary": "The rapid development in large language models (LLMs) has transformed the\nlandscape of natural language processing and understanding (NLP/NLU), offering\nsignificant benefits across various domains. However, when applied to\nscientific research, these powerful models exhibit critical failure modes\nrelated to scientific integrity and trustworthiness. Existing general-purpose\nLLM guardrails are insufficient to address these unique challenges in the\nscientific domain. We provide comprehensive guidelines for deploying LLM\nguardrails in the scientific domain. We identify specific challenges --\nincluding time sensitivity, knowledge contextualization, conflict resolution,\nand intellectual property concerns -- and propose a guideline framework for the\nguardrails that can align with scientific needs. These guardrail dimensions\ninclude trustworthiness, ethics & bias, safety, and legal aspects. We also\noutline in detail the implementation strategies that employ white-box,\nblack-box, and gray-box methodologies that can be enforced within scientific\ncontexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development in large language models (LLMs) has transformed the\nlandscape of natural language processing and understanding (NLP/NLU), offering\nsignificant benefits across various domains. However, when applied to\nscientific research, these powerful models exhibit critical failure modes\nrelated to scientific integrity and trustworthiness. Existing general-purpose\nLLM guardrails are insufficient to address these unique challenges in the\nscientific domain. We provide comprehensive guidelines for deploying LLM\nguardrails in the scientific domain. We identify specific challenges --\nincluding time sensitivity, knowledge contextualization, conflict resolution,\nand intellectual property concerns -- and propose a guideline framework for the\nguardrails that can align with scientific needs. These guardrail dimensions\ninclude trustworthiness, ethics & bias, safety, and legal aspects. We also\noutline in detail the implementation strategies that employ white-box,\nblack-box, and gray-box methodologies that can be enforced within scientific\ncontexts."
                },
                "authors": [
                    {
                        "name": "Nishan Pantha"
                    },
                    {
                        "name": "Muthukumaran Ramasubramanian"
                    },
                    {
                        "name": "Iksha Gurung"
                    },
                    {
                        "name": "Manil Maskey"
                    },
                    {
                        "name": "Rahul Ramachandran"
                    }
                ],
                "author_detail": {
                    "name": "Rahul Ramachandran"
                },
                "author": "Rahul Ramachandran",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08181v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08181v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.01198v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.01198v5",
                "updated": "2024-12-04T16:46:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    16,
                    46,
                    47,
                    2,
                    339,
                    0
                ],
                "published": "2023-10-02T13:37:28Z",
                "published_parsed": [
                    2023,
                    10,
                    2,
                    13,
                    37,
                    28,
                    0,
                    275,
                    0
                ],
                "title": "Likelihood Based Inference for ARMA Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Likelihood Based Inference for ARMA Models"
                },
                "summary": "Autoregressive moving average (ARMA) models are widely used for analyzing\ntime series data. However, standard likelihood-based inference methodology for\nARMA models has avoidable limitations. We show that common ARMA likelihood\nmaximization strategies often lead to sub-optimal parameter estimates. While\nthis possibility has been previously identified, no routinely applicable\nalgorithm has been developed to resolve the issue. We introduce a novel random\ninitialization algorithm, designed to take advantage of the structure of the\nARMA likelihood function, which overcomes these optimization problems.\nAdditionally, we show that profile confidence intervals provide superior\nconfidence intervals to those based on the Fisher information matrix. The\nefficacy of the proposed methodology is demonstrated through a data analysis\nexample and a series of simulation studies. This work makes a significant\ncontribution to statistical practice by identifying and resolving\nunder-recognized shortcomings of existing procedures that frequently arise in\nscientific and industrial applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive moving average (ARMA) models are widely used for analyzing\ntime series data. However, standard likelihood-based inference methodology for\nARMA models has avoidable limitations. We show that common ARMA likelihood\nmaximization strategies often lead to sub-optimal parameter estimates. While\nthis possibility has been previously identified, no routinely applicable\nalgorithm has been developed to resolve the issue. We introduce a novel random\ninitialization algorithm, designed to take advantage of the structure of the\nARMA likelihood function, which overcomes these optimization problems.\nAdditionally, we show that profile confidence intervals provide superior\nconfidence intervals to those based on the Fisher information matrix. The\nefficacy of the proposed methodology is demonstrated through a data analysis\nexample and a series of simulation studies. This work makes a significant\ncontribution to statistical practice by identifying and resolving\nunder-recognized shortcomings of existing procedures that frequently arise in\nscientific and industrial applications."
                },
                "authors": [
                    {
                        "name": "Jesse Wheeler"
                    },
                    {
                        "name": "Edward L. Ionides"
                    }
                ],
                "author_detail": {
                    "name": "Edward L. Ionides"
                },
                "author": "Edward L. Ionides",
                "arxiv_comment": "The developmental version of the R package used in this paper is\n  available at the following GitHub repository:\n  git@github.com:jeswheel/arima2.git",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.01198v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.01198v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03455v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03455v1",
                "updated": "2024-12-04T16:43:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    16,
                    43,
                    51,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T16:43:51Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    16,
                    43,
                    51,
                    2,
                    339,
                    0
                ],
                "title": "A High Incidence of Central Star Formation Inferred from the Color\n  Gradients of Galaxies at $z>4$",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A High Incidence of Central Star Formation Inferred from the Color\n  Gradients of Galaxies at $z>4$"
                },
                "summary": "We study the rest-frame ultraviolet-optical color gradients of 669 galaxies\nat $4<z<8$ by characterizing the wavelength dependence of their structural\nparameters derived from simultaneously fitting the seven-band NIRCam images\nacquired with the James Webb Space Telescope. Distinct from trends observed at\nlower redshifts, where most galaxies exhibit negative color gradients whereby\ngalaxy centers are redder than their outskirts, in high-redshift galaxies\npositive color gradients match or even outnumber negative color gradients. The\ncolor gradients principally reflect radial variations in stellar population\ninstead of dust reddening or contamination from active galactic nuclei. The\nsign and magnitude of the color profile depend systematically on the global\nproperties of the galaxy: positive color gradients, characteristic of centrally\nconcentrated star formation or outside-in growth, preferentially inhabit\ngalaxies of lower stellar mass, smaller size, and bluer spectral energy\ndistribution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the rest-frame ultraviolet-optical color gradients of 669 galaxies\nat $4<z<8$ by characterizing the wavelength dependence of their structural\nparameters derived from simultaneously fitting the seven-band NIRCam images\nacquired with the James Webb Space Telescope. Distinct from trends observed at\nlower redshifts, where most galaxies exhibit negative color gradients whereby\ngalaxy centers are redder than their outskirts, in high-redshift galaxies\npositive color gradients match or even outnumber negative color gradients. The\ncolor gradients principally reflect radial variations in stellar population\ninstead of dust reddening or contamination from active galactic nuclei. The\nsign and magnitude of the color profile depend systematically on the global\nproperties of the galaxy: positive color gradients, characteristic of centrally\nconcentrated star formation or outside-in growth, preferentially inhabit\ngalaxies of lower stellar mass, smaller size, and bluer spectral energy\ndistribution."
                },
                "authors": [
                    {
                        "name": "Bingcheng Jin"
                    },
                    {
                        "name": "Luis C. Ho"
                    },
                    {
                        "name": "Wen Sun"
                    }
                ],
                "author_detail": {
                    "name": "Wen Sun"
                },
                "arxiv_affiliation": "PKU, KIAA",
                "author": "Wen Sun",
                "arxiv_comment": "21 pages, 16 figures, submitted to ApJ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03455v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03455v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03454v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03454v1",
                "updated": "2024-12-04T16:42:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    16,
                    42,
                    56,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T16:42:56Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    16,
                    42,
                    56,
                    2,
                    339,
                    0
                ],
                "title": "Decoding Long-duration Gravitational Waves from Binary Neutron Stars\n  with Machine Learning: Parameter Estimation and Equations of State",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoding Long-duration Gravitational Waves from Binary Neutron Stars\n  with Machine Learning: Parameter Estimation and Equations of State"
                },
                "summary": "Gravitational waves (GWs) from binary neutron stars (BNSs) offer valuable\nunderstanding of the nature of compact objects and hadronic matter. However,\ntheir analysis requires substantial computational resources due to the\nchallenges in Bayesian stochastic sampling. The third-generation (3G) GW\ndetectors are expected to detect BNS signals with significantly increased\nsignal duration, detection rates, and signal strength, leading to a major\ncomputational burden in the 3G era. We demonstrate a machine learning-based\nworkflow capable of producing source parameter estimation and constraints on\nequations of state (EOSs) for hours-long BNS signals in seconds with minimal\nhardware costs. We employ efficient compressions on the GW data and EOS using\nneural networks, based on which we build normalizing flows for inferences.\nGiven that full Bayesian analysis is prohibitively time-intensive, we validate\nour model against (semi-)analytical predictions. Additionally, we estimate the\ncomputational demands of BNS signal analysis in the 3G era, showing that the\nmachine learning methods will be crucial for future catalog-level analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gravitational waves (GWs) from binary neutron stars (BNSs) offer valuable\nunderstanding of the nature of compact objects and hadronic matter. However,\ntheir analysis requires substantial computational resources due to the\nchallenges in Bayesian stochastic sampling. The third-generation (3G) GW\ndetectors are expected to detect BNS signals with significantly increased\nsignal duration, detection rates, and signal strength, leading to a major\ncomputational burden in the 3G era. We demonstrate a machine learning-based\nworkflow capable of producing source parameter estimation and constraints on\nequations of state (EOSs) for hours-long BNS signals in seconds with minimal\nhardware costs. We employ efficient compressions on the GW data and EOS using\nneural networks, based on which we build normalizing flows for inferences.\nGiven that full Bayesian analysis is prohibitively time-intensive, we validate\nour model against (semi-)analytical predictions. Additionally, we estimate the\ncomputational demands of BNS signal analysis in the 3G era, showing that the\nmachine learning methods will be crucial for future catalog-level analysis."
                },
                "authors": [
                    {
                        "name": "Qian Hu"
                    },
                    {
                        "name": "Jessica Irwin"
                    },
                    {
                        "name": "Qi Sun"
                    },
                    {
                        "name": "Christopher Messenger"
                    },
                    {
                        "name": "Lami Suleiman"
                    },
                    {
                        "name": "Ik Siong Heng"
                    },
                    {
                        "name": "John Veitch"
                    }
                ],
                "author_detail": {
                    "name": "John Veitch"
                },
                "author": "John Veitch",
                "arxiv_comment": "8 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03454v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03454v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01083v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01083v3",
                "updated": "2024-12-04T16:39:07Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    16,
                    39,
                    7,
                    2,
                    339,
                    0
                ],
                "published": "2024-09-02T09:11:28Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    9,
                    11,
                    28,
                    0,
                    246,
                    0
                ],
                "title": "Affordance-based Robot Manipulation with Flow Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Affordance-based Robot Manipulation with Flow Matching"
                },
                "summary": "We present a framework for assistive robot manipulation, which focuses on two\nfundamental challenges: first, efficiently adapting large-scale models to\ndownstream scene affordance understanding tasks, especially in daily living\nscenarios where gathering multi-task data involving humans requires strenuous\neffort; second, effectively learning robot trajectories by grounding the visual\naffordance model. We tackle the first challenge by employing a\nparameter-efficient prompt tuning method that prepends learnable text prompts\nto the frozen vision model to predict manipulation affordances in multi-task\nscenarios. Then we propose to learn robot trajectories guided by affordances in\na supervised Flow Matching method. Flow matching represents a robot visuomotor\npolicy as a conditional process of flowing random waypoints to desired robot\ntrajectories. Finally, we introduce a real-world dataset with 10 tasks across\nActivities of Daily Living to test our framework. Our extensive evaluation\nhighlights that the proposed prompt tuning method for learning manipulation\naffordance with language prompter achieves competitive performance and even\noutperforms other finetuning protocols across data scales, while satisfying\nparameter efficiency. Learning multi-task robot trajectories with flow matching\npolicy also leads to consistently better results than alternative behavior\ncloning methods, including marginally better generalization performance and\nprominently faster inference than diffusion policy with DDPM. Our framework\nseamlessly unifies affordance model learning and trajectory generation with\nflow matching for robot manipulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a framework for assistive robot manipulation, which focuses on two\nfundamental challenges: first, efficiently adapting large-scale models to\ndownstream scene affordance understanding tasks, especially in daily living\nscenarios where gathering multi-task data involving humans requires strenuous\neffort; second, effectively learning robot trajectories by grounding the visual\naffordance model. We tackle the first challenge by employing a\nparameter-efficient prompt tuning method that prepends learnable text prompts\nto the frozen vision model to predict manipulation affordances in multi-task\nscenarios. Then we propose to learn robot trajectories guided by affordances in\na supervised Flow Matching method. Flow matching represents a robot visuomotor\npolicy as a conditional process of flowing random waypoints to desired robot\ntrajectories. Finally, we introduce a real-world dataset with 10 tasks across\nActivities of Daily Living to test our framework. Our extensive evaluation\nhighlights that the proposed prompt tuning method for learning manipulation\naffordance with language prompter achieves competitive performance and even\noutperforms other finetuning protocols across data scales, while satisfying\nparameter efficiency. Learning multi-task robot trajectories with flow matching\npolicy also leads to consistently better results than alternative behavior\ncloning methods, including marginally better generalization performance and\nprominently faster inference than diffusion policy with DDPM. Our framework\nseamlessly unifies affordance model learning and trajectory generation with\nflow matching for robot manipulation."
                },
                "authors": [
                    {
                        "name": "Fan Zhang"
                    },
                    {
                        "name": "Michael Gienger"
                    }
                ],
                "author_detail": {
                    "name": "Michael Gienger"
                },
                "author": "Michael Gienger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01083v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01083v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03766v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03766v2",
                "updated": "2024-12-04T16:39:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    16,
                    39,
                    4,
                    2,
                    339,
                    0
                ],
                "published": "2024-11-06T08:59:44Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    8,
                    59,
                    44,
                    2,
                    311,
                    0
                ],
                "title": "Number Cookbook: Number Understanding of Language Models and How to\n  Improve It",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Number Cookbook: Number Understanding of Language Models and How to\n  Improve It"
                },
                "summary": "Large language models (LLMs) can solve an increasing number of complex\nreasoning tasks while making surprising mistakes in basic numerical\nunderstanding and processing (such as 9.11 > 9.9). The latter ability is\nessential for tackling complex arithmetic and mathematical problems and serves\nas a foundation for most reasoning tasks, but previous work paid little\nattention to it or only discussed several restricted tasks (like integer\naddition). In this paper, we comprehensively investigate the numerical\nunderstanding and processing ability (NUPA) of LLMs. Firstly, we introduce a\nbenchmark covering four common numerical representations and 17 distinct\nnumerical tasks in four major categories, resulting in 41 meaningful\ncombinations in total. These tasks are derived from primary and secondary\neducation curricula, encompassing nearly all everyday numerical understanding\nand processing scenarios, and the rules of these tasks are very simple and\nclear. Through the benchmark, we find that current LLMs fail frequently in many\nof the tasks. To study the problem, we train small models with existing and\npotential techniques for enhancing NUPA (such as tokenizers, PEs, and number\nformats), comprehensively evaluating their effectiveness using our testbed. We\nalso finetune practical-scale LLMs on our proposed NUPA tasks and find that 1)\nnaive finetuning can improve NUPA a lot on many but not all tasks, and 2)\nsurprisingly, techniques designed to enhance NUPA prove ineffective for\nfinetuning pretrained models. We further explore the impact of chain-of-thought\ntechniques on NUPA. Our work provides a more detailed and comprehensive\nunderstanding of NUPA in LLMs. Our benchmark and code are released at\nhttps://github.com/GraphPKU/number_cookbook.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) can solve an increasing number of complex\nreasoning tasks while making surprising mistakes in basic numerical\nunderstanding and processing (such as 9.11 > 9.9). The latter ability is\nessential for tackling complex arithmetic and mathematical problems and serves\nas a foundation for most reasoning tasks, but previous work paid little\nattention to it or only discussed several restricted tasks (like integer\naddition). In this paper, we comprehensively investigate the numerical\nunderstanding and processing ability (NUPA) of LLMs. Firstly, we introduce a\nbenchmark covering four common numerical representations and 17 distinct\nnumerical tasks in four major categories, resulting in 41 meaningful\ncombinations in total. These tasks are derived from primary and secondary\neducation curricula, encompassing nearly all everyday numerical understanding\nand processing scenarios, and the rules of these tasks are very simple and\nclear. Through the benchmark, we find that current LLMs fail frequently in many\nof the tasks. To study the problem, we train small models with existing and\npotential techniques for enhancing NUPA (such as tokenizers, PEs, and number\nformats), comprehensively evaluating their effectiveness using our testbed. We\nalso finetune practical-scale LLMs on our proposed NUPA tasks and find that 1)\nnaive finetuning can improve NUPA a lot on many but not all tasks, and 2)\nsurprisingly, techniques designed to enhance NUPA prove ineffective for\nfinetuning pretrained models. We further explore the impact of chain-of-thought\ntechniques on NUPA. Our work provides a more detailed and comprehensive\nunderstanding of NUPA in LLMs. Our benchmark and code are released at\nhttps://github.com/GraphPKU/number_cookbook."
                },
                "authors": [
                    {
                        "name": "Haotong Yang"
                    },
                    {
                        "name": "Yi Hu"
                    },
                    {
                        "name": "Shijia Kang"
                    },
                    {
                        "name": "Zhouchen Lin"
                    },
                    {
                        "name": "Muhan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Muhan Zhang"
                },
                "author": "Muhan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03766v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03766v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03446v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03446v1",
                "updated": "2024-12-04T16:34:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    16,
                    34,
                    35,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T16:34:35Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    16,
                    34,
                    35,
                    2,
                    339,
                    0
                ],
                "title": "From Words to Workflows: Automating Business Processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Words to Workflows: Automating Business Processes"
                },
                "summary": "As businesses increasingly rely on automation to streamline operations, the\nlimitations of Robotic Process Automation (RPA) have become apparent,\nparticularly its dependence on expert knowledge and inability to handle complex\ndecision-making tasks. Recent advancements in Artificial Intelligence (AI),\nparticularly Generative AI (GenAI) and Large Language Models (LLMs), have paved\nthe way for Intelligent Automation (IA), which integrates cognitive\ncapabilities to overcome the shortcomings of RPA. This paper introduces\nText2Workflow, a novel method that automatically generates workflows from\nnatural language user requests. Unlike traditional automation approaches,\nText2Workflow offers a generalized solution for automating any business\nprocess, translating user inputs into a sequence of executable steps\nrepresented in JavaScript Object Notation (JSON) format. Leveraging the\ndecision-making and instruction-following capabilities of LLMs, this method\nprovides a scalable, adaptable framework that enables users to visualize and\nexecute workflows with minimal manual intervention. This research outlines the\nText2Workflow methodology and its broader implications for automating complex\nbusiness processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As businesses increasingly rely on automation to streamline operations, the\nlimitations of Robotic Process Automation (RPA) have become apparent,\nparticularly its dependence on expert knowledge and inability to handle complex\ndecision-making tasks. Recent advancements in Artificial Intelligence (AI),\nparticularly Generative AI (GenAI) and Large Language Models (LLMs), have paved\nthe way for Intelligent Automation (IA), which integrates cognitive\ncapabilities to overcome the shortcomings of RPA. This paper introduces\nText2Workflow, a novel method that automatically generates workflows from\nnatural language user requests. Unlike traditional automation approaches,\nText2Workflow offers a generalized solution for automating any business\nprocess, translating user inputs into a sequence of executable steps\nrepresented in JavaScript Object Notation (JSON) format. Leveraging the\ndecision-making and instruction-following capabilities of LLMs, this method\nprovides a scalable, adaptable framework that enables users to visualize and\nexecute workflows with minimal manual intervention. This research outlines the\nText2Workflow methodology and its broader implications for automating complex\nbusiness processes."
                },
                "authors": [
                    {
                        "name": "Laura Minkova"
                    },
                    {
                        "name": "Jessica Lpez Espejel"
                    },
                    {
                        "name": "Taki Eddine Toufik Djaidja"
                    },
                    {
                        "name": "Walid Dahhane"
                    },
                    {
                        "name": "El Hassane Ettifouri"
                    }
                ],
                "author_detail": {
                    "name": "El Hassane Ettifouri"
                },
                "author": "El Hassane Ettifouri",
                "arxiv_comment": "Under review at Elsevier's Engineering Applications of Artificial\n  Intelligence",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03446v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03446v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16362v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16362v2",
                "updated": "2024-12-04T16:30:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    16,
                    30,
                    46,
                    2,
                    339,
                    0
                ],
                "published": "2024-11-25T13:15:31Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    13,
                    15,
                    31,
                    0,
                    330,
                    0
                ],
                "title": "Optimal switching strategies in multi-drug therapies for chronic\n  diseases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal switching strategies in multi-drug therapies for chronic\n  diseases"
                },
                "summary": "Antimicrobial resistance is a threat to public health with millions of deaths\nlinked to drug resistant infections every year. To mitigate resistance, common\nstrategies that are used are combination therapies and therapy switching.\nHowever, the stochastic nature of pathogenic mutation makes the optimization of\nthese strategies challenging. Here, we propose a two-scale stochastic model\nthat considers the effective evolution of therapies in a multidimensional\nefficacy space, where each dimension represents the efficacy of a specific drug\nin the therapy. The diffusion of therapies within this space is subject to\nstochastic resets, representing therapy switches. The boundaries of the space,\ninferred from coarser pathogen-host dynamics, can be either reflecting or\nabsorbing. Reflecting boundaries impede full recovery of the host, while\nabsorbing boundaries represent the development of antimicrobial resistance,\nleading to therapy failure. We derive analytical expressions for the average\nabsorption times, accounting for both continuous and discrete genomic changes\nusing the frameworks of Langevin and Master equations, respectively. These\nexpressions allow us to evaluate the relevance of times between drug-switches\nand the number of simultaneous drugs in relation to typical timescales for drug\nresistance development. We also explore realistic scenarios where therapy\nconstraints are imposed to the number of administered therapies and/or their\ncosts, finding non-trivial optimal drug-switching protocols that maximize the\ntime before antimicrobial resistance develops while reducing therapy costs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Antimicrobial resistance is a threat to public health with millions of deaths\nlinked to drug resistant infections every year. To mitigate resistance, common\nstrategies that are used are combination therapies and therapy switching.\nHowever, the stochastic nature of pathogenic mutation makes the optimization of\nthese strategies challenging. Here, we propose a two-scale stochastic model\nthat considers the effective evolution of therapies in a multidimensional\nefficacy space, where each dimension represents the efficacy of a specific drug\nin the therapy. The diffusion of therapies within this space is subject to\nstochastic resets, representing therapy switches. The boundaries of the space,\ninferred from coarser pathogen-host dynamics, can be either reflecting or\nabsorbing. Reflecting boundaries impede full recovery of the host, while\nabsorbing boundaries represent the development of antimicrobial resistance,\nleading to therapy failure. We derive analytical expressions for the average\nabsorption times, accounting for both continuous and discrete genomic changes\nusing the frameworks of Langevin and Master equations, respectively. These\nexpressions allow us to evaluate the relevance of times between drug-switches\nand the number of simultaneous drugs in relation to typical timescales for drug\nresistance development. We also explore realistic scenarios where therapy\nconstraints are imposed to the number of administered therapies and/or their\ncosts, finding non-trivial optimal drug-switching protocols that maximize the\ntime before antimicrobial resistance develops while reducing therapy costs."
                },
                "authors": [
                    {
                        "name": "Juan Magalang"
                    },
                    {
                        "name": "Javier Aguilar"
                    },
                    {
                        "name": "Jose Perico Esguerra"
                    },
                    {
                        "name": "dgar Roldn"
                    },
                    {
                        "name": "Daniel Sanchez-Taltavull"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Sanchez-Taltavull"
                },
                "author": "Daniel Sanchez-Taltavull",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16362v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16362v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.PE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.PE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02205v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02205v2",
                "updated": "2024-12-04T16:12:08Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    16,
                    12,
                    8,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-03T06:47:15Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    6,
                    47,
                    15,
                    1,
                    338,
                    0
                ],
                "title": "DataLab: A Unified Platform for LLM-Powered Business Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DataLab: A Unified Platform for LLM-Powered Business Intelligence"
                },
                "summary": "Business intelligence (BI) transforms large volumes of data within modern\norganizations into actionable insights for informed decision-making. Recently,\nlarge language model (LLM)-based agents have streamlined the BI workflow by\nautomatically performing task planning, reasoning, and actions in executable\nenvironments based on natural language (NL) queries. However, existing\napproaches primarily focus on individual BI tasks such as NL2SQL and NL2VIS.\nThe fragmentation of tasks across different data roles and tools lead to\ninefficiencies and potential errors due to the iterative and collaborative\nnature of BI. In this paper, we introduce DataLab, a unified BI platform that\nintegrates a one-stop LLM-based agent framework with an augmented computational\nnotebook interface. DataLab supports a wide range of BI tasks for different\ndata roles by seamlessly combining LLM assistance with user customization\nwithin a single environment. To achieve this unification, we design a domain\nknowledge incorporation module tailored for enterprise-specific BI tasks, an\ninter-agent communication mechanism to facilitate information sharing across\nthe BI workflow, and a cell-based context management strategy to enhance\ncontext utilization efficiency in BI notebooks. Extensive experiments\ndemonstrate that DataLab achieves state-of-the-art performance on various BI\ntasks across popular research benchmarks. Moreover, DataLab maintains high\neffectiveness and efficiency on real-world datasets from Tencent, achieving up\nto a 58.58% increase in accuracy and a 61.65% reduction in token cost on\nenterprise-specific BI tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Business intelligence (BI) transforms large volumes of data within modern\norganizations into actionable insights for informed decision-making. Recently,\nlarge language model (LLM)-based agents have streamlined the BI workflow by\nautomatically performing task planning, reasoning, and actions in executable\nenvironments based on natural language (NL) queries. However, existing\napproaches primarily focus on individual BI tasks such as NL2SQL and NL2VIS.\nThe fragmentation of tasks across different data roles and tools lead to\ninefficiencies and potential errors due to the iterative and collaborative\nnature of BI. In this paper, we introduce DataLab, a unified BI platform that\nintegrates a one-stop LLM-based agent framework with an augmented computational\nnotebook interface. DataLab supports a wide range of BI tasks for different\ndata roles by seamlessly combining LLM assistance with user customization\nwithin a single environment. To achieve this unification, we design a domain\nknowledge incorporation module tailored for enterprise-specific BI tasks, an\ninter-agent communication mechanism to facilitate information sharing across\nthe BI workflow, and a cell-based context management strategy to enhance\ncontext utilization efficiency in BI notebooks. Extensive experiments\ndemonstrate that DataLab achieves state-of-the-art performance on various BI\ntasks across popular research benchmarks. Moreover, DataLab maintains high\neffectiveness and efficiency on real-world datasets from Tencent, achieving up\nto a 58.58% increase in accuracy and a 61.65% reduction in token cost on\nenterprise-specific BI tasks."
                },
                "authors": [
                    {
                        "name": "Luoxuan Weng"
                    },
                    {
                        "name": "Yinghao Tang"
                    },
                    {
                        "name": "Yingchaojie Feng"
                    },
                    {
                        "name": "Zhuo Chang"
                    },
                    {
                        "name": "Peng Chen"
                    },
                    {
                        "name": "Ruiqin Chen"
                    },
                    {
                        "name": "Haozhe Feng"
                    },
                    {
                        "name": "Chen Hou"
                    },
                    {
                        "name": "Danqing Huang"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Huaming Rao"
                    },
                    {
                        "name": "Haonan Wang"
                    },
                    {
                        "name": "Canshi Wei"
                    },
                    {
                        "name": "Xiaofeng Yang"
                    },
                    {
                        "name": "Yuhui Zhang"
                    },
                    {
                        "name": "Yifeng Zheng"
                    },
                    {
                        "name": "Xiuqi Huang"
                    },
                    {
                        "name": "Minfeng Zhu"
                    },
                    {
                        "name": "Yuxin Ma"
                    },
                    {
                        "name": "Bin Cui"
                    },
                    {
                        "name": "Wei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wei Chen"
                },
                "author": "Wei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02205v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02205v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03420v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03420v1",
                "updated": "2024-12-04T16:00:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    16,
                    0,
                    14,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T16:00:14Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    16,
                    0,
                    14,
                    2,
                    339,
                    0
                ],
                "title": "Automated Test-Case Generation for REST APIs Using Model Inference\n  Search Heuristic",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Test-Case Generation for REST APIs Using Model Inference\n  Search Heuristic"
                },
                "summary": "The rising popularity of the microservice architectural style has led to a\ngrowing demand for automated testing approaches tailored to these systems.\nEvoMaster is a state-of-the-art tool that uses Evolutionary Algorithms (EAs) to\nautomatically generate test cases for microservices' REST APIs. One limitation\nof these EAs is the use of unit-level search heuristics, such as branch\ndistances, which focus on fine-grained code coverage and may not effectively\ncapture the complex, interconnected behaviors characteristic of system-level\ntesting. To address this limitation, we propose a new search heuristic (MISH)\nthat uses real-time automaton learning to guide the test case generation\nprocess. We capture the sequential call patterns exhibited by a test case by\nlearning an automaton from the stream of log events outputted by different\nmicroservices within the same system. Therefore, MISH learns a representation\nof the systemwide behavior, allowing us to define the fitness of a test case\nbased on the path it traverses within the inferred automaton. We empirically\nevaluate MISH's effectiveness on six real-world benchmark microservice\napplications and compare it against a state-of-the-art technique, MOSA, for\ntesting REST APIs. Our evaluation shows promising results for using MISH to\nguide the automated test case generation within EvoMaster.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rising popularity of the microservice architectural style has led to a\ngrowing demand for automated testing approaches tailored to these systems.\nEvoMaster is a state-of-the-art tool that uses Evolutionary Algorithms (EAs) to\nautomatically generate test cases for microservices' REST APIs. One limitation\nof these EAs is the use of unit-level search heuristics, such as branch\ndistances, which focus on fine-grained code coverage and may not effectively\ncapture the complex, interconnected behaviors characteristic of system-level\ntesting. To address this limitation, we propose a new search heuristic (MISH)\nthat uses real-time automaton learning to guide the test case generation\nprocess. We capture the sequential call patterns exhibited by a test case by\nlearning an automaton from the stream of log events outputted by different\nmicroservices within the same system. Therefore, MISH learns a representation\nof the systemwide behavior, allowing us to define the fitness of a test case\nbased on the path it traverses within the inferred automaton. We empirically\nevaluate MISH's effectiveness on six real-world benchmark microservice\napplications and compare it against a state-of-the-art technique, MOSA, for\ntesting REST APIs. Our evaluation shows promising results for using MISH to\nguide the automated test case generation within EvoMaster."
                },
                "authors": [
                    {
                        "name": "Clinton Cao"
                    },
                    {
                        "name": "Annibale Panichella"
                    },
                    {
                        "name": "Sicco Verwer"
                    }
                ],
                "author_detail": {
                    "name": "Sicco Verwer"
                },
                "author": "Sicco Verwer",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03420v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03420v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03409v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03409v1",
                "updated": "2024-12-04T15:48:59Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    15,
                    48,
                    59,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T15:48:59Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    15,
                    48,
                    59,
                    2,
                    339,
                    0
                ],
                "title": "PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following\n  Models Need for Efficient Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following\n  Models Need for Efficient Generation"
                },
                "summary": "Recently, large vision-language models (LVLMs) have rapidly gained popularity\nfor their strong generation and reasoning capabilities given diverse multimodal\ninputs. However, these models incur significant computational and memory\noverhead during inference, which greatly hinders the efficient deployment in\npractical scenarios. The extensive key-value (KV) cache, necessitated by the\nlengthy input and output sequences, notably contributes to the high inference\ncost. Based on this, recent works have investigated ways to reduce the KV cache\nsize for higher efficiency. Although effective, they generally overlook the\ndistinct importance distributions of KV vectors across layers and maintain the\nsame cache size for each layer during the next token prediction. This results\nin the significant contextual information loss for certain layers, leading to\nnotable performance decline. To address this, we present PrefixKV. It reframes\nthe challenge of determining KV cache sizes for all layers into the task of\nsearching for the optimal global prefix configuration. With an adaptive\nlayer-wise KV retention recipe based on binary search, the maximum contextual\ninformation can thus be preserved in each layer, facilitating the generation.\nExtensive experiments demonstrate that our method achieves the state-of-the-art\nperformance compared with others. It exhibits superior inference efficiency and\ngeneration quality trade-offs, showing promising potential for practical\napplications. Code is available at \\url{https://github.com/THU-MIG/PrefixKV}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large vision-language models (LVLMs) have rapidly gained popularity\nfor their strong generation and reasoning capabilities given diverse multimodal\ninputs. However, these models incur significant computational and memory\noverhead during inference, which greatly hinders the efficient deployment in\npractical scenarios. The extensive key-value (KV) cache, necessitated by the\nlengthy input and output sequences, notably contributes to the high inference\ncost. Based on this, recent works have investigated ways to reduce the KV cache\nsize for higher efficiency. Although effective, they generally overlook the\ndistinct importance distributions of KV vectors across layers and maintain the\nsame cache size for each layer during the next token prediction. This results\nin the significant contextual information loss for certain layers, leading to\nnotable performance decline. To address this, we present PrefixKV. It reframes\nthe challenge of determining KV cache sizes for all layers into the task of\nsearching for the optimal global prefix configuration. With an adaptive\nlayer-wise KV retention recipe based on binary search, the maximum contextual\ninformation can thus be preserved in each layer, facilitating the generation.\nExtensive experiments demonstrate that our method achieves the state-of-the-art\nperformance compared with others. It exhibits superior inference efficiency and\ngeneration quality trade-offs, showing promising potential for practical\napplications. Code is available at \\url{https://github.com/THU-MIG/PrefixKV}."
                },
                "authors": [
                    {
                        "name": "Ao Wang"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Jianchao Tan"
                    },
                    {
                        "name": "Kefeng Zhang"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Zijia Lin"
                    },
                    {
                        "name": "Jungong Han"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "arxiv_comment": "12 pages, 5 figures;",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03409v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03409v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03398v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03398v1",
                "updated": "2024-12-04T15:27:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    15,
                    27,
                    39,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T15:27:39Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    15,
                    27,
                    39,
                    2,
                    339,
                    0
                ],
                "title": "RedStone: Curating General, Code, Math, and QA Data for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RedStone: Curating General, Code, Math, and QA Data for Large Language\n  Models"
                },
                "summary": "Pre-training Large Language Models (LLMs) on high-quality, meticulously\ncurated datasets is widely recognized as critical for enhancing their\nperformance and generalization capabilities. This study explores the untapped\npotential of Common Crawl as a comprehensive and flexible resource for\npre-training LLMs, addressing both general-purpose language understanding and\nspecialized domain knowledge. We introduce RedStone, an innovative and scalable\npipeline engineered to extract and process data from Common Crawl, facilitating\nthe creation of extensive and varied pre-training datasets. Unlike traditional\ndatasets, which often require expensive curation and domain-specific expertise,\nRedStone leverages the breadth of Common Crawl to deliver datasets tailored to\na wide array of domains. In this work, we exemplify its capability by\nconstructing pre-training datasets across multiple fields, including general\nlanguage understanding, code, mathematics, and question-answering tasks. The\nflexibility of RedStone allows for easy adaptation to other specialized\ndomains, significantly lowering the barrier to creating valuable\ndomain-specific datasets. Our findings demonstrate that Common Crawl, when\nharnessed through effective pipelines like RedStone, can serve as a rich,\nrenewable source of pre-training data, unlocking new avenues for domain\nadaptation and knowledge discovery in LLMs. This work also underscores the\nimportance of innovative data acquisition strategies and highlights the role of\nweb-scale data as a powerful resource in the continued evolution of LLMs.\nRedStone code and data samples will be publicly available at\n\\url{https://aka.ms/redstone}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pre-training Large Language Models (LLMs) on high-quality, meticulously\ncurated datasets is widely recognized as critical for enhancing their\nperformance and generalization capabilities. This study explores the untapped\npotential of Common Crawl as a comprehensive and flexible resource for\npre-training LLMs, addressing both general-purpose language understanding and\nspecialized domain knowledge. We introduce RedStone, an innovative and scalable\npipeline engineered to extract and process data from Common Crawl, facilitating\nthe creation of extensive and varied pre-training datasets. Unlike traditional\ndatasets, which often require expensive curation and domain-specific expertise,\nRedStone leverages the breadth of Common Crawl to deliver datasets tailored to\na wide array of domains. In this work, we exemplify its capability by\nconstructing pre-training datasets across multiple fields, including general\nlanguage understanding, code, mathematics, and question-answering tasks. The\nflexibility of RedStone allows for easy adaptation to other specialized\ndomains, significantly lowering the barrier to creating valuable\ndomain-specific datasets. Our findings demonstrate that Common Crawl, when\nharnessed through effective pipelines like RedStone, can serve as a rich,\nrenewable source of pre-training data, unlocking new avenues for domain\nadaptation and knowledge discovery in LLMs. This work also underscores the\nimportance of innovative data acquisition strategies and highlights the role of\nweb-scale data as a powerful resource in the continued evolution of LLMs.\nRedStone code and data samples will be publicly available at\n\\url{https://aka.ms/redstone}."
                },
                "authors": [
                    {
                        "name": "Yaoyao Chang"
                    },
                    {
                        "name": "Lei Cui"
                    },
                    {
                        "name": "Li Dong"
                    },
                    {
                        "name": "Shaohan Huang"
                    },
                    {
                        "name": "Yangyu Huang"
                    },
                    {
                        "name": "Yupan Huang"
                    },
                    {
                        "name": "Scarlett Li"
                    },
                    {
                        "name": "Tengchao Lv"
                    },
                    {
                        "name": "Shuming Ma"
                    },
                    {
                        "name": "Qinzheng Sun"
                    },
                    {
                        "name": "Wenhui Wang"
                    },
                    {
                        "name": "Furu Wei"
                    },
                    {
                        "name": "Ying Xin"
                    },
                    {
                        "name": "Mao Yang"
                    },
                    {
                        "name": "Qiufeng Yin"
                    },
                    {
                        "name": "Xingxing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xingxing Zhang"
                },
                "author": "Xingxing Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03398v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03398v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.19732v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.19732v4",
                "updated": "2024-12-04T15:20:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    15,
                    20,
                    35,
                    2,
                    339,
                    0
                ],
                "published": "2024-05-30T06:24:14Z",
                "published_parsed": [
                    2024,
                    5,
                    30,
                    6,
                    24,
                    14,
                    3,
                    151,
                    0
                ],
                "title": "LLM as a Complementary Optimizer to Gradient Descent: A Case Study in\n  Prompt Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM as a Complementary Optimizer to Gradient Descent: A Case Study in\n  Prompt Tuning"
                },
                "summary": "Mastering a skill generally relies on both hands-on experience from doers and\ninsightful, high-level guidance by mentors. Will this strategy also work well\nfor solving complex non-convex optimization problems? Here, a common\ngradient-based optimizer acts like a disciplined doer, making locally optimal\nupdates at each step. Large Language Models (LLMs) can also search for better\nsolutions by inferring from natural language instructions, akin to a high-level\nmentor. In this paper, we show that these two participators are complementary\nto each other and can effectively collaborate as a combined optimization\nframework. The collaborative optimization is achieved by alternating between\nthe gradient-based and LLM-based optimizers. We instruct LLMs to generate\npossibly improved solutions by taking parameter trajectories recorded during\nthe previous stage of gradient-based optimization into account. Inferred\nresults of LLMs are used as restarting points for the next stage of gradient\noptimization. We verify the effectiveness of this optimization framework on\nprompt tuning. By leveraging both the locally rigorous gradient-based optimizer\nand the high-level deductive LLM-based optimizer, the combined optimization\nmethod consistently yields improvements over competitive baselines on a variety\nof tasks. Our results demonstrate the synergistic effect of conventional\ngradient-based optimization and the inference ability of LLMs. The code is\nreleased at https://github.com/guozix/LLM-catalyst.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mastering a skill generally relies on both hands-on experience from doers and\ninsightful, high-level guidance by mentors. Will this strategy also work well\nfor solving complex non-convex optimization problems? Here, a common\ngradient-based optimizer acts like a disciplined doer, making locally optimal\nupdates at each step. Large Language Models (LLMs) can also search for better\nsolutions by inferring from natural language instructions, akin to a high-level\nmentor. In this paper, we show that these two participators are complementary\nto each other and can effectively collaborate as a combined optimization\nframework. The collaborative optimization is achieved by alternating between\nthe gradient-based and LLM-based optimizers. We instruct LLMs to generate\npossibly improved solutions by taking parameter trajectories recorded during\nthe previous stage of gradient-based optimization into account. Inferred\nresults of LLMs are used as restarting points for the next stage of gradient\noptimization. We verify the effectiveness of this optimization framework on\nprompt tuning. By leveraging both the locally rigorous gradient-based optimizer\nand the high-level deductive LLM-based optimizer, the combined optimization\nmethod consistently yields improvements over competitive baselines on a variety\nof tasks. Our results demonstrate the synergistic effect of conventional\ngradient-based optimization and the inference ability of LLMs. The code is\nreleased at https://github.com/guozix/LLM-catalyst."
                },
                "authors": [
                    {
                        "name": "Zixian Guo"
                    },
                    {
                        "name": "Ming Liu"
                    },
                    {
                        "name": "Zhilong Ji"
                    },
                    {
                        "name": "Jinfeng Bai"
                    },
                    {
                        "name": "Yiwen Guo"
                    },
                    {
                        "name": "Wangmeng Zuo"
                    }
                ],
                "author_detail": {
                    "name": "Wangmeng Zuo"
                },
                "author": "Wangmeng Zuo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.19732v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.19732v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16310v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16310v3",
                "updated": "2024-12-04T15:12:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    15,
                    12,
                    6,
                    2,
                    339,
                    0
                ],
                "published": "2024-11-25T11:57:48Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    11,
                    57,
                    48,
                    0,
                    330,
                    0
                ],
                "title": "Functionality understanding and segmentation in 3D scenes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Functionality understanding and segmentation in 3D scenes"
                },
                "summary": "Understanding functionalities in 3D scenes involves interpreting natural\nlanguage descriptions to locate functional interactive objects, such as handles\nand buttons, in a 3D environment. Functionality understanding is highly\nchallenging, as it requires both world knowledge to interpret language and\nspatial perception to identify fine-grained objects. For example, given a task\nlike 'turn on the ceiling light', an embodied AI agent must infer that it needs\nto locate the light switch, even though the switch is not explicitly mentioned\nin the task description. To date, no dedicated methods have been developed for\nthis problem. In this paper, we introduce Fun3DU, the first approach designed\nfor functionality understanding in 3D scenes. Fun3DU uses a language model to\nparse the task description through Chain-of-Thought reasoning in order to\nidentify the object of interest. The identified object is segmented across\nmultiple views of the captured scene by using a vision and language model. The\nsegmentation results from each view are lifted in 3D and aggregated into the\npoint cloud using geometric information. Fun3DU is training-free, relying\nentirely on pre-trained models. We evaluate Fun3DU on SceneFun3D, the most\nrecent and only dataset to benchmark this task, which comprises over 3000 task\ndescriptions on 230 scenes. Our method significantly outperforms\nstate-of-the-art open-vocabulary 3D segmentation approaches. Project page:\nhttps://jcorsetti.github.io/fun3du",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding functionalities in 3D scenes involves interpreting natural\nlanguage descriptions to locate functional interactive objects, such as handles\nand buttons, in a 3D environment. Functionality understanding is highly\nchallenging, as it requires both world knowledge to interpret language and\nspatial perception to identify fine-grained objects. For example, given a task\nlike 'turn on the ceiling light', an embodied AI agent must infer that it needs\nto locate the light switch, even though the switch is not explicitly mentioned\nin the task description. To date, no dedicated methods have been developed for\nthis problem. In this paper, we introduce Fun3DU, the first approach designed\nfor functionality understanding in 3D scenes. Fun3DU uses a language model to\nparse the task description through Chain-of-Thought reasoning in order to\nidentify the object of interest. The identified object is segmented across\nmultiple views of the captured scene by using a vision and language model. The\nsegmentation results from each view are lifted in 3D and aggregated into the\npoint cloud using geometric information. Fun3DU is training-free, relying\nentirely on pre-trained models. We evaluate Fun3DU on SceneFun3D, the most\nrecent and only dataset to benchmark this task, which comprises over 3000 task\ndescriptions on 230 scenes. Our method significantly outperforms\nstate-of-the-art open-vocabulary 3D segmentation approaches. Project page:\nhttps://jcorsetti.github.io/fun3du"
                },
                "authors": [
                    {
                        "name": "Jaime Corsetti"
                    },
                    {
                        "name": "Francesco Giuliari"
                    },
                    {
                        "name": "Alice Fasoli"
                    },
                    {
                        "name": "Davide Boscaini"
                    },
                    {
                        "name": "Fabio Poiesi"
                    }
                ],
                "author_detail": {
                    "name": "Fabio Poiesi"
                },
                "author": "Fabio Poiesi",
                "arxiv_comment": "Technical report. 20 pages, 12 figures, 7 tables. Fixed main diagram",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16310v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16310v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03378v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03378v1",
                "updated": "2024-12-04T15:05:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    15,
                    5,
                    43,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T15:05:43Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    15,
                    5,
                    43,
                    2,
                    339,
                    0
                ],
                "title": "Volumetrically Consistent 3D Gaussian Rasterization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Volumetrically Consistent 3D Gaussian Rasterization"
                },
                "summary": "Recently, 3D Gaussian Splatting (3DGS) has enabled photorealistic view\nsynthesis at high inference speeds. However, its splatting-based rendering\nmodel makes several approximations to the rendering equation, reducing physical\naccuracy. We show that splatting and its approximations are unnecessary, even\nwithin a rasterizer; we instead volumetrically integrate 3D Gaussians directly\nto compute the transmittance across them analytically. We use this analytic\ntransmittance to derive more physically-accurate alpha values than 3DGS, which\ncan directly be used within their framework. The result is a method that more\nclosely follows the volume rendering equation (similar to ray-tracing) while\nenjoying the speed benefits of rasterization. Our method represents opaque\nsurfaces with higher accuracy and fewer points than 3DGS. This enables it to\noutperform 3DGS for view synthesis (measured in SSIM and LPIPS). Being\nvolumetrically consistent also enables our method to work out of the box for\ntomography. We match the state-of-the-art 3DGS-based tomography method with\nfewer points. Being volumetrically consistent also enables our method to work\nout of the box for tomography. We match the state-of-the-art 3DGS-based\ntomography method with fewer points.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, 3D Gaussian Splatting (3DGS) has enabled photorealistic view\nsynthesis at high inference speeds. However, its splatting-based rendering\nmodel makes several approximations to the rendering equation, reducing physical\naccuracy. We show that splatting and its approximations are unnecessary, even\nwithin a rasterizer; we instead volumetrically integrate 3D Gaussians directly\nto compute the transmittance across them analytically. We use this analytic\ntransmittance to derive more physically-accurate alpha values than 3DGS, which\ncan directly be used within their framework. The result is a method that more\nclosely follows the volume rendering equation (similar to ray-tracing) while\nenjoying the speed benefits of rasterization. Our method represents opaque\nsurfaces with higher accuracy and fewer points than 3DGS. This enables it to\noutperform 3DGS for view synthesis (measured in SSIM and LPIPS). Being\nvolumetrically consistent also enables our method to work out of the box for\ntomography. We match the state-of-the-art 3DGS-based tomography method with\nfewer points. Being volumetrically consistent also enables our method to work\nout of the box for tomography. We match the state-of-the-art 3DGS-based\ntomography method with fewer points."
                },
                "authors": [
                    {
                        "name": "Chinmay Talegaonkar"
                    },
                    {
                        "name": "Yash Belhe"
                    },
                    {
                        "name": "Ravi Ramamoorthi"
                    },
                    {
                        "name": "Nicholas Antipa"
                    }
                ],
                "author_detail": {
                    "name": "Nicholas Antipa"
                },
                "author": "Nicholas Antipa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03378v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03378v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15903v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15903v2",
                "updated": "2024-12-04T15:01:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    15,
                    1,
                    47,
                    2,
                    339,
                    0
                ],
                "published": "2024-08-28T16:15:45Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    16,
                    15,
                    45,
                    2,
                    241,
                    0
                ],
                "title": "LLM-Based Multi-Hop Question Answering with Knowledge Graph Integration\n  in Evolving Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Based Multi-Hop Question Answering with Knowledge Graph Integration\n  in Evolving Environments"
                },
                "summary": "The important challenge of keeping knowledge in Large Language Models (LLMs)\nup-to-date has led to the development of various methods for incorporating new\nfacts. However, existing methods for such knowledge editing still face\ndifficulties with multi-hop questions that require accurate fact identification\nand sequential logical reasoning, particularly among numerous fact updates. To\ntackle these challenges, this paper introduces Graph Memory-based Editing for\nLarge Language Models (GMeLLo), a straightforward and effective method that\nmerges the explicit knowledge representation of Knowledge Graphs (KGs) with the\nlinguistic flexibility of LLMs. Beyond merely leveraging LLMs for question\nanswering, GMeLLo employs these models to convert free-form language into\nstructured queries and fact triples, facilitating seamless interaction with KGs\nfor rapid updates and precise multi-hop reasoning. Our results show that GMeLLo\nsignificantly surpasses current state-of-the-art (SOTA) knowledge editing\nmethods in the multi-hop question answering benchmark, MQuAKE, especially in\nscenarios with extensive knowledge edits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The important challenge of keeping knowledge in Large Language Models (LLMs)\nup-to-date has led to the development of various methods for incorporating new\nfacts. However, existing methods for such knowledge editing still face\ndifficulties with multi-hop questions that require accurate fact identification\nand sequential logical reasoning, particularly among numerous fact updates. To\ntackle these challenges, this paper introduces Graph Memory-based Editing for\nLarge Language Models (GMeLLo), a straightforward and effective method that\nmerges the explicit knowledge representation of Knowledge Graphs (KGs) with the\nlinguistic flexibility of LLMs. Beyond merely leveraging LLMs for question\nanswering, GMeLLo employs these models to convert free-form language into\nstructured queries and fact triples, facilitating seamless interaction with KGs\nfor rapid updates and precise multi-hop reasoning. Our results show that GMeLLo\nsignificantly surpasses current state-of-the-art (SOTA) knowledge editing\nmethods in the multi-hop question answering benchmark, MQuAKE, especially in\nscenarios with extensive knowledge edits."
                },
                "authors": [
                    {
                        "name": "Ruirui Chen"
                    },
                    {
                        "name": "Weifeng Jiang"
                    },
                    {
                        "name": "Chengwei Qin"
                    },
                    {
                        "name": "Ishaan Singh Rawal"
                    },
                    {
                        "name": "Cheston Tan"
                    },
                    {
                        "name": "Dongkyu Choi"
                    },
                    {
                        "name": "Bo Xiong"
                    },
                    {
                        "name": "Bo Ai"
                    }
                ],
                "author_detail": {
                    "name": "Bo Ai"
                },
                "author": "Bo Ai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15903v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15903v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03359v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03359v1",
                "updated": "2024-12-04T14:45:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    14,
                    45,
                    9,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T14:45:09Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    14,
                    45,
                    9,
                    2,
                    339,
                    0
                ],
                "title": "WiS Platform: Enhancing Evaluation of LLM-Based Multi-Agent Systems\n  Through Game-Based Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WiS Platform: Enhancing Evaluation of LLM-Based Multi-Agent Systems\n  Through Game-Based Analysis"
                },
                "summary": "Recent advancements in autonomous multi-agent systems (MAS) based on large\nlanguage models (LLMs) have enhanced the application scenarios and improved the\ncapability of LLMs to handle complex tasks. Despite demonstrating\neffectiveness, existing studies still evidently struggle to evaluate, analysis,\nand reproducibility of LLM-based MAS. In this paper, to facilitate the research\non LLM-based MAS, we introduce an open, scalable, and real-time updated\nplatform for accessing and analyzing the LLM-based MAS based on the games Who\nis Spy?\" (WiS). Our platform is featured with three main worths: (1) a unified\nmodel evaluate interface that supports models available on Hugging Face; (2)\nreal-time updated leaderboard for model evaluation; (3) a comprehensive\nevaluation covering game-winning rates, attacking, defense strategies, and\nreasoning of LLMs. To rigorously test WiS, we conduct extensive experiments\ncoverage of various open- and closed-source LLMs, we find that different agents\nexhibit distinct and intriguing behaviors in the game. The experimental results\ndemonstrate the effectiveness and efficiency of our platform in evaluating\nLLM-based MAS. Our platform and its documentation are publicly available at\n\\url{https://whoisspy.ai/}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in autonomous multi-agent systems (MAS) based on large\nlanguage models (LLMs) have enhanced the application scenarios and improved the\ncapability of LLMs to handle complex tasks. Despite demonstrating\neffectiveness, existing studies still evidently struggle to evaluate, analysis,\nand reproducibility of LLM-based MAS. In this paper, to facilitate the research\non LLM-based MAS, we introduce an open, scalable, and real-time updated\nplatform for accessing and analyzing the LLM-based MAS based on the games Who\nis Spy?\" (WiS). Our platform is featured with three main worths: (1) a unified\nmodel evaluate interface that supports models available on Hugging Face; (2)\nreal-time updated leaderboard for model evaluation; (3) a comprehensive\nevaluation covering game-winning rates, attacking, defense strategies, and\nreasoning of LLMs. To rigorously test WiS, we conduct extensive experiments\ncoverage of various open- and closed-source LLMs, we find that different agents\nexhibit distinct and intriguing behaviors in the game. The experimental results\ndemonstrate the effectiveness and efficiency of our platform in evaluating\nLLM-based MAS. Our platform and its documentation are publicly available at\n\\url{https://whoisspy.ai/}"
                },
                "authors": [
                    {
                        "name": "Chengwei Hu"
                    },
                    {
                        "name": "Jianhui Zheng"
                    },
                    {
                        "name": "Yancheng He"
                    },
                    {
                        "name": "Hangyu Guo"
                    },
                    {
                        "name": "Junguang Jiang"
                    },
                    {
                        "name": "Han Zhu"
                    },
                    {
                        "name": "Kai Sun"
                    },
                    {
                        "name": "Yuning Jiang"
                    },
                    {
                        "name": "Wenbo Su"
                    },
                    {
                        "name": "Bo Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zheng"
                },
                "author": "Bo Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03359v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03359v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03353v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03353v1",
                "updated": "2024-12-04T14:36:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    14,
                    36,
                    44,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T14:36:44Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    14,
                    36,
                    44,
                    2,
                    339,
                    0
                ],
                "title": "MOVE: Multi-skill Omnidirectional Legged Locomotion with Limited View in\n  3D Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MOVE: Multi-skill Omnidirectional Legged Locomotion with Limited View in\n  3D Environments"
                },
                "summary": "Legged robots possess inherent advantages in traversing complex 3D terrains.\nHowever, previous work on low-cost quadruped robots with egocentric vision\nsystems has been limited by a narrow front-facing view and exteroceptive noise,\nrestricting omnidirectional mobility in such environments. While building a\nvoxel map through a hierarchical structure can refine exteroception processing,\nit introduces significant computational overhead, noise, and delays. In this\npaper, we present MOVE, a one-stage end-to-end learning framework capable of\nmulti-skill omnidirectional legged locomotion with limited view in 3D\nenvironments, just like what a real animal can do. When movement aligns with\nthe robot's line of sight, exteroceptive perception enhances locomotion,\nenabling extreme climbing and leaping. When vision is obstructed or the\ndirection of movement lies outside the robot's field of view, the robot relies\non proprioception for tasks like crawling and climbing stairs. We integrate all\nthese skills into a single neural network by introducing a pseudo-siamese\nnetwork structure combining supervised and contrastive learning which helps the\nrobot infer its surroundings beyond its field of view. Experiments in both\nsimulations and real-world scenarios demonstrate the robustness of our method,\nbroadening the operational environments for robotics with egocentric vision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Legged robots possess inherent advantages in traversing complex 3D terrains.\nHowever, previous work on low-cost quadruped robots with egocentric vision\nsystems has been limited by a narrow front-facing view and exteroceptive noise,\nrestricting omnidirectional mobility in such environments. While building a\nvoxel map through a hierarchical structure can refine exteroception processing,\nit introduces significant computational overhead, noise, and delays. In this\npaper, we present MOVE, a one-stage end-to-end learning framework capable of\nmulti-skill omnidirectional legged locomotion with limited view in 3D\nenvironments, just like what a real animal can do. When movement aligns with\nthe robot's line of sight, exteroceptive perception enhances locomotion,\nenabling extreme climbing and leaping. When vision is obstructed or the\ndirection of movement lies outside the robot's field of view, the robot relies\non proprioception for tasks like crawling and climbing stairs. We integrate all\nthese skills into a single neural network by introducing a pseudo-siamese\nnetwork structure combining supervised and contrastive learning which helps the\nrobot infer its surroundings beyond its field of view. Experiments in both\nsimulations and real-world scenarios demonstrate the robustness of our method,\nbroadening the operational environments for robotics with egocentric vision."
                },
                "authors": [
                    {
                        "name": "Songbo Li"
                    },
                    {
                        "name": "Shixin Luo"
                    },
                    {
                        "name": "Jun Wu"
                    },
                    {
                        "name": "Qiuguo Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Qiuguo Zhu"
                },
                "author": "Qiuguo Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03353v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03353v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.16995v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.16995v2",
                "updated": "2024-12-04T14:33:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    14,
                    33,
                    44,
                    2,
                    339,
                    0
                ],
                "published": "2024-06-24T08:36:40Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    8,
                    36,
                    40,
                    0,
                    176,
                    0
                ],
                "title": "tcrLM: a lightweight protein language model for predicting T cell\n  receptor and epitope binding specificity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "tcrLM: a lightweight protein language model for predicting T cell\n  receptor and epitope binding specificity"
                },
                "summary": "The anti-cancer immune response relies on the bindings between T-cell\nreceptors (TCRs) and antigens, which elicits adaptive immunity to eliminate\ntumor cells. This ability of the immune system to respond to novel various\nneoantigens arises from the immense diversity of TCR repository. However, TCR\ndiversity poses a significant challenge on accurately predicting antigen-TCR\nbindings. In this study, we introduce a lightweight masked language model,\ntermed tcrLM, to address this challenge. Our approach involves randomly masking\nsegments of TCR sequences and training tcrLM to infer the masked segments,\nthereby enabling the extraction of expressive features from TCR sequences. To\nfurther enhance robustness, we incorporate virtual adversarial training into\ntcrLM. We construct the largest TCR CDR3 sequence set with more than 100\nmillion distinct sequences, and pretrain tcrLM on these sequences. The\npre-trained encoder is subsequently applied to predict TCR-antigen binding\nspecificity. We evaluate model performance on three test datasets: independent,\nexternal, and COVID-19 test set. The results demonstrate that tcrLM not only\nsurpasses existing TCR-antigen binding prediction methods, but also outperforms\nother mainstream protein language models. More interestingly, tcrLM effectively\ncaptures the biochemical properties and positional preference of amino acids\nwithin TCR sequences. Additionally, the predicted TCR-neoantigen binding scores\nindicates the immunotherapy responses and clinical outcomes in a melanoma\ncohort. These findings demonstrate the potential of tcrLM in predicting\nTCR-antigen binding specificity, with significant implications for advancing\nimmunotherapy and personalized medicine.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The anti-cancer immune response relies on the bindings between T-cell\nreceptors (TCRs) and antigens, which elicits adaptive immunity to eliminate\ntumor cells. This ability of the immune system to respond to novel various\nneoantigens arises from the immense diversity of TCR repository. However, TCR\ndiversity poses a significant challenge on accurately predicting antigen-TCR\nbindings. In this study, we introduce a lightweight masked language model,\ntermed tcrLM, to address this challenge. Our approach involves randomly masking\nsegments of TCR sequences and training tcrLM to infer the masked segments,\nthereby enabling the extraction of expressive features from TCR sequences. To\nfurther enhance robustness, we incorporate virtual adversarial training into\ntcrLM. We construct the largest TCR CDR3 sequence set with more than 100\nmillion distinct sequences, and pretrain tcrLM on these sequences. The\npre-trained encoder is subsequently applied to predict TCR-antigen binding\nspecificity. We evaluate model performance on three test datasets: independent,\nexternal, and COVID-19 test set. The results demonstrate that tcrLM not only\nsurpasses existing TCR-antigen binding prediction methods, but also outperforms\nother mainstream protein language models. More interestingly, tcrLM effectively\ncaptures the biochemical properties and positional preference of amino acids\nwithin TCR sequences. Additionally, the predicted TCR-neoantigen binding scores\nindicates the immunotherapy responses and clinical outcomes in a melanoma\ncohort. These findings demonstrate the potential of tcrLM in predicting\nTCR-antigen binding specificity, with significant implications for advancing\nimmunotherapy and personalized medicine."
                },
                "authors": [
                    {
                        "name": "Xing Fang"
                    },
                    {
                        "name": "Chenpeng Yu"
                    },
                    {
                        "name": "Shiye Tian"
                    },
                    {
                        "name": "Hui Liu"
                    }
                ],
                "author_detail": {
                    "name": "Hui Liu"
                },
                "author": "Hui Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.16995v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.16995v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.QM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.03644v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.03644v2",
                "updated": "2024-12-04T14:27:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    14,
                    27,
                    6,
                    2,
                    339,
                    0
                ],
                "published": "2024-05-06T17:07:28Z",
                "published_parsed": [
                    2024,
                    5,
                    6,
                    17,
                    7,
                    28,
                    0,
                    127,
                    0
                ],
                "title": "When LLMs Meet Cybersecurity: A Systematic Literature Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When LLMs Meet Cybersecurity: A Systematic Literature Review"
                },
                "summary": "The rapid development of large language models (LLMs) has opened new avenues\nacross various fields, including cybersecurity, which faces an evolving threat\nlandscape and demand for innovative technologies. Despite initial explorations\ninto the application of LLMs in cybersecurity, there is a lack of a\ncomprehensive overview of this research area. This paper addresses this gap by\nproviding a systematic literature review, covering the analysis of over 300\nworks, encompassing 25 LLMs and more than 10 downstream scenarios. Our\ncomprehensive overview addresses three key research questions: the construction\nof cybersecurity-oriented LLMs, the application of LLMs to various\ncybersecurity tasks, the challenges and further research in this area. This\nstudy aims to shed light on the extensive potential of LLMs in enhancing\ncybersecurity practices and serve as a valuable resource for applying LLMs in\nthis field. We also maintain and regularly update a list of practical guides on\nLLMs for cybersecurity at https://github.com/tmylla/Awesome-LLM4Cybersecurity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of large language models (LLMs) has opened new avenues\nacross various fields, including cybersecurity, which faces an evolving threat\nlandscape and demand for innovative technologies. Despite initial explorations\ninto the application of LLMs in cybersecurity, there is a lack of a\ncomprehensive overview of this research area. This paper addresses this gap by\nproviding a systematic literature review, covering the analysis of over 300\nworks, encompassing 25 LLMs and more than 10 downstream scenarios. Our\ncomprehensive overview addresses three key research questions: the construction\nof cybersecurity-oriented LLMs, the application of LLMs to various\ncybersecurity tasks, the challenges and further research in this area. This\nstudy aims to shed light on the extensive potential of LLMs in enhancing\ncybersecurity practices and serve as a valuable resource for applying LLMs in\nthis field. We also maintain and regularly update a list of practical guides on\nLLMs for cybersecurity at https://github.com/tmylla/Awesome-LLM4Cybersecurity."
                },
                "authors": [
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Haoyu Bu"
                    },
                    {
                        "name": "Hui Wen"
                    },
                    {
                        "name": "Yongji Liu"
                    },
                    {
                        "name": "Haiqiang Fei"
                    },
                    {
                        "name": "Rongrong Xi"
                    },
                    {
                        "name": "Lun Li"
                    },
                    {
                        "name": "Yun Yang"
                    },
                    {
                        "name": "Hongsong Zhu"
                    },
                    {
                        "name": "Dan Meng"
                    }
                ],
                "author_detail": {
                    "name": "Dan Meng"
                },
                "author": "Dan Meng",
                "arxiv_comment": "We have updated the related papers up to Aug 31st, with 50+ new\n  papers added",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.03644v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.03644v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03343v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03343v1",
                "updated": "2024-12-04T14:23:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    14,
                    23,
                    16,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T14:23:16Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    14,
                    23,
                    16,
                    2,
                    339,
                    0
                ],
                "title": "Improving Linguistic Diversity of Large Language Models with Possibility\n  Exploration Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Linguistic Diversity of Large Language Models with Possibility\n  Exploration Fine-Tuning"
                },
                "summary": "While Large Language Models (LLMs) have made significant strides in\nreplicating human-like abilities, there are concerns about a reduction in the\nlinguistic diversity of their outputs. This results in the homogenization of\nviewpoints and perspectives, as well as the underrepresentation of specific\ndemographic groups. Although several fine-tuning and prompting techniques have\nbeen suggested to tackle the issue, they are often tailored to specific tasks\nor come with a substantial increase in computational cost and latency. This\nmakes them challenging to apply to applications that demand very low latency,\nsuch as chatbots and virtual assistants. We propose Possibility Exploration\nFine-Tuning (PEFT), a task-agnostic framework that enhances the text diversity\nof LLMs without increasing latency or computational cost. Given the same\nprompt, models fine-tuned with PEFT can simultaneously generate multiple\ndiverse responses, each corresponding with a controllable possibility number.\nExperiments on dialogue and story generation tasks demonstrate that PEFT\nsignificantly enhances the diversity of LLM outputs, as evidenced by lower\nsimilarity between candidate responses. Since PEFT emphasizes semantic\ndiversity over lexical diversity, it can also notably reduce demographic bias\nin dialogue systems. The implementations and datasets are available in our\nrepository: https://github.com/mailong25/peft_diversity",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large Language Models (LLMs) have made significant strides in\nreplicating human-like abilities, there are concerns about a reduction in the\nlinguistic diversity of their outputs. This results in the homogenization of\nviewpoints and perspectives, as well as the underrepresentation of specific\ndemographic groups. Although several fine-tuning and prompting techniques have\nbeen suggested to tackle the issue, they are often tailored to specific tasks\nor come with a substantial increase in computational cost and latency. This\nmakes them challenging to apply to applications that demand very low latency,\nsuch as chatbots and virtual assistants. We propose Possibility Exploration\nFine-Tuning (PEFT), a task-agnostic framework that enhances the text diversity\nof LLMs without increasing latency or computational cost. Given the same\nprompt, models fine-tuned with PEFT can simultaneously generate multiple\ndiverse responses, each corresponding with a controllable possibility number.\nExperiments on dialogue and story generation tasks demonstrate that PEFT\nsignificantly enhances the diversity of LLM outputs, as evidenced by lower\nsimilarity between candidate responses. Since PEFT emphasizes semantic\ndiversity over lexical diversity, it can also notably reduce demographic bias\nin dialogue systems. The implementations and datasets are available in our\nrepository: https://github.com/mailong25/peft_diversity"
                },
                "authors": [
                    {
                        "name": "Long Mai"
                    },
                    {
                        "name": "Julie Carson-Berndsen"
                    }
                ],
                "author_detail": {
                    "name": "Julie Carson-Berndsen"
                },
                "author": "Julie Carson-Berndsen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03343v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03343v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01951v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01951v2",
                "updated": "2024-12-04T14:20:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    14,
                    20,
                    21,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-02T20:24:17Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    20,
                    24,
                    17,
                    0,
                    337,
                    0
                ],
                "title": "Self-Improvement in Language Models: The Sharpening Mechanism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Improvement in Language Models: The Sharpening Mechanism"
                },
                "summary": "Recent work in language modeling has raised the possibility of\nself-improvement, where a language models evaluates and refines its own\ngenerations to achieve higher performance without external feedback. It is\nimpossible for this self-improvement to create information that is not already\nin the model, so why should we expect that this will lead to improved\ncapabilities? We offer a new perspective on the capabilities of\nself-improvement through a lens we refer to as sharpening. Motivated by the\nobservation that language models are often better at verifying response quality\nthan they are at generating correct responses, we formalize self-improvement as\nusing the model itself as a verifier during post-training in order to\n``sharpen'' the model to one placing large mass on high-quality sequences,\nthereby amortizing the expensive inference-time computation of generating good\nsequences. We begin by introducing a new statistical framework for sharpening\nin which the learner aims to sharpen a pre-trained base policy via sample\naccess, and establish fundamental limits. Then we analyze two natural families\nof self-improvement algorithms based on SFT and RLHF. We find that (i) the\nSFT-based approach is minimax optimal whenever the initial model has sufficient\ncoverage, but (ii) the RLHF-based approach can improve over SFT-based\nself-improvement by leveraging online exploration, bypassing the need for\ncoverage. Finally, we empirically validate the sharpening mechanism via\ninference-time and amortization experiments. We view these findings as a\nstarting point toward a foundational understanding that can guide the design\nand evaluation of self-improvement algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work in language modeling has raised the possibility of\nself-improvement, where a language models evaluates and refines its own\ngenerations to achieve higher performance without external feedback. It is\nimpossible for this self-improvement to create information that is not already\nin the model, so why should we expect that this will lead to improved\ncapabilities? We offer a new perspective on the capabilities of\nself-improvement through a lens we refer to as sharpening. Motivated by the\nobservation that language models are often better at verifying response quality\nthan they are at generating correct responses, we formalize self-improvement as\nusing the model itself as a verifier during post-training in order to\n``sharpen'' the model to one placing large mass on high-quality sequences,\nthereby amortizing the expensive inference-time computation of generating good\nsequences. We begin by introducing a new statistical framework for sharpening\nin which the learner aims to sharpen a pre-trained base policy via sample\naccess, and establish fundamental limits. Then we analyze two natural families\nof self-improvement algorithms based on SFT and RLHF. We find that (i) the\nSFT-based approach is minimax optimal whenever the initial model has sufficient\ncoverage, but (ii) the RLHF-based approach can improve over SFT-based\nself-improvement by leveraging online exploration, bypassing the need for\ncoverage. Finally, we empirically validate the sharpening mechanism via\ninference-time and amortization experiments. We view these findings as a\nstarting point toward a foundational understanding that can guide the design\nand evaluation of self-improvement algorithms."
                },
                "authors": [
                    {
                        "name": "Audrey Huang"
                    },
                    {
                        "name": "Adam Block"
                    },
                    {
                        "name": "Dylan J. Foster"
                    },
                    {
                        "name": "Dhruv Rohatgi"
                    },
                    {
                        "name": "Cyril Zhang"
                    },
                    {
                        "name": "Max Simchowitz"
                    },
                    {
                        "name": "Jordan T. Ash"
                    },
                    {
                        "name": "Akshay Krishnamurthy"
                    }
                ],
                "author_detail": {
                    "name": "Akshay Krishnamurthy"
                },
                "author": "Akshay Krishnamurthy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01951v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01951v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03338v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03338v1",
                "updated": "2024-12-04T14:13:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    14,
                    13,
                    38,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T14:13:38Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    14,
                    13,
                    38,
                    2,
                    339,
                    0
                ],
                "title": "AI-Driven Day-to-Day Route Choice",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-Driven Day-to-Day Route Choice"
                },
                "summary": "Understanding travelers' route choices can help policymakers devise optimal\noperational and planning strategies for both normal and abnormal circumstances.\nHowever, existing choice modeling methods often rely on predefined assumptions\nand struggle to capture the dynamic and adaptive nature of travel behavior.\nRecently, Large Language Models (LLMs) have emerged as a promising alternative,\ndemonstrating remarkable ability to replicate human-like behaviors across\nvarious fields. Despite this potential, their capacity to accurately simulate\nhuman route choice behavior in transportation contexts remains doubtful. To\nsatisfy this curiosity, this paper investigates the potential of LLMs for route\nchoice modeling by introducing an LLM-empowered agent, \"LLMTraveler.\" This\nagent integrates an LLM as its core, equipped with a memory system that learns\nfrom past experiences and makes decisions by balancing retrieved data and\npersonality traits. The study systematically evaluates the LLMTraveler's\nability to replicate human-like decision-making through two stages: (1)\nanalyzing its route-switching behavior in single origin-destination (OD) pair\ncongestion game scenarios, where it demonstrates patterns align with laboratory\ndata but are not fully explained by traditional models, and (2) testing its\ncapacity to model day-to-day (DTD) adaptive learning behaviors on the Ortuzar\nand Willumsen (OW) network, producing results comparable to Multinomial Logit\n(MNL) and Reinforcement Learning (RL) models. These experiments demonstrate\nthat the framework can partially replicate human-like decision-making in route\nchoice while providing natural language explanations for its decisions. This\ncapability offers valuable insights for transportation policymaking, such as\nsimulating traveler responses to new policies or changes in the network.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding travelers' route choices can help policymakers devise optimal\noperational and planning strategies for both normal and abnormal circumstances.\nHowever, existing choice modeling methods often rely on predefined assumptions\nand struggle to capture the dynamic and adaptive nature of travel behavior.\nRecently, Large Language Models (LLMs) have emerged as a promising alternative,\ndemonstrating remarkable ability to replicate human-like behaviors across\nvarious fields. Despite this potential, their capacity to accurately simulate\nhuman route choice behavior in transportation contexts remains doubtful. To\nsatisfy this curiosity, this paper investigates the potential of LLMs for route\nchoice modeling by introducing an LLM-empowered agent, \"LLMTraveler.\" This\nagent integrates an LLM as its core, equipped with a memory system that learns\nfrom past experiences and makes decisions by balancing retrieved data and\npersonality traits. The study systematically evaluates the LLMTraveler's\nability to replicate human-like decision-making through two stages: (1)\nanalyzing its route-switching behavior in single origin-destination (OD) pair\ncongestion game scenarios, where it demonstrates patterns align with laboratory\ndata but are not fully explained by traditional models, and (2) testing its\ncapacity to model day-to-day (DTD) adaptive learning behaviors on the Ortuzar\nand Willumsen (OW) network, producing results comparable to Multinomial Logit\n(MNL) and Reinforcement Learning (RL) models. These experiments demonstrate\nthat the framework can partially replicate human-like decision-making in route\nchoice while providing natural language explanations for its decisions. This\ncapability offers valuable insights for transportation policymaking, such as\nsimulating traveler responses to new policies or changes in the network."
                },
                "authors": [
                    {
                        "name": "Leizhen Wang"
                    },
                    {
                        "name": "Peibo Duan"
                    },
                    {
                        "name": "Zhengbing He"
                    },
                    {
                        "name": "Cheng Lyu"
                    },
                    {
                        "name": "Xin Chen"
                    },
                    {
                        "name": "Nan Zheng"
                    },
                    {
                        "name": "Li Yao"
                    },
                    {
                        "name": "Zhenliang Ma"
                    }
                ],
                "author_detail": {
                    "name": "Zhenliang Ma"
                },
                "author": "Zhenliang Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03338v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03338v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03324v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03324v1",
                "updated": "2024-12-04T13:56:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    13,
                    56,
                    44,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T13:56:44Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    13,
                    56,
                    44,
                    2,
                    339,
                    0
                ],
                "title": "A Stitch in Time Saves Nine: Small VLM is a Precise Guidance for\n  accelerating Large VLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Stitch in Time Saves Nine: Small VLM is a Precise Guidance for\n  accelerating Large VLMs"
                },
                "summary": "Vision-language models (VLMs) have shown remarkable success across various\nmulti-modal tasks, yet large VLMs encounter significant efficiency challenges\ndue to processing numerous visual tokens. A promising approach to accelerating\nlarge VLM inference is using partial information, such as attention maps from\nspecific layers, to assess token importance and prune less essential tokens.\nHowever, our study reveals three key insights: (i) Partial attention\ninformation is insufficient for accurately identifying critical visual tokens,\nresulting in suboptimal performance, especially at low token retention ratios;\n(ii) Global attention information, such as the attention map aggregated across\nall layers, more effectively preserves essential tokens and maintains\ncomparable performance under aggressive pruning. However, the attention maps\nfrom all layers requires a full inference pass, which increases computational\nload and is therefore impractical in existing methods; and (iii) The global\nattention map aggregated from a small VLM closely resembles that of a large\nVLM, suggesting an efficient alternative. Based on these findings, we introduce\na \\textbf{training-free} method, \\underline{\\textbf{S}}mall VLM\n\\underline{\\textbf{G}}uidance for accelerating \\underline{\\textbf{L}}arge VLMs\n(\\textbf{SGL}). Specifically, we employ the attention map aggregated from a\nsmall VLM to guide visual token pruning in a large VLM. Additionally, an early\nexiting mechanism is developed to fully use the small VLM's predictions,\ndynamically invoking the larger VLM only when necessary, yielding a superior\ntrade-off between accuracy and computation. Extensive evaluations across 11\nbenchmarks demonstrate the effectiveness and generalizability of SGL, achieving\nup to 91\\% pruning ratio for visual tokens while retaining competitive\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) have shown remarkable success across various\nmulti-modal tasks, yet large VLMs encounter significant efficiency challenges\ndue to processing numerous visual tokens. A promising approach to accelerating\nlarge VLM inference is using partial information, such as attention maps from\nspecific layers, to assess token importance and prune less essential tokens.\nHowever, our study reveals three key insights: (i) Partial attention\ninformation is insufficient for accurately identifying critical visual tokens,\nresulting in suboptimal performance, especially at low token retention ratios;\n(ii) Global attention information, such as the attention map aggregated across\nall layers, more effectively preserves essential tokens and maintains\ncomparable performance under aggressive pruning. However, the attention maps\nfrom all layers requires a full inference pass, which increases computational\nload and is therefore impractical in existing methods; and (iii) The global\nattention map aggregated from a small VLM closely resembles that of a large\nVLM, suggesting an efficient alternative. Based on these findings, we introduce\na \\textbf{training-free} method, \\underline{\\textbf{S}}mall VLM\n\\underline{\\textbf{G}}uidance for accelerating \\underline{\\textbf{L}}arge VLMs\n(\\textbf{SGL}). Specifically, we employ the attention map aggregated from a\nsmall VLM to guide visual token pruning in a large VLM. Additionally, an early\nexiting mechanism is developed to fully use the small VLM's predictions,\ndynamically invoking the larger VLM only when necessary, yielding a superior\ntrade-off between accuracy and computation. Extensive evaluations across 11\nbenchmarks demonstrate the effectiveness and generalizability of SGL, achieving\nup to 91\\% pruning ratio for visual tokens while retaining competitive\nperformance."
                },
                "authors": [
                    {
                        "name": "Wangbo Zhao"
                    },
                    {
                        "name": "Yizeng Han"
                    },
                    {
                        "name": "Jiasheng Tang"
                    },
                    {
                        "name": "Zhikai Li"
                    },
                    {
                        "name": "Yibing Song"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Zhangyang Wang"
                    },
                    {
                        "name": "Yang You"
                    }
                ],
                "author_detail": {
                    "name": "Yang You"
                },
                "author": "Yang You",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03324v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03324v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03321v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03321v1",
                "updated": "2024-12-04T13:55:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    13,
                    55,
                    14,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T13:55:14Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    13,
                    55,
                    14,
                    2,
                    339,
                    0
                ],
                "title": "Scalable Bayesian Tensor Ring Factorization for Multiway Data Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable Bayesian Tensor Ring Factorization for Multiway Data Analysis"
                },
                "summary": "Tensor decompositions play a crucial role in numerous applications related to\nmulti-way data analysis. By employing a Bayesian framework with\nsparsity-inducing priors, Bayesian Tensor Ring (BTR) factorization offers\nprobabilistic estimates and an effective approach for automatically adapting\nthe tensor ring rank during the learning process. However, previous BTR method\nemploys an Automatic Relevance Determination (ARD) prior, which can lead to\nsub-optimal solutions. Besides, it solely focuses on continuous data, whereas\nmany applications involve discrete data. More importantly, it relies on the\nCoordinate-Ascent Variational Inference (CAVI) algorithm, which is inadequate\nfor handling large tensors with extensive observations. These limitations\ngreatly limit its application scales and scopes, making it suitable only for\nsmall-scale problems, such as image/video completion. To address these issues,\nwe propose a novel BTR model that incorporates a nonparametric Multiplicative\nGamma Process (MGP) prior, known for its superior accuracy in identifying\nlatent structures. To handle discrete data, we introduce the P\\'olya-Gamma\naugmentation for closed-form updates. Furthermore, we develop an efficient\nGibbs sampler for consistent posterior simulation, which reduces the\ncomputational complexity of previous VI algorithm by two orders, and an online\nEM algorithm that is scalable to extremely large tensors. To showcase the\nadvantages of our model, we conduct extensive experiments on both simulation\ndata and real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tensor decompositions play a crucial role in numerous applications related to\nmulti-way data analysis. By employing a Bayesian framework with\nsparsity-inducing priors, Bayesian Tensor Ring (BTR) factorization offers\nprobabilistic estimates and an effective approach for automatically adapting\nthe tensor ring rank during the learning process. However, previous BTR method\nemploys an Automatic Relevance Determination (ARD) prior, which can lead to\nsub-optimal solutions. Besides, it solely focuses on continuous data, whereas\nmany applications involve discrete data. More importantly, it relies on the\nCoordinate-Ascent Variational Inference (CAVI) algorithm, which is inadequate\nfor handling large tensors with extensive observations. These limitations\ngreatly limit its application scales and scopes, making it suitable only for\nsmall-scale problems, such as image/video completion. To address these issues,\nwe propose a novel BTR model that incorporates a nonparametric Multiplicative\nGamma Process (MGP) prior, known for its superior accuracy in identifying\nlatent structures. To handle discrete data, we introduce the P\\'olya-Gamma\naugmentation for closed-form updates. Furthermore, we develop an efficient\nGibbs sampler for consistent posterior simulation, which reduces the\ncomputational complexity of previous VI algorithm by two orders, and an online\nEM algorithm that is scalable to extremely large tensors. To showcase the\nadvantages of our model, we conduct extensive experiments on both simulation\ndata and real-world applications."
                },
                "authors": [
                    {
                        "name": "Zerui Tao"
                    },
                    {
                        "name": "Toshihisa Tanaka"
                    },
                    {
                        "name": "Qibin Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Qibin Zhao"
                },
                "author": "Qibin Zhao",
                "arxiv_comment": "ICONIP 2023",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03321v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03321v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03312v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03312v1",
                "updated": "2024-12-04T13:44:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    13,
                    44,
                    56,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T13:44:56Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    13,
                    44,
                    56,
                    2,
                    339,
                    0
                ],
                "title": "Path-Guided Particle-based Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Path-Guided Particle-based Sampling"
                },
                "summary": "Particle-based Bayesian inference methods by sampling from a partition-free\ntarget (posterior) distribution, e.g., Stein variational gradient descent\n(SVGD), have attracted significant attention. We propose a path-guided\nparticle-based sampling~(PGPS) method based on a novel Log-weighted Shrinkage\n(LwS) density path linking an initial distribution to the target distribution.\nWe propose to utilize a Neural network to learn a vector field motivated by the\nFokker-Planck equation of the designed density path. Particles, initiated from\nthe initial distribution, evolve according to the ordinary differential\nequation defined by the vector field. The distribution of these particles is\nguided along a density path from the initial distribution to the target\ndistribution. The proposed LwS density path allows for an efficient search of\nmodes of the target distribution while canonical methods fail. We theoretically\nanalyze the Wasserstein distance of the distribution of the PGPS-generated\nsamples and the target distribution due to approximation and discretization\nerrors. Practically, the proposed PGPS-LwS method demonstrates higher Bayesian\ninference accuracy and better calibration ability in experiments conducted on\nboth synthetic and real-world Bayesian learning tasks, compared to baselines,\nsuch as SVGD and Langevin dynamics, etc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Particle-based Bayesian inference methods by sampling from a partition-free\ntarget (posterior) distribution, e.g., Stein variational gradient descent\n(SVGD), have attracted significant attention. We propose a path-guided\nparticle-based sampling~(PGPS) method based on a novel Log-weighted Shrinkage\n(LwS) density path linking an initial distribution to the target distribution.\nWe propose to utilize a Neural network to learn a vector field motivated by the\nFokker-Planck equation of the designed density path. Particles, initiated from\nthe initial distribution, evolve according to the ordinary differential\nequation defined by the vector field. The distribution of these particles is\nguided along a density path from the initial distribution to the target\ndistribution. The proposed LwS density path allows for an efficient search of\nmodes of the target distribution while canonical methods fail. We theoretically\nanalyze the Wasserstein distance of the distribution of the PGPS-generated\nsamples and the target distribution due to approximation and discretization\nerrors. Practically, the proposed PGPS-LwS method demonstrates higher Bayesian\ninference accuracy and better calibration ability in experiments conducted on\nboth synthetic and real-world Bayesian learning tasks, compared to baselines,\nsuch as SVGD and Langevin dynamics, etc."
                },
                "authors": [
                    {
                        "name": "Mingzhou Fan"
                    },
                    {
                        "name": "Ruida Zhou"
                    },
                    {
                        "name": "Chao Tian"
                    },
                    {
                        "name": "Xiaoning Qian"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoning Qian"
                },
                "author": "Xiaoning Qian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03312v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03312v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14845v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14845v2",
                "updated": "2024-12-04T13:43:28Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    13,
                    43,
                    28,
                    2,
                    339,
                    0
                ],
                "published": "2024-08-27T07:56:35Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    7,
                    56,
                    35,
                    1,
                    240,
                    0
                ],
                "title": "AAVENUE: Detecting LLM Biases on NLU Tasks in AAVE via a Novel Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AAVENUE: Detecting LLM Biases on NLU Tasks in AAVE via a Novel Benchmark"
                },
                "summary": "Detecting biases in natural language understanding (NLU) for African American\nVernacular English (AAVE) is crucial to developing inclusive natural language\nprocessing (NLP) systems. To address dialect-induced performance discrepancies,\nwe introduce AAVENUE ({AAVE} {N}atural Language {U}nderstanding {E}valuation),\na benchmark for evaluating large language model (LLM) performance on NLU tasks\nin AAVE and Standard American English (SAE). AAVENUE builds upon and extends\nexisting benchmarks like VALUE, replacing deterministic syntactic and\nmorphological transformations with a more flexible methodology leveraging\nLLM-based translation with few-shot prompting, improving performance across our\nevaluation metrics when translating key tasks from the GLUE and SuperGLUE\nbenchmarks. We compare AAVENUE and VALUE translations using five popular LLMs\nand a comprehensive set of metrics including fluency, BARTScore, quality,\ncoherence, and understandability. Additionally, we recruit fluent AAVE speakers\nto validate our translations for authenticity. Our evaluations reveal that LLMs\nconsistently perform better on SAE tasks than AAVE-translated versions,\nunderscoring inherent biases and highlighting the need for more inclusive NLP\nmodels. We have open-sourced our source code on GitHub and created a website to\nshowcase our work at https://aavenue.live.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting biases in natural language understanding (NLU) for African American\nVernacular English (AAVE) is crucial to developing inclusive natural language\nprocessing (NLP) systems. To address dialect-induced performance discrepancies,\nwe introduce AAVENUE ({AAVE} {N}atural Language {U}nderstanding {E}valuation),\na benchmark for evaluating large language model (LLM) performance on NLU tasks\nin AAVE and Standard American English (SAE). AAVENUE builds upon and extends\nexisting benchmarks like VALUE, replacing deterministic syntactic and\nmorphological transformations with a more flexible methodology leveraging\nLLM-based translation with few-shot prompting, improving performance across our\nevaluation metrics when translating key tasks from the GLUE and SuperGLUE\nbenchmarks. We compare AAVENUE and VALUE translations using five popular LLMs\nand a comprehensive set of metrics including fluency, BARTScore, quality,\ncoherence, and understandability. Additionally, we recruit fluent AAVE speakers\nto validate our translations for authenticity. Our evaluations reveal that LLMs\nconsistently perform better on SAE tasks than AAVE-translated versions,\nunderscoring inherent biases and highlighting the need for more inclusive NLP\nmodels. We have open-sourced our source code on GitHub and created a website to\nshowcase our work at https://aavenue.live."
                },
                "authors": [
                    {
                        "name": "Abhay Gupta"
                    },
                    {
                        "name": "Philip Meng"
                    },
                    {
                        "name": "Ece Yurtseven"
                    },
                    {
                        "name": "Sean O'Brien"
                    },
                    {
                        "name": "Kevin Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Zhu"
                },
                "author": "Kevin Zhu",
                "arxiv_comment": "Published at NLP4PI @ EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14845v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14845v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17686v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17686v2",
                "updated": "2024-12-04T13:39:01Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    13,
                    39,
                    1,
                    2,
                    339,
                    0
                ],
                "published": "2024-11-26T18:53:51Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    53,
                    51,
                    1,
                    331,
                    0
                ],
                "title": "Rethinking Token Reduction in MLLMs: Towards a Unified Paradigm for\n  Training-Free Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Token Reduction in MLLMs: Towards a Unified Paradigm for\n  Training-Free Acceleration"
                },
                "summary": "To accelerate the inference of heavy Multimodal Large Language Models\n(MLLMs), this study rethinks the current landscape of training-free token\nreduction research. We regret to find that the critical components of existing\nmethods are tightly intertwined, with their interconnections and effects\nremaining unclear for comparison, transfer, and expansion. Therefore, we\npropose a unified ''filter-correlate-compress'' paradigm that decomposes the\ntoken reduction into three distinct stages within a pipeline, maintaining\nconsistent design objectives and elements while allowing for unique\nimplementations. We additionally demystify the popular works and subsume them\ninto our paradigm to showcase its universality. Finally, we offer a suite of\nmethods grounded in the paradigm, striking a balance between speed and accuracy\nthroughout different phases of the inference. Experimental results across 10\nbenchmarks indicate that our methods can achieve up to an 82.4% reduction in\nFLOPs with a minimal impact on performance, simultaneously surpassing\nstate-of-the-art training-free methods. Our project page is at\nhttps://ficoco-accelerate.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To accelerate the inference of heavy Multimodal Large Language Models\n(MLLMs), this study rethinks the current landscape of training-free token\nreduction research. We regret to find that the critical components of existing\nmethods are tightly intertwined, with their interconnections and effects\nremaining unclear for comparison, transfer, and expansion. Therefore, we\npropose a unified ''filter-correlate-compress'' paradigm that decomposes the\ntoken reduction into three distinct stages within a pipeline, maintaining\nconsistent design objectives and elements while allowing for unique\nimplementations. We additionally demystify the popular works and subsume them\ninto our paradigm to showcase its universality. Finally, we offer a suite of\nmethods grounded in the paradigm, striking a balance between speed and accuracy\nthroughout different phases of the inference. Experimental results across 10\nbenchmarks indicate that our methods can achieve up to an 82.4% reduction in\nFLOPs with a minimal impact on performance, simultaneously surpassing\nstate-of-the-art training-free methods. Our project page is at\nhttps://ficoco-accelerate.github.io/."
                },
                "authors": [
                    {
                        "name": "Yuhang Han"
                    },
                    {
                        "name": "Xuyang Liu"
                    },
                    {
                        "name": "Pengxiang Ding"
                    },
                    {
                        "name": "Donglin Wang"
                    },
                    {
                        "name": "Honggang Chen"
                    },
                    {
                        "name": "Qingsen Yan"
                    },
                    {
                        "name": "Siteng Huang"
                    }
                ],
                "author_detail": {
                    "name": "Siteng Huang"
                },
                "author": "Siteng Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17686v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17686v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12403v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12403v2",
                "updated": "2024-12-04T13:32:05Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    13,
                    32,
                    5,
                    2,
                    339,
                    0
                ],
                "published": "2024-08-22T13:49:22Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    13,
                    49,
                    22,
                    3,
                    235,
                    0
                ],
                "title": "Quantifying the $S_8$ tension and evidence for interacting dark energy\n  from redshift-space distortion measurements",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantifying the $S_8$ tension and evidence for interacting dark energy\n  from redshift-space distortion measurements"
                },
                "summary": "In recent years, Cosmic Microwave Background (CMB) observations, Weak Lensing\nsurveys, and $f\\sigma_8(z)$ measurements from Redshift-Space Distortions (RSD)\nhave revealed a significant ($\\sim$3$-$5$\\sigma$) discrepancy in the inferred\nvalue of the matter clustering parameter $S_8$. In this work, we investigate\nthe implications of RSD for a cosmological framework postulating an interaction\nbetween Dark Energy (DE) and Dark Matter (DM). We explore scenarios where DM\ncan transfer energy-momentum to DE or vice versa. The energy-momentum flow is\ncharacterized by the strength and the sign of the coupling parameter $\\xi$. Our\nbaseline analysis combines RSD measurements with the latest data from Baryon\nAcoustic Oscillations (BAO) observed by DESI, Type Ia Supernovae from the\nPantheonPlus sample, and CMB data from Planck. We demonstrate that RSD\nmeasurements provide significant additional information imposing new and strong\nupper bounds on possible interaction in the dark sector. Models with $\\xi > 0$\ncan effectively alleviate the tension in $S_8$, presenting them as compelling\nalternatives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Cosmic Microwave Background (CMB) observations, Weak Lensing\nsurveys, and $f\\sigma_8(z)$ measurements from Redshift-Space Distortions (RSD)\nhave revealed a significant ($\\sim$3$-$5$\\sigma$) discrepancy in the inferred\nvalue of the matter clustering parameter $S_8$. In this work, we investigate\nthe implications of RSD for a cosmological framework postulating an interaction\nbetween Dark Energy (DE) and Dark Matter (DM). We explore scenarios where DM\ncan transfer energy-momentum to DE or vice versa. The energy-momentum flow is\ncharacterized by the strength and the sign of the coupling parameter $\\xi$. Our\nbaseline analysis combines RSD measurements with the latest data from Baryon\nAcoustic Oscillations (BAO) observed by DESI, Type Ia Supernovae from the\nPantheonPlus sample, and CMB data from Planck. We demonstrate that RSD\nmeasurements provide significant additional information imposing new and strong\nupper bounds on possible interaction in the dark sector. Models with $\\xi > 0$\ncan effectively alleviate the tension in $S_8$, presenting them as compelling\nalternatives."
                },
                "authors": [
                    {
                        "name": "Miguel A. Sabogal"
                    },
                    {
                        "name": "Emanuelly Silva"
                    },
                    {
                        "name": "Rafael C. Nunes"
                    },
                    {
                        "name": "Suresh Kumar"
                    },
                    {
                        "name": "Eleonora Di Valentino"
                    },
                    {
                        "name": "William Giar"
                    }
                ],
                "author_detail": {
                    "name": "William Giar"
                },
                "author": "William Giar",
                "arxiv_doi": "10.1103/PhysRevD.110.123508",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevD.110.123508",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.12403v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12403v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "13 pages, 6 figures. Comments welcome and appreciated",
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03295v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03295v1",
                "updated": "2024-12-04T13:14:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    13,
                    14,
                    35,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T13:14:35Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    13,
                    14,
                    35,
                    2,
                    339,
                    0
                ],
                "title": "Digital twin inference from multi-physical simulation data of DED\n  additive manufacturing processes with neural ODEs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital twin inference from multi-physical simulation data of DED\n  additive manufacturing processes with neural ODEs"
                },
                "summary": "A digital twin is a virtual representation that accurately replicates its\nphysical counterpart, fostering bi-directional real-time data exchange\nthroughout the entire process lifecycle. For Laser Directed Energy Deposition\nof Wire (DED-LB/w) additive manufacturing processes, digital twins may help to\ncontrol the residual stress design in build parts. This study focuses on\nproviding faster-than-real-time and highly accurate surrogate models for the\nformation of residual stresses by employing neural ordinary differential\nequations. The approach enables accurate prediction of temperatures and altered\nstructural properties like stress tensor components. The developed surrogates\ncan ultimately facilitate on-the-fly re-optimization of the ongoing\nmanufacturing process to achieve desired structural outcomes. Consequently,\nthis building block contributes significantly to realizing digital twins and\nthe first-time-right paradigm in additive manufacturing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A digital twin is a virtual representation that accurately replicates its\nphysical counterpart, fostering bi-directional real-time data exchange\nthroughout the entire process lifecycle. For Laser Directed Energy Deposition\nof Wire (DED-LB/w) additive manufacturing processes, digital twins may help to\ncontrol the residual stress design in build parts. This study focuses on\nproviding faster-than-real-time and highly accurate surrogate models for the\nformation of residual stresses by employing neural ordinary differential\nequations. The approach enables accurate prediction of temperatures and altered\nstructural properties like stress tensor components. The developed surrogates\ncan ultimately facilitate on-the-fly re-optimization of the ongoing\nmanufacturing process to achieve desired structural outcomes. Consequently,\nthis building block contributes significantly to realizing digital twins and\nthe first-time-right paradigm in additive manufacturing."
                },
                "authors": [
                    {
                        "name": "Maximilian Kannapinn"
                    },
                    {
                        "name": "Fabian Roth"
                    },
                    {
                        "name": "Oliver Weeger"
                    }
                ],
                "author_detail": {
                    "name": "Oliver Weeger"
                },
                "author": "Oliver Weeger",
                "arxiv_comment": "Presented at the ICCE 2024, Darmstadt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03295v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03295v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03293v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03293v1",
                "updated": "2024-12-04T13:11:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    13,
                    11,
                    38,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T13:11:38Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    13,
                    11,
                    38,
                    2,
                    339,
                    0
                ],
                "title": "Diffusion-VLA: Scaling Robot Foundation Models via Unified Diffusion and\n  Autoregression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-VLA: Scaling Robot Foundation Models via Unified Diffusion and\n  Autoregression"
                },
                "summary": "In this paper, we present DiffusionVLA, a novel framework that seamlessly\ncombines the autoregression model with the diffusion model for learning\nvisuomotor policy. Central to our approach is a next-token prediction\nobjective, enabling the model to reason effectively over the user's query in\nthe context of current observations. Subsequently, a diffusion model is\nattached to generate robust action outputs. To enhance policy learning through\nself-reasoning, we introduce a novel reasoning injection module that integrates\nreasoning phrases directly into the policy learning process. The whole\nframework is simple and flexible, making it easy to deploy and upgrade. We\nconduct extensive experiments using multiple real robots to validate the\neffectiveness of DiffusionVLA. Our tests include a challenging factory sorting\ntask, where DiffusionVLA successfully categorizes objects, including those not\nseen during training. We observe that the reasoning module makes the model\ninterpretable. It allows observers to understand the model thought process and\nidentify potential causes of policy failures. Additionally, we test\nDiffusionVLA on a zero-shot bin-picking task, achieving 63.7\\% accuracy on 102\npreviously unseen objects. Our method demonstrates robustness to visual\nchanges, such as distractors and new backgrounds, and easily adapts to new\nembodiments. Furthermore, DiffusionVLA can follow novel instructions and retain\nconversational ability. Notably, DiffusionVLA is data-efficient and fast at\ninference; our smallest DiffusionVLA-2B runs 82Hz on a single A6000 GPU and can\ntrain from scratch on less than 50 demonstrations for a complex task. Finally,\nwe scale the model from 2B to 72B parameters, showcasing improved\ngeneralization capabilities with increased model size.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present DiffusionVLA, a novel framework that seamlessly\ncombines the autoregression model with the diffusion model for learning\nvisuomotor policy. Central to our approach is a next-token prediction\nobjective, enabling the model to reason effectively over the user's query in\nthe context of current observations. Subsequently, a diffusion model is\nattached to generate robust action outputs. To enhance policy learning through\nself-reasoning, we introduce a novel reasoning injection module that integrates\nreasoning phrases directly into the policy learning process. The whole\nframework is simple and flexible, making it easy to deploy and upgrade. We\nconduct extensive experiments using multiple real robots to validate the\neffectiveness of DiffusionVLA. Our tests include a challenging factory sorting\ntask, where DiffusionVLA successfully categorizes objects, including those not\nseen during training. We observe that the reasoning module makes the model\ninterpretable. It allows observers to understand the model thought process and\nidentify potential causes of policy failures. Additionally, we test\nDiffusionVLA on a zero-shot bin-picking task, achieving 63.7\\% accuracy on 102\npreviously unseen objects. Our method demonstrates robustness to visual\nchanges, such as distractors and new backgrounds, and easily adapts to new\nembodiments. Furthermore, DiffusionVLA can follow novel instructions and retain\nconversational ability. Notably, DiffusionVLA is data-efficient and fast at\ninference; our smallest DiffusionVLA-2B runs 82Hz on a single A6000 GPU and can\ntrain from scratch on less than 50 demonstrations for a complex task. Finally,\nwe scale the model from 2B to 72B parameters, showcasing improved\ngeneralization capabilities with increased model size."
                },
                "authors": [
                    {
                        "name": "Junjie Wen"
                    },
                    {
                        "name": "Minjie Zhu"
                    },
                    {
                        "name": "Yichen Zhu"
                    },
                    {
                        "name": "Zhibin Tang"
                    },
                    {
                        "name": "Jinming Li"
                    },
                    {
                        "name": "Zhongyi Zhou"
                    },
                    {
                        "name": "Chengmeng Li"
                    },
                    {
                        "name": "Xiaoyu Liu"
                    },
                    {
                        "name": "Yaxin Peng"
                    },
                    {
                        "name": "Chaomin Shen"
                    },
                    {
                        "name": "Feifei Feng"
                    }
                ],
                "author_detail": {
                    "name": "Feifei Feng"
                },
                "author": "Feifei Feng",
                "arxiv_comment": "The project page is available at: http://diffusion-vla.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03293v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03293v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15957v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15957v3",
                "updated": "2024-12-04T12:54:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    12,
                    54,
                    44,
                    2,
                    339,
                    0
                ],
                "published": "2024-10-21T12:36:27Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    12,
                    36,
                    27,
                    0,
                    295,
                    0
                ],
                "title": "CamI2V: Camera-Controlled Image-to-Video Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CamI2V: Camera-Controlled Image-to-Video Diffusion Model"
                },
                "summary": "Recent advancements have integrated camera pose as a user-friendly and\nphysics-informed condition in video diffusion models, enabling precise camera\ncontrol. In this paper, we identify one of the key challenges as effectively\nmodeling noisy cross-frame interactions to enhance geometry consistency and\ncamera controllability. We innovatively associate the quality of a condition\nwith its ability to reduce uncertainty and interpret noisy cross-frame features\nas a form of noisy condition. Recognizing that noisy conditions provide\ndeterministic information while also introducing randomness and potential\nmisguidance due to added noise, we propose applying epipolar attention to only\naggregate features along corresponding epipolar lines, thereby accessing an\noptimal amount of noisy conditions. Additionally, we address scenarios where\nepipolar lines disappear, commonly caused by rapid camera movements, dynamic\nobjects, or occlusions, ensuring robust performance in diverse environments.\nFurthermore, we develop a more robust and reproducible evaluation pipeline to\naddress the inaccuracies and instabilities of existing camera control metrics.\nOur method achieves a 25.64% improvement in camera controllability on the\nRealEstate10K dataset without compromising dynamics or generation quality and\ndemonstrates strong generalization to out-of-domain images. Training and\ninference require only 24GB and 12GB of memory, respectively, for 16-frame\nsequences at 256x256 resolution. We will release all checkpoints, along with\ntraining and evaluation code. Dynamic videos are best viewed at\nhttps://zgctroy.github.io/CamI2V.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements have integrated camera pose as a user-friendly and\nphysics-informed condition in video diffusion models, enabling precise camera\ncontrol. In this paper, we identify one of the key challenges as effectively\nmodeling noisy cross-frame interactions to enhance geometry consistency and\ncamera controllability. We innovatively associate the quality of a condition\nwith its ability to reduce uncertainty and interpret noisy cross-frame features\nas a form of noisy condition. Recognizing that noisy conditions provide\ndeterministic information while also introducing randomness and potential\nmisguidance due to added noise, we propose applying epipolar attention to only\naggregate features along corresponding epipolar lines, thereby accessing an\noptimal amount of noisy conditions. Additionally, we address scenarios where\nepipolar lines disappear, commonly caused by rapid camera movements, dynamic\nobjects, or occlusions, ensuring robust performance in diverse environments.\nFurthermore, we develop a more robust and reproducible evaluation pipeline to\naddress the inaccuracies and instabilities of existing camera control metrics.\nOur method achieves a 25.64% improvement in camera controllability on the\nRealEstate10K dataset without compromising dynamics or generation quality and\ndemonstrates strong generalization to out-of-domain images. Training and\ninference require only 24GB and 12GB of memory, respectively, for 16-frame\nsequences at 256x256 resolution. We will release all checkpoints, along with\ntraining and evaluation code. Dynamic videos are best viewed at\nhttps://zgctroy.github.io/CamI2V."
                },
                "authors": [
                    {
                        "name": "Guangcong Zheng"
                    },
                    {
                        "name": "Teng Li"
                    },
                    {
                        "name": "Rui Jiang"
                    },
                    {
                        "name": "Yehao Lu"
                    },
                    {
                        "name": "Tao Wu"
                    },
                    {
                        "name": "Xi Li"
                    }
                ],
                "author_detail": {
                    "name": "Xi Li"
                },
                "author": "Xi Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15957v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15957v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.11268v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.11268v5",
                "updated": "2024-12-04T12:43:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    12,
                    43,
                    30,
                    2,
                    339,
                    0
                ],
                "published": "2023-09-20T12:51:13Z",
                "published_parsed": [
                    2023,
                    9,
                    20,
                    12,
                    51,
                    13,
                    2,
                    263,
                    0
                ],
                "title": "StructChart: On the Schema, Metric, and Augmentation for Visual Chart\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StructChart: On the Schema, Metric, and Augmentation for Visual Chart\n  Understanding"
                },
                "summary": "Charts are common in literature across various scientific fields, conveying\nrich information easily accessible to readers. Current chart-related tasks\nfocus on either chart perception that extracts information from the visual\ncharts, or chart reasoning given the extracted data, e.g. in a tabular form. In\nthis paper, we introduce StructChart, a novel framework that leverages\nStructured Triplet Representations (STR) to achieve a unified and\nlabel-efficient approach to chart perception and reasoning tasks, which is\ngenerally applicable to different downstream tasks, beyond the\nquestion-answering task as specifically studied in peer works. Specifically,\nStructChart first reformulates the chart data from the tubular form (linearized\nCSV) to STR, which can friendlily reduce the task gap between chart perception\nand reasoning. We then propose a Structuring Chart-oriented Representation\nMetric (SCRM) to quantitatively evaluate the chart perception task performance.\nTo augment the training, we further explore the potential of Large Language\nModels (LLMs) to enhance the diversity in both chart visual style and\nstatistical information. Extensive experiments on various chart-related tasks\ndemonstrate the effectiveness and potential of a unified chart\nperception-reasoning paradigm to push the frontier of chart understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Charts are common in literature across various scientific fields, conveying\nrich information easily accessible to readers. Current chart-related tasks\nfocus on either chart perception that extracts information from the visual\ncharts, or chart reasoning given the extracted data, e.g. in a tabular form. In\nthis paper, we introduce StructChart, a novel framework that leverages\nStructured Triplet Representations (STR) to achieve a unified and\nlabel-efficient approach to chart perception and reasoning tasks, which is\ngenerally applicable to different downstream tasks, beyond the\nquestion-answering task as specifically studied in peer works. Specifically,\nStructChart first reformulates the chart data from the tubular form (linearized\nCSV) to STR, which can friendlily reduce the task gap between chart perception\nand reasoning. We then propose a Structuring Chart-oriented Representation\nMetric (SCRM) to quantitatively evaluate the chart perception task performance.\nTo augment the training, we further explore the potential of Large Language\nModels (LLMs) to enhance the diversity in both chart visual style and\nstatistical information. Extensive experiments on various chart-related tasks\ndemonstrate the effectiveness and potential of a unified chart\nperception-reasoning paradigm to push the frontier of chart understanding."
                },
                "authors": [
                    {
                        "name": "Renqiu Xia"
                    },
                    {
                        "name": "Haoyang Peng"
                    },
                    {
                        "name": "Hancheng Ye"
                    },
                    {
                        "name": "Mingsheng Li"
                    },
                    {
                        "name": "Xiangchao Yan"
                    },
                    {
                        "name": "Peng Ye"
                    },
                    {
                        "name": "Botian Shi"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Junchi Yan"
                    },
                    {
                        "name": "Bo Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zhang"
                },
                "author": "Bo Zhang",
                "arxiv_comment": "All codes, models and SimChart9K data are available for downloading\n  at: https://github.com/UniModal4Reasoning/ChartVLM and\n  https://github.com/UniModal4Reasoning/SimChart9K",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.11268v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.11268v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01946v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01946v2",
                "updated": "2024-12-04T12:19:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    12,
                    19,
                    35,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-02T20:14:46Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    20,
                    14,
                    46,
                    0,
                    337,
                    0
                ],
                "title": "The Reality of AI and Biorisk",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Reality of AI and Biorisk"
                },
                "summary": "To accurately and confidently answer the question 'could an AI model or\nsystem increase biorisk', it is necessary to have both a sound theoretical\nthreat model for how AI models or systems could increase biorisk and a robust\nmethod for testing that threat model. This paper provides an analysis of\nexisting available research surrounding two AI and biorisk threat models: 1)\naccess to information and planning via large language models (LLMs), and 2) the\nuse of AI-enabled biological tools (BTs) in synthesizing novel biological\nartifacts. We find that existing studies around AI-related biorisk are nascent,\noften speculative in nature, or limited in terms of their methodological\nmaturity and transparency. The available literature suggests that current LLMs\nand BTs do not pose an immediate risk, and more work is needed to develop\nrigorous approaches to understanding how future models could increase biorisks.\nWe end with recommendations about how empirical work can be expanded to more\nprecisely target biorisk and ensure rigor and validity of findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To accurately and confidently answer the question 'could an AI model or\nsystem increase biorisk', it is necessary to have both a sound theoretical\nthreat model for how AI models or systems could increase biorisk and a robust\nmethod for testing that threat model. This paper provides an analysis of\nexisting available research surrounding two AI and biorisk threat models: 1)\naccess to information and planning via large language models (LLMs), and 2) the\nuse of AI-enabled biological tools (BTs) in synthesizing novel biological\nartifacts. We find that existing studies around AI-related biorisk are nascent,\noften speculative in nature, or limited in terms of their methodological\nmaturity and transparency. The available literature suggests that current LLMs\nand BTs do not pose an immediate risk, and more work is needed to develop\nrigorous approaches to understanding how future models could increase biorisks.\nWe end with recommendations about how empirical work can be expanded to more\nprecisely target biorisk and ensure rigor and validity of findings."
                },
                "authors": [
                    {
                        "name": "Aidan Peppin"
                    },
                    {
                        "name": "Anka Reuel"
                    },
                    {
                        "name": "Stephen Casper"
                    },
                    {
                        "name": "Elliot Jones"
                    },
                    {
                        "name": "Andrew Strait"
                    },
                    {
                        "name": "Usman Anwar"
                    },
                    {
                        "name": "Anurag Agrawal"
                    },
                    {
                        "name": "Sayash Kapoor"
                    },
                    {
                        "name": "Sanmi Koyejo"
                    },
                    {
                        "name": "Marie Pellat"
                    },
                    {
                        "name": "Rishi Bommasani"
                    },
                    {
                        "name": "Nick Frosst"
                    },
                    {
                        "name": "Sara Hooker"
                    }
                ],
                "author_detail": {
                    "name": "Sara Hooker"
                },
                "author": "Sara Hooker",
                "arxiv_comment": "Updated to correct author affiliations",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01946v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01946v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03267v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03267v1",
                "updated": "2024-12-04T12:18:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    12,
                    18,
                    21,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T12:18:21Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    12,
                    18,
                    21,
                    2,
                    339,
                    0
                ],
                "title": "Detecting abnormal heart sound using mobile phones and on-device IConNet",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting abnormal heart sound using mobile phones and on-device IConNet"
                },
                "summary": "Given the global prevalence of cardiovascular diseases, there is a pressing\nneed for easily accessible early screening methods. Typically, this requires\nmedical practitioners to investigate heart auscultations for irregular sounds,\nfollowed by echocardiography and electrocardiography tests. To democratize\nearly diagnosis, we present a user-friendly solution for abnormal heart sound\ndetection, utilizing mobile phones and a lightweight neural network optimized\nfor on-device inference. Unlike previous approaches reliant on specialized\nstethoscopes, our method directly analyzes audio recordings, facilitated by a\nnovel architecture known as IConNet. IConNet, an Interpretable Convolutional\nNeural Network, harnesses insights from audio signal processing, enhancing\nefficiency and providing transparency in neural pattern extraction from raw\nwaveform signals. This is a significant step towards trustworthy AI in\nhealthcare, aiding in remote health monitoring efforts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Given the global prevalence of cardiovascular diseases, there is a pressing\nneed for easily accessible early screening methods. Typically, this requires\nmedical practitioners to investigate heart auscultations for irregular sounds,\nfollowed by echocardiography and electrocardiography tests. To democratize\nearly diagnosis, we present a user-friendly solution for abnormal heart sound\ndetection, utilizing mobile phones and a lightweight neural network optimized\nfor on-device inference. Unlike previous approaches reliant on specialized\nstethoscopes, our method directly analyzes audio recordings, facilitated by a\nnovel architecture known as IConNet. IConNet, an Interpretable Convolutional\nNeural Network, harnesses insights from audio signal processing, enhancing\nefficiency and providing transparency in neural pattern extraction from raw\nwaveform signals. This is a significant step towards trustworthy AI in\nhealthcare, aiding in remote health monitoring efforts."
                },
                "authors": [
                    {
                        "name": "Linh Vu"
                    },
                    {
                        "name": "Thu Tran"
                    }
                ],
                "author_detail": {
                    "name": "Thu Tran"
                },
                "author": "Thu Tran",
                "arxiv_comment": "N2Women'24 Workshop, MobiSys 2024, Tokyo, Japan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03267v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03267v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16316v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16316v5",
                "updated": "2024-12-04T12:17:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    12,
                    17,
                    31,
                    2,
                    339,
                    0
                ],
                "published": "2024-11-25T12:09:43Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    12,
                    9,
                    43,
                    0,
                    330,
                    0
                ],
                "title": "Monocular Lane Detection Based on Deep Learning: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Monocular Lane Detection Based on Deep Learning: A Survey"
                },
                "summary": "Lane detection plays an important role in autonomous driving perception\nsystems. As deep learning algorithms gain popularity, monocular lane detection\nmethods based on them have demonstrated superior performance and emerged as a\nkey research direction in autonomous driving perception. The core designs of\nthese algorithmic frameworks can be summarized as follows: (1) Task paradigm,\nfocusing on lane instance-level discrimination; (2) Lane modeling, representing\nlanes as a set of learnable parameters in the neural network; (3) Global\ncontext supplementation, enhancing inference on the obscure lanes; (4)\nPerspective effect elimination, providing accurate 3D lanes for downstream\napplications. From these perspectives, this paper presents a comprehensive\noverview of existing methods, encompassing both the increasingly mature 2D lane\ndetection approaches and the developing 3D lane detection works. Besides, this\npaper compares the performance of mainstream methods on different benchmarks\nand investigates their inference speed under a unified setting for fair\ncomparison. Moreover, we present some extended works on lane detection,\nincluding multi-task perception, video lane detection, online high-definition\nmap construction, and lane topology reasoning, to offer readers a comprehensive\nroadmap for the evolution of lane detection. Finally, we point out some\npotential future research directions in this field. We exhaustively collect the\npapers and codes of existing works at\nhttps://github.com/Core9724/Awesome-Lane-Detection and will keep tracing the\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lane detection plays an important role in autonomous driving perception\nsystems. As deep learning algorithms gain popularity, monocular lane detection\nmethods based on them have demonstrated superior performance and emerged as a\nkey research direction in autonomous driving perception. The core designs of\nthese algorithmic frameworks can be summarized as follows: (1) Task paradigm,\nfocusing on lane instance-level discrimination; (2) Lane modeling, representing\nlanes as a set of learnable parameters in the neural network; (3) Global\ncontext supplementation, enhancing inference on the obscure lanes; (4)\nPerspective effect elimination, providing accurate 3D lanes for downstream\napplications. From these perspectives, this paper presents a comprehensive\noverview of existing methods, encompassing both the increasingly mature 2D lane\ndetection approaches and the developing 3D lane detection works. Besides, this\npaper compares the performance of mainstream methods on different benchmarks\nand investigates their inference speed under a unified setting for fair\ncomparison. Moreover, we present some extended works on lane detection,\nincluding multi-task perception, video lane detection, online high-definition\nmap construction, and lane topology reasoning, to offer readers a comprehensive\nroadmap for the evolution of lane detection. Finally, we point out some\npotential future research directions in this field. We exhaustively collect the\npapers and codes of existing works at\nhttps://github.com/Core9724/Awesome-Lane-Detection and will keep tracing the\nresearch."
                },
                "authors": [
                    {
                        "name": "Xin He"
                    },
                    {
                        "name": "Haiyun Guo"
                    },
                    {
                        "name": "Kuan Zhu"
                    },
                    {
                        "name": "Bingke Zhu"
                    },
                    {
                        "name": "Xu Zhao"
                    },
                    {
                        "name": "Jianwu Fang"
                    },
                    {
                        "name": "Jinqiao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jinqiao Wang"
                },
                "author": "Jinqiao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16316v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16316v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03253v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03253v1",
                "updated": "2024-12-04T11:52:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    11,
                    52,
                    3,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T11:52:03Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    11,
                    52,
                    3,
                    2,
                    339,
                    0
                ],
                "title": "Alignment at Pre-training! Towards Native Alignment for Arabic LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alignment at Pre-training! Towards Native Alignment for Arabic LLMs"
                },
                "summary": "The alignment of large language models (LLMs) is critical for developing\neffective and safe language models. Traditional approaches focus on aligning\nmodels during the instruction tuning or reinforcement learning stages, referred\nto in this paper as `post alignment'. We argue that alignment during the\npre-training phase, which we term `native alignment', warrants investigation.\nNative alignment aims to prevent unaligned content from the beginning, rather\nthan relying on post-hoc processing. This approach leverages extensively\naligned pre-training data to enhance the effectiveness and usability of\npre-trained models. Our study specifically explores the application of native\nalignment in the context of Arabic LLMs. We conduct comprehensive experiments\nand ablation studies to evaluate the impact of native alignment on model\nperformance and alignment stability. Additionally, we release open-source\nArabic LLMs that demonstrate state-of-the-art performance on various\nbenchmarks, providing significant benefits to the Arabic LLM community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The alignment of large language models (LLMs) is critical for developing\neffective and safe language models. Traditional approaches focus on aligning\nmodels during the instruction tuning or reinforcement learning stages, referred\nto in this paper as `post alignment'. We argue that alignment during the\npre-training phase, which we term `native alignment', warrants investigation.\nNative alignment aims to prevent unaligned content from the beginning, rather\nthan relying on post-hoc processing. This approach leverages extensively\naligned pre-training data to enhance the effectiveness and usability of\npre-trained models. Our study specifically explores the application of native\nalignment in the context of Arabic LLMs. We conduct comprehensive experiments\nand ablation studies to evaluate the impact of native alignment on model\nperformance and alignment stability. Additionally, we release open-source\nArabic LLMs that demonstrate state-of-the-art performance on various\nbenchmarks, providing significant benefits to the Arabic LLM community."
                },
                "authors": [
                    {
                        "name": "Juhao Liang"
                    },
                    {
                        "name": "Zhenyang Cai"
                    },
                    {
                        "name": "Jianqing Zhu"
                    },
                    {
                        "name": "Huang Huang"
                    },
                    {
                        "name": "Kewei Zong"
                    },
                    {
                        "name": "Bang An"
                    },
                    {
                        "name": "Mosen Alharthi"
                    },
                    {
                        "name": "Juncai He"
                    },
                    {
                        "name": "Lian Zhang"
                    },
                    {
                        "name": "Haizhou Li"
                    },
                    {
                        "name": "Benyou Wang"
                    },
                    {
                        "name": "Jinchao Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jinchao Xu"
                },
                "author": "Jinchao Xu",
                "arxiv_comment": "Accepted to NeurIPS 2024 main conference. see\n  https://github.com/FreedomIntelligence/AceGPT-v2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03253v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03253v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03250v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03250v1",
                "updated": "2024-12-04T11:49:22Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    11,
                    49,
                    22,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T11:49:22Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    11,
                    49,
                    22,
                    2,
                    339,
                    0
                ],
                "title": "Controlling the Mutation in Large Language Models for the Efficient\n  Evolution of Algorithms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Controlling the Mutation in Large Language Models for the Efficient\n  Evolution of Algorithms"
                },
                "summary": "The integration of Large Language Models (LLMs) with evolutionary computation\n(EC) has introduced a promising paradigm for automating the design of\nmetaheuristic algorithms. However, existing frameworks, such as the Large\nLanguage Model Evolutionary Algorithm (LLaMEA), often lack precise control over\nmutation mechanisms, leading to inefficiencies in solution space exploration\nand potentially suboptimal convergence. This paper introduces a novel approach\nto mutation control within LLM-driven evolutionary frameworks, inspired by\ntheory of genetic algorithms. Specifically, we propose dynamic mutation prompts\nthat adaptively regulate mutation rates, leveraging a heavy-tailed power-law\ndistribution to balance exploration and exploitation. Experiments using\nGPT-3.5-turbo and GPT-4o models demonstrate that GPT-3.5-turbo fails to adhere\nto the specific mutation instructions, while GPT-4o is able to adapt its\nmutation based on the prompt engineered dynamic prompts. Further experiments\nshow that the introduction of these dynamic rates can improve the convergence\nspeed and adaptability of LLaMEA, when using GPT-4o. This work sets the\nstarting point for better controlled LLM-based mutations in code optimization\ntasks, paving the way for further advancements in automated metaheuristic\ndesign.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Large Language Models (LLMs) with evolutionary computation\n(EC) has introduced a promising paradigm for automating the design of\nmetaheuristic algorithms. However, existing frameworks, such as the Large\nLanguage Model Evolutionary Algorithm (LLaMEA), often lack precise control over\nmutation mechanisms, leading to inefficiencies in solution space exploration\nand potentially suboptimal convergence. This paper introduces a novel approach\nto mutation control within LLM-driven evolutionary frameworks, inspired by\ntheory of genetic algorithms. Specifically, we propose dynamic mutation prompts\nthat adaptively regulate mutation rates, leveraging a heavy-tailed power-law\ndistribution to balance exploration and exploitation. Experiments using\nGPT-3.5-turbo and GPT-4o models demonstrate that GPT-3.5-turbo fails to adhere\nto the specific mutation instructions, while GPT-4o is able to adapt its\nmutation based on the prompt engineered dynamic prompts. Further experiments\nshow that the introduction of these dynamic rates can improve the convergence\nspeed and adaptability of LLaMEA, when using GPT-4o. This work sets the\nstarting point for better controlled LLM-based mutations in code optimization\ntasks, paving the way for further advancements in automated metaheuristic\ndesign."
                },
                "authors": [
                    {
                        "name": "Haoran Yin"
                    },
                    {
                        "name": "Anna V. Kononova"
                    },
                    {
                        "name": "Thomas Bck"
                    },
                    {
                        "name": "Niki van Stein"
                    }
                ],
                "author_detail": {
                    "name": "Niki van Stein"
                },
                "author": "Niki van Stein",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03250v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03250v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10083v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10083v3",
                "updated": "2024-12-04T11:49:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    11,
                    49,
                    4,
                    2,
                    339,
                    0
                ],
                "published": "2024-11-15T10:01:52Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    10,
                    1,
                    52,
                    4,
                    320,
                    0
                ],
                "title": "Xmodel-1.5: An 1B-scale Multilingual LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Xmodel-1.5: An 1B-scale Multilingual LLM"
                },
                "summary": "We introduce Xmodel-1.5, a 1-billion-parameter multilingual large language\nmodel pretrained on 2 trillion tokens, designed for balanced performance and\nscalability. Unlike most large models that use the BPE tokenizer, Xmodel-1.5\nemploys a custom unigram tokenizer with 65,280 tokens, optimizing both\nefficiency and accuracy. The model delivers competitive results across multiple\nlanguages, including Thai, Arabic, French, Chinese, and English, outperforming\nAlibaba's PolyLM-1.7B on respective evaluation datasets. Xmodel-1.5 excels in\nbenchmarks like mMMLU and PIQA, and achieves state-of-the-art results in Thai.\nTo support low-resource language research, we release Xdata_Thai, a\nThai-specific evaluation dataset featuring unique linguistic challenges such as\ngendered particles and idioms. While the model demonstrates strong performance,\nthere is still room for improvement in handling culturally specific nuances. We\nhope this work contributes to advancements in multilingual AI research. Models\nand code are publicly available on GitHub at\nhttps://github.com/XiaoduoAILab/XmodelLM-1.5",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Xmodel-1.5, a 1-billion-parameter multilingual large language\nmodel pretrained on 2 trillion tokens, designed for balanced performance and\nscalability. Unlike most large models that use the BPE tokenizer, Xmodel-1.5\nemploys a custom unigram tokenizer with 65,280 tokens, optimizing both\nefficiency and accuracy. The model delivers competitive results across multiple\nlanguages, including Thai, Arabic, French, Chinese, and English, outperforming\nAlibaba's PolyLM-1.7B on respective evaluation datasets. Xmodel-1.5 excels in\nbenchmarks like mMMLU and PIQA, and achieves state-of-the-art results in Thai.\nTo support low-resource language research, we release Xdata_Thai, a\nThai-specific evaluation dataset featuring unique linguistic challenges such as\ngendered particles and idioms. While the model demonstrates strong performance,\nthere is still room for improvement in handling culturally specific nuances. We\nhope this work contributes to advancements in multilingual AI research. Models\nand code are publicly available on GitHub at\nhttps://github.com/XiaoduoAILab/XmodelLM-1.5"
                },
                "authors": [
                    {
                        "name": "Wang Qun"
                    },
                    {
                        "name": "Liu Yang"
                    },
                    {
                        "name": "Lin Qingquan"
                    },
                    {
                        "name": "Jiang Ling"
                    }
                ],
                "author_detail": {
                    "name": "Jiang Ling"
                },
                "author": "Jiang Ling",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10083v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10083v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03248v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03248v1",
                "updated": "2024-12-04T11:47:57Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    11,
                    47,
                    57,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T11:47:57Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    11,
                    47,
                    57,
                    2,
                    339,
                    0
                ],
                "title": "AIM: Adaptive Inference of Multi-Modal LLMs via Token Merging and\n  Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AIM: Adaptive Inference of Multi-Modal LLMs via Token Merging and\n  Pruning"
                },
                "summary": "Large language models (LLMs) have enabled the creation of multi-modal LLMs\nthat exhibit strong comprehension of visual data such as images and videos.\nHowever, these models usually rely on extensive visual tokens from visual\nencoders, leading to high computational demands, which limits their\napplicability in resource-constrained environments and for long-context tasks.\nIn this work, we propose a training-free adaptive inference method for\nmulti-modal LLMs that can accommodate a broad range of efficiency requirements\nwith a minimum performance drop. Our method consists of a) iterative token\nmerging based on embedding similarity before LLMs, and b) progressive token\npruning within LLM layers based on multi-modal importance. With a minimalist\ndesign, our method can be applied to both video and image LLMs. Extensive\nexperiments on diverse video and image benchmarks demonstrate that, our method\nsubstantially reduces computation load (e.g., a $\\textbf{7-fold}$ reduction in\nFLOPs) while preserving the performance of video and image LLMs. Further, under\na similar computational cost, our method outperforms the state-of-the-art\nmethods in long video understanding (e.g., $\\textbf{+4.6}$ on MLVU).\nAdditionally, our in-depth analysis provides insights into token redundancy and\nLLM layer behaviors, offering guidance for future research in designing\nefficient multi-modal LLMs. Our code will be available at\nhttps://github.com/LaVi-Lab/AIM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have enabled the creation of multi-modal LLMs\nthat exhibit strong comprehension of visual data such as images and videos.\nHowever, these models usually rely on extensive visual tokens from visual\nencoders, leading to high computational demands, which limits their\napplicability in resource-constrained environments and for long-context tasks.\nIn this work, we propose a training-free adaptive inference method for\nmulti-modal LLMs that can accommodate a broad range of efficiency requirements\nwith a minimum performance drop. Our method consists of a) iterative token\nmerging based on embedding similarity before LLMs, and b) progressive token\npruning within LLM layers based on multi-modal importance. With a minimalist\ndesign, our method can be applied to both video and image LLMs. Extensive\nexperiments on diverse video and image benchmarks demonstrate that, our method\nsubstantially reduces computation load (e.g., a $\\textbf{7-fold}$ reduction in\nFLOPs) while preserving the performance of video and image LLMs. Further, under\na similar computational cost, our method outperforms the state-of-the-art\nmethods in long video understanding (e.g., $\\textbf{+4.6}$ on MLVU).\nAdditionally, our in-depth analysis provides insights into token redundancy and\nLLM layer behaviors, offering guidance for future research in designing\nefficient multi-modal LLMs. Our code will be available at\nhttps://github.com/LaVi-Lab/AIM."
                },
                "authors": [
                    {
                        "name": "Yiwu Zhong"
                    },
                    {
                        "name": "Zhuoming Liu"
                    },
                    {
                        "name": "Yin Li"
                    },
                    {
                        "name": "Liwei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Liwei Wang"
                },
                "author": "Liwei Wang",
                "arxiv_comment": "12 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03248v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03248v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02626v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02626v2",
                "updated": "2024-12-04T11:45:34Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    11,
                    45,
                    34,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-03T17:54:12Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    17,
                    54,
                    12,
                    1,
                    338,
                    0
                ],
                "title": "Time-Reversal Provides Unsupervised Feedback to LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-Reversal Provides Unsupervised Feedback to LLMs"
                },
                "summary": "Large Language Models (LLMs) are typically trained to predict in the forward\ndirection of time. However, recent works have shown that prompting these models\nto look back and critique their own generations can produce useful feedback.\nMotivated by this, we explore the question of whether LLMs can be empowered to\nthink (predict and score) backwards to provide unsupervised feedback that\ncomplements forward LLMs. Towards this, we introduce Time Reversed Language\nModels (TRLMs), which can score and generate queries when conditioned on\nresponses, effectively functioning in the reverse direction of time. Further,\nto effectively infer in the response to query direction, we pre-train and\nfine-tune a language model (TRLM-Ba) in the reverse token order from scratch.\nWe show empirically (and theoretically in a stylized setting) that\ntime-reversed models can indeed complement forward model predictions when used\nto score the query given response for re-ranking multiple forward generations.\nWe obtain up to 5\\% improvement on the widely used AlpacaEval Leaderboard over\nthe competent baseline of best-of-N re-ranking using self log-perplexity\nscores. We further show that TRLM scoring outperforms conventional forward\nscoring of response given query, resulting in significant gains in applications\nsuch as citation generation and passage retrieval. We next leverage the\ngenerative ability of TRLM to augment or provide unsupervised feedback to input\nsafety filters of LLMs, demonstrating a drastic reduction in false negative\nrate with negligible impact on false positive rates against several attacks\npublished on the popular JailbreakBench leaderboard.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are typically trained to predict in the forward\ndirection of time. However, recent works have shown that prompting these models\nto look back and critique their own generations can produce useful feedback.\nMotivated by this, we explore the question of whether LLMs can be empowered to\nthink (predict and score) backwards to provide unsupervised feedback that\ncomplements forward LLMs. Towards this, we introduce Time Reversed Language\nModels (TRLMs), which can score and generate queries when conditioned on\nresponses, effectively functioning in the reverse direction of time. Further,\nto effectively infer in the response to query direction, we pre-train and\nfine-tune a language model (TRLM-Ba) in the reverse token order from scratch.\nWe show empirically (and theoretically in a stylized setting) that\ntime-reversed models can indeed complement forward model predictions when used\nto score the query given response for re-ranking multiple forward generations.\nWe obtain up to 5\\% improvement on the widely used AlpacaEval Leaderboard over\nthe competent baseline of best-of-N re-ranking using self log-perplexity\nscores. We further show that TRLM scoring outperforms conventional forward\nscoring of response given query, resulting in significant gains in applications\nsuch as citation generation and passage retrieval. We next leverage the\ngenerative ability of TRLM to augment or provide unsupervised feedback to input\nsafety filters of LLMs, demonstrating a drastic reduction in false negative\nrate with negligible impact on false positive rates against several attacks\npublished on the popular JailbreakBench leaderboard."
                },
                "authors": [
                    {
                        "name": "Yerram Varun"
                    },
                    {
                        "name": "Rahul Madhavan"
                    },
                    {
                        "name": "Sravanti Addepalli"
                    },
                    {
                        "name": "Arun Suggala"
                    },
                    {
                        "name": "Karthikeyan Shanmugam"
                    },
                    {
                        "name": "Prateek Jain"
                    }
                ],
                "author_detail": {
                    "name": "Prateek Jain"
                },
                "author": "Prateek Jain",
                "arxiv_comment": "Accepted as a spotlight in NeurIPS 2024",
                "arxiv_journal_ref": "Varun, Y., Madhavan, R., Addepalli, S., Suggala, A., Shanmugam,\n  K., & Jain, P. Time-Reversal Provides Unsupervised Feedback to LLMs. In The\n  Thirty-Eighth Annual Conference on Neural Information Processing Systems\n  (NeurIPS), 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02626v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02626v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03235v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03235v1",
                "updated": "2024-12-04T11:36:37Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    11,
                    36,
                    37,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T11:36:37Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    11,
                    36,
                    37,
                    2,
                    339,
                    0
                ],
                "title": "Does Safety Training of LLMs Generalize to Semantically Related Natural\n  Prompts?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does Safety Training of LLMs Generalize to Semantically Related Natural\n  Prompts?"
                },
                "summary": "Large Language Models (LLMs) are known to be susceptible to crafted\nadversarial attacks or jailbreaks that lead to the generation of objectionable\ncontent despite being aligned to human preferences using safety fine-tuning\nmethods. While the large dimensionality of input token space makes it\ninevitable to find adversarial prompts that can jailbreak these models, we aim\nto evaluate whether safety fine-tuned LLMs are safe against natural prompts\nwhich are semantically related to toxic seed prompts that elicit safe responses\nafter alignment. We surprisingly find that popular aligned LLMs such as GPT-4\ncan be compromised using naive prompts that are NOT even crafted with an\nobjective of jailbreaking the model. Furthermore, we empirically show that\ngiven a seed prompt that elicits a toxic response from an unaligned model, one\ncan systematically generate several semantically related natural prompts that\ncan jailbreak aligned LLMs. Towards this, we propose a method of Response\nGuided Question Augmentation (ReG-QA) to evaluate the generalization of safety\naligned LLMs to natural prompts, that first generates several toxic answers\ngiven a seed question using an unaligned LLM (Q to A), and further leverages an\nLLM to generate questions that are likely to produce these answers (A to Q). We\ninterestingly find that safety fine-tuned LLMs such as GPT-4o are vulnerable to\nproducing natural jailbreak questions from unsafe content (without denial) and\ncan thus be used for the latter (A to Q) step. We obtain attack success rates\nthat are comparable to/ better than leading adversarial attack methods on the\nJailbreakBench leaderboard, while being significantly more stable against\ndefenses such as Smooth-LLM and Synonym Substitution, which are effective\nagainst existing all attacks on the leaderboard.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are known to be susceptible to crafted\nadversarial attacks or jailbreaks that lead to the generation of objectionable\ncontent despite being aligned to human preferences using safety fine-tuning\nmethods. While the large dimensionality of input token space makes it\ninevitable to find adversarial prompts that can jailbreak these models, we aim\nto evaluate whether safety fine-tuned LLMs are safe against natural prompts\nwhich are semantically related to toxic seed prompts that elicit safe responses\nafter alignment. We surprisingly find that popular aligned LLMs such as GPT-4\ncan be compromised using naive prompts that are NOT even crafted with an\nobjective of jailbreaking the model. Furthermore, we empirically show that\ngiven a seed prompt that elicits a toxic response from an unaligned model, one\ncan systematically generate several semantically related natural prompts that\ncan jailbreak aligned LLMs. Towards this, we propose a method of Response\nGuided Question Augmentation (ReG-QA) to evaluate the generalization of safety\naligned LLMs to natural prompts, that first generates several toxic answers\ngiven a seed question using an unaligned LLM (Q to A), and further leverages an\nLLM to generate questions that are likely to produce these answers (A to Q). We\ninterestingly find that safety fine-tuned LLMs such as GPT-4o are vulnerable to\nproducing natural jailbreak questions from unsafe content (without denial) and\ncan thus be used for the latter (A to Q) step. We obtain attack success rates\nthat are comparable to/ better than leading adversarial attack methods on the\nJailbreakBench leaderboard, while being significantly more stable against\ndefenses such as Smooth-LLM and Synonym Substitution, which are effective\nagainst existing all attacks on the leaderboard."
                },
                "authors": [
                    {
                        "name": "Sravanti Addepalli"
                    },
                    {
                        "name": "Yerram Varun"
                    },
                    {
                        "name": "Arun Suggala"
                    },
                    {
                        "name": "Karthikeyan Shanmugam"
                    },
                    {
                        "name": "Prateek Jain"
                    }
                ],
                "author_detail": {
                    "name": "Prateek Jain"
                },
                "author": "Prateek Jain",
                "arxiv_comment": "Accepted at the Safe Generative AI Workshop @ NeurIPS 2024",
                "arxiv_journal_ref": "Addepalli, S., Varun, Y., Suggala, A., Shanmugam, K. and Jain, P.,\n  Does Safety Training of LLMs Generalize to Semantically Related Natural\n  Prompts?. In Neurips Safe Generative AI Workshop 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03235v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03235v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03223v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03223v1",
                "updated": "2024-12-04T11:18:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    11,
                    18,
                    32,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T11:18:32Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    11,
                    18,
                    32,
                    2,
                    339,
                    0
                ],
                "title": "Linq-Embed-Mistral Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linq-Embed-Mistral Technical Report"
                },
                "summary": "This report explores the enhancement of text retrieval performance using\nadvanced data refinement techniques. We develop\nLinq-Embed-Mistral\\footnote{\\url{https://huggingface.co/Linq-AI-Research/Linq-Embed-Mistral}}\nby building on the E5-mistral and Mistral-7B-v0.1 models, focusing on\nsophisticated data crafting, data filtering, and negative mining methods, which\nare highly tailored to each task, applied to both existing benchmark dataset\nand highly tailored synthetic dataset generated via large language models\n(LLMs). Linq-Embed-Mistral excels in the MTEB benchmarks (as of May 29, 2024),\nachieving an average score of 68.2 across 56 datasets, and ranks 1st among all\nmodels for retrieval tasks on the MTEB leaderboard with a performance score of\n60.2. This performance underscores its superior capability in enhancing search\nprecision and reliability. Our contributions include advanced data refinement\nmethods that significantly improve model performance on benchmark and synthetic\ndatasets, techniques for homogeneous task ordering and mixed task fine-tuning\nto enhance model generalization and stability, and a streamlined evaluation\nprocess using 4-bit precision and a light retrieval evaluation set, which\naccelerates validation without sacrificing accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This report explores the enhancement of text retrieval performance using\nadvanced data refinement techniques. We develop\nLinq-Embed-Mistral\\footnote{\\url{https://huggingface.co/Linq-AI-Research/Linq-Embed-Mistral}}\nby building on the E5-mistral and Mistral-7B-v0.1 models, focusing on\nsophisticated data crafting, data filtering, and negative mining methods, which\nare highly tailored to each task, applied to both existing benchmark dataset\nand highly tailored synthetic dataset generated via large language models\n(LLMs). Linq-Embed-Mistral excels in the MTEB benchmarks (as of May 29, 2024),\nachieving an average score of 68.2 across 56 datasets, and ranks 1st among all\nmodels for retrieval tasks on the MTEB leaderboard with a performance score of\n60.2. This performance underscores its superior capability in enhancing search\nprecision and reliability. Our contributions include advanced data refinement\nmethods that significantly improve model performance on benchmark and synthetic\ndatasets, techniques for homogeneous task ordering and mixed task fine-tuning\nto enhance model generalization and stability, and a streamlined evaluation\nprocess using 4-bit precision and a light retrieval evaluation set, which\naccelerates validation without sacrificing accuracy."
                },
                "authors": [
                    {
                        "name": "Chanyeol Choi"
                    },
                    {
                        "name": "Junseong Kim"
                    },
                    {
                        "name": "Seolhwa Lee"
                    },
                    {
                        "name": "Jihoon Kwon"
                    },
                    {
                        "name": "Sangmo Gu"
                    },
                    {
                        "name": "Yejin Kim"
                    },
                    {
                        "name": "Minkyung Cho"
                    },
                    {
                        "name": "Jy-yong Sohn"
                    }
                ],
                "author_detail": {
                    "name": "Jy-yong Sohn"
                },
                "author": "Jy-yong Sohn",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03223v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03223v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03220v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03220v1",
                "updated": "2024-12-04T11:14:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    11,
                    14,
                    6,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T11:14:06Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    11,
                    14,
                    6,
                    2,
                    339,
                    0
                ],
                "title": "Survey of different Large Language Model Architectures: Trends,\n  Benchmarks, and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Survey of different Large Language Model Architectures: Trends,\n  Benchmarks, and Challenges"
                },
                "summary": "Large Language Models (LLMs) represent a class of deep learning models adept\nat understanding natural language and generating coherent responses to various\nprompts or queries. These models far exceed the complexity of conventional\nneural networks, often encompassing dozens of neural network layers and\ncontaining billions to trillions of parameters. They are typically trained on\nvast datasets, utilizing architectures based on transformer blocks. Present-day\nLLMs are multi-functional, capable of performing a range of tasks from text\ngeneration and language translation to question answering, as well as code\ngeneration and analysis. An advanced subset of these models, known as\nMultimodal Large Language Models (MLLMs), extends LLM capabilities to process\nand interpret multiple data modalities, including images, audio, and video.\nThis enhancement empowers MLLMs with capabilities like video editing, image\ncomprehension, and captioning for visual content. This survey provides a\ncomprehensive overview of the recent advancements in LLMs. We begin by tracing\nthe evolution of LLMs and subsequently delve into the advent and nuances of\nMLLMs. We analyze emerging state-of-the-art MLLMs, exploring their technical\nfeatures, strengths, and limitations. Additionally, we present a comparative\nanalysis of these models and discuss their challenges, potential limitations,\nand prospects for future development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) represent a class of deep learning models adept\nat understanding natural language and generating coherent responses to various\nprompts or queries. These models far exceed the complexity of conventional\nneural networks, often encompassing dozens of neural network layers and\ncontaining billions to trillions of parameters. They are typically trained on\nvast datasets, utilizing architectures based on transformer blocks. Present-day\nLLMs are multi-functional, capable of performing a range of tasks from text\ngeneration and language translation to question answering, as well as code\ngeneration and analysis. An advanced subset of these models, known as\nMultimodal Large Language Models (MLLMs), extends LLM capabilities to process\nand interpret multiple data modalities, including images, audio, and video.\nThis enhancement empowers MLLMs with capabilities like video editing, image\ncomprehension, and captioning for visual content. This survey provides a\ncomprehensive overview of the recent advancements in LLMs. We begin by tracing\nthe evolution of LLMs and subsequently delve into the advent and nuances of\nMLLMs. We analyze emerging state-of-the-art MLLMs, exploring their technical\nfeatures, strengths, and limitations. Additionally, we present a comparative\nanalysis of these models and discuss their challenges, potential limitations,\nand prospects for future development."
                },
                "authors": [
                    {
                        "name": "Minghao Shao"
                    },
                    {
                        "name": "Abdul Basit"
                    },
                    {
                        "name": "Ramesh Karri"
                    },
                    {
                        "name": "Muhammad Shafique"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Shafique"
                },
                "author": "Muhammad Shafique",
                "arxiv_doi": "10.1109/ACCESS.2024.3482107",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ACCESS.2024.3482107",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.03220v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03220v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03214v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03214v1",
                "updated": "2024-12-04T11:05:01Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    11,
                    5,
                    1,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T11:05:01Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    11,
                    5,
                    1,
                    2,
                    339,
                    0
                ],
                "title": "Continual Low-Rank Scaled Dot-product Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual Low-Rank Scaled Dot-product Attention"
                },
                "summary": "Transformers are widely used for their ability to capture data relations in\nsequence processing, with great success for a wide range of static tasks.\nHowever, the computational and memory footprint of their main component, i.e.,\nthe Scaled Dot-product Attention, is commonly overlooked. This makes their\nadoption in applications involving stream data processing with constraints in\nresponse latency, computational and memory resources infeasible. Some works\nhave proposed methods to lower the computational cost of transformers, i.e.\nlow-rank approximations, sparsity in attention, and efficient formulations for\nContinual Inference. In this paper, we introduce a new formulation of the\nScaled Dot-product Attention based on the Nystr\\\"om approximation that is\nsuitable for Continual Inference. In experiments on Online Audio Classification\nand Online Action Detection tasks, the proposed Continual Scaled Dot-product\nAttention can lower the number of operations by up to three orders of magnitude\ncompared to the original Transformers while retaining the predictive\nperformance of competing models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers are widely used for their ability to capture data relations in\nsequence processing, with great success for a wide range of static tasks.\nHowever, the computational and memory footprint of their main component, i.e.,\nthe Scaled Dot-product Attention, is commonly overlooked. This makes their\nadoption in applications involving stream data processing with constraints in\nresponse latency, computational and memory resources infeasible. Some works\nhave proposed methods to lower the computational cost of transformers, i.e.\nlow-rank approximations, sparsity in attention, and efficient formulations for\nContinual Inference. In this paper, we introduce a new formulation of the\nScaled Dot-product Attention based on the Nystr\\\"om approximation that is\nsuitable for Continual Inference. In experiments on Online Audio Classification\nand Online Action Detection tasks, the proposed Continual Scaled Dot-product\nAttention can lower the number of operations by up to three orders of magnitude\ncompared to the original Transformers while retaining the predictive\nperformance of competing models."
                },
                "authors": [
                    {
                        "name": "Gins Carreto Picn"
                    },
                    {
                        "name": "Illia Oleksiienko"
                    },
                    {
                        "name": "Lukas Hedegaard"
                    },
                    {
                        "name": "Arian Bakhtiarnia"
                    },
                    {
                        "name": "Alexandros Iosifidis"
                    }
                ],
                "author_detail": {
                    "name": "Alexandros Iosifidis"
                },
                "author": "Alexandros Iosifidis",
                "arxiv_comment": "11 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03214v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03214v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03213v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03213v1",
                "updated": "2024-12-04T10:58:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    10,
                    58,
                    27,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T10:58:27Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    10,
                    58,
                    27,
                    2,
                    339,
                    0
                ],
                "title": "ClusterKV: Manipulating LLM KV Cache in Semantic Space for Recallable\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ClusterKV: Manipulating LLM KV Cache in Semantic Space for Recallable\n  Compression"
                },
                "summary": "Large Language Models (LLMs) have been widely deployed in a variety of\napplications, and the context length is rapidly increasing to handle tasks such\nas long-document QA and complex logical reasoning. However, long context poses\nsignificant challenges for inference efficiency, including high memory costs of\nkey-value (KV) cache and increased latency due to extensive memory accesses.\nRecent works have proposed compressing KV cache to approximate computation, but\nthese methods either evict tokens permanently, never recalling them for later\ninference, or recall previous tokens at the granularity of pages divided by\ntextual positions. Both approaches degrade the model accuracy and output\nquality. To achieve efficient and accurate recallable KV cache compression, we\nintroduce ClusterKV, which recalls tokens at the granularity of semantic\nclusters. We design and implement efficient algorithms and systems for\nclustering, selection, indexing and caching. Experiment results show that\nClusterKV attains negligible accuracy loss across various tasks with 32k\ncontext lengths, using only a 1k to 2k KV cache budget, and achieves up to a\n2$\\times$ speedup in latency and a 2.5$\\times$ improvement in decoding\nthroughput. Compared to SoTA recallable KV compression methods, ClusterKV\ndemonstrates higher model accuracy and output quality, while maintaining or\nexceeding inference efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been widely deployed in a variety of\napplications, and the context length is rapidly increasing to handle tasks such\nas long-document QA and complex logical reasoning. However, long context poses\nsignificant challenges for inference efficiency, including high memory costs of\nkey-value (KV) cache and increased latency due to extensive memory accesses.\nRecent works have proposed compressing KV cache to approximate computation, but\nthese methods either evict tokens permanently, never recalling them for later\ninference, or recall previous tokens at the granularity of pages divided by\ntextual positions. Both approaches degrade the model accuracy and output\nquality. To achieve efficient and accurate recallable KV cache compression, we\nintroduce ClusterKV, which recalls tokens at the granularity of semantic\nclusters. We design and implement efficient algorithms and systems for\nclustering, selection, indexing and caching. Experiment results show that\nClusterKV attains negligible accuracy loss across various tasks with 32k\ncontext lengths, using only a 1k to 2k KV cache budget, and achieves up to a\n2$\\times$ speedup in latency and a 2.5$\\times$ improvement in decoding\nthroughput. Compared to SoTA recallable KV compression methods, ClusterKV\ndemonstrates higher model accuracy and output quality, while maintaining or\nexceeding inference efficiency."
                },
                "authors": [
                    {
                        "name": "Guangda Liu"
                    },
                    {
                        "name": "Chengwei Li"
                    },
                    {
                        "name": "Jieru Zhao"
                    },
                    {
                        "name": "Chenqi Zhang"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03213v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03213v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16926v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16926v2",
                "updated": "2024-12-04T10:52:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    10,
                    52,
                    4,
                    2,
                    339,
                    0
                ],
                "published": "2024-10-22T11:57:32Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    11,
                    57,
                    32,
                    1,
                    296,
                    0
                ],
                "title": "Pyramid Vector Quantization for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pyramid Vector Quantization for LLMs"
                },
                "summary": "Recent works on compression of large language models (LLM) using quantization\nconsidered reparameterizing the architecture such that weights are distributed\non the sphere. This demonstratively improves the ability to quantize by\nincreasing the mathematical notion of coherence, resulting in fewer weight\noutliers without affecting the network output. In this work, we aim to further\nexploit this spherical geometry of the weights when performing quantization by\nconsidering Pyramid Vector Quantization (PVQ) for large language models.\nArranging points evenly on the sphere is notoriously difficult, especially in\nhigh dimensions, and in case approximate solutions exists, representing points\nexplicitly in a codebook is typically not feasible due to its additional memory\ncost. Instead, PVQ uses a fixed integer lattice on the sphere by projecting\npoints onto the 1-sphere, which allows for efficient encoding and decoding\nwithout requiring an explicit codebook in memory. To obtain a practical\nalgorithm, we propose to combine PVQ with scale quantization for which we\nderive theoretically optimal quantizations, under empirically verified\nassumptions. Further, we extend pyramid vector quantization to use Hessian\ninformation to minimize quantization error under expected feature activations,\ninstead of only relying on weight magnitudes. Experimentally, we achieves\nstate-of-the-art quantization performance with pareto-optimal trade-off between\nperformance and bits per weight and bits per activation, compared to compared\nmethods. On weight-only, we find that we can quantize a Llama-3 70B model to\n3.25 bits per weight and retain 98\\% accuracy on downstream tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent works on compression of large language models (LLM) using quantization\nconsidered reparameterizing the architecture such that weights are distributed\non the sphere. This demonstratively improves the ability to quantize by\nincreasing the mathematical notion of coherence, resulting in fewer weight\noutliers without affecting the network output. In this work, we aim to further\nexploit this spherical geometry of the weights when performing quantization by\nconsidering Pyramid Vector Quantization (PVQ) for large language models.\nArranging points evenly on the sphere is notoriously difficult, especially in\nhigh dimensions, and in case approximate solutions exists, representing points\nexplicitly in a codebook is typically not feasible due to its additional memory\ncost. Instead, PVQ uses a fixed integer lattice on the sphere by projecting\npoints onto the 1-sphere, which allows for efficient encoding and decoding\nwithout requiring an explicit codebook in memory. To obtain a practical\nalgorithm, we propose to combine PVQ with scale quantization for which we\nderive theoretically optimal quantizations, under empirically verified\nassumptions. Further, we extend pyramid vector quantization to use Hessian\ninformation to minimize quantization error under expected feature activations,\ninstead of only relying on weight magnitudes. Experimentally, we achieves\nstate-of-the-art quantization performance with pareto-optimal trade-off between\nperformance and bits per weight and bits per activation, compared to compared\nmethods. On weight-only, we find that we can quantize a Llama-3 70B model to\n3.25 bits per weight and retain 98\\% accuracy on downstream tasks."
                },
                "authors": [
                    {
                        "name": "Tycho F. A. van der Ouderaa"
                    },
                    {
                        "name": "Maximilian L. Croci"
                    },
                    {
                        "name": "Agrin Hilmkil"
                    },
                    {
                        "name": "James Hensman"
                    }
                ],
                "author_detail": {
                    "name": "James Hensman"
                },
                "author": "James Hensman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16926v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16926v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00850v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00850v2",
                "updated": "2024-12-04T10:45:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    10,
                    45,
                    41,
                    2,
                    339,
                    0
                ],
                "published": "2024-10-30T11:16:04Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    11,
                    16,
                    4,
                    2,
                    304,
                    0
                ],
                "title": "GWQ: Gradient-Aware Weight Quantization for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GWQ: Gradient-Aware Weight Quantization for Large Language Models"
                },
                "summary": "Large language models (LLMs) show impressive performance in solving complex\nlanguage tasks. However, its large number of parameters present significant\nchallenges for the deployment and application of the model on edge devices.\nCompressing large language models to low bits can enable them to run on\nresource-constrained devices, often leading to performance degradation. To\naddress this problem, we propose gradient-aware weight quantization (GWQ), the\nfirst quantization approach for low-bit weight quantization that leverages\ngradients to localize outliers, requiring only a minimal amount of calibration\ndata for outlier detection. GWQ retains the weights corresponding to the top 1%\noutliers preferentially at FP16 precision, while the remaining non-outlier\nweights are stored in a low-bit format. GWQ found experimentally that utilizing\nthe sensitive weights in the gradient localization model is more scientific\ncompared to utilizing the sensitive weights in the Hessian matrix localization\nmodel. Compared to current quantization methods, GWQ can be applied to multiple\nlanguage models and achieves lower PPL on the WikiText2 and C4 dataset. In the\nzero-shot task, GWQ quantized models have higher accuracy compared to other\nquantization methods. GWQ is also suitable for multimodal model quantization,\nand the quantized Qwen-VL family model is more accurate than other methods.\nZero-shot target detection task dataset RefCOCO outperforms the current\nstat-of-the-arts method SPQR. GWQ achieves 1.2 times inference speedup in\ncomparison to the original model, and effectively reduces the inference memory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) show impressive performance in solving complex\nlanguage tasks. However, its large number of parameters present significant\nchallenges for the deployment and application of the model on edge devices.\nCompressing large language models to low bits can enable them to run on\nresource-constrained devices, often leading to performance degradation. To\naddress this problem, we propose gradient-aware weight quantization (GWQ), the\nfirst quantization approach for low-bit weight quantization that leverages\ngradients to localize outliers, requiring only a minimal amount of calibration\ndata for outlier detection. GWQ retains the weights corresponding to the top 1%\noutliers preferentially at FP16 precision, while the remaining non-outlier\nweights are stored in a low-bit format. GWQ found experimentally that utilizing\nthe sensitive weights in the gradient localization model is more scientific\ncompared to utilizing the sensitive weights in the Hessian matrix localization\nmodel. Compared to current quantization methods, GWQ can be applied to multiple\nlanguage models and achieves lower PPL on the WikiText2 and C4 dataset. In the\nzero-shot task, GWQ quantized models have higher accuracy compared to other\nquantization methods. GWQ is also suitable for multimodal model quantization,\nand the quantized Qwen-VL family model is more accurate than other methods.\nZero-shot target detection task dataset RefCOCO outperforms the current\nstat-of-the-arts method SPQR. GWQ achieves 1.2 times inference speedup in\ncomparison to the original model, and effectively reduces the inference memory."
                },
                "authors": [
                    {
                        "name": "Yihua Shao"
                    },
                    {
                        "name": "Siyu Liang"
                    },
                    {
                        "name": "Zijian Ling"
                    },
                    {
                        "name": "Minxi Yan"
                    },
                    {
                        "name": "Haiyang Liu"
                    },
                    {
                        "name": "Siyu Chen"
                    },
                    {
                        "name": "Ziyang Yan"
                    },
                    {
                        "name": "Chenyu Zhang"
                    },
                    {
                        "name": "Haotong Qin"
                    },
                    {
                        "name": "Michele Magno"
                    },
                    {
                        "name": "Yang Yang"
                    },
                    {
                        "name": "Zhen Lei"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Jingcai Guo"
                    },
                    {
                        "name": "Ling Shao"
                    },
                    {
                        "name": "Hao Tang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Tang"
                },
                "author": "Hao Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00850v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00850v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03205v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03205v1",
                "updated": "2024-12-04T10:44:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    10,
                    44,
                    50,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T10:44:50Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    10,
                    44,
                    50,
                    2,
                    339,
                    0
                ],
                "title": "U-MATH: A University-Level Benchmark for Evaluating Mathematical Skills\n  in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "U-MATH: A University-Level Benchmark for Evaluating Mathematical Skills\n  in LLMs"
                },
                "summary": "The current evaluation of mathematical skills in LLMs is limited, as existing\nbenchmarks are either relatively small, primarily focus on elementary and\nhigh-school problems, or lack diversity in topics. Additionally, the inclusion\nof visual elements in tasks remains largely under-explored.\n  To address these gaps, we introduce U-MATH, a novel benchmark of 1,100\nunpublished open-ended university-level problems sourced from teaching\nmaterials. It is balanced across six core subjects, with 20% of multimodal\nproblems. Given the open-ended nature of U-MATH problems, we employ an LLM to\njudge the correctness of generated solutions. To this end, we release\n$\\mu$-MATH, a dataset to evaluate the LLMs' capabilities in judging solutions.\n  The evaluation of general domain, math-specific, and multimodal LLMs\nhighlights the challenges presented by U-MATH. Our findings reveal that LLMs\nachieve a maximum accuracy of only 63% on text-based tasks, with even lower 45%\non visual problems. The solution assessment proves challenging for LLMs, with\nthe best LLM judge having an F1-score of 80% on $\\mu$-MATH.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current evaluation of mathematical skills in LLMs is limited, as existing\nbenchmarks are either relatively small, primarily focus on elementary and\nhigh-school problems, or lack diversity in topics. Additionally, the inclusion\nof visual elements in tasks remains largely under-explored.\n  To address these gaps, we introduce U-MATH, a novel benchmark of 1,100\nunpublished open-ended university-level problems sourced from teaching\nmaterials. It is balanced across six core subjects, with 20% of multimodal\nproblems. Given the open-ended nature of U-MATH problems, we employ an LLM to\njudge the correctness of generated solutions. To this end, we release\n$\\mu$-MATH, a dataset to evaluate the LLMs' capabilities in judging solutions.\n  The evaluation of general domain, math-specific, and multimodal LLMs\nhighlights the challenges presented by U-MATH. Our findings reveal that LLMs\nachieve a maximum accuracy of only 63% on text-based tasks, with even lower 45%\non visual problems. The solution assessment proves challenging for LLMs, with\nthe best LLM judge having an F1-score of 80% on $\\mu$-MATH."
                },
                "authors": [
                    {
                        "name": "Konstantin Chernyshev"
                    },
                    {
                        "name": "Vitaliy Polshkov"
                    },
                    {
                        "name": "Ekaterina Artemova"
                    },
                    {
                        "name": "Alex Myasnikov"
                    },
                    {
                        "name": "Vlad Stepanov"
                    },
                    {
                        "name": "Alexei Miasnikov"
                    },
                    {
                        "name": "Sergei Tilga"
                    }
                ],
                "author_detail": {
                    "name": "Sergei Tilga"
                },
                "author": "Sergei Tilga",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03205v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03205v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.07744v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.07744v2",
                "updated": "2024-12-04T10:39:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    10,
                    39,
                    30,
                    2,
                    339,
                    0
                ],
                "published": "2024-04-11T13:42:24Z",
                "published_parsed": [
                    2024,
                    4,
                    11,
                    13,
                    42,
                    24,
                    3,
                    102,
                    0
                ],
                "title": "Exploring the Current Star Formation Rate and Nebula Contribution of\n  Galaxies at z \\textless\\ 0.4 with FADO",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Current Star Formation Rate and Nebula Contribution of\n  Galaxies at z \\textless\\ 0.4 with FADO"
                },
                "summary": "The star formation rate (SFR) is a crucial astrophysical tracer for\nunderstanding the formation and evolution of galaxies, determining the\ninteraction between interstellar medium properties and star formation, thereby\ninferring the evolutionary laws of cosmic star formation history and cosmic\nenergy density. The mainstream approach to studying the stellar content in\ngalaxies relies on pure stellar population synthesis models. However, these\nmethods fail to account for the contamination of SFR caused by nebular gas\nradiation. Recent studies have indicated that neglecting nebular radiation\ncontamination appears non-negligible in galaxies with intense star-forming\nactivities and at relatively high redshifts, potentially leading to\noverestimating stellar masses. However, there is currently limited targeted\nresearch, particularly regarding galaxies at redshifts (z $<$ 0.4). In this\nwork, a total of 6511 star-formation galaxies (SFGs) are selected from the\nSDSS-DR18, and FADO fits their spectra. This tool can exclude nebular radiation\ncontributions in the spectral fitting. A tentative work is carried out to\nexplore the SFR of these galaxies. Our results show that the median value of\n$H_{\\alpha}$ obtained by FADO fitting is $\\sim$ 0.0146 dex higher than that\nobtained from the pure stellar population synthesis model {\\it qsofitmore}. It\nhas been demonstrated that the nebular emission shows a trend of increasing\nwith redshift. We also studied the impact on the star-forming main\nsequence(SFMS) at low to intermediate redshifts. By comparing two spectral\nfitting software packages, we found that although the contribution of nebular\nemission is minimal, it generally shows an increasing trend with redshift. We\nanticipate that by combining optical and near-infrared spectral data, the\ninfluence of nebulae may become more prominent in star-forming galaxies at\nhigher redshifts (e.g., up to z $\\sim$ 2).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The star formation rate (SFR) is a crucial astrophysical tracer for\nunderstanding the formation and evolution of galaxies, determining the\ninteraction between interstellar medium properties and star formation, thereby\ninferring the evolutionary laws of cosmic star formation history and cosmic\nenergy density. The mainstream approach to studying the stellar content in\ngalaxies relies on pure stellar population synthesis models. However, these\nmethods fail to account for the contamination of SFR caused by nebular gas\nradiation. Recent studies have indicated that neglecting nebular radiation\ncontamination appears non-negligible in galaxies with intense star-forming\nactivities and at relatively high redshifts, potentially leading to\noverestimating stellar masses. However, there is currently limited targeted\nresearch, particularly regarding galaxies at redshifts (z $<$ 0.4). In this\nwork, a total of 6511 star-formation galaxies (SFGs) are selected from the\nSDSS-DR18, and FADO fits their spectra. This tool can exclude nebular radiation\ncontributions in the spectral fitting. A tentative work is carried out to\nexplore the SFR of these galaxies. Our results show that the median value of\n$H_{\\alpha}$ obtained by FADO fitting is $\\sim$ 0.0146 dex higher than that\nobtained from the pure stellar population synthesis model {\\it qsofitmore}. It\nhas been demonstrated that the nebular emission shows a trend of increasing\nwith redshift. We also studied the impact on the star-forming main\nsequence(SFMS) at low to intermediate redshifts. By comparing two spectral\nfitting software packages, we found that although the contribution of nebular\nemission is minimal, it generally shows an increasing trend with redshift. We\nanticipate that by combining optical and near-infrared spectral data, the\ninfluence of nebulae may become more prominent in star-forming galaxies at\nhigher redshifts (e.g., up to z $\\sim$ 2)."
                },
                "authors": [
                    {
                        "name": "Yaosong Yu"
                    },
                    {
                        "name": "Qihang Chen"
                    },
                    {
                        "name": "Liang Jing"
                    }
                ],
                "author_detail": {
                    "name": "Liang Jing"
                },
                "author": "Liang Jing",
                "arxiv_comment": "14 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.07744v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.07744v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18053v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18053v3",
                "updated": "2024-12-04T10:35:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    10,
                    35,
                    51,
                    2,
                    339,
                    0
                ],
                "published": "2024-09-26T16:58:04Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    16,
                    58,
                    4,
                    3,
                    270,
                    0
                ],
                "title": "DualAD: Dual-Layer Planning for Reasoning in Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DualAD: Dual-Layer Planning for Reasoning in Autonomous Driving"
                },
                "summary": "We present a novel autonomous driving framework, DualAD, designed to imitate\nhuman reasoning during driving. DualAD comprises two layers: a rule-based\nmotion planner at the bottom layer that handles routine driving tasks requiring\nminimal reasoning, and an upper layer featuring a rule-based text encoder that\nconverts driving scenarios from absolute states into text description. This\ntext is then processed by a large language model (LLM) to make driving\ndecisions. The upper layer intervenes in the bottom layer's decisions when\npotential danger is detected, mimicking human reasoning in critical situations.\nClosed-loop experiments demonstrate that DualAD, using a zero-shot pre-trained\nmodel, significantly outperforms rule-based motion planners that lack reasoning\nabilities. Our experiments also highlight the effectiveness of the text\nencoder, which considerably enhances the model's scenario understanding.\nAdditionally, the integrated DualAD model improves with stronger LLMs,\nindicating the framework's potential for further enhancement. Code and\nbenchmarks are available at github.com/TUM-AVS/DualAD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel autonomous driving framework, DualAD, designed to imitate\nhuman reasoning during driving. DualAD comprises two layers: a rule-based\nmotion planner at the bottom layer that handles routine driving tasks requiring\nminimal reasoning, and an upper layer featuring a rule-based text encoder that\nconverts driving scenarios from absolute states into text description. This\ntext is then processed by a large language model (LLM) to make driving\ndecisions. The upper layer intervenes in the bottom layer's decisions when\npotential danger is detected, mimicking human reasoning in critical situations.\nClosed-loop experiments demonstrate that DualAD, using a zero-shot pre-trained\nmodel, significantly outperforms rule-based motion planners that lack reasoning\nabilities. Our experiments also highlight the effectiveness of the text\nencoder, which considerably enhances the model's scenario understanding.\nAdditionally, the integrated DualAD model improves with stronger LLMs,\nindicating the framework's potential for further enhancement. Code and\nbenchmarks are available at github.com/TUM-AVS/DualAD."
                },
                "authors": [
                    {
                        "name": "Dingrui Wang"
                    },
                    {
                        "name": "Marc Kaufeld"
                    },
                    {
                        "name": "Johannes Betz"
                    }
                ],
                "author_detail": {
                    "name": "Johannes Betz"
                },
                "author": "Johannes Betz",
                "arxiv_comment": "Autonomous Driving, Large Language Models (LLMs), Human Reasoning,\n  Critical Scenario",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18053v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18053v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.08004v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.08004v2",
                "updated": "2024-12-04T10:35:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    10,
                    35,
                    25,
                    2,
                    339,
                    0
                ],
                "published": "2024-03-12T18:12:50Z",
                "published_parsed": [
                    2024,
                    3,
                    12,
                    18,
                    12,
                    50,
                    1,
                    72,
                    0
                ],
                "title": "Leveraging LLMs for On-the-Fly Instruction Guided Image Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging LLMs for On-the-Fly Instruction Guided Image Editing"
                },
                "summary": "The combination of language processing and image processing keeps attracting\nincreased interest given recent impressive advances that leverage the combined\nstrengths of both domains of research. Among these advances, the task of\nediting an image on the basis solely of a natural language instruction stands\nout as a most challenging endeavour. While recent approaches for this task\nresort, in one way or other, to some form of preliminary preparation, training\nor fine-tuning, this paper explores a novel approach: We propose a\npreparation-free method that permits instruction-guided image editing on the\nfly. This approach is organized along three steps properly orchestrated that\nresort to image captioning and DDIM inversion, followed by obtaining the edit\ndirection embedding, followed by image editing proper. While dispensing with\npreliminary preparation, our approach demonstrates to be effective and\ncompetitive, outperforming recent, state of the art models for this task when\nevaluated on the MAGICBRUSH dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The combination of language processing and image processing keeps attracting\nincreased interest given recent impressive advances that leverage the combined\nstrengths of both domains of research. Among these advances, the task of\nediting an image on the basis solely of a natural language instruction stands\nout as a most challenging endeavour. While recent approaches for this task\nresort, in one way or other, to some form of preliminary preparation, training\nor fine-tuning, this paper explores a novel approach: We propose a\npreparation-free method that permits instruction-guided image editing on the\nfly. This approach is organized along three steps properly orchestrated that\nresort to image captioning and DDIM inversion, followed by obtaining the edit\ndirection embedding, followed by image editing proper. While dispensing with\npreliminary preparation, our approach demonstrates to be effective and\ncompetitive, outperforming recent, state of the art models for this task when\nevaluated on the MAGICBRUSH dataset."
                },
                "authors": [
                    {
                        "name": "Rodrigo Santos"
                    },
                    {
                        "name": "Joo Silva"
                    },
                    {
                        "name": "Antnio Branco"
                    }
                ],
                "author_detail": {
                    "name": "Antnio Branco"
                },
                "author": "Antnio Branco",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.08004v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.08004v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.06209v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.06209v3",
                "updated": "2024-12-04T10:33:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    10,
                    33,
                    18,
                    2,
                    339,
                    0
                ],
                "published": "2024-04-09T10:58:21Z",
                "published_parsed": [
                    2024,
                    4,
                    9,
                    10,
                    58,
                    21,
                    1,
                    100,
                    0
                ],
                "title": "Elephants Never Forget: Memorization and Learning of Tabular Data in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Elephants Never Forget: Memorization and Learning of Tabular Data in\n  Large Language Models"
                },
                "summary": "While many have shown how Large Language Models (LLMs) can be applied to a\ndiverse set of tasks, the critical issues of data contamination and\nmemorization are often glossed over. In this work, we address this concern for\ntabular data. Specifically, we introduce a variety of different techniques to\nassess whether a language model has seen a tabular dataset during training.\nThis investigation reveals that LLMs have memorized many popular tabular\ndatasets verbatim. We then compare the few-shot learning performance of LLMs on\ndatasets that were seen during training to the performance on datasets released\nafter training. We find that LLMs perform better on datasets seen during\ntraining, indicating that memorization leads to overfitting. At the same time,\nLLMs show non-trivial performance on novel datasets and are surprisingly robust\nto data transformations. We then investigate the in-context statistical\nlearning abilities of LLMs. While LLMs are significantly better than random at\nsolving statistical classification problems, the sample efficiency of few-shot\nlearning lags behind traditional statistical learning algorithms, especially as\nthe dimension of the problem increases. This suggests that much of the observed\nfew-shot performance on novel real-world datasets is due to the LLM's world\nknowledge. Overall, our results highlight the importance of testing whether an\nLLM has seen an evaluation dataset during pre-training. We release the\nhttps://github.com/interpretml/LLM-Tabular-Memorization-Checker Python package\nto test LLMs for memorization of tabular datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While many have shown how Large Language Models (LLMs) can be applied to a\ndiverse set of tasks, the critical issues of data contamination and\nmemorization are often glossed over. In this work, we address this concern for\ntabular data. Specifically, we introduce a variety of different techniques to\nassess whether a language model has seen a tabular dataset during training.\nThis investigation reveals that LLMs have memorized many popular tabular\ndatasets verbatim. We then compare the few-shot learning performance of LLMs on\ndatasets that were seen during training to the performance on datasets released\nafter training. We find that LLMs perform better on datasets seen during\ntraining, indicating that memorization leads to overfitting. At the same time,\nLLMs show non-trivial performance on novel datasets and are surprisingly robust\nto data transformations. We then investigate the in-context statistical\nlearning abilities of LLMs. While LLMs are significantly better than random at\nsolving statistical classification problems, the sample efficiency of few-shot\nlearning lags behind traditional statistical learning algorithms, especially as\nthe dimension of the problem increases. This suggests that much of the observed\nfew-shot performance on novel real-world datasets is due to the LLM's world\nknowledge. Overall, our results highlight the importance of testing whether an\nLLM has seen an evaluation dataset during pre-training. We release the\nhttps://github.com/interpretml/LLM-Tabular-Memorization-Checker Python package\nto test LLMs for memorization of tabular datasets."
                },
                "authors": [
                    {
                        "name": "Sebastian Bordt"
                    },
                    {
                        "name": "Harsha Nori"
                    },
                    {
                        "name": "Vanessa Rodrigues"
                    },
                    {
                        "name": "Besmira Nushi"
                    },
                    {
                        "name": "Rich Caruana"
                    }
                ],
                "author_detail": {
                    "name": "Rich Caruana"
                },
                "author": "Rich Caruana",
                "arxiv_comment": "COLM camera ready, fix typo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.06209v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.06209v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03194v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03194v1",
                "updated": "2024-12-04T10:29:33Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    10,
                    29,
                    33,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T10:29:33Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    10,
                    29,
                    33,
                    2,
                    339,
                    0
                ],
                "title": "Coupled Boundary Element and Finite Volume Methods for Modeling\n  Fluid-Induced Seismicity in Fault Networks within Low-Permeability Rocks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coupled Boundary Element and Finite Volume Methods for Modeling\n  Fluid-Induced Seismicity in Fault Networks within Low-Permeability Rocks"
                },
                "summary": "To better understand the mechanics of injection-induced seismicity, we\ndeveloped a two-dimensional numerical code to simulate both seismic and\naseismic slip on non-planar faults and fault networks driven by fluid diffusion\nalong permeable faults. Our approach integrates a boundary element method to\nmodel fault slip governed by rate-and-state friction with a finite volume\nmethod for simulating fluid diffusion along fault networks. We demonstrate the\nmethod's capabilities with two illustrative examples: (1) fluid injection\ninducing slow slip on a primary rough, rate-strengthening fault, which\nsubsequently triggers microseismicity on secondary, smaller faults, and (2)\nfluid injection on a single fault in a network of intersecting faults, leading\nto fluid diffusion and reactivation of slip throughout the network. In both\ncases, the simulated slow slip migrates more rapidly than the fluid pressure\ndiffusion front. The observed migration patterns of microseismicity in the\nfirst example and slow slip in the second example resemble diffusion processes\nbut involve diffusivity values that differ significantly from the fault\nhydraulic diffusivity. These results support the conclusion that the\nmicroseismicity front is not a direct proxy for the fluid diffusion front and\ncannot be used to directly infer hydraulic diffusivity, consistently with some\ndecametric scale in-situ experiments of fault activation under controlled\nconditions. This work highlights the importance of distinguishing between\nmechanical and hydrological processes in the analysis of induced seismicity,\nproviding a powerful tool for improving our understanding of fault behavior in\nresponse to fluid injection, in particular when a network of faults is\ninvolved.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To better understand the mechanics of injection-induced seismicity, we\ndeveloped a two-dimensional numerical code to simulate both seismic and\naseismic slip on non-planar faults and fault networks driven by fluid diffusion\nalong permeable faults. Our approach integrates a boundary element method to\nmodel fault slip governed by rate-and-state friction with a finite volume\nmethod for simulating fluid diffusion along fault networks. We demonstrate the\nmethod's capabilities with two illustrative examples: (1) fluid injection\ninducing slow slip on a primary rough, rate-strengthening fault, which\nsubsequently triggers microseismicity on secondary, smaller faults, and (2)\nfluid injection on a single fault in a network of intersecting faults, leading\nto fluid diffusion and reactivation of slip throughout the network. In both\ncases, the simulated slow slip migrates more rapidly than the fluid pressure\ndiffusion front. The observed migration patterns of microseismicity in the\nfirst example and slow slip in the second example resemble diffusion processes\nbut involve diffusivity values that differ significantly from the fault\nhydraulic diffusivity. These results support the conclusion that the\nmicroseismicity front is not a direct proxy for the fluid diffusion front and\ncannot be used to directly infer hydraulic diffusivity, consistently with some\ndecametric scale in-situ experiments of fault activation under controlled\nconditions. This work highlights the importance of distinguishing between\nmechanical and hydrological processes in the analysis of induced seismicity,\nproviding a powerful tool for improving our understanding of fault behavior in\nresponse to fluid injection, in particular when a network of faults is\ninvolved."
                },
                "authors": [
                    {
                        "name": "Pierre Romanet"
                    },
                    {
                        "name": "Marco Maria Scuderi"
                    },
                    {
                        "name": "Jean-Paul Ampuero"
                    },
                    {
                        "name": "Stephanie Chaillat"
                    },
                    {
                        "name": "Frederic Cappa"
                    }
                ],
                "author_detail": {
                    "name": "Frederic Cappa"
                },
                "author": "Frederic Cappa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03194v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03194v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.geo-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.geo-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2212.04400v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2212.04400v3",
                "updated": "2024-12-04T10:21:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    10,
                    21,
                    56,
                    2,
                    339,
                    0
                ],
                "published": "2022-12-08T16:57:52Z",
                "published_parsed": [
                    2022,
                    12,
                    8,
                    16,
                    57,
                    52,
                    3,
                    342,
                    0
                ],
                "title": "The Lifebelt Particle Filter for robust estimation from low-valued count\n  data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Lifebelt Particle Filter for robust estimation from low-valued count\n  data"
                },
                "summary": "Particle filtering methods can be applied to estimation problems in discrete\nspaces on bounded domains, to sample from and marginalise over unknown hidden\nstates. As in continuous settings, problems such as particle degradation can\narise: proposed particles can be incompatible with the data, lying in low\nprobability regions or outside the boundary constraints, and the discrete\nsystem could result in all particles having weights of zero. In this paper we\nintroduce the Lifebelt Particle Filter (LBPF), a novel method for robust\nlikelihood estimation in low-valued count problems. The LBPF combines a\nstandard particle filter with one (or more) lifebelt particles which, by\nconstruction, lie within the boundaries of the discrete random variables, and\ntherefore are compatible with the data. A mixture of resampled and\nnon-resampled particles allows for the preservation of the lifebelt particle,\nwhich, together with the remaining particle swarm, provides samples from the\nfiltering distribution, and can be used to generate unbiased estimates of the\nlikelihood. The main benefit of the LBPF is that only one or few, wisely\nchosen, particles are sufficient to prevent particle collapse. Differently from\nother methods, there is no need to increase the number of particles, and\ntherefore the computational effort, in regions of the parameter space that\ngenerate less likely hidden states. The LBPF can be used within a\npseudo-marginal scheme to draw inferences on static parameters, $\n\\boldsymbol{\\theta} $, governing the system. We address here the estimation of\na parameter governing probabilities of deaths and recoveries of hospitalised\npatients during an epidemic.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Particle filtering methods can be applied to estimation problems in discrete\nspaces on bounded domains, to sample from and marginalise over unknown hidden\nstates. As in continuous settings, problems such as particle degradation can\narise: proposed particles can be incompatible with the data, lying in low\nprobability regions or outside the boundary constraints, and the discrete\nsystem could result in all particles having weights of zero. In this paper we\nintroduce the Lifebelt Particle Filter (LBPF), a novel method for robust\nlikelihood estimation in low-valued count problems. The LBPF combines a\nstandard particle filter with one (or more) lifebelt particles which, by\nconstruction, lie within the boundaries of the discrete random variables, and\ntherefore are compatible with the data. A mixture of resampled and\nnon-resampled particles allows for the preservation of the lifebelt particle,\nwhich, together with the remaining particle swarm, provides samples from the\nfiltering distribution, and can be used to generate unbiased estimates of the\nlikelihood. The main benefit of the LBPF is that only one or few, wisely\nchosen, particles are sufficient to prevent particle collapse. Differently from\nother methods, there is no need to increase the number of particles, and\ntherefore the computational effort, in regions of the parameter space that\ngenerate less likely hidden states. The LBPF can be used within a\npseudo-marginal scheme to draw inferences on static parameters, $\n\\boldsymbol{\\theta} $, governing the system. We address here the estimation of\na parameter governing probabilities of deaths and recoveries of hospitalised\npatients during an epidemic."
                },
                "authors": [
                    {
                        "name": "Alice Corbella"
                    },
                    {
                        "name": "Trevelyan J. McKinley"
                    },
                    {
                        "name": "Paul J. Birrell"
                    },
                    {
                        "name": "Daniela De Angelis"
                    },
                    {
                        "name": "Anne M. Presanis"
                    },
                    {
                        "name": "Gareth O. Roberts"
                    },
                    {
                        "name": "Simon E. F. Spencer"
                    }
                ],
                "author_detail": {
                    "name": "Simon E. F. Spencer"
                },
                "author": "Simon E. F. Spencer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2212.04400v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2212.04400v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03187v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03187v1",
                "updated": "2024-12-04T10:15:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    10,
                    15,
                    12,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T10:15:12Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    10,
                    15,
                    12,
                    2,
                    339,
                    0
                ],
                "title": "Weighted-Reward Preference Optimization for Implicit Model Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Weighted-Reward Preference Optimization for Implicit Model Fusion"
                },
                "summary": "While fusing heterogeneous open-source LLMs with varying architectures and\nsizes can potentially integrate the strengths of different models, existing\nfusion methods face significant challenges, such as vocabulary alignment and\nmerging distribution matrices. These procedures are not only complex but also\nprone to introducing noise and errors. In this paper, we propose an implicit\nfusion method, Weighted-Reward Preference Optimization (WRPO), which leverages\npreference optimization between the source LLMs and the target LLM to transfer\ntheir capabilities effectively. WRPO eliminates the need for vocabulary\nalignment and matrix fusion and can be efficiently scaled to accommodate\nvarious LLMs. To address distributional deviations between the source and\ntarget LLMs, WRPO introduces a progressive adaptation strategy that gradually\nshifts reliance on preferred examples from the target LLM to the source LLMs.\nExtensive experiments on the MT-Bench, AlpacaEval-2, and Arena-Hard benchmarks\ndemonstrate that WRPO consistently outperforms existing knowledge fusion\nmethods and various fine-tuning baselines. When applied to LLaMA3-8B-Instruct\nas the target model, WRPO achieves a length-controlled win rate of 55.9%\nagainst GPT-4-Preview-1106 on AlpacaEval-2 and a win rate of 46.2% against\nGPT-4-0314 on Arena-Hard. Our code is available at\n\\url{https://github.com/SLIT-AI/WRPO}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While fusing heterogeneous open-source LLMs with varying architectures and\nsizes can potentially integrate the strengths of different models, existing\nfusion methods face significant challenges, such as vocabulary alignment and\nmerging distribution matrices. These procedures are not only complex but also\nprone to introducing noise and errors. In this paper, we propose an implicit\nfusion method, Weighted-Reward Preference Optimization (WRPO), which leverages\npreference optimization between the source LLMs and the target LLM to transfer\ntheir capabilities effectively. WRPO eliminates the need for vocabulary\nalignment and matrix fusion and can be efficiently scaled to accommodate\nvarious LLMs. To address distributional deviations between the source and\ntarget LLMs, WRPO introduces a progressive adaptation strategy that gradually\nshifts reliance on preferred examples from the target LLM to the source LLMs.\nExtensive experiments on the MT-Bench, AlpacaEval-2, and Arena-Hard benchmarks\ndemonstrate that WRPO consistently outperforms existing knowledge fusion\nmethods and various fine-tuning baselines. When applied to LLaMA3-8B-Instruct\nas the target model, WRPO achieves a length-controlled win rate of 55.9%\nagainst GPT-4-Preview-1106 on AlpacaEval-2 and a win rate of 46.2% against\nGPT-4-0314 on Arena-Hard. Our code is available at\n\\url{https://github.com/SLIT-AI/WRPO}."
                },
                "authors": [
                    {
                        "name": "Ziyi Yang"
                    },
                    {
                        "name": "Fanqi Wan"
                    },
                    {
                        "name": "Longguang Zhong"
                    },
                    {
                        "name": "Tianyuan Shi"
                    },
                    {
                        "name": "Xiaojun Quan"
                    }
                ],
                "author_detail": {
                    "name": "Xiaojun Quan"
                },
                "author": "Xiaojun Quan",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03187v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03187v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03179v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03179v1",
                "updated": "2024-12-04T10:05:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    10,
                    5,
                    47,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T10:05:47Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    10,
                    5,
                    47,
                    2,
                    339,
                    0
                ],
                "title": "Optimizing Dense Visual Predictions Through Multi-Task Coherence and\n  Prioritization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Dense Visual Predictions Through Multi-Task Coherence and\n  Prioritization"
                },
                "summary": "Multi-Task Learning (MTL) involves the concurrent training of multiple tasks,\noffering notable advantages for dense prediction tasks in computer vision. MTL\nnot only reduces training and inference time as opposed to having multiple\nsingle-task models, but also enhances task accuracy through the interaction of\nmultiple tasks. However, existing methods face limitations. They often rely on\nsuboptimal cross-task interactions, resulting in task-specific predictions with\npoor geometric and predictive coherence. In addition, many approaches use\ninadequate loss weighting strategies, which do not address the inherent\nvariability in task evolution during training. To overcome these challenges, we\npropose an advanced MTL model specifically designed for dense vision tasks. Our\nmodel leverages state-of-the-art vision transformers with task-specific\ndecoders. To enhance cross-task coherence, we introduce a trace-back method\nthat improves both cross-task geometric and predictive features. Furthermore,\nwe present a novel dynamic task balancing approach that projects task losses\nonto a common scale and prioritizes more challenging tasks during training.\nExtensive experiments demonstrate the superiority of our method, establishing\nnew state-of-the-art performance across two benchmark datasets. The code is\navailable at:https://github.com/Klodivio355/MT-CP",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Task Learning (MTL) involves the concurrent training of multiple tasks,\noffering notable advantages for dense prediction tasks in computer vision. MTL\nnot only reduces training and inference time as opposed to having multiple\nsingle-task models, but also enhances task accuracy through the interaction of\nmultiple tasks. However, existing methods face limitations. They often rely on\nsuboptimal cross-task interactions, resulting in task-specific predictions with\npoor geometric and predictive coherence. In addition, many approaches use\ninadequate loss weighting strategies, which do not address the inherent\nvariability in task evolution during training. To overcome these challenges, we\npropose an advanced MTL model specifically designed for dense vision tasks. Our\nmodel leverages state-of-the-art vision transformers with task-specific\ndecoders. To enhance cross-task coherence, we introduce a trace-back method\nthat improves both cross-task geometric and predictive features. Furthermore,\nwe present a novel dynamic task balancing approach that projects task losses\nonto a common scale and prioritizes more challenging tasks during training.\nExtensive experiments demonstrate the superiority of our method, establishing\nnew state-of-the-art performance across two benchmark datasets. The code is\navailable at:https://github.com/Klodivio355/MT-CP"
                },
                "authors": [
                    {
                        "name": "Maxime Fontana"
                    },
                    {
                        "name": "Michael Spratling"
                    },
                    {
                        "name": "Miaojing Shi"
                    }
                ],
                "author_detail": {
                    "name": "Miaojing Shi"
                },
                "author": "Miaojing Shi",
                "arxiv_comment": "Accepted by WACV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03179v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03179v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15017v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15017v4",
                "updated": "2024-12-04T09:54:59Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    9,
                    54,
                    59,
                    2,
                    339,
                    0
                ],
                "published": "2024-07-22T06:15:59Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    6,
                    15,
                    59,
                    0,
                    204,
                    0
                ],
                "title": "Knowledge Mechanisms in Large Language Models: A Survey and Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Mechanisms in Large Language Models: A Survey and Perspective"
                },
                "summary": "Understanding knowledge mechanisms in Large Language Models (LLMs) is crucial\nfor advancing towards trustworthy AGI. This paper reviews knowledge mechanism\nanalysis from a novel taxonomy including knowledge utilization and evolution.\nKnowledge utilization delves into the mechanism of memorization, comprehension\nand application, and creation. Knowledge evolution focuses on the dynamic\nprogression of knowledge within individual and group LLMs. Moreover, we discuss\nwhat knowledge LLMs have learned, the reasons for the fragility of parametric\nknowledge, and the potential dark knowledge (hypothesis) that will be\nchallenging to address. We hope this work can help understand knowledge in LLMs\nand provide insights for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding knowledge mechanisms in Large Language Models (LLMs) is crucial\nfor advancing towards trustworthy AGI. This paper reviews knowledge mechanism\nanalysis from a novel taxonomy including knowledge utilization and evolution.\nKnowledge utilization delves into the mechanism of memorization, comprehension\nand application, and creation. Knowledge evolution focuses on the dynamic\nprogression of knowledge within individual and group LLMs. Moreover, we discuss\nwhat knowledge LLMs have learned, the reasons for the fragility of parametric\nknowledge, and the potential dark knowledge (hypothesis) that will be\nchallenging to address. We hope this work can help understand knowledge in LLMs\nand provide insights for future research."
                },
                "authors": [
                    {
                        "name": "Mengru Wang"
                    },
                    {
                        "name": "Yunzhi Yao"
                    },
                    {
                        "name": "Ziwen Xu"
                    },
                    {
                        "name": "Shuofei Qiao"
                    },
                    {
                        "name": "Shumin Deng"
                    },
                    {
                        "name": "Peng Wang"
                    },
                    {
                        "name": "Xiang Chen"
                    },
                    {
                        "name": "Jia-Chen Gu"
                    },
                    {
                        "name": "Yong Jiang"
                    },
                    {
                        "name": "Pengjun Xie"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Huajun Chen"
                    },
                    {
                        "name": "Ningyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ningyu Zhang"
                },
                "author": "Ningyu Zhang",
                "arxiv_comment": "EMNLP 2024 Findings; 39 pages (v4)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15017v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15017v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01289v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01289v2",
                "updated": "2024-12-04T09:51:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    9,
                    51,
                    16,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-02T09:02:28Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    9,
                    2,
                    28,
                    0,
                    337,
                    0
                ],
                "title": "Enhancing Perception Capabilities of Multimodal LLMs with Training-Free\n  Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Perception Capabilities of Multimodal LLMs with Training-Free\n  Fusion"
                },
                "summary": "Multimodal LLMs (MLLMs) equip language models with visual capabilities by\naligning vision encoders with language models. Existing methods to enhance the\nvisual perception of MLLMs often involve designing more powerful vision\nencoders, which requires exploring a vast design space and re-aligning each\npotential encoder with the language model, resulting in prohibitively high\ntraining costs. In this paper, we introduce VisionFuse, a novel integration\nframework that efficiently utilizes multiple vision encoders from off-the-shelf\nMLLMs to enhance visual perception without requiring additional training. Our\napproach is motivated by the observation that different MLLMs tend to focus on\ndistinct regions given the same query and image. Moreover, we find that the\nfeature distributions of vision encoders within an MLLM family, a group of\nMLLMs sharing the same pretrained LLM, are highly aligned. Building on these\ninsights, VisionFuse enriches the visual context by concatenating the tokens\ngenerated by the vision encoders of selected MLLMs within a family. By merging\nthe parameters of language models from these MLLMs, VisionFuse allows a single\nlanguage model to align with various vision encoders, significantly reducing\ndeployment overhead. We conduct comprehensive evaluations across multiple\nmultimodal benchmarks using various MLLM combinations, demonstrating\nsubstantial improvements in multimodal tasks. Notably, when integrating\nMiniGemini-8B and SLIME-8B, VisionFuse achieves an average performance increase\nof over 4%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal LLMs (MLLMs) equip language models with visual capabilities by\naligning vision encoders with language models. Existing methods to enhance the\nvisual perception of MLLMs often involve designing more powerful vision\nencoders, which requires exploring a vast design space and re-aligning each\npotential encoder with the language model, resulting in prohibitively high\ntraining costs. In this paper, we introduce VisionFuse, a novel integration\nframework that efficiently utilizes multiple vision encoders from off-the-shelf\nMLLMs to enhance visual perception without requiring additional training. Our\napproach is motivated by the observation that different MLLMs tend to focus on\ndistinct regions given the same query and image. Moreover, we find that the\nfeature distributions of vision encoders within an MLLM family, a group of\nMLLMs sharing the same pretrained LLM, are highly aligned. Building on these\ninsights, VisionFuse enriches the visual context by concatenating the tokens\ngenerated by the vision encoders of selected MLLMs within a family. By merging\nthe parameters of language models from these MLLMs, VisionFuse allows a single\nlanguage model to align with various vision encoders, significantly reducing\ndeployment overhead. We conduct comprehensive evaluations across multiple\nmultimodal benchmarks using various MLLM combinations, demonstrating\nsubstantial improvements in multimodal tasks. Notably, when integrating\nMiniGemini-8B and SLIME-8B, VisionFuse achieves an average performance increase\nof over 4%."
                },
                "authors": [
                    {
                        "name": "Zhuokun Chen"
                    },
                    {
                        "name": "Jinwu Hu"
                    },
                    {
                        "name": "Zeshuai Deng"
                    },
                    {
                        "name": "Yufeng Wang"
                    },
                    {
                        "name": "Bohan Zhuang"
                    },
                    {
                        "name": "Mingkui Tan"
                    }
                ],
                "author_detail": {
                    "name": "Mingkui Tan"
                },
                "author": "Mingkui Tan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01289v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01289v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03162v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03162v1",
                "updated": "2024-12-04T09:39:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    9,
                    39,
                    56,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T09:39:56Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    9,
                    39,
                    56,
                    2,
                    339,
                    0
                ],
                "title": "LLM-Twin: A Generated-Persona Approach for Survey Pre-Testing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Twin: A Generated-Persona Approach for Survey Pre-Testing"
                },
                "summary": "Surveys are widely used in social sciences to understand human behavior, but\ntheir implementation often involves iterative adjustments that demand\nsignificant effort and resources. To this end, researchers have increasingly\nturned to large language models (LLMs) to simulate human behavior. While\nexisting studies have focused on distributional similarities, individual-level\ncomparisons remain underexplored. Building upon prior work, we investigate\nwhether providing LLMs with respondents' prior information can replicate both\nstatistical distributions and individual decision-making patterns using Partial\nLeast Squares Structural Equation Modeling (PLS-SEM), a well-established causal\nanalysis method. We also introduce the concept of the LLM-Twin, user personas\ngenerated by supplying respondent-specific information to the LLM. By comparing\nresponses generated by the LLM-Twin with actual individual survey responses, we\nassess its effectiveness in replicating individual-level outcomes. Our findings\nshow that: (1) PLS-SEM analysis shows LLM-generated responses align with human\nresponses, (2) LLMs, when provided with respondent-specific information, are\ncapable of reproducing individual human responses, and (3) LLM-Twin responses\nclosely follow human responses at the individual level. These findings\nhighlight the potential of LLMs as a complementary tool for pre-testing surveys\nand optimizing research design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surveys are widely used in social sciences to understand human behavior, but\ntheir implementation often involves iterative adjustments that demand\nsignificant effort and resources. To this end, researchers have increasingly\nturned to large language models (LLMs) to simulate human behavior. While\nexisting studies have focused on distributional similarities, individual-level\ncomparisons remain underexplored. Building upon prior work, we investigate\nwhether providing LLMs with respondents' prior information can replicate both\nstatistical distributions and individual decision-making patterns using Partial\nLeast Squares Structural Equation Modeling (PLS-SEM), a well-established causal\nanalysis method. We also introduce the concept of the LLM-Twin, user personas\ngenerated by supplying respondent-specific information to the LLM. By comparing\nresponses generated by the LLM-Twin with actual individual survey responses, we\nassess its effectiveness in replicating individual-level outcomes. Our findings\nshow that: (1) PLS-SEM analysis shows LLM-generated responses align with human\nresponses, (2) LLMs, when provided with respondent-specific information, are\ncapable of reproducing individual human responses, and (3) LLM-Twin responses\nclosely follow human responses at the individual level. These findings\nhighlight the potential of LLMs as a complementary tool for pre-testing surveys\nand optimizing research design."
                },
                "authors": [
                    {
                        "name": "Sunwoong Kim"
                    },
                    {
                        "name": "Jongho Jeong"
                    },
                    {
                        "name": "Jin Soo Han"
                    },
                    {
                        "name": "Donghyuk Shin"
                    }
                ],
                "author_detail": {
                    "name": "Donghyuk Shin"
                },
                "author": "Donghyuk Shin",
                "arxiv_comment": "11 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03162v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03162v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03160v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03160v1",
                "updated": "2024-12-04T09:38:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    9,
                    38,
                    11,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T09:38:11Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    9,
                    38,
                    11,
                    2,
                    339,
                    0
                ],
                "title": "Byte BPE Tokenization as an Inverse string Homomorphism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Byte BPE Tokenization as an Inverse string Homomorphism"
                },
                "summary": "Tokenization is an important preprocessing step in the training and inference\nof large language models (LLMs). While there has been extensive research on the\nexpressive power of the neural achitectures used in LLMs, the impact of\ntokenization has not been well understood. In this work, we demonstrate that\ntokenization, irrespective of the algorithm used, acts as an inverse\nhomomorphism between strings and tokens. This suggests that the character space\nof the source language and the token space of the tokenized language are\nhomomorphic, preserving the structural properties of the source language.\nAdditionally, we explore the concept of proper tokenization, which refers to an\nunambiguous tokenization returned from the tokenizer. Our analysis reveals that\nthe expressiveness of neural architectures in recognizing context-free\nlanguages is not affected by tokenization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tokenization is an important preprocessing step in the training and inference\nof large language models (LLMs). While there has been extensive research on the\nexpressive power of the neural achitectures used in LLMs, the impact of\ntokenization has not been well understood. In this work, we demonstrate that\ntokenization, irrespective of the algorithm used, acts as an inverse\nhomomorphism between strings and tokens. This suggests that the character space\nof the source language and the token space of the tokenized language are\nhomomorphic, preserving the structural properties of the source language.\nAdditionally, we explore the concept of proper tokenization, which refers to an\nunambiguous tokenization returned from the tokenizer. Our analysis reveals that\nthe expressiveness of neural architectures in recognizing context-free\nlanguages is not affected by tokenization."
                },
                "authors": [
                    {
                        "name": "Saibo Geng"
                    },
                    {
                        "name": "Sankalp Gambhir"
                    },
                    {
                        "name": "Chris Wendler"
                    },
                    {
                        "name": "Robert West"
                    }
                ],
                "author_detail": {
                    "name": "Robert West"
                },
                "author": "Robert West",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03160v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03160v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19330v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19330v2",
                "updated": "2024-12-04T09:27:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    9,
                    27,
                    25,
                    2,
                    339,
                    0
                ],
                "published": "2024-11-28T19:00:01Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    19,
                    0,
                    1,
                    3,
                    333,
                    0
                ],
                "title": "A direct measurement of the electron density turbulence parameter $C_1$\n  towards the magnetar XTE J1810-197",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A direct measurement of the electron density turbulence parameter $C_1$\n  towards the magnetar XTE J1810-197"
                },
                "summary": "We report the first, direct measurement of the electron density turbulence\nparameter $C_1$, enabled by 550-750 MHz observations with the upgraded Giant\nMetrewave Radio Telescope. The parameter $C_1$ depends on the power law index\nof the wavenumber spectrum of electron density inhomogeneities in the ionized\ninterstellar medium. Radio waves propagating through the inhomogeneous ionized\nmedium suffer multipath propagation, as a result of which the pulsed emission\nfrom a neutron star undergoes scatter broadening. Consequently, interference\nbetween the delayed copies of the scatter-broadened electric field manifests as\nscintillation. We measure a scintillation bandwidth $\\Delta\\nu_d=149\\pm3$ Hz as\nwell as a scatter-broadening timescale $\\tau_d=1.22\\pm0.09$ ms at 650 MHz\ntowards the magnetar XTE J1810-197, using which we estimate $C_1=1.14\\pm0.09$\ndirectly from the uncertainty relation. This is also the first reported direct\nmeasurement of a scintillation bandwidth of order 100 Hz. We describe the\nmethods employed to obtain these results and discuss their implications in\ngeneral, as well as for the magnetar XTE J1810-197. We also discuss how such,\neffectively in-situ, measurements of $C_1$ can aid in inferring the wavenumber\nspectrum power law index and hence quantitatively discriminate between the\nvarious possible scattering scenarios in the ionized medium.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report the first, direct measurement of the electron density turbulence\nparameter $C_1$, enabled by 550-750 MHz observations with the upgraded Giant\nMetrewave Radio Telescope. The parameter $C_1$ depends on the power law index\nof the wavenumber spectrum of electron density inhomogeneities in the ionized\ninterstellar medium. Radio waves propagating through the inhomogeneous ionized\nmedium suffer multipath propagation, as a result of which the pulsed emission\nfrom a neutron star undergoes scatter broadening. Consequently, interference\nbetween the delayed copies of the scatter-broadened electric field manifests as\nscintillation. We measure a scintillation bandwidth $\\Delta\\nu_d=149\\pm3$ Hz as\nwell as a scatter-broadening timescale $\\tau_d=1.22\\pm0.09$ ms at 650 MHz\ntowards the magnetar XTE J1810-197, using which we estimate $C_1=1.14\\pm0.09$\ndirectly from the uncertainty relation. This is also the first reported direct\nmeasurement of a scintillation bandwidth of order 100 Hz. We describe the\nmethods employed to obtain these results and discuss their implications in\ngeneral, as well as for the magnetar XTE J1810-197. We also discuss how such,\neffectively in-situ, measurements of $C_1$ can aid in inferring the wavenumber\nspectrum power law index and hence quantitatively discriminate between the\nvarious possible scattering scenarios in the ionized medium."
                },
                "authors": [
                    {
                        "name": "Visweshwar Ram Marthi"
                    },
                    {
                        "name": "Yogesh Maan"
                    }
                ],
                "author_detail": {
                    "name": "Yogesh Maan"
                },
                "author": "Yogesh Maan",
                "arxiv_comment": "9 pages, 3 figures, submitted to ApJL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19330v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19330v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00809v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00809v2",
                "updated": "2024-12-04T09:26:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    9,
                    26,
                    47,
                    2,
                    339,
                    0
                ],
                "published": "2024-10-23T16:16:15Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    16,
                    16,
                    15,
                    2,
                    297,
                    0
                ],
                "title": "Adaptive Dense Reward: Understanding the Gap Between Action and Reward\n  Space in Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Dense Reward: Understanding the Gap Between Action and Reward\n  Space in Alignment"
                },
                "summary": "Reinforcement Learning from Human Feedback (RLHF) has proven highly effective\nin aligning Large Language Models (LLMs) with human preferences. However, the\noriginal RLHF typically optimizes under an overall reward, which can lead to a\nsuboptimal learning process. This limitation stems from RLHF's lack of\nawareness regarding which specific tokens should be reinforced or suppressed.\nMoreover, conflicts in supervision can arise, for instance, when a chosen\nresponse includes erroneous tokens, while a rejected response contains accurate\nelements. To rectify these shortcomings, increasing dense reward methods, such\nas step-wise and token-wise RLHF, have been proposed. However, these existing\nmethods are limited to specific tasks (like mathematics). In this paper, we\npropose the ``Adaptive Message-wise RLHF'' method, which robustly applies to\nvarious tasks. By defining pivot tokens as key indicators, our approach\nadaptively identifies essential information and converts sequence-level\nsupervision into fine-grained, subsequence-level supervision. This aligns the\ndensity of rewards and action spaces more closely with the information density\nof the input. Experiments demonstrate that our method can be integrated into\nvarious training methods, significantly mitigating hallucinations and\ncatastrophic forgetting problems, while outperforming other methods on multiple\nevaluation metrics. Our method improves the success rate on adversarial samples\nby 10\\% compared to the sample-wise approach, and achieves a 1.3\\% improvement\non evaluation benchmarks such as MMLU, GSM8K, HumanEval, etc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning from Human Feedback (RLHF) has proven highly effective\nin aligning Large Language Models (LLMs) with human preferences. However, the\noriginal RLHF typically optimizes under an overall reward, which can lead to a\nsuboptimal learning process. This limitation stems from RLHF's lack of\nawareness regarding which specific tokens should be reinforced or suppressed.\nMoreover, conflicts in supervision can arise, for instance, when a chosen\nresponse includes erroneous tokens, while a rejected response contains accurate\nelements. To rectify these shortcomings, increasing dense reward methods, such\nas step-wise and token-wise RLHF, have been proposed. However, these existing\nmethods are limited to specific tasks (like mathematics). In this paper, we\npropose the ``Adaptive Message-wise RLHF'' method, which robustly applies to\nvarious tasks. By defining pivot tokens as key indicators, our approach\nadaptively identifies essential information and converts sequence-level\nsupervision into fine-grained, subsequence-level supervision. This aligns the\ndensity of rewards and action spaces more closely with the information density\nof the input. Experiments demonstrate that our method can be integrated into\nvarious training methods, significantly mitigating hallucinations and\ncatastrophic forgetting problems, while outperforming other methods on multiple\nevaluation metrics. Our method improves the success rate on adversarial samples\nby 10\\% compared to the sample-wise approach, and achieves a 1.3\\% improvement\non evaluation benchmarks such as MMLU, GSM8K, HumanEval, etc."
                },
                "authors": [
                    {
                        "name": "Yanshi Li"
                    },
                    {
                        "name": "Shaopan Xiong"
                    },
                    {
                        "name": "Gengru Chen"
                    },
                    {
                        "name": "Xiaoyang Li"
                    },
                    {
                        "name": "Yijia Luo"
                    },
                    {
                        "name": "Xingyao Zhang"
                    },
                    {
                        "name": "Yanhui Huang"
                    },
                    {
                        "name": "Xingyuan Bu"
                    },
                    {
                        "name": "Yingshui Tan"
                    },
                    {
                        "name": "Chun Yuan"
                    },
                    {
                        "name": "Jiamang Wang"
                    },
                    {
                        "name": "Wenbo Su"
                    },
                    {
                        "name": "Bo Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zheng"
                },
                "author": "Bo Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00809v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00809v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03151v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03151v1",
                "updated": "2024-12-04T09:18:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    9,
                    18,
                    54,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T09:18:54Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    9,
                    18,
                    54,
                    2,
                    339,
                    0
                ],
                "title": "Large Language Models show both individual and collective creativity\n  comparable to humans",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models show both individual and collective creativity\n  comparable to humans"
                },
                "summary": "Artificial intelligence has, so far, largely automated routine tasks, but\nwhat does it mean for the future of work if Large Language Models (LLMs) show\ncreativity comparable to humans? To measure the creativity of LLMs\nholistically, the current study uses 13 creative tasks spanning three domains.\nWe benchmark the LLMs against individual humans, and also take a novel approach\nby comparing them to the collective creativity of groups of humans. We find\nthat the best LLMs (Claude and GPT-4) rank in the 52nd percentile against\nhumans, and overall LLMs excel in divergent thinking and problem solving but\nlag in creative writing. When questioned 10 times, an LLM's collective\ncreativity is equivalent to 8-10 humans. When more responses are requested, two\nadditional responses of LLMs equal one extra human. Ultimately, LLMs, when\noptimally applied, may compete with a small group of humans in the future of\nwork.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial intelligence has, so far, largely automated routine tasks, but\nwhat does it mean for the future of work if Large Language Models (LLMs) show\ncreativity comparable to humans? To measure the creativity of LLMs\nholistically, the current study uses 13 creative tasks spanning three domains.\nWe benchmark the LLMs against individual humans, and also take a novel approach\nby comparing them to the collective creativity of groups of humans. We find\nthat the best LLMs (Claude and GPT-4) rank in the 52nd percentile against\nhumans, and overall LLMs excel in divergent thinking and problem solving but\nlag in creative writing. When questioned 10 times, an LLM's collective\ncreativity is equivalent to 8-10 humans. When more responses are requested, two\nadditional responses of LLMs equal one extra human. Ultimately, LLMs, when\noptimally applied, may compete with a small group of humans in the future of\nwork."
                },
                "authors": [
                    {
                        "name": "Luning Sun"
                    },
                    {
                        "name": "Yuzhuo Yuan"
                    },
                    {
                        "name": "Yuan Yao"
                    },
                    {
                        "name": "Yanyan Li"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Xing Xie"
                    },
                    {
                        "name": "Xiting Wang"
                    },
                    {
                        "name": "Fang Luo"
                    },
                    {
                        "name": "David Stillwell"
                    }
                ],
                "author_detail": {
                    "name": "David Stillwell"
                },
                "author": "David Stillwell",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03151v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03151v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03150v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03150v1",
                "updated": "2024-12-04T09:17:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    9,
                    17,
                    47,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T09:17:47Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    9,
                    17,
                    47,
                    2,
                    339,
                    0
                ],
                "title": "Appearance Matching Adapter for Exemplar-based Semantic Image Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Appearance Matching Adapter for Exemplar-based Semantic Image Synthesis"
                },
                "summary": "Exemplar-based semantic image synthesis aims to generate images aligned with\ngiven semantic content while preserving the appearance of an exemplar image.\nConventional structure-guidance models, such as ControlNet, are limited in that\nthey cannot directly utilize exemplar images as input, relying instead solely\non text prompts to control appearance. Recent tuning-free approaches address\nthis limitation by transferring local appearance from the exemplar image to the\nsynthesized image through implicit cross-image matching in the augmented\nself-attention mechanism of pre-trained diffusion models. However, these\nmethods face challenges when applied to content-rich scenes with significant\ngeometric deformations, such as driving scenes. In this paper, we propose the\nAppearance Matching Adapter (AM-Adapter), a learnable framework that enhances\ncross-image matching within augmented self-attention by incorporating semantic\ninformation from segmentation maps. To effectively disentangle generation and\nmatching processes, we adopt a stage-wise training approach. Initially, we\ntrain the structure-guidance and generation networks, followed by training the\nAM-Adapter while keeping the other networks frozen. During inference, we\nintroduce an automated exemplar retrieval method to efficiently select exemplar\nimage-segmentation pairs. Despite utilizing a limited number of learnable\nparameters, our method achieves state-of-the-art performance, excelling in both\nsemantic alignment preservation and local appearance fidelity. Extensive\nablation studies further validate our design choices. Code and pre-trained\nweights will be publicly available.: https://cvlab-kaist.github.io/AM-Adapter/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exemplar-based semantic image synthesis aims to generate images aligned with\ngiven semantic content while preserving the appearance of an exemplar image.\nConventional structure-guidance models, such as ControlNet, are limited in that\nthey cannot directly utilize exemplar images as input, relying instead solely\non text prompts to control appearance. Recent tuning-free approaches address\nthis limitation by transferring local appearance from the exemplar image to the\nsynthesized image through implicit cross-image matching in the augmented\nself-attention mechanism of pre-trained diffusion models. However, these\nmethods face challenges when applied to content-rich scenes with significant\ngeometric deformations, such as driving scenes. In this paper, we propose the\nAppearance Matching Adapter (AM-Adapter), a learnable framework that enhances\ncross-image matching within augmented self-attention by incorporating semantic\ninformation from segmentation maps. To effectively disentangle generation and\nmatching processes, we adopt a stage-wise training approach. Initially, we\ntrain the structure-guidance and generation networks, followed by training the\nAM-Adapter while keeping the other networks frozen. During inference, we\nintroduce an automated exemplar retrieval method to efficiently select exemplar\nimage-segmentation pairs. Despite utilizing a limited number of learnable\nparameters, our method achieves state-of-the-art performance, excelling in both\nsemantic alignment preservation and local appearance fidelity. Extensive\nablation studies further validate our design choices. Code and pre-trained\nweights will be publicly available.: https://cvlab-kaist.github.io/AM-Adapter/"
                },
                "authors": [
                    {
                        "name": "Siyoon Jin"
                    },
                    {
                        "name": "Jisu Nam"
                    },
                    {
                        "name": "Jiyoung Kim"
                    },
                    {
                        "name": "Dahyun Chung"
                    },
                    {
                        "name": "Yeong-Seok Kim"
                    },
                    {
                        "name": "Joonhyung Park"
                    },
                    {
                        "name": "Heonjeong Chu"
                    },
                    {
                        "name": "Seungryong Kim"
                    }
                ],
                "author_detail": {
                    "name": "Seungryong Kim"
                },
                "author": "Seungryong Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03150v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03150v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03148v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03148v1",
                "updated": "2024-12-04T09:14:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    9,
                    14,
                    56,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T09:14:56Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    9,
                    14,
                    56,
                    2,
                    339,
                    0
                ],
                "title": "Fine-Grained Behavior Simulation with Role-Playing Large Language Model\n  on Social Media",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Grained Behavior Simulation with Role-Playing Large Language Model\n  on Social Media"
                },
                "summary": "Large language models (LLMs) have demonstrated impressive capabilities in\nrole-playing tasks. However, there is limited research on whether LLMs can\naccurately simulate user behavior in real-world scenarios, such as social\nmedia. This requires models to effectively analyze a user's history and\nsimulate their role. In this paper, we introduce \\textbf{FineRob}, a novel\nfine-grained behavior simulation dataset. We collect the complete behavioral\nhistory of 1,866 distinct users across three social media platforms. Each\nbehavior is decomposed into three fine-grained elements: object, type, and\ncontent, resulting in 78.6k QA records. Based on FineRob, we identify two\ndominant reasoning patterns in LLMs' behavior simulation processes and propose\nthe \\textbf{OM-CoT} fine-tuning method to enhance the capability. Through\ncomprehensive experiments, we conduct an in-depth analysis of key factors of\nbehavior simulation and also demonstrate the effectiveness of OM-CoT\napproach\\footnote{Code and dataset are available at\n\\url{https://github.com/linkseed18612254945/FineRob}}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated impressive capabilities in\nrole-playing tasks. However, there is limited research on whether LLMs can\naccurately simulate user behavior in real-world scenarios, such as social\nmedia. This requires models to effectively analyze a user's history and\nsimulate their role. In this paper, we introduce \\textbf{FineRob}, a novel\nfine-grained behavior simulation dataset. We collect the complete behavioral\nhistory of 1,866 distinct users across three social media platforms. Each\nbehavior is decomposed into three fine-grained elements: object, type, and\ncontent, resulting in 78.6k QA records. Based on FineRob, we identify two\ndominant reasoning patterns in LLMs' behavior simulation processes and propose\nthe \\textbf{OM-CoT} fine-tuning method to enhance the capability. Through\ncomprehensive experiments, we conduct an in-depth analysis of key factors of\nbehavior simulation and also demonstrate the effectiveness of OM-CoT\napproach\\footnote{Code and dataset are available at\n\\url{https://github.com/linkseed18612254945/FineRob}}"
                },
                "authors": [
                    {
                        "name": "Kun Li"
                    },
                    {
                        "name": "Chenwei Dai"
                    },
                    {
                        "name": "Wei Zhou"
                    },
                    {
                        "name": "Songlin Hu"
                    }
                ],
                "author_detail": {
                    "name": "Songlin Hu"
                },
                "author": "Songlin Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03148v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03148v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03145v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03145v1",
                "updated": "2024-12-04T09:11:33Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    9,
                    11,
                    33,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T09:11:33Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    9,
                    11,
                    33,
                    2,
                    339,
                    0
                ],
                "title": "Topological Trajectory Classification and Landmark Inference on\n  Simplicial Complexes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Topological Trajectory Classification and Landmark Inference on\n  Simplicial Complexes"
                },
                "summary": "We consider the problem of classifying trajectories on a discrete or\ndiscretised 2-dimensional manifold modelled by a simplicial complex. Previous\nworks have proposed to project the trajectories into the harmonic eigenspace of\nthe Hodge Laplacian, and then cluster the resulting embeddings. However, if the\nconsidered space has vanishing homology (i.e., no \"holes\"), then the harmonic\nspace of the 1-Hodge Laplacian is trivial and thus the approach fails. Here we\npropose to view this issue akin to a sensor placement problem and present an\nalgorithm that aims to learn \"optimal holes\" to distinguish a set of given\ntrajectory classes. Specifically, given a set of labelled trajectories, which\nwe interpret as edge-flows on the underlying simplicial complex, we search for\n2-simplices whose deletion results in an optimal separation of the trajectory\nlabels according to the corresponding spectral embedding of the trajectories\ninto the harmonic space. Finally, we generalise this approach to the\nunsupervised setting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the problem of classifying trajectories on a discrete or\ndiscretised 2-dimensional manifold modelled by a simplicial complex. Previous\nworks have proposed to project the trajectories into the harmonic eigenspace of\nthe Hodge Laplacian, and then cluster the resulting embeddings. However, if the\nconsidered space has vanishing homology (i.e., no \"holes\"), then the harmonic\nspace of the 1-Hodge Laplacian is trivial and thus the approach fails. Here we\npropose to view this issue akin to a sensor placement problem and present an\nalgorithm that aims to learn \"optimal holes\" to distinguish a set of given\ntrajectory classes. Specifically, given a set of labelled trajectories, which\nwe interpret as edge-flows on the underlying simplicial complex, we search for\n2-simplices whose deletion results in an optimal separation of the trajectory\nlabels according to the corresponding spectral embedding of the trajectories\ninto the harmonic space. Finally, we generalise this approach to the\nunsupervised setting."
                },
                "authors": [
                    {
                        "name": "Vincent P. Grande"
                    },
                    {
                        "name": "Josef Hoppe"
                    },
                    {
                        "name": "Florian Frantzen"
                    },
                    {
                        "name": "Michael T. Schaub"
                    }
                ],
                "author_detail": {
                    "name": "Michael T. Schaub"
                },
                "author": "Michael T. Schaub",
                "arxiv_comment": "5 pages, 4 figures, Accepted at the 58th Annual Asilomar Conference\n  on Signals, Systems, and Computers 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03145v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03145v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03138v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03138v1",
                "updated": "2024-12-04T09:02:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    9,
                    2,
                    46,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T09:02:46Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    9,
                    2,
                    46,
                    2,
                    339,
                    0
                ],
                "title": "Optimal bounds on a tree inference algorithm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal bounds on a tree inference algorithm"
                },
                "summary": "This paper tightens the best known analysis of Hein's 1989 algorithm to infer\nthe topology of a weighted tree based on the lengths of paths between its\nleaves. It shows that the number of length queries required for a degree-$k$\ntree of $n$ leaves is $O(n k \\log_k n)$, which is the lower bound. It also\npresents a family of trees for which the performance is asymptotically better,\nand shows that no such family exists for a competing $O(n k \\log_k n)$\nalgorithm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper tightens the best known analysis of Hein's 1989 algorithm to infer\nthe topology of a weighted tree based on the lengths of paths between its\nleaves. It shows that the number of length queries required for a degree-$k$\ntree of $n$ leaves is $O(n k \\log_k n)$, which is the lower bound. It also\npresents a family of trees for which the performance is asymptotically better,\nand shows that no such family exists for a competing $O(n k \\log_k n)$\nalgorithm."
                },
                "authors": [
                    {
                        "name": "Jack Gardiner"
                    },
                    {
                        "name": "Lachlan L. H. Andrew"
                    },
                    {
                        "name": "Junhao Gan"
                    },
                    {
                        "name": "Jean Honorio"
                    },
                    {
                        "name": "Seeun William Umboh"
                    }
                ],
                "author_detail": {
                    "name": "Seeun William Umboh"
                },
                "author": "Seeun William Umboh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03138v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03138v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.16539v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.16539v4",
                "updated": "2024-12-04T08:56:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    56,
                    17,
                    2,
                    339,
                    0
                ],
                "published": "2024-03-25T08:31:14Z",
                "published_parsed": [
                    2024,
                    3,
                    25,
                    8,
                    31,
                    14,
                    0,
                    85,
                    0
                ],
                "title": "Data-Efficient 3D Visual Grounding via Order-Aware Referring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-Efficient 3D Visual Grounding via Order-Aware Referring"
                },
                "summary": "3D visual grounding aims to identify the target object within a 3D point\ncloud scene referred to by a natural language description. Previous works\nusually require significant data relating to point color and their descriptions\nto exploit the corresponding complicated verbo-visual relations. In our work,\nwe introduce Vigor, a novel Data-Efficient 3D Visual Grounding framework via\nOrder-aware Referring. Vigor leverages LLM to produce a desirable referential\norder from the input description for 3D visual grounding. With the proposed\nstacked object-referring blocks, the predicted anchor objects in the above\norder allow one to locate the target object progressively without supervision\non the identities of anchor objects or exact relations between anchor/target\nobjects. In addition, we present an order-aware warm-up training strategy,\nwhich augments referential orders for pre-training the visual grounding\nframework. This allows us to better capture the complex verbo-visual relations\nand benefit the desirable data-efficient learning scheme. Experimental results\non the NR3D and ScanRefer datasets demonstrate our superiority in low-resource\nscenarios. In particular, Vigor surpasses current state-of-the-art frameworks\nby 9.3% and 7.6% grounding accuracy under 1% data and 10% data settings on the\nNR3D dataset, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D visual grounding aims to identify the target object within a 3D point\ncloud scene referred to by a natural language description. Previous works\nusually require significant data relating to point color and their descriptions\nto exploit the corresponding complicated verbo-visual relations. In our work,\nwe introduce Vigor, a novel Data-Efficient 3D Visual Grounding framework via\nOrder-aware Referring. Vigor leverages LLM to produce a desirable referential\norder from the input description for 3D visual grounding. With the proposed\nstacked object-referring blocks, the predicted anchor objects in the above\norder allow one to locate the target object progressively without supervision\non the identities of anchor objects or exact relations between anchor/target\nobjects. In addition, we present an order-aware warm-up training strategy,\nwhich augments referential orders for pre-training the visual grounding\nframework. This allows us to better capture the complex verbo-visual relations\nand benefit the desirable data-efficient learning scheme. Experimental results\non the NR3D and ScanRefer datasets demonstrate our superiority in low-resource\nscenarios. In particular, Vigor surpasses current state-of-the-art frameworks\nby 9.3% and 7.6% grounding accuracy under 1% data and 10% data settings on the\nNR3D dataset, respectively."
                },
                "authors": [
                    {
                        "name": "Tung-Yu Wu"
                    },
                    {
                        "name": "Sheng-Yu Huang"
                    },
                    {
                        "name": "Yu-Chiang Frank Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Chiang Frank Wang"
                },
                "author": "Yu-Chiang Frank Wang",
                "arxiv_comment": "accepted to WACV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.16539v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.16539v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.09861v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.09861v2",
                "updated": "2024-12-04T08:52:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    52,
                    51,
                    2,
                    339,
                    0
                ],
                "published": "2024-02-15T10:29:06Z",
                "published_parsed": [
                    2024,
                    2,
                    15,
                    10,
                    29,
                    6,
                    3,
                    46,
                    0
                ],
                "title": "Testing mirror symmetry in the Universe with LIGO-Virgo black-hole\n  mergers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Testing mirror symmetry in the Universe with LIGO-Virgo black-hole\n  mergers"
                },
                "summary": "Certain precessing black-hole mergers produce gravitational waves with net\ncircular polarization, understood as an imbalance between right- and\nleft-handed amplitudes. According to the Cosmological Principle, such emission\nmust average to zero across all binary mergers in our Universe to preserve\nmirror-reflection symmetry at very large scales. We present a new independent\ngravitational-wave test of this hypothesis. Using a novel observable based on\nthe Chern-Pontryagin pseudo-scalar, we measure the emission of net circular\npolarization across 47 black-hole mergers recently analyzed by Islam et. al.\nwith a state-of-the art model for precessing black-hole mergers in General\nRelativity. The average value obtained is consistent with zero. Remarkably,\nhowever, we find that at least $82\\%$ of the analysed sources must have\nproduced net circular polarization. Of these, GW200129 shows strong evidence\nfor mirror asymmetry, with a Bayes Factor of 12.6 or, equivalently, $93.1\\%$\nprobability. We obtain consistent (although stronger) results of $97.5\\%$ and\n$94.3\\%$ respectively using public results on this event from Hannam et. al.\nand performing our own parameter inference. This finding further implies\nevidence of astrophysical sources that can spontaneously emit circularly\npolarized photons by quantum effects. Forthcoming black-hole merger detections\nwill enable stronger constraints on large-scale mirror asymmetry and the\nCosmological Principle.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Certain precessing black-hole mergers produce gravitational waves with net\ncircular polarization, understood as an imbalance between right- and\nleft-handed amplitudes. According to the Cosmological Principle, such emission\nmust average to zero across all binary mergers in our Universe to preserve\nmirror-reflection symmetry at very large scales. We present a new independent\ngravitational-wave test of this hypothesis. Using a novel observable based on\nthe Chern-Pontryagin pseudo-scalar, we measure the emission of net circular\npolarization across 47 black-hole mergers recently analyzed by Islam et. al.\nwith a state-of-the art model for precessing black-hole mergers in General\nRelativity. The average value obtained is consistent with zero. Remarkably,\nhowever, we find that at least $82\\%$ of the analysed sources must have\nproduced net circular polarization. Of these, GW200129 shows strong evidence\nfor mirror asymmetry, with a Bayes Factor of 12.6 or, equivalently, $93.1\\%$\nprobability. We obtain consistent (although stronger) results of $97.5\\%$ and\n$94.3\\%$ respectively using public results on this event from Hannam et. al.\nand performing our own parameter inference. This finding further implies\nevidence of astrophysical sources that can spontaneously emit circularly\npolarized photons by quantum effects. Forthcoming black-hole merger detections\nwill enable stronger constraints on large-scale mirror asymmetry and the\nCosmological Principle."
                },
                "authors": [
                    {
                        "name": "Juan Caldern Bustillo"
                    },
                    {
                        "name": "Adrian del Rio"
                    },
                    {
                        "name": "Nicolas Sanchis-Gual"
                    },
                    {
                        "name": "Koustav Chandra"
                    },
                    {
                        "name": "Samson H. W. Leong"
                    }
                ],
                "author_detail": {
                    "name": "Samson H. W. Leong"
                },
                "author": "Samson H. W. Leong",
                "arxiv_comment": "10 pages, 6 Figures, 3 Appendixes. Version accepted for publication\n  in Physical Review Letters",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.09861v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.09861v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03131v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03131v1",
                "updated": "2024-12-04T08:51:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    51,
                    23,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T08:51:23Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    51,
                    23,
                    2,
                    339,
                    0
                ],
                "title": "Unifying KV Cache Compression for Large Language Models with LeanKV",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying KV Cache Compression for Large Language Models with LeanKV"
                },
                "summary": "Large language models (LLMs) demonstrate exceptional performance but incur\nhigh serving costs due to substantial memory demands, with the key-value (KV)\ncache being a primary bottleneck. Existing KV cache compression methods,\nincluding quantization and pruning, struggle with limitations such as uniform\ntreatment of keys and values and static memory allocation across attention\nheads. To address these challenges, we introduce LeanKV, a unified KV cache\ncompression framework that enhances LLM serving efficiency without compromising\naccuracy through three innovations: (1) Hetero-KV quantization, which stores\nkeys at a higher precision than values to reflect their greater impact on\nattention computations; (2) per-head dynamic sparsity, which allocates memory\nbased on token importance per head and per request; and (3) unified KV\ncompression, integrating mixed-precision quantization and selective pruning to\nenable a smooth tradeoff between model accuracy and memory efficiency. To\nefficiently support these techniques, LeanKV introduces systems optimizations\nincluding unified paging and on-GPU parallel memory management. Implemented on\nvLLM, LeanKV compresses the KV cache by $3.0\\times$ to $5.0\\times$ without\naccuracy loss and up to $11.0\\times$ with under 5% accuracy loss, enhancing\nthroughput by $1.9\\times$ to $2.5\\times$, and up to $6.9\\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate exceptional performance but incur\nhigh serving costs due to substantial memory demands, with the key-value (KV)\ncache being a primary bottleneck. Existing KV cache compression methods,\nincluding quantization and pruning, struggle with limitations such as uniform\ntreatment of keys and values and static memory allocation across attention\nheads. To address these challenges, we introduce LeanKV, a unified KV cache\ncompression framework that enhances LLM serving efficiency without compromising\naccuracy through three innovations: (1) Hetero-KV quantization, which stores\nkeys at a higher precision than values to reflect their greater impact on\nattention computations; (2) per-head dynamic sparsity, which allocates memory\nbased on token importance per head and per request; and (3) unified KV\ncompression, integrating mixed-precision quantization and selective pruning to\nenable a smooth tradeoff between model accuracy and memory efficiency. To\nefficiently support these techniques, LeanKV introduces systems optimizations\nincluding unified paging and on-GPU parallel memory management. Implemented on\nvLLM, LeanKV compresses the KV cache by $3.0\\times$ to $5.0\\times$ without\naccuracy loss and up to $11.0\\times$ with under 5% accuracy loss, enhancing\nthroughput by $1.9\\times$ to $2.5\\times$, and up to $6.9\\times$."
                },
                "authors": [
                    {
                        "name": "Yanqi Zhang"
                    },
                    {
                        "name": "Yuwei Hu"
                    },
                    {
                        "name": "Runyuan Zhao"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03131v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03131v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07663v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07663v2",
                "updated": "2024-12-04T08:47:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    47,
                    23,
                    2,
                    339,
                    0
                ],
                "published": "2024-10-10T07:12:46Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    7,
                    12,
                    46,
                    3,
                    284,
                    0
                ],
                "title": "TDDSR: Single-Step Diffusion with Two Discriminators for Super\n  Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TDDSR: Single-Step Diffusion with Two Discriminators for Super\n  Resolution"
                },
                "summary": "Super-resolution methods are increasingly becoming popular for both\nreal-world and face-specific tasks. Many existing approaches, however, rely on\nsimplistic degradation models, which limits their ability to handle complex and\nunknown degradation patterns effectively. While diffusion-based\nsuper-resolution techniques have recently shown impressive results, they are\nstill constrained by the need for numerous inference steps. To address this, we\npropose TDDSR, an efficient single-step diffusion-based super-resolution\nmethod. Our method, distilled from a pre-trained teacher model and based on a\ndiffusion network, performs super-resolution in a single step. It integrates a\nlearnable diffusion-based downsampler to capture diverse degradation patterns\nand employs two discriminators, one for high-resolution and one for\nlow-resolution images, to enhance the overall performance. Experimental results\ndemonstrate its effectiveness across real-world and face-specific SR tasks,\nachieving performance beyond other state-of-the-art models and comparable to\nprevious diffusion methods with multiple sampling steps.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Super-resolution methods are increasingly becoming popular for both\nreal-world and face-specific tasks. Many existing approaches, however, rely on\nsimplistic degradation models, which limits their ability to handle complex and\nunknown degradation patterns effectively. While diffusion-based\nsuper-resolution techniques have recently shown impressive results, they are\nstill constrained by the need for numerous inference steps. To address this, we\npropose TDDSR, an efficient single-step diffusion-based super-resolution\nmethod. Our method, distilled from a pre-trained teacher model and based on a\ndiffusion network, performs super-resolution in a single step. It integrates a\nlearnable diffusion-based downsampler to capture diverse degradation patterns\nand employs two discriminators, one for high-resolution and one for\nlow-resolution images, to enhance the overall performance. Experimental results\ndemonstrate its effectiveness across real-world and face-specific SR tasks,\nachieving performance beyond other state-of-the-art models and comparable to\nprevious diffusion methods with multiple sampling steps."
                },
                "authors": [
                    {
                        "name": "Sohwi Kim"
                    },
                    {
                        "name": "Tae-Kyun Kim"
                    }
                ],
                "author_detail": {
                    "name": "Tae-Kyun Kim"
                },
                "author": "Tae-Kyun Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07663v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07663v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03126v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03126v1",
                "updated": "2024-12-04T08:43:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    43,
                    46,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T08:43:46Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    43,
                    46,
                    2,
                    339,
                    0
                ],
                "title": "Completing the Functional Approach in Object-Oriented Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Completing the Functional Approach in Object-Oriented Languages"
                },
                "summary": "Over the last two decades practically all object-oriented programming\nlanguages have introduced features that are well-known from functional\nprogramming languages. But many features that were introduced were fragmentary.\nIn Java-TX we address the latter features and propose a completion. Java-TX\n(i.e. Type eXtended) is a language based on Java. The predominant new features\nare global type inference and real function types for lambda expressions.\nGlobal type inference means that all type annotations can be omitted, and the\ncompiler infers them without losing the static type property. We introduce the\nfunction types in a similar fashion as in Scala but additionally integrated\nthem into the Java target-typing as proposed in the so-called strawman\napproach. In this paper, we provide an integrated presentation of all Java-TX\nfeatures. The focus is therby on the automatic inference of type parameters for\nclasses and their methods, and on the heterogeneous translation of function\ntypes, which permits the preservation of the argument and return types in\nbytecode.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Over the last two decades practically all object-oriented programming\nlanguages have introduced features that are well-known from functional\nprogramming languages. But many features that were introduced were fragmentary.\nIn Java-TX we address the latter features and propose a completion. Java-TX\n(i.e. Type eXtended) is a language based on Java. The predominant new features\nare global type inference and real function types for lambda expressions.\nGlobal type inference means that all type annotations can be omitted, and the\ncompiler infers them without losing the static type property. We introduce the\nfunction types in a similar fashion as in Scala but additionally integrated\nthem into the Java target-typing as proposed in the so-called strawman\napproach. In this paper, we provide an integrated presentation of all Java-TX\nfeatures. The focus is therby on the automatic inference of type parameters for\nclasses and their methods, and on the heterogeneous translation of function\ntypes, which permits the preservation of the argument and return types in\nbytecode."
                },
                "authors": [
                    {
                        "name": "Martin Pluemicke"
                    }
                ],
                "author_detail": {
                    "name": "Martin Pluemicke"
                },
                "author": "Martin Pluemicke",
                "arxiv_doi": "10.4204/EPTCS.413.4",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.4204/EPTCS.413.4",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.03126v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03126v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "In Proceedings PT 2024, arXiv:2412.01856",
                "arxiv_journal_ref": "EPTCS 413, 2024, pp. 43-56",
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.1.1; D.1.5; D.3.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03123v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03123v1",
                "updated": "2024-12-04T08:43:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    43,
                    12,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T08:43:12Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    43,
                    12,
                    2,
                    339,
                    0
                ],
                "title": "Robust Multi-bit Text Watermark with LLM-based Paraphrasers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Multi-bit Text Watermark with LLM-based Paraphrasers"
                },
                "summary": "We propose an imperceptible multi-bit text watermark embedded by paraphrasing\nwith LLMs. We fine-tune a pair of LLM paraphrasers that are designed to behave\ndifferently so that their paraphrasing difference reflected in the text\nsemantics can be identified by a trained decoder. To embed our multi-bit\nwatermark, we use two paraphrasers alternatively to encode the pre-defined\nbinary code at the sentence level. Then we use a text classifier as the decoder\nto decode each bit of the watermark. Through extensive experiments, we show\nthat our watermarks can achieve over 99.99\\% detection AUC with small (1.1B)\ntext paraphrasers while keeping the semantic information of the original\nsentence. More importantly, our pipeline is robust under word substitution and\nsentence paraphrasing perturbations and generalizes well to\nout-of-distributional data. We also show the stealthiness of our watermark with\nLLM-based evaluation. We open-source the code:\nhttps://github.com/xiaojunxu/multi-bit-text-watermark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose an imperceptible multi-bit text watermark embedded by paraphrasing\nwith LLMs. We fine-tune a pair of LLM paraphrasers that are designed to behave\ndifferently so that their paraphrasing difference reflected in the text\nsemantics can be identified by a trained decoder. To embed our multi-bit\nwatermark, we use two paraphrasers alternatively to encode the pre-defined\nbinary code at the sentence level. Then we use a text classifier as the decoder\nto decode each bit of the watermark. Through extensive experiments, we show\nthat our watermarks can achieve over 99.99\\% detection AUC with small (1.1B)\ntext paraphrasers while keeping the semantic information of the original\nsentence. More importantly, our pipeline is robust under word substitution and\nsentence paraphrasing perturbations and generalizes well to\nout-of-distributional data. We also show the stealthiness of our watermark with\nLLM-based evaluation. We open-source the code:\nhttps://github.com/xiaojunxu/multi-bit-text-watermark."
                },
                "authors": [
                    {
                        "name": "Xiaojun Xu"
                    },
                    {
                        "name": "Jinghan Jia"
                    },
                    {
                        "name": "Yuanshun Yao"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Hang Li"
                    }
                ],
                "author_detail": {
                    "name": "Hang Li"
                },
                "author": "Hang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03123v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03123v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.16473v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.16473v3",
                "updated": "2024-12-04T08:40:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    40,
                    41,
                    2,
                    339,
                    0
                ],
                "published": "2024-02-26T10:42:25Z",
                "published_parsed": [
                    2024,
                    2,
                    26,
                    10,
                    42,
                    25,
                    0,
                    57,
                    0
                ],
                "title": "DCVSMNet: Double Cost Volume Stereo Matching Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DCVSMNet: Double Cost Volume Stereo Matching Network"
                },
                "summary": "We introduce Double Cost Volume Stereo Matching Network(DCVSMNet) which is a\nnovel architecture characterised by by two small upper (group-wise) and lower\n(norm correlation) cost volumes. Each cost volume is processed separately, and\na coupling module is proposed to fuse the geometry information extracted from\nthe upper and lower cost volumes. DCVSMNet is a fast stereo matching network\nwith a 67 ms inference time and strong generalization ability which can produce\ncompetitive results compared to state-of-the-art methods. The results on\nseveral bench mark datasets show that DCVSMNet achieves better accuracy than\nmethods such as CGI-Stereo and BGNet at the cost of greater inference time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Double Cost Volume Stereo Matching Network(DCVSMNet) which is a\nnovel architecture characterised by by two small upper (group-wise) and lower\n(norm correlation) cost volumes. Each cost volume is processed separately, and\na coupling module is proposed to fuse the geometry information extracted from\nthe upper and lower cost volumes. DCVSMNet is a fast stereo matching network\nwith a 67 ms inference time and strong generalization ability which can produce\ncompetitive results compared to state-of-the-art methods. The results on\nseveral bench mark datasets show that DCVSMNet achieves better accuracy than\nmethods such as CGI-Stereo and BGNet at the cost of greater inference time."
                },
                "authors": [
                    {
                        "name": "Mahmoud Tahmasebi"
                    },
                    {
                        "name": "Saif Huq"
                    },
                    {
                        "name": "Kevin Meehan"
                    },
                    {
                        "name": "Marion McAfee"
                    }
                ],
                "author_detail": {
                    "name": "Marion McAfee"
                },
                "author": "Marion McAfee",
                "arxiv_doi": "10.1016/j.neucom.2024.129002",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.neucom.2024.129002",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2402.16473v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.16473v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "page 129002, 2024",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.04566v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.04566v4",
                "updated": "2024-12-04T08:36:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    36,
                    13,
                    2,
                    339,
                    0
                ],
                "published": "2024-04-06T09:27:04Z",
                "published_parsed": [
                    2024,
                    4,
                    6,
                    9,
                    27,
                    4,
                    5,
                    97,
                    0
                ],
                "title": "Efficient and Green Large Language Models for Software Engineering:\n  Literature Review, Vision, and the Road Ahead",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient and Green Large Language Models for Software Engineering:\n  Literature Review, Vision, and the Road Ahead"
                },
                "summary": "Large Language Models (LLMs) have recently shown remarkable capabilities in\nvarious software engineering tasks, spurring the rapid growth of the Large\nLanguage Models for Software Engineering (LLM4SE) area. However, limited\nattention has been paid to developing efficient LLM4SE techniques that demand\nminimal computational cost, time, and memory resources, as well as green LLM4SE\nsolutions that reduce energy consumption, water usage, and carbon emissions.\n  This paper aims to redirect the focus of the research community towards the\nefficiency and greenness of LLM4SE, while also sharing potential research\ndirections to achieve this goal. It commences with a brief overview of the\nsignificance of LLM4SE and highlights the need for efficient and green LLM4SE\nsolutions. Subsequently, the paper presents a vision for a future where\nefficient and green LLM4SE revolutionizes the LLM-based software engineering\ntool landscape, benefiting various stakeholders, including industry, individual\npractitioners, and society. The paper then delineates a roadmap for future\nresearch, outlining specific research paths and potential solutions for the\nresearch community to pursue. While not intended to be a definitive guide, the\npaper aims to inspire further progress, with the ultimate goal of establishing\nefficient and green LLM4SE as a central element in the future of software\nengineering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have recently shown remarkable capabilities in\nvarious software engineering tasks, spurring the rapid growth of the Large\nLanguage Models for Software Engineering (LLM4SE) area. However, limited\nattention has been paid to developing efficient LLM4SE techniques that demand\nminimal computational cost, time, and memory resources, as well as green LLM4SE\nsolutions that reduce energy consumption, water usage, and carbon emissions.\n  This paper aims to redirect the focus of the research community towards the\nefficiency and greenness of LLM4SE, while also sharing potential research\ndirections to achieve this goal. It commences with a brief overview of the\nsignificance of LLM4SE and highlights the need for efficient and green LLM4SE\nsolutions. Subsequently, the paper presents a vision for a future where\nefficient and green LLM4SE revolutionizes the LLM-based software engineering\ntool landscape, benefiting various stakeholders, including industry, individual\npractitioners, and society. The paper then delineates a roadmap for future\nresearch, outlining specific research paths and potential solutions for the\nresearch community to pursue. While not intended to be a definitive guide, the\npaper aims to inspire further progress, with the ultimate goal of establishing\nefficient and green LLM4SE as a central element in the future of software\nengineering."
                },
                "authors": [
                    {
                        "name": "Jieke Shi"
                    },
                    {
                        "name": "Zhou Yang"
                    },
                    {
                        "name": "David Lo"
                    }
                ],
                "author_detail": {
                    "name": "David Lo"
                },
                "author": "David Lo",
                "arxiv_comment": "Accepted by ACM Transactions on Software Engineering and Methodology\n  (TOSEM), 23 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.04566v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.04566v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22420v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22420v2",
                "updated": "2024-12-04T08:26:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    26,
                    26,
                    2,
                    339,
                    0
                ],
                "published": "2024-10-29T18:01:53Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    18,
                    1,
                    53,
                    1,
                    303,
                    0
                ],
                "title": "Inferring redshift and galaxy properties via a multi-task neural net\n  with probabilistic outputs: An application to simulated MOONS spectra",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring redshift and galaxy properties via a multi-task neural net\n  with probabilistic outputs: An application to simulated MOONS spectra"
                },
                "summary": "The era of large-scale astronomical surveys demands innovative approaches for\nrapid and accurate analysis of extensive spectral data, and a promising\ndirection in which to address this challenge is offered by machine learning.\nHere, we introduce a new pipeline, M-TOPnet (Multi-Task network Outputting\nProbabilities), which employs a convolutional neural network with residual\nlearning to simultaneously derive redshift and other key physical properties of\ngalaxies from their spectra. Our tool efficiently encodes spectral information\ninto a latent space, employing distinct downstream branches for each physical\nquantity, thereby benefiting from multi-task learning. Notably, our method\nhandles the redshift output as a probability distribution, allowing for a more\nrefined and robust estimation of this critical parameter. We demonstrate\npreliminary results using simulated data from the MOONS instrument, which will\nsoon be operating at the ESO/VLT. We highlight the effectiveness of our tool in\naccurately predicting the redshift, stellar mass, and star formation rate of\ngalaxies at z>~1-3, even for faint sources (m_H ~ 24) for which traditional\nmethods often struggle. Through analysis of the output probability\ndistributions, we demonstrate that our pipeline enables robust quality\nscreening of the results, achieving accuracy rates of up to 99% in redshift\ndetermination (defined as predictions within |Delta_z| < 0.01 relative to the\ntrue redshift) with 8h exposure spectra, while automatically identifying\npotentially problematic cases. Our pipeline thus emerges as a powerful solution\nfor the upcoming challenges in observational astronomy, combining precision,\ninterpretability, and efficiency, all aspects that are crucial for analysing\nthe massive datasets expected from next-generation instruments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The era of large-scale astronomical surveys demands innovative approaches for\nrapid and accurate analysis of extensive spectral data, and a promising\ndirection in which to address this challenge is offered by machine learning.\nHere, we introduce a new pipeline, M-TOPnet (Multi-Task network Outputting\nProbabilities), which employs a convolutional neural network with residual\nlearning to simultaneously derive redshift and other key physical properties of\ngalaxies from their spectra. Our tool efficiently encodes spectral information\ninto a latent space, employing distinct downstream branches for each physical\nquantity, thereby benefiting from multi-task learning. Notably, our method\nhandles the redshift output as a probability distribution, allowing for a more\nrefined and robust estimation of this critical parameter. We demonstrate\npreliminary results using simulated data from the MOONS instrument, which will\nsoon be operating at the ESO/VLT. We highlight the effectiveness of our tool in\naccurately predicting the redshift, stellar mass, and star formation rate of\ngalaxies at z>~1-3, even for faint sources (m_H ~ 24) for which traditional\nmethods often struggle. Through analysis of the output probability\ndistributions, we demonstrate that our pipeline enables robust quality\nscreening of the results, achieving accuracy rates of up to 99% in redshift\ndetermination (defined as predictions within |Delta_z| < 0.01 relative to the\ntrue redshift) with 8h exposure spectra, while automatically identifying\npotentially problematic cases. Our pipeline thus emerges as a powerful solution\nfor the upcoming challenges in observational astronomy, combining precision,\ninterpretability, and efficiency, all aspects that are crucial for analysing\nthe massive datasets expected from next-generation instruments."
                },
                "authors": [
                    {
                        "name": "Michele Ginolfi"
                    },
                    {
                        "name": "Filippo Mannucci"
                    },
                    {
                        "name": "Francesco Belfiore"
                    },
                    {
                        "name": "Alessandro Marconi"
                    },
                    {
                        "name": "Nicholas Boardman"
                    },
                    {
                        "name": "Lucia Pozzetti"
                    },
                    {
                        "name": "Micol Bolzonella"
                    },
                    {
                        "name": "Enrico Di Teodoro"
                    },
                    {
                        "name": "Giovanni Cresci"
                    },
                    {
                        "name": "Vivienne Wild"
                    },
                    {
                        "name": "Myriam Rodrigues"
                    },
                    {
                        "name": "Roberto Maiolino"
                    },
                    {
                        "name": "Michele Cirasuolo"
                    },
                    {
                        "name": "Ernesto Oliva"
                    }
                ],
                "author_detail": {
                    "name": "Ernesto Oliva"
                },
                "author": "Ernesto Oliva",
                "arxiv_comment": "accepted by A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22420v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22420v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2412.03563v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03563v1",
                "updated": "2024-12-04T18:56:37Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    18,
                    56,
                    37,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T18:56:37Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    18,
                    56,
                    37,
                    2,
                    339,
                    0
                ],
                "title": "From Individual to Society: A Survey on Social Simulation Driven by\n  Large Language Model-based Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Individual to Society: A Survey on Social Simulation Driven by\n  Large Language Model-based Agents"
                },
                "summary": "Traditional sociological research often relies on human participation, which,\nthough effective, is expensive, challenging to scale, and with ethical\nconcerns. Recent advancements in large language models (LLMs) highlight their\npotential to simulate human behavior, enabling the replication of individual\nresponses and facilitating studies on many interdisciplinary studies. In this\npaper, we conduct a comprehensive survey of this field, illustrating the recent\nprogress in simulation driven by LLM-empowered agents. We categorize the\nsimulations into three types: (1) Individual Simulation, which mimics specific\nindividuals or demographic groups; (2) Scenario Simulation, where multiple\nagents collaborate to achieve goals within specific contexts; and (3) Society\nSimulation, which models interactions within agent societies to reflect the\ncomplexity and variety of real-world dynamics. These simulations follow a\nprogression, ranging from detailed individual modeling to large-scale societal\nphenomena. We provide a detailed discussion of each simulation type, including\nthe architecture or key components of the simulation, the classification of\nobjectives or scenarios and the evaluation method. Afterward, we summarize\ncommonly used datasets and benchmarks. Finally, we discuss the trends across\nthese three types of simulation. A repository for the related sources is at\n{\\url{https://github.com/FudanDISC/SocialAgent}}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional sociological research often relies on human participation, which,\nthough effective, is expensive, challenging to scale, and with ethical\nconcerns. Recent advancements in large language models (LLMs) highlight their\npotential to simulate human behavior, enabling the replication of individual\nresponses and facilitating studies on many interdisciplinary studies. In this\npaper, we conduct a comprehensive survey of this field, illustrating the recent\nprogress in simulation driven by LLM-empowered agents. We categorize the\nsimulations into three types: (1) Individual Simulation, which mimics specific\nindividuals or demographic groups; (2) Scenario Simulation, where multiple\nagents collaborate to achieve goals within specific contexts; and (3) Society\nSimulation, which models interactions within agent societies to reflect the\ncomplexity and variety of real-world dynamics. These simulations follow a\nprogression, ranging from detailed individual modeling to large-scale societal\nphenomena. We provide a detailed discussion of each simulation type, including\nthe architecture or key components of the simulation, the classification of\nobjectives or scenarios and the evaluation method. Afterward, we summarize\ncommonly used datasets and benchmarks. Finally, we discuss the trends across\nthese three types of simulation. A repository for the related sources is at\n{\\url{https://github.com/FudanDISC/SocialAgent}}."
                },
                "authors": [
                    {
                        "name": "Xinyi Mou"
                    },
                    {
                        "name": "Xuanwen Ding"
                    },
                    {
                        "name": "Qi He"
                    },
                    {
                        "name": "Liang Wang"
                    },
                    {
                        "name": "Jingcong Liang"
                    },
                    {
                        "name": "Xinnong Zhang"
                    },
                    {
                        "name": "Libo Sun"
                    },
                    {
                        "name": "Jiayu Lin"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Xuanjing Huang"
                    },
                    {
                        "name": "Zhongyu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Zhongyu Wei"
                },
                "author": "Zhongyu Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03563v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03563v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.10182v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.10182v3",
                "updated": "2024-12-04T18:48:28Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    18,
                    48,
                    28,
                    2,
                    339,
                    0
                ],
                "published": "2024-03-15T10:38:48Z",
                "published_parsed": [
                    2024,
                    3,
                    15,
                    10,
                    38,
                    48,
                    4,
                    75,
                    0
                ],
                "title": "Fast and reliable uncertainty quantification with neural network\n  ensembles for industrial image classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast and reliable uncertainty quantification with neural network\n  ensembles for industrial image classification"
                },
                "summary": "Image classification with neural networks (NNs) is widely used in industrial\nprocesses, situations where the model likely encounters unknown objects during\ndeployment, i.e., out-of-distribution (OOD) data. Worryingly, NNs tend to make\nconfident yet incorrect predictions when confronted with OOD data. To increase\nthe models' reliability, they should quantify the uncertainty in their own\npredictions, communicating when the output should (not) be trusted. Deep\nensembles, composed of multiple independent NNs, have been shown to perform\nstrongly but are computationally expensive. Recent research has proposed more\nefficient NN ensembles, namely the snapshot, batch, and multi-input\nmulti-output ensemble. This study investigates the predictive and uncertainty\nperformance of efficient NN ensembles in the context of image classification\nfor industrial processes. It is the first to provide a comprehensive comparison\nand it proposes a novel Diversity Quality metric to quantify the ensembles'\nperformance on the in-distribution and OOD sets in one single metric. The\nresults highlight the batch ensemble as a cost-effective and competitive\nalternative to the deep ensemble. It matches the deep ensemble in both\nuncertainty and accuracy while exhibiting considerable savings in training\ntime, test time, and memory storage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Image classification with neural networks (NNs) is widely used in industrial\nprocesses, situations where the model likely encounters unknown objects during\ndeployment, i.e., out-of-distribution (OOD) data. Worryingly, NNs tend to make\nconfident yet incorrect predictions when confronted with OOD data. To increase\nthe models' reliability, they should quantify the uncertainty in their own\npredictions, communicating when the output should (not) be trusted. Deep\nensembles, composed of multiple independent NNs, have been shown to perform\nstrongly but are computationally expensive. Recent research has proposed more\nefficient NN ensembles, namely the snapshot, batch, and multi-input\nmulti-output ensemble. This study investigates the predictive and uncertainty\nperformance of efficient NN ensembles in the context of image classification\nfor industrial processes. It is the first to provide a comprehensive comparison\nand it proposes a novel Diversity Quality metric to quantify the ensembles'\nperformance on the in-distribution and OOD sets in one single metric. The\nresults highlight the batch ensemble as a cost-effective and competitive\nalternative to the deep ensemble. It matches the deep ensemble in both\nuncertainty and accuracy while exhibiting considerable savings in training\ntime, test time, and memory storage."
                },
                "authors": [
                    {
                        "name": "Arthur Thuy"
                    },
                    {
                        "name": "Dries F. Benoit"
                    }
                ],
                "author_detail": {
                    "name": "Dries F. Benoit"
                },
                "author": "Dries F. Benoit",
                "arxiv_comment": "Submitted to Annals of Operations Research",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.10182v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.10182v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03550v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03550v1",
                "updated": "2024-12-04T18:47:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    18,
                    47,
                    11,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T18:47:11Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    18,
                    47,
                    11,
                    2,
                    339,
                    0
                ],
                "title": "Teaching an Old Dog New Tricks: Verifiable FHE Using Commodity Hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teaching an Old Dog New Tricks: Verifiable FHE Using Commodity Hardware"
                },
                "summary": "We present Argos, a simple approach for adding verifiability to fully\nhomomorphic encryption (FHE) schemes using trusted hardware. Traditional\napproaches to verifiable FHE require expensive cryptographic proofs, which\nincur an overhead of up to seven orders of magnitude on top of FHE, making them\nimpractical.\n  With Argos, we show that trusted hardware can be securely used to provide\nverifiability for FHE computations, with minimal overhead relative to the\nbaseline FHE computation. An important contribution of Argos is showing that\nthe major security pitfall associated with trusted hardware, microarchitectural\nside channels, can be completely mitigated by excluding any secrets from the\nCPU and the memory hierarchy. This is made possible by focusing on building a\nplatform that only enforces program and data integrity and not confidentiality\n(which is sufficient for verifiable FHE, since all data remain encrypted at all\ntimes). All secrets related to the attestation mechanism are kept in a separate\ncoprocessor (e.g., a TPM) inaccessible to any software-based attacker. Relying\non a discrete TPM typically incurs significant performance overhead, which is\nwhy (insecure) software-based TPMs are used in practice. As a second\ncontribution, we show that for FHE applications, the attestation protocol can\nbe adapted to only incur a fixed cost.\n  Argos requires no dedicated hardware extensions and is supported on commodity\nprocessors from 2008 onward. Our prototype implementation introduces 6%\noverhead to the FHE evaluation, and 8% for more complex protocols. In\nparticular, we show that Argos can be adapted for real-world applications of\nFHE, such as PIR and PSI. By demonstrating how to combine cryptography with\ntrusted hardware, Argos paves the way for widespread deployment of FHE-based\nprotocols beyond the semi-honest setting, without the overhead of cryptographic\nproofs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Argos, a simple approach for adding verifiability to fully\nhomomorphic encryption (FHE) schemes using trusted hardware. Traditional\napproaches to verifiable FHE require expensive cryptographic proofs, which\nincur an overhead of up to seven orders of magnitude on top of FHE, making them\nimpractical.\n  With Argos, we show that trusted hardware can be securely used to provide\nverifiability for FHE computations, with minimal overhead relative to the\nbaseline FHE computation. An important contribution of Argos is showing that\nthe major security pitfall associated with trusted hardware, microarchitectural\nside channels, can be completely mitigated by excluding any secrets from the\nCPU and the memory hierarchy. This is made possible by focusing on building a\nplatform that only enforces program and data integrity and not confidentiality\n(which is sufficient for verifiable FHE, since all data remain encrypted at all\ntimes). All secrets related to the attestation mechanism are kept in a separate\ncoprocessor (e.g., a TPM) inaccessible to any software-based attacker. Relying\non a discrete TPM typically incurs significant performance overhead, which is\nwhy (insecure) software-based TPMs are used in practice. As a second\ncontribution, we show that for FHE applications, the attestation protocol can\nbe adapted to only incur a fixed cost.\n  Argos requires no dedicated hardware extensions and is supported on commodity\nprocessors from 2008 onward. Our prototype implementation introduces 6%\noverhead to the FHE evaluation, and 8% for more complex protocols. In\nparticular, we show that Argos can be adapted for real-world applications of\nFHE, such as PIR and PSI. By demonstrating how to combine cryptography with\ntrusted hardware, Argos paves the way for widespread deployment of FHE-based\nprotocols beyond the semi-honest setting, without the overhead of cryptographic\nproofs."
                },
                "authors": [
                    {
                        "name": "Jules Drean"
                    },
                    {
                        "name": "Fisher Jepsen"
                    },
                    {
                        "name": "Edward Suh"
                    },
                    {
                        "name": "Srini Devadas"
                    },
                    {
                        "name": "Aamer Jaleel"
                    },
                    {
                        "name": "Gururaj Saileshwar"
                    }
                ],
                "author_detail": {
                    "name": "Gururaj Saileshwar"
                },
                "author": "Gururaj Saileshwar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03550v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03550v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19379v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19379v2",
                "updated": "2024-12-04T18:40:24Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    18,
                    40,
                    24,
                    2,
                    339,
                    0
                ],
                "published": "2024-11-28T21:10:20Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    21,
                    10,
                    20,
                    3,
                    333,
                    0
                ],
                "title": "Marconi: Prefix Caching for the Era of Hybrid LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Marconi: Prefix Caching for the Era of Hybrid LLMs"
                },
                "summary": "Hybrid models that combine the language modeling capabilities of Attention\nlayers with the efficiency of Recurrent layers (e.g., State Space Models) have\ngained traction in practically supporting long contexts in Large Language Model\nserving. Yet, the unique properties of these models complicate the usage of\ncomplementary efficiency optimizations such as prefix caching that skip\nredundant computations across requests. Most notably, their use of in-place\nstate updates for recurrent layers precludes rolling back cache entries for\npartial sequence overlaps, and instead mandates only exact-match cache hits;\nthe effect is a deluge of (large) cache entries per sequence, most of which\nyield minimal reuse opportunities. We present Marconi, the first system that\nsupports efficient prefix caching with Hybrid LLMs. Key to Marconi are its\nnovel admission and eviction policies that more judiciously assess potential\ncache entries based not only on recency, but also on (1) forecasts of their\nreuse likelihood across a taxonomy of different hit scenarios, and (2) the\ncompute savings that hits deliver relative to memory footprints. Across diverse\nworkloads and Hybrid models, Marconi achieves up to 34.4$\\times$ higher token\nhit rates (71.1% or 617 ms lower TTFT) compared to state-of-the-art prefix\ncaching systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid models that combine the language modeling capabilities of Attention\nlayers with the efficiency of Recurrent layers (e.g., State Space Models) have\ngained traction in practically supporting long contexts in Large Language Model\nserving. Yet, the unique properties of these models complicate the usage of\ncomplementary efficiency optimizations such as prefix caching that skip\nredundant computations across requests. Most notably, their use of in-place\nstate updates for recurrent layers precludes rolling back cache entries for\npartial sequence overlaps, and instead mandates only exact-match cache hits;\nthe effect is a deluge of (large) cache entries per sequence, most of which\nyield minimal reuse opportunities. We present Marconi, the first system that\nsupports efficient prefix caching with Hybrid LLMs. Key to Marconi are its\nnovel admission and eviction policies that more judiciously assess potential\ncache entries based not only on recency, but also on (1) forecasts of their\nreuse likelihood across a taxonomy of different hit scenarios, and (2) the\ncompute savings that hits deliver relative to memory footprints. Across diverse\nworkloads and Hybrid models, Marconi achieves up to 34.4$\\times$ higher token\nhit rates (71.1% or 617 ms lower TTFT) compared to state-of-the-art prefix\ncaching systems."
                },
                "authors": [
                    {
                        "name": "Rui Pan"
                    },
                    {
                        "name": "Zhuang Wang"
                    },
                    {
                        "name": "Zhen Jia"
                    },
                    {
                        "name": "Can Karakus"
                    },
                    {
                        "name": "Luca Zancato"
                    },
                    {
                        "name": "Tri Dao"
                    },
                    {
                        "name": "Yida Wang"
                    },
                    {
                        "name": "Ravi Netravali"
                    }
                ],
                "author_detail": {
                    "name": "Ravi Netravali"
                },
                "author": "Ravi Netravali",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19379v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19379v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03537v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03537v1",
                "updated": "2024-12-04T18:32:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    18,
                    32,
                    42,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T18:32:42Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    18,
                    32,
                    42,
                    2,
                    339,
                    0
                ],
                "title": "Evaluating Gender Bias Transfer between Pre-trained and Prompt-Adapted\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Gender Bias Transfer between Pre-trained and Prompt-Adapted\n  Language Models"
                },
                "summary": "Large language models (LLMs) are increasingly being adapted to achieve\ntask-specificity for deployment in real-world decision systems. Several\nprevious works have investigated the bias transfer hypothesis (BTH) by studying\nthe effect of the fine-tuning adaptation strategy on model fairness to find\nthat fairness in pre-trained masked language models have limited effect on the\nfairness of models when adapted using fine-tuning. In this work, we expand the\nstudy of BTH to causal models under prompt adaptations, as prompting is an\naccessible, and compute-efficient way to deploy models in real-world systems.\nIn contrast to previous works, we establish that intrinsic biases in\npre-trained Mistral, Falcon and Llama models are strongly correlated (rho >=\n0.94) with biases when the same models are zero- and few-shot prompted, using a\npronoun co-reference resolution task. Further, we find that bias transfer\nremains strongly correlated even when LLMs are specifically prompted to exhibit\nfair or biased behavior (rho >= 0.92), and few-shot length and stereotypical\ncomposition are varied (rho >= 0.97). Our findings highlight the importance of\nensuring fairness in pre-trained LLMs, especially when they are later used to\nperform downstream tasks via prompt adaptation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly being adapted to achieve\ntask-specificity for deployment in real-world decision systems. Several\nprevious works have investigated the bias transfer hypothesis (BTH) by studying\nthe effect of the fine-tuning adaptation strategy on model fairness to find\nthat fairness in pre-trained masked language models have limited effect on the\nfairness of models when adapted using fine-tuning. In this work, we expand the\nstudy of BTH to causal models under prompt adaptations, as prompting is an\naccessible, and compute-efficient way to deploy models in real-world systems.\nIn contrast to previous works, we establish that intrinsic biases in\npre-trained Mistral, Falcon and Llama models are strongly correlated (rho >=\n0.94) with biases when the same models are zero- and few-shot prompted, using a\npronoun co-reference resolution task. Further, we find that bias transfer\nremains strongly correlated even when LLMs are specifically prompted to exhibit\nfair or biased behavior (rho >= 0.92), and few-shot length and stereotypical\ncomposition are varied (rho >= 0.97). Our findings highlight the importance of\nensuring fairness in pre-trained LLMs, especially when they are later used to\nperform downstream tasks via prompt adaptation."
                },
                "authors": [
                    {
                        "name": "Natalie Mackraz"
                    },
                    {
                        "name": "Nivedha Sivakumar"
                    },
                    {
                        "name": "Samira Khorshidi"
                    },
                    {
                        "name": "Krishna Patel"
                    },
                    {
                        "name": "Barry-John Theobald"
                    },
                    {
                        "name": "Luca Zappella"
                    },
                    {
                        "name": "Nicholas Apostoloff"
                    }
                ],
                "author_detail": {
                    "name": "Nicholas Apostoloff"
                },
                "author": "Nicholas Apostoloff",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03537v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03537v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03531v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03531v1",
                "updated": "2024-12-04T18:26:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    18,
                    26,
                    13,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T18:26:13Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    18,
                    26,
                    13,
                    2,
                    339,
                    0
                ],
                "title": "A Review on Scientific Knowledge Extraction using Large Language Models\n  in Biomedical Sciences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Review on Scientific Knowledge Extraction using Large Language Models\n  in Biomedical Sciences"
                },
                "summary": "The rapid advancement of large language models (LLMs) has opened new\nboundaries in the extraction and synthesis of medical knowledge, particularly\nwithin evidence synthesis. This paper reviews the state-of-the-art applications\nof LLMs in the biomedical domain, exploring their effectiveness in automating\ncomplex tasks such as evidence synthesis and data extraction from a biomedical\ncorpus of documents. While LLMs demonstrate remarkable potential, significant\nchallenges remain, including issues related to hallucinations, contextual\nunderstanding, and the ability to generalize across diverse medical tasks. We\nhighlight critical gaps in the current research literature, particularly the\nneed for unified benchmarks to standardize evaluations and ensure reliability\nin real-world applications. In addition, we propose directions for future\nresearch, emphasizing the integration of state-of-the-art techniques such as\nretrieval-augmented generation (RAG) to enhance LLM performance in evidence\nsynthesis. By addressing these challenges and utilizing the strengths of LLMs,\nwe aim to improve access to medical literature and facilitate meaningful\ndiscoveries in healthcare.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) has opened new\nboundaries in the extraction and synthesis of medical knowledge, particularly\nwithin evidence synthesis. This paper reviews the state-of-the-art applications\nof LLMs in the biomedical domain, exploring their effectiveness in automating\ncomplex tasks such as evidence synthesis and data extraction from a biomedical\ncorpus of documents. While LLMs demonstrate remarkable potential, significant\nchallenges remain, including issues related to hallucinations, contextual\nunderstanding, and the ability to generalize across diverse medical tasks. We\nhighlight critical gaps in the current research literature, particularly the\nneed for unified benchmarks to standardize evaluations and ensure reliability\nin real-world applications. In addition, we propose directions for future\nresearch, emphasizing the integration of state-of-the-art techniques such as\nretrieval-augmented generation (RAG) to enhance LLM performance in evidence\nsynthesis. By addressing these challenges and utilizing the strengths of LLMs,\nwe aim to improve access to medical literature and facilitate meaningful\ndiscoveries in healthcare."
                },
                "authors": [
                    {
                        "name": "Gabriel Lino Garcia"
                    },
                    {
                        "name": "Joo Renato Ribeiro Manesco"
                    },
                    {
                        "name": "Pedro Henrique Paiola"
                    },
                    {
                        "name": "Lucas Miranda"
                    },
                    {
                        "name": "Maria Paola de Salvo"
                    },
                    {
                        "name": "Joo Paulo Papa"
                    }
                ],
                "author_detail": {
                    "name": "Joo Paulo Papa"
                },
                "author": "Joo Paulo Papa",
                "arxiv_comment": "9 pages, 1 table, 1 figure, conference paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03531v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03531v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06304v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06304v4",
                "updated": "2024-12-04T18:14:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    18,
                    14,
                    31,
                    2,
                    339,
                    0
                ],
                "published": "2024-08-12T17:17:16Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    17,
                    17,
                    16,
                    0,
                    225,
                    0
                ],
                "title": "Control-Flow Attestation: Concepts, Solutions, and Open Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Control-Flow Attestation: Concepts, Solutions, and Open Challenges"
                },
                "summary": "Control-flow attestation unifies the worlds of control-flow integrity and\nplatform attestation by measuring and reporting a target's run-time behaviour\nto a verifier. Trust assurances in the target are provided by testing whether\nits execution follows an authorised control-flow path. The problem has been\nexplored in various settings, such as assessing the trustworthiness of cloud\nplatforms, cyber-physical systems, and Internet of Things devices. Despite a\nsignificant number of proposals being made in recent years, the area remains\nfragmented, with different adversarial behaviours, verification paradigms, and\ndeployment challenges being addressed. In this paper, we present the first\nsurvey of control-flow attestation, examining the core ideas and solutions in\nstate-of-the-art schemes. In total, we survey over 30 papers published between\n2016--2024, consolidate and compare their key features, and pose several\nchallenges and recommendations for future research in the area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Control-flow attestation unifies the worlds of control-flow integrity and\nplatform attestation by measuring and reporting a target's run-time behaviour\nto a verifier. Trust assurances in the target are provided by testing whether\nits execution follows an authorised control-flow path. The problem has been\nexplored in various settings, such as assessing the trustworthiness of cloud\nplatforms, cyber-physical systems, and Internet of Things devices. Despite a\nsignificant number of proposals being made in recent years, the area remains\nfragmented, with different adversarial behaviours, verification paradigms, and\ndeployment challenges being addressed. In this paper, we present the first\nsurvey of control-flow attestation, examining the core ideas and solutions in\nstate-of-the-art schemes. In total, we survey over 30 papers published between\n2016--2024, consolidate and compare their key features, and pose several\nchallenges and recommendations for future research in the area."
                },
                "authors": [
                    {
                        "name": "Zhanyu Sha"
                    },
                    {
                        "name": "Carlton Shepherd"
                    },
                    {
                        "name": "Amir Rafi"
                    },
                    {
                        "name": "Konstantinos Markantonakis"
                    }
                ],
                "author_detail": {
                    "name": "Konstantinos Markantonakis"
                },
                "author": "Konstantinos Markantonakis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06304v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06304v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03516v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03516v1",
                "updated": "2024-12-04T17:57:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    17,
                    57,
                    39,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T17:57:39Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    17,
                    57,
                    39,
                    2,
                    339,
                    0
                ],
                "title": "You're (Not) My Type -- Can LLMs Generate Feedback of Specific Types for\n  Introductory Programming Tasks?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "You're (Not) My Type -- Can LLMs Generate Feedback of Specific Types for\n  Introductory Programming Tasks?"
                },
                "summary": "Background: Feedback as one of the most influential factors for learning has\nbeen subject to a great body of research. It plays a key role in the\ndevelopment of educational technology systems and is traditionally rooted in\ndeterministic feedback defined by experts and their experience. However, with\nthe rise of generative AI and especially Large Language Models (LLMs), we\nexpect feedback as part of learning systems to transform, especially for the\ncontext of programming. In the past, it was challenging to automate feedback\nfor learners of programming. LLMs may create new possibilities to provide\nricher, and more individual feedback than ever before.\n  Objectives: This paper aims to generate specific types of feedback for\nintroductory programming tasks using LLMs. We revisit existing feedback\ntaxonomies to capture the specifics of the generated feedback, such as\nrandomness, uncertainty, and degrees of variation.\n  Methods: We iteratively designed prompts for the generation of specific\nfeedback types (as part of existing feedback taxonomies) in response to\nauthentic student programs. We then evaluated the generated output and\ndetermined to what extent it reflected certain feedback types.\n  Results and Conclusion: The present work provides a better understanding of\ndifferent feedback dimensions and characteristics. The results have\nimplications for future feedback research with regard to, for example, feedback\neffects and learners' informational needs. It further provides a basis for the\ndevelopment of new tools and learning systems for novice programmers including\nfeedback generated by AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background: Feedback as one of the most influential factors for learning has\nbeen subject to a great body of research. It plays a key role in the\ndevelopment of educational technology systems and is traditionally rooted in\ndeterministic feedback defined by experts and their experience. However, with\nthe rise of generative AI and especially Large Language Models (LLMs), we\nexpect feedback as part of learning systems to transform, especially for the\ncontext of programming. In the past, it was challenging to automate feedback\nfor learners of programming. LLMs may create new possibilities to provide\nricher, and more individual feedback than ever before.\n  Objectives: This paper aims to generate specific types of feedback for\nintroductory programming tasks using LLMs. We revisit existing feedback\ntaxonomies to capture the specifics of the generated feedback, such as\nrandomness, uncertainty, and degrees of variation.\n  Methods: We iteratively designed prompts for the generation of specific\nfeedback types (as part of existing feedback taxonomies) in response to\nauthentic student programs. We then evaluated the generated output and\ndetermined to what extent it reflected certain feedback types.\n  Results and Conclusion: The present work provides a better understanding of\ndifferent feedback dimensions and characteristics. The results have\nimplications for future feedback research with regard to, for example, feedback\neffects and learners' informational needs. It further provides a basis for the\ndevelopment of new tools and learning systems for novice programmers including\nfeedback generated by AI."
                },
                "authors": [
                    {
                        "name": "Dominic Lohr"
                    },
                    {
                        "name": "Hieke Keuning"
                    },
                    {
                        "name": "Natalie Kiesler"
                    }
                ],
                "author_detail": {
                    "name": "Natalie Kiesler"
                },
                "author": "Natalie Kiesler",
                "arxiv_comment": "Accepted at Journal of Computer Assisted Learning (2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03516v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03516v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.16860v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.16860v2",
                "updated": "2024-12-04T17:57:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    17,
                    57,
                    32,
                    2,
                    339,
                    0
                ],
                "published": "2024-06-24T17:59:42Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    17,
                    59,
                    42,
                    0,
                    176,
                    0
                ],
                "title": "Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs"
                },
                "summary": "We introduce Cambrian-1, a family of multimodal LLMs (MLLMs) designed with a\nvision-centric approach. While stronger language models can enhance multimodal\ncapabilities, the design choices for vision components are often insufficiently\nexplored and disconnected from visual representation learning research. This\ngap hinders accurate sensory grounding in real-world scenarios. Our study uses\nLLMs and visual instruction tuning as an interface to evaluate various visual\nrepresentations, offering new insights into different models and architectures\n-- self-supervised, strongly supervised, or combinations thereof -- based on\nexperiments with over 20 vision encoders. We critically examine existing MLLM\nbenchmarks, address the difficulties involved in consolidating and interpreting\nresults from various tasks, and introduce a new vision-centric benchmark,\nCV-Bench. To further improve visual grounding, we propose the Spatial Vision\nAggregator (SVA), a dynamic and spatially-aware connector that integrates\nhigh-resolution vision features with LLMs while reducing the number of tokens.\nAdditionally, we discuss the curation of high-quality visual instruction-tuning\ndata from publicly available sources, emphasizing the importance of data source\nbalancing and distribution ratio. Collectively, Cambrian-1 not only achieves\nstate-of-the-art performance but also serves as a comprehensive, open cookbook\nfor instruction-tuned MLLMs. We provide model weights, code, supporting tools,\ndatasets, and detailed instruction-tuning and evaluation recipes. We hope our\nrelease will inspire and accelerate advancements in multimodal systems and\nvisual representation learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Cambrian-1, a family of multimodal LLMs (MLLMs) designed with a\nvision-centric approach. While stronger language models can enhance multimodal\ncapabilities, the design choices for vision components are often insufficiently\nexplored and disconnected from visual representation learning research. This\ngap hinders accurate sensory grounding in real-world scenarios. Our study uses\nLLMs and visual instruction tuning as an interface to evaluate various visual\nrepresentations, offering new insights into different models and architectures\n-- self-supervised, strongly supervised, or combinations thereof -- based on\nexperiments with over 20 vision encoders. We critically examine existing MLLM\nbenchmarks, address the difficulties involved in consolidating and interpreting\nresults from various tasks, and introduce a new vision-centric benchmark,\nCV-Bench. To further improve visual grounding, we propose the Spatial Vision\nAggregator (SVA), a dynamic and spatially-aware connector that integrates\nhigh-resolution vision features with LLMs while reducing the number of tokens.\nAdditionally, we discuss the curation of high-quality visual instruction-tuning\ndata from publicly available sources, emphasizing the importance of data source\nbalancing and distribution ratio. Collectively, Cambrian-1 not only achieves\nstate-of-the-art performance but also serves as a comprehensive, open cookbook\nfor instruction-tuned MLLMs. We provide model weights, code, supporting tools,\ndatasets, and detailed instruction-tuning and evaluation recipes. We hope our\nrelease will inspire and accelerate advancements in multimodal systems and\nvisual representation learning."
                },
                "authors": [
                    {
                        "name": "Shengbang Tong"
                    },
                    {
                        "name": "Ellis Brown"
                    },
                    {
                        "name": "Penghao Wu"
                    },
                    {
                        "name": "Sanghyun Woo"
                    },
                    {
                        "name": "Manoj Middepogu"
                    },
                    {
                        "name": "Sai Charitha Akula"
                    },
                    {
                        "name": "Jihan Yang"
                    },
                    {
                        "name": "Shusheng Yang"
                    },
                    {
                        "name": "Adithya Iyer"
                    },
                    {
                        "name": "Xichen Pan"
                    },
                    {
                        "name": "Ziteng Wang"
                    },
                    {
                        "name": "Rob Fergus"
                    },
                    {
                        "name": "Yann LeCun"
                    },
                    {
                        "name": "Saining Xie"
                    }
                ],
                "author_detail": {
                    "name": "Saining Xie"
                },
                "author": "Saining Xie",
                "arxiv_comment": "NeurIPS 2024 (Oral). Website at https://cambrian-mllm.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.16860v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.16860v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11376v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11376v2",
                "updated": "2024-12-04T17:45:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    17,
                    45,
                    14,
                    2,
                    339,
                    0
                ],
                "published": "2024-09-17T17:23:44Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    17,
                    23,
                    44,
                    1,
                    261,
                    0
                ],
                "title": "Towards Time Series Reasoning with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Time Series Reasoning with LLMs"
                },
                "summary": "Multi-modal large language models (MLLMs) have enabled numerous advances in\nunderstanding and reasoning in domains like vision, but we have not yet seen\nthis broad success for time-series. Although prior works on time-series MLLMs\nhave shown promising performance in time-series forecasting, very few works\nshow how an LLM could be used for time-series reasoning in natural language. We\npropose a novel multi-modal time-series LLM approach that learns generalizable\ninformation across various domains with powerful zero-shot performance. First,\nwe train a lightweight time-series encoder on top of an LLM to directly extract\ntime-series information. Then, we fine-tune our model with chain-of-thought\naugmented time-series tasks to encourage the model to generate reasoning paths.\nWe show that our model learns a latent representation that reflects specific\ntime-series features (e.g. slope, frequency), as well as outperforming GPT-4o\non a set of zero-shot reasoning tasks on a variety of domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-modal large language models (MLLMs) have enabled numerous advances in\nunderstanding and reasoning in domains like vision, but we have not yet seen\nthis broad success for time-series. Although prior works on time-series MLLMs\nhave shown promising performance in time-series forecasting, very few works\nshow how an LLM could be used for time-series reasoning in natural language. We\npropose a novel multi-modal time-series LLM approach that learns generalizable\ninformation across various domains with powerful zero-shot performance. First,\nwe train a lightweight time-series encoder on top of an LLM to directly extract\ntime-series information. Then, we fine-tune our model with chain-of-thought\naugmented time-series tasks to encourage the model to generate reasoning paths.\nWe show that our model learns a latent representation that reflects specific\ntime-series features (e.g. slope, frequency), as well as outperforming GPT-4o\non a set of zero-shot reasoning tasks on a variety of domains."
                },
                "authors": [
                    {
                        "name": "Winnie Chow"
                    },
                    {
                        "name": "Lauren Gardiner"
                    },
                    {
                        "name": "Haraldur T. Hallgrmsson"
                    },
                    {
                        "name": "Maxwell A. Xu"
                    },
                    {
                        "name": "Shirley You Ren"
                    }
                ],
                "author_detail": {
                    "name": "Shirley You Ren"
                },
                "author": "Shirley You Ren",
                "arxiv_comment": "Oral Presentation at 2024 NeurIPS Workshop on Time Series in the Age\n  of Large Models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11376v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11376v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13928v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13928v2",
                "updated": "2024-12-04T17:03:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    17,
                    3,
                    13,
                    2,
                    339,
                    0
                ],
                "published": "2024-10-17T17:56:01Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    17,
                    56,
                    1,
                    3,
                    291,
                    0
                ],
                "title": "Automatically Interpreting Millions of Features in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatically Interpreting Millions of Features in Large Language Models"
                },
                "summary": "While the activations of neurons in deep neural networks usually do not have\na simple human-understandable interpretation, sparse autoencoders (SAEs) can be\nused to transform these activations into a higher-dimensional latent space\nwhich may be more easily interpretable. However, these SAEs can have millions\nof distinct latent features, making it infeasible for humans to manually\ninterpret each one. In this work, we build an open-source automated pipeline to\ngenerate and evaluate natural language explanations for SAE features using\nLLMs. We test our framework on SAEs of varying sizes, activation functions, and\nlosses, trained on two different open-weight LLMs. We introduce five new\ntechniques to score the quality of explanations that are cheaper to run than\nthe previous state of the art. One of these techniques, intervention scoring,\nevaluates the interpretability of the effects of intervening on a feature,\nwhich we find explains features that are not recalled by existing methods. We\npropose guidelines for generating better explanations that remain valid for a\nbroader set of activating contexts, and discuss pitfalls with existing scoring\ntechniques. We use our explanations to measure the semantic similarity of\nindependently trained SAEs, and find that SAEs trained on nearby layers of the\nresidual stream are highly similar. Our large-scale analysis confirms that SAE\nlatents are indeed much more interpretable than neurons, even when neurons are\nsparsified using top-$k$ postprocessing. Our code is available at\nhttps://github.com/EleutherAI/sae-auto-interp, and our explanations are\navailable at\nhttps://huggingface.co/datasets/EleutherAI/auto_interp_explanations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While the activations of neurons in deep neural networks usually do not have\na simple human-understandable interpretation, sparse autoencoders (SAEs) can be\nused to transform these activations into a higher-dimensional latent space\nwhich may be more easily interpretable. However, these SAEs can have millions\nof distinct latent features, making it infeasible for humans to manually\ninterpret each one. In this work, we build an open-source automated pipeline to\ngenerate and evaluate natural language explanations for SAE features using\nLLMs. We test our framework on SAEs of varying sizes, activation functions, and\nlosses, trained on two different open-weight LLMs. We introduce five new\ntechniques to score the quality of explanations that are cheaper to run than\nthe previous state of the art. One of these techniques, intervention scoring,\nevaluates the interpretability of the effects of intervening on a feature,\nwhich we find explains features that are not recalled by existing methods. We\npropose guidelines for generating better explanations that remain valid for a\nbroader set of activating contexts, and discuss pitfalls with existing scoring\ntechniques. We use our explanations to measure the semantic similarity of\nindependently trained SAEs, and find that SAEs trained on nearby layers of the\nresidual stream are highly similar. Our large-scale analysis confirms that SAE\nlatents are indeed much more interpretable than neurons, even when neurons are\nsparsified using top-$k$ postprocessing. Our code is available at\nhttps://github.com/EleutherAI/sae-auto-interp, and our explanations are\navailable at\nhttps://huggingface.co/datasets/EleutherAI/auto_interp_explanations."
                },
                "authors": [
                    {
                        "name": "Gonalo Paulo"
                    },
                    {
                        "name": "Alex Mallen"
                    },
                    {
                        "name": "Caden Juang"
                    },
                    {
                        "name": "Nora Belrose"
                    }
                ],
                "author_detail": {
                    "name": "Nora Belrose"
                },
                "author": "Nora Belrose",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13928v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13928v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03467v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03467v1",
                "updated": "2024-12-04T16:56:20Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    16,
                    56,
                    20,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T16:56:20Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    16,
                    56,
                    20,
                    2,
                    339,
                    0
                ],
                "title": "Training-Free Mitigation of Language Reasoning Degradation After\n  Multimodal Instruction Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Mitigation of Language Reasoning Degradation After\n  Multimodal Instruction Tuning"
                },
                "summary": "Multimodal models typically combine a powerful large language model (LLM)\nwith a vision encoder and are then trained on multimodal data via instruction\ntuning. While this process adapts LLMs to multimodal settings, it remains\nunclear whether this adaptation compromises their original language reasoning\ncapabilities. In this work, we explore the effects of multimodal instruction\ntuning on language reasoning performance. We focus on LLaVA, a leading\nmultimodal framework that integrates LLMs such as Vicuna or Mistral with the\nCLIP vision encoder. We compare the performance of the original LLMs with their\nmultimodal-adapted counterparts across eight language reasoning tasks. Our\nexperiments yield several key insights. First, the impact of multimodal\nlearning varies between Vicuna and Mistral: we observe a degradation in\nlanguage reasoning for Mistral but improvements for Vicuna across most tasks.\nSecond, while multimodal instruction learning consistently degrades performance\non mathematical reasoning tasks (e.g., GSM8K), it enhances performance on\ncommonsense reasoning tasks (e.g., CommonsenseQA). Finally, we demonstrate that\na training-free model merging technique can effectively mitigate the language\nreasoning degradation observed in multimodal-adapted Mistral and even improve\nperformance on visual tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal models typically combine a powerful large language model (LLM)\nwith a vision encoder and are then trained on multimodal data via instruction\ntuning. While this process adapts LLMs to multimodal settings, it remains\nunclear whether this adaptation compromises their original language reasoning\ncapabilities. In this work, we explore the effects of multimodal instruction\ntuning on language reasoning performance. We focus on LLaVA, a leading\nmultimodal framework that integrates LLMs such as Vicuna or Mistral with the\nCLIP vision encoder. We compare the performance of the original LLMs with their\nmultimodal-adapted counterparts across eight language reasoning tasks. Our\nexperiments yield several key insights. First, the impact of multimodal\nlearning varies between Vicuna and Mistral: we observe a degradation in\nlanguage reasoning for Mistral but improvements for Vicuna across most tasks.\nSecond, while multimodal instruction learning consistently degrades performance\non mathematical reasoning tasks (e.g., GSM8K), it enhances performance on\ncommonsense reasoning tasks (e.g., CommonsenseQA). Finally, we demonstrate that\na training-free model merging technique can effectively mitigate the language\nreasoning degradation observed in multimodal-adapted Mistral and even improve\nperformance on visual tasks."
                },
                "authors": [
                    {
                        "name": "Neale Ratzlaff"
                    },
                    {
                        "name": "Man Luo"
                    },
                    {
                        "name": "Xin Su"
                    },
                    {
                        "name": "Vasudev Lal"
                    },
                    {
                        "name": "Phillip Howard"
                    }
                ],
                "author_detail": {
                    "name": "Phillip Howard"
                },
                "author": "Phillip Howard",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03467v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03467v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08181v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08181v2",
                "updated": "2024-12-04T16:55:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    16,
                    55,
                    18,
                    2,
                    339,
                    0
                ],
                "published": "2024-11-12T20:57:12Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    20,
                    57,
                    12,
                    1,
                    317,
                    0
                ],
                "title": "Challenges in Guardrailing Large Language Models for Science",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Challenges in Guardrailing Large Language Models for Science"
                },
                "summary": "The rapid development in large language models (LLMs) has transformed the\nlandscape of natural language processing and understanding (NLP/NLU), offering\nsignificant benefits across various domains. However, when applied to\nscientific research, these powerful models exhibit critical failure modes\nrelated to scientific integrity and trustworthiness. Existing general-purpose\nLLM guardrails are insufficient to address these unique challenges in the\nscientific domain. We provide comprehensive guidelines for deploying LLM\nguardrails in the scientific domain. We identify specific challenges --\nincluding time sensitivity, knowledge contextualization, conflict resolution,\nand intellectual property concerns -- and propose a guideline framework for the\nguardrails that can align with scientific needs. These guardrail dimensions\ninclude trustworthiness, ethics & bias, safety, and legal aspects. We also\noutline in detail the implementation strategies that employ white-box,\nblack-box, and gray-box methodologies that can be enforced within scientific\ncontexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development in large language models (LLMs) has transformed the\nlandscape of natural language processing and understanding (NLP/NLU), offering\nsignificant benefits across various domains. However, when applied to\nscientific research, these powerful models exhibit critical failure modes\nrelated to scientific integrity and trustworthiness. Existing general-purpose\nLLM guardrails are insufficient to address these unique challenges in the\nscientific domain. We provide comprehensive guidelines for deploying LLM\nguardrails in the scientific domain. We identify specific challenges --\nincluding time sensitivity, knowledge contextualization, conflict resolution,\nand intellectual property concerns -- and propose a guideline framework for the\nguardrails that can align with scientific needs. These guardrail dimensions\ninclude trustworthiness, ethics & bias, safety, and legal aspects. We also\noutline in detail the implementation strategies that employ white-box,\nblack-box, and gray-box methodologies that can be enforced within scientific\ncontexts."
                },
                "authors": [
                    {
                        "name": "Nishan Pantha"
                    },
                    {
                        "name": "Muthukumaran Ramasubramanian"
                    },
                    {
                        "name": "Iksha Gurung"
                    },
                    {
                        "name": "Manil Maskey"
                    },
                    {
                        "name": "Rahul Ramachandran"
                    }
                ],
                "author_detail": {
                    "name": "Rahul Ramachandran"
                },
                "author": "Rahul Ramachandran",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08181v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08181v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03766v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03766v2",
                "updated": "2024-12-04T16:39:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    16,
                    39,
                    4,
                    2,
                    339,
                    0
                ],
                "published": "2024-11-06T08:59:44Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    8,
                    59,
                    44,
                    2,
                    311,
                    0
                ],
                "title": "Number Cookbook: Number Understanding of Language Models and How to\n  Improve It",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Number Cookbook: Number Understanding of Language Models and How to\n  Improve It"
                },
                "summary": "Large language models (LLMs) can solve an increasing number of complex\nreasoning tasks while making surprising mistakes in basic numerical\nunderstanding and processing (such as 9.11 > 9.9). The latter ability is\nessential for tackling complex arithmetic and mathematical problems and serves\nas a foundation for most reasoning tasks, but previous work paid little\nattention to it or only discussed several restricted tasks (like integer\naddition). In this paper, we comprehensively investigate the numerical\nunderstanding and processing ability (NUPA) of LLMs. Firstly, we introduce a\nbenchmark covering four common numerical representations and 17 distinct\nnumerical tasks in four major categories, resulting in 41 meaningful\ncombinations in total. These tasks are derived from primary and secondary\neducation curricula, encompassing nearly all everyday numerical understanding\nand processing scenarios, and the rules of these tasks are very simple and\nclear. Through the benchmark, we find that current LLMs fail frequently in many\nof the tasks. To study the problem, we train small models with existing and\npotential techniques for enhancing NUPA (such as tokenizers, PEs, and number\nformats), comprehensively evaluating their effectiveness using our testbed. We\nalso finetune practical-scale LLMs on our proposed NUPA tasks and find that 1)\nnaive finetuning can improve NUPA a lot on many but not all tasks, and 2)\nsurprisingly, techniques designed to enhance NUPA prove ineffective for\nfinetuning pretrained models. We further explore the impact of chain-of-thought\ntechniques on NUPA. Our work provides a more detailed and comprehensive\nunderstanding of NUPA in LLMs. Our benchmark and code are released at\nhttps://github.com/GraphPKU/number_cookbook.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) can solve an increasing number of complex\nreasoning tasks while making surprising mistakes in basic numerical\nunderstanding and processing (such as 9.11 > 9.9). The latter ability is\nessential for tackling complex arithmetic and mathematical problems and serves\nas a foundation for most reasoning tasks, but previous work paid little\nattention to it or only discussed several restricted tasks (like integer\naddition). In this paper, we comprehensively investigate the numerical\nunderstanding and processing ability (NUPA) of LLMs. Firstly, we introduce a\nbenchmark covering four common numerical representations and 17 distinct\nnumerical tasks in four major categories, resulting in 41 meaningful\ncombinations in total. These tasks are derived from primary and secondary\neducation curricula, encompassing nearly all everyday numerical understanding\nand processing scenarios, and the rules of these tasks are very simple and\nclear. Through the benchmark, we find that current LLMs fail frequently in many\nof the tasks. To study the problem, we train small models with existing and\npotential techniques for enhancing NUPA (such as tokenizers, PEs, and number\nformats), comprehensively evaluating their effectiveness using our testbed. We\nalso finetune practical-scale LLMs on our proposed NUPA tasks and find that 1)\nnaive finetuning can improve NUPA a lot on many but not all tasks, and 2)\nsurprisingly, techniques designed to enhance NUPA prove ineffective for\nfinetuning pretrained models. We further explore the impact of chain-of-thought\ntechniques on NUPA. Our work provides a more detailed and comprehensive\nunderstanding of NUPA in LLMs. Our benchmark and code are released at\nhttps://github.com/GraphPKU/number_cookbook."
                },
                "authors": [
                    {
                        "name": "Haotong Yang"
                    },
                    {
                        "name": "Yi Hu"
                    },
                    {
                        "name": "Shijia Kang"
                    },
                    {
                        "name": "Zhouchen Lin"
                    },
                    {
                        "name": "Muhan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Muhan Zhang"
                },
                "author": "Muhan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03766v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03766v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03446v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03446v1",
                "updated": "2024-12-04T16:34:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    16,
                    34,
                    35,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T16:34:35Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    16,
                    34,
                    35,
                    2,
                    339,
                    0
                ],
                "title": "From Words to Workflows: Automating Business Processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Words to Workflows: Automating Business Processes"
                },
                "summary": "As businesses increasingly rely on automation to streamline operations, the\nlimitations of Robotic Process Automation (RPA) have become apparent,\nparticularly its dependence on expert knowledge and inability to handle complex\ndecision-making tasks. Recent advancements in Artificial Intelligence (AI),\nparticularly Generative AI (GenAI) and Large Language Models (LLMs), have paved\nthe way for Intelligent Automation (IA), which integrates cognitive\ncapabilities to overcome the shortcomings of RPA. This paper introduces\nText2Workflow, a novel method that automatically generates workflows from\nnatural language user requests. Unlike traditional automation approaches,\nText2Workflow offers a generalized solution for automating any business\nprocess, translating user inputs into a sequence of executable steps\nrepresented in JavaScript Object Notation (JSON) format. Leveraging the\ndecision-making and instruction-following capabilities of LLMs, this method\nprovides a scalable, adaptable framework that enables users to visualize and\nexecute workflows with minimal manual intervention. This research outlines the\nText2Workflow methodology and its broader implications for automating complex\nbusiness processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As businesses increasingly rely on automation to streamline operations, the\nlimitations of Robotic Process Automation (RPA) have become apparent,\nparticularly its dependence on expert knowledge and inability to handle complex\ndecision-making tasks. Recent advancements in Artificial Intelligence (AI),\nparticularly Generative AI (GenAI) and Large Language Models (LLMs), have paved\nthe way for Intelligent Automation (IA), which integrates cognitive\ncapabilities to overcome the shortcomings of RPA. This paper introduces\nText2Workflow, a novel method that automatically generates workflows from\nnatural language user requests. Unlike traditional automation approaches,\nText2Workflow offers a generalized solution for automating any business\nprocess, translating user inputs into a sequence of executable steps\nrepresented in JavaScript Object Notation (JSON) format. Leveraging the\ndecision-making and instruction-following capabilities of LLMs, this method\nprovides a scalable, adaptable framework that enables users to visualize and\nexecute workflows with minimal manual intervention. This research outlines the\nText2Workflow methodology and its broader implications for automating complex\nbusiness processes."
                },
                "authors": [
                    {
                        "name": "Laura Minkova"
                    },
                    {
                        "name": "Jessica Lpez Espejel"
                    },
                    {
                        "name": "Taki Eddine Toufik Djaidja"
                    },
                    {
                        "name": "Walid Dahhane"
                    },
                    {
                        "name": "El Hassane Ettifouri"
                    }
                ],
                "author_detail": {
                    "name": "El Hassane Ettifouri"
                },
                "author": "El Hassane Ettifouri",
                "arxiv_comment": "Under review at Elsevier's Engineering Applications of Artificial\n  Intelligence",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03446v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03446v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03427v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03427v1",
                "updated": "2024-12-04T16:17:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    16,
                    17,
                    9,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T16:17:09Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    16,
                    17,
                    9,
                    2,
                    339,
                    0
                ],
                "title": "Assessing Foundation Models' Transferability to Physiological Signals in\n  Precision Medicine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing Foundation Models' Transferability to Physiological Signals in\n  Precision Medicine"
                },
                "summary": "The success of precision medicine requires computational models that can\neffectively process and interpret diverse physiological signals across\nheterogeneous patient populations. While foundation models have demonstrated\nremarkable transfer capabilities across various domains, their effectiveness in\nhandling individual-specific physiological signals - crucial for precision\nmedicine - remains largely unexplored. This work introduces a systematic\npipeline for rapidly and efficiently evaluating foundation models' transfer\ncapabilities in medical contexts. Our pipeline employs a three-stage approach.\nFirst, it leverages physiological simulation software to generate diverse,\nclinically relevant scenarios, particularly focusing on data-scarce medical\nconditions. This simulation-based approach enables both targeted capability\nassessment and subsequent model fine-tuning. Second, the pipeline projects\nthese simulated signals through the foundation model to obtain embeddings,\nwhich are then evaluated using linear methods. This evaluation quantifies the\nmodel's ability to capture three critical aspects: physiological feature\nindependence, temporal dynamics preservation, and medical scenario\ndifferentiation. Finally, the pipeline validates these representations through\nspecific downstream medical tasks. Initial testing of our pipeline on the\nMoirai time series foundation model revealed significant limitations in\nphysiological signal processing, including feature entanglement, temporal\ndynamics distortion, and reduced scenario discrimination. These findings\nsuggest that current foundation models may require substantial architectural\nmodifications or targeted fine-tuning before deployment in clinical settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The success of precision medicine requires computational models that can\neffectively process and interpret diverse physiological signals across\nheterogeneous patient populations. While foundation models have demonstrated\nremarkable transfer capabilities across various domains, their effectiveness in\nhandling individual-specific physiological signals - crucial for precision\nmedicine - remains largely unexplored. This work introduces a systematic\npipeline for rapidly and efficiently evaluating foundation models' transfer\ncapabilities in medical contexts. Our pipeline employs a three-stage approach.\nFirst, it leverages physiological simulation software to generate diverse,\nclinically relevant scenarios, particularly focusing on data-scarce medical\nconditions. This simulation-based approach enables both targeted capability\nassessment and subsequent model fine-tuning. Second, the pipeline projects\nthese simulated signals through the foundation model to obtain embeddings,\nwhich are then evaluated using linear methods. This evaluation quantifies the\nmodel's ability to capture three critical aspects: physiological feature\nindependence, temporal dynamics preservation, and medical scenario\ndifferentiation. Finally, the pipeline validates these representations through\nspecific downstream medical tasks. Initial testing of our pipeline on the\nMoirai time series foundation model revealed significant limitations in\nphysiological signal processing, including feature entanglement, temporal\ndynamics distortion, and reduced scenario discrimination. These findings\nsuggest that current foundation models may require substantial architectural\nmodifications or targeted fine-tuning before deployment in clinical settings."
                },
                "authors": [
                    {
                        "name": "Matthias Christenson"
                    },
                    {
                        "name": "Cove Geary"
                    },
                    {
                        "name": "Brian Locke"
                    },
                    {
                        "name": "Pranav Koirala"
                    },
                    {
                        "name": "Warren Woodrich Pettine"
                    }
                ],
                "author_detail": {
                    "name": "Warren Woodrich Pettine"
                },
                "author": "Warren Woodrich Pettine",
                "arxiv_comment": "Presented at the precision medicine workshop at the AI in Medicine\n  conference (2024) in Salt Lake City",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03427v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03427v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02205v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02205v2",
                "updated": "2024-12-04T16:12:08Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    16,
                    12,
                    8,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-03T06:47:15Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    6,
                    47,
                    15,
                    1,
                    338,
                    0
                ],
                "title": "DataLab: A Unified Platform for LLM-Powered Business Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DataLab: A Unified Platform for LLM-Powered Business Intelligence"
                },
                "summary": "Business intelligence (BI) transforms large volumes of data within modern\norganizations into actionable insights for informed decision-making. Recently,\nlarge language model (LLM)-based agents have streamlined the BI workflow by\nautomatically performing task planning, reasoning, and actions in executable\nenvironments based on natural language (NL) queries. However, existing\napproaches primarily focus on individual BI tasks such as NL2SQL and NL2VIS.\nThe fragmentation of tasks across different data roles and tools lead to\ninefficiencies and potential errors due to the iterative and collaborative\nnature of BI. In this paper, we introduce DataLab, a unified BI platform that\nintegrates a one-stop LLM-based agent framework with an augmented computational\nnotebook interface. DataLab supports a wide range of BI tasks for different\ndata roles by seamlessly combining LLM assistance with user customization\nwithin a single environment. To achieve this unification, we design a domain\nknowledge incorporation module tailored for enterprise-specific BI tasks, an\ninter-agent communication mechanism to facilitate information sharing across\nthe BI workflow, and a cell-based context management strategy to enhance\ncontext utilization efficiency in BI notebooks. Extensive experiments\ndemonstrate that DataLab achieves state-of-the-art performance on various BI\ntasks across popular research benchmarks. Moreover, DataLab maintains high\neffectiveness and efficiency on real-world datasets from Tencent, achieving up\nto a 58.58% increase in accuracy and a 61.65% reduction in token cost on\nenterprise-specific BI tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Business intelligence (BI) transforms large volumes of data within modern\norganizations into actionable insights for informed decision-making. Recently,\nlarge language model (LLM)-based agents have streamlined the BI workflow by\nautomatically performing task planning, reasoning, and actions in executable\nenvironments based on natural language (NL) queries. However, existing\napproaches primarily focus on individual BI tasks such as NL2SQL and NL2VIS.\nThe fragmentation of tasks across different data roles and tools lead to\ninefficiencies and potential errors due to the iterative and collaborative\nnature of BI. In this paper, we introduce DataLab, a unified BI platform that\nintegrates a one-stop LLM-based agent framework with an augmented computational\nnotebook interface. DataLab supports a wide range of BI tasks for different\ndata roles by seamlessly combining LLM assistance with user customization\nwithin a single environment. To achieve this unification, we design a domain\nknowledge incorporation module tailored for enterprise-specific BI tasks, an\ninter-agent communication mechanism to facilitate information sharing across\nthe BI workflow, and a cell-based context management strategy to enhance\ncontext utilization efficiency in BI notebooks. Extensive experiments\ndemonstrate that DataLab achieves state-of-the-art performance on various BI\ntasks across popular research benchmarks. Moreover, DataLab maintains high\neffectiveness and efficiency on real-world datasets from Tencent, achieving up\nto a 58.58% increase in accuracy and a 61.65% reduction in token cost on\nenterprise-specific BI tasks."
                },
                "authors": [
                    {
                        "name": "Luoxuan Weng"
                    },
                    {
                        "name": "Yinghao Tang"
                    },
                    {
                        "name": "Yingchaojie Feng"
                    },
                    {
                        "name": "Zhuo Chang"
                    },
                    {
                        "name": "Peng Chen"
                    },
                    {
                        "name": "Ruiqin Chen"
                    },
                    {
                        "name": "Haozhe Feng"
                    },
                    {
                        "name": "Chen Hou"
                    },
                    {
                        "name": "Danqing Huang"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Huaming Rao"
                    },
                    {
                        "name": "Haonan Wang"
                    },
                    {
                        "name": "Canshi Wei"
                    },
                    {
                        "name": "Xiaofeng Yang"
                    },
                    {
                        "name": "Yuhui Zhang"
                    },
                    {
                        "name": "Yifeng Zheng"
                    },
                    {
                        "name": "Xiuqi Huang"
                    },
                    {
                        "name": "Minfeng Zhu"
                    },
                    {
                        "name": "Yuxin Ma"
                    },
                    {
                        "name": "Bin Cui"
                    },
                    {
                        "name": "Wei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wei Chen"
                },
                "author": "Wei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02205v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02205v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03409v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03409v1",
                "updated": "2024-12-04T15:48:59Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    15,
                    48,
                    59,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T15:48:59Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    15,
                    48,
                    59,
                    2,
                    339,
                    0
                ],
                "title": "PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following\n  Models Need for Efficient Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following\n  Models Need for Efficient Generation"
                },
                "summary": "Recently, large vision-language models (LVLMs) have rapidly gained popularity\nfor their strong generation and reasoning capabilities given diverse multimodal\ninputs. However, these models incur significant computational and memory\noverhead during inference, which greatly hinders the efficient deployment in\npractical scenarios. The extensive key-value (KV) cache, necessitated by the\nlengthy input and output sequences, notably contributes to the high inference\ncost. Based on this, recent works have investigated ways to reduce the KV cache\nsize for higher efficiency. Although effective, they generally overlook the\ndistinct importance distributions of KV vectors across layers and maintain the\nsame cache size for each layer during the next token prediction. This results\nin the significant contextual information loss for certain layers, leading to\nnotable performance decline. To address this, we present PrefixKV. It reframes\nthe challenge of determining KV cache sizes for all layers into the task of\nsearching for the optimal global prefix configuration. With an adaptive\nlayer-wise KV retention recipe based on binary search, the maximum contextual\ninformation can thus be preserved in each layer, facilitating the generation.\nExtensive experiments demonstrate that our method achieves the state-of-the-art\nperformance compared with others. It exhibits superior inference efficiency and\ngeneration quality trade-offs, showing promising potential for practical\napplications. Code is available at \\url{https://github.com/THU-MIG/PrefixKV}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large vision-language models (LVLMs) have rapidly gained popularity\nfor their strong generation and reasoning capabilities given diverse multimodal\ninputs. However, these models incur significant computational and memory\noverhead during inference, which greatly hinders the efficient deployment in\npractical scenarios. The extensive key-value (KV) cache, necessitated by the\nlengthy input and output sequences, notably contributes to the high inference\ncost. Based on this, recent works have investigated ways to reduce the KV cache\nsize for higher efficiency. Although effective, they generally overlook the\ndistinct importance distributions of KV vectors across layers and maintain the\nsame cache size for each layer during the next token prediction. This results\nin the significant contextual information loss for certain layers, leading to\nnotable performance decline. To address this, we present PrefixKV. It reframes\nthe challenge of determining KV cache sizes for all layers into the task of\nsearching for the optimal global prefix configuration. With an adaptive\nlayer-wise KV retention recipe based on binary search, the maximum contextual\ninformation can thus be preserved in each layer, facilitating the generation.\nExtensive experiments demonstrate that our method achieves the state-of-the-art\nperformance compared with others. It exhibits superior inference efficiency and\ngeneration quality trade-offs, showing promising potential for practical\napplications. Code is available at \\url{https://github.com/THU-MIG/PrefixKV}."
                },
                "authors": [
                    {
                        "name": "Ao Wang"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Jianchao Tan"
                    },
                    {
                        "name": "Kefeng Zhang"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Zijia Lin"
                    },
                    {
                        "name": "Jungong Han"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "arxiv_comment": "12 pages, 5 figures;",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03409v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03409v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03398v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03398v1",
                "updated": "2024-12-04T15:27:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    15,
                    27,
                    39,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T15:27:39Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    15,
                    27,
                    39,
                    2,
                    339,
                    0
                ],
                "title": "RedStone: Curating General, Code, Math, and QA Data for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RedStone: Curating General, Code, Math, and QA Data for Large Language\n  Models"
                },
                "summary": "Pre-training Large Language Models (LLMs) on high-quality, meticulously\ncurated datasets is widely recognized as critical for enhancing their\nperformance and generalization capabilities. This study explores the untapped\npotential of Common Crawl as a comprehensive and flexible resource for\npre-training LLMs, addressing both general-purpose language understanding and\nspecialized domain knowledge. We introduce RedStone, an innovative and scalable\npipeline engineered to extract and process data from Common Crawl, facilitating\nthe creation of extensive and varied pre-training datasets. Unlike traditional\ndatasets, which often require expensive curation and domain-specific expertise,\nRedStone leverages the breadth of Common Crawl to deliver datasets tailored to\na wide array of domains. In this work, we exemplify its capability by\nconstructing pre-training datasets across multiple fields, including general\nlanguage understanding, code, mathematics, and question-answering tasks. The\nflexibility of RedStone allows for easy adaptation to other specialized\ndomains, significantly lowering the barrier to creating valuable\ndomain-specific datasets. Our findings demonstrate that Common Crawl, when\nharnessed through effective pipelines like RedStone, can serve as a rich,\nrenewable source of pre-training data, unlocking new avenues for domain\nadaptation and knowledge discovery in LLMs. This work also underscores the\nimportance of innovative data acquisition strategies and highlights the role of\nweb-scale data as a powerful resource in the continued evolution of LLMs.\nRedStone code and data samples will be publicly available at\n\\url{https://aka.ms/redstone}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pre-training Large Language Models (LLMs) on high-quality, meticulously\ncurated datasets is widely recognized as critical for enhancing their\nperformance and generalization capabilities. This study explores the untapped\npotential of Common Crawl as a comprehensive and flexible resource for\npre-training LLMs, addressing both general-purpose language understanding and\nspecialized domain knowledge. We introduce RedStone, an innovative and scalable\npipeline engineered to extract and process data from Common Crawl, facilitating\nthe creation of extensive and varied pre-training datasets. Unlike traditional\ndatasets, which often require expensive curation and domain-specific expertise,\nRedStone leverages the breadth of Common Crawl to deliver datasets tailored to\na wide array of domains. In this work, we exemplify its capability by\nconstructing pre-training datasets across multiple fields, including general\nlanguage understanding, code, mathematics, and question-answering tasks. The\nflexibility of RedStone allows for easy adaptation to other specialized\ndomains, significantly lowering the barrier to creating valuable\ndomain-specific datasets. Our findings demonstrate that Common Crawl, when\nharnessed through effective pipelines like RedStone, can serve as a rich,\nrenewable source of pre-training data, unlocking new avenues for domain\nadaptation and knowledge discovery in LLMs. This work also underscores the\nimportance of innovative data acquisition strategies and highlights the role of\nweb-scale data as a powerful resource in the continued evolution of LLMs.\nRedStone code and data samples will be publicly available at\n\\url{https://aka.ms/redstone}."
                },
                "authors": [
                    {
                        "name": "Yaoyao Chang"
                    },
                    {
                        "name": "Lei Cui"
                    },
                    {
                        "name": "Li Dong"
                    },
                    {
                        "name": "Shaohan Huang"
                    },
                    {
                        "name": "Yangyu Huang"
                    },
                    {
                        "name": "Yupan Huang"
                    },
                    {
                        "name": "Scarlett Li"
                    },
                    {
                        "name": "Tengchao Lv"
                    },
                    {
                        "name": "Shuming Ma"
                    },
                    {
                        "name": "Qinzheng Sun"
                    },
                    {
                        "name": "Wenhui Wang"
                    },
                    {
                        "name": "Furu Wei"
                    },
                    {
                        "name": "Ying Xin"
                    },
                    {
                        "name": "Mao Yang"
                    },
                    {
                        "name": "Qiufeng Yin"
                    },
                    {
                        "name": "Xingxing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xingxing Zhang"
                },
                "author": "Xingxing Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03398v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03398v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.19732v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.19732v4",
                "updated": "2024-12-04T15:20:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    15,
                    20,
                    35,
                    2,
                    339,
                    0
                ],
                "published": "2024-05-30T06:24:14Z",
                "published_parsed": [
                    2024,
                    5,
                    30,
                    6,
                    24,
                    14,
                    3,
                    151,
                    0
                ],
                "title": "LLM as a Complementary Optimizer to Gradient Descent: A Case Study in\n  Prompt Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM as a Complementary Optimizer to Gradient Descent: A Case Study in\n  Prompt Tuning"
                },
                "summary": "Mastering a skill generally relies on both hands-on experience from doers and\ninsightful, high-level guidance by mentors. Will this strategy also work well\nfor solving complex non-convex optimization problems? Here, a common\ngradient-based optimizer acts like a disciplined doer, making locally optimal\nupdates at each step. Large Language Models (LLMs) can also search for better\nsolutions by inferring from natural language instructions, akin to a high-level\nmentor. In this paper, we show that these two participators are complementary\nto each other and can effectively collaborate as a combined optimization\nframework. The collaborative optimization is achieved by alternating between\nthe gradient-based and LLM-based optimizers. We instruct LLMs to generate\npossibly improved solutions by taking parameter trajectories recorded during\nthe previous stage of gradient-based optimization into account. Inferred\nresults of LLMs are used as restarting points for the next stage of gradient\noptimization. We verify the effectiveness of this optimization framework on\nprompt tuning. By leveraging both the locally rigorous gradient-based optimizer\nand the high-level deductive LLM-based optimizer, the combined optimization\nmethod consistently yields improvements over competitive baselines on a variety\nof tasks. Our results demonstrate the synergistic effect of conventional\ngradient-based optimization and the inference ability of LLMs. The code is\nreleased at https://github.com/guozix/LLM-catalyst.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mastering a skill generally relies on both hands-on experience from doers and\ninsightful, high-level guidance by mentors. Will this strategy also work well\nfor solving complex non-convex optimization problems? Here, a common\ngradient-based optimizer acts like a disciplined doer, making locally optimal\nupdates at each step. Large Language Models (LLMs) can also search for better\nsolutions by inferring from natural language instructions, akin to a high-level\nmentor. In this paper, we show that these two participators are complementary\nto each other and can effectively collaborate as a combined optimization\nframework. The collaborative optimization is achieved by alternating between\nthe gradient-based and LLM-based optimizers. We instruct LLMs to generate\npossibly improved solutions by taking parameter trajectories recorded during\nthe previous stage of gradient-based optimization into account. Inferred\nresults of LLMs are used as restarting points for the next stage of gradient\noptimization. We verify the effectiveness of this optimization framework on\nprompt tuning. By leveraging both the locally rigorous gradient-based optimizer\nand the high-level deductive LLM-based optimizer, the combined optimization\nmethod consistently yields improvements over competitive baselines on a variety\nof tasks. Our results demonstrate the synergistic effect of conventional\ngradient-based optimization and the inference ability of LLMs. The code is\nreleased at https://github.com/guozix/LLM-catalyst."
                },
                "authors": [
                    {
                        "name": "Zixian Guo"
                    },
                    {
                        "name": "Ming Liu"
                    },
                    {
                        "name": "Zhilong Ji"
                    },
                    {
                        "name": "Jinfeng Bai"
                    },
                    {
                        "name": "Yiwen Guo"
                    },
                    {
                        "name": "Wangmeng Zuo"
                    }
                ],
                "author_detail": {
                    "name": "Wangmeng Zuo"
                },
                "author": "Wangmeng Zuo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.19732v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.19732v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15903v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15903v2",
                "updated": "2024-12-04T15:01:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    15,
                    1,
                    47,
                    2,
                    339,
                    0
                ],
                "published": "2024-08-28T16:15:45Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    16,
                    15,
                    45,
                    2,
                    241,
                    0
                ],
                "title": "LLM-Based Multi-Hop Question Answering with Knowledge Graph Integration\n  in Evolving Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Based Multi-Hop Question Answering with Knowledge Graph Integration\n  in Evolving Environments"
                },
                "summary": "The important challenge of keeping knowledge in Large Language Models (LLMs)\nup-to-date has led to the development of various methods for incorporating new\nfacts. However, existing methods for such knowledge editing still face\ndifficulties with multi-hop questions that require accurate fact identification\nand sequential logical reasoning, particularly among numerous fact updates. To\ntackle these challenges, this paper introduces Graph Memory-based Editing for\nLarge Language Models (GMeLLo), a straightforward and effective method that\nmerges the explicit knowledge representation of Knowledge Graphs (KGs) with the\nlinguistic flexibility of LLMs. Beyond merely leveraging LLMs for question\nanswering, GMeLLo employs these models to convert free-form language into\nstructured queries and fact triples, facilitating seamless interaction with KGs\nfor rapid updates and precise multi-hop reasoning. Our results show that GMeLLo\nsignificantly surpasses current state-of-the-art (SOTA) knowledge editing\nmethods in the multi-hop question answering benchmark, MQuAKE, especially in\nscenarios with extensive knowledge edits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The important challenge of keeping knowledge in Large Language Models (LLMs)\nup-to-date has led to the development of various methods for incorporating new\nfacts. However, existing methods for such knowledge editing still face\ndifficulties with multi-hop questions that require accurate fact identification\nand sequential logical reasoning, particularly among numerous fact updates. To\ntackle these challenges, this paper introduces Graph Memory-based Editing for\nLarge Language Models (GMeLLo), a straightforward and effective method that\nmerges the explicit knowledge representation of Knowledge Graphs (KGs) with the\nlinguistic flexibility of LLMs. Beyond merely leveraging LLMs for question\nanswering, GMeLLo employs these models to convert free-form language into\nstructured queries and fact triples, facilitating seamless interaction with KGs\nfor rapid updates and precise multi-hop reasoning. Our results show that GMeLLo\nsignificantly surpasses current state-of-the-art (SOTA) knowledge editing\nmethods in the multi-hop question answering benchmark, MQuAKE, especially in\nscenarios with extensive knowledge edits."
                },
                "authors": [
                    {
                        "name": "Ruirui Chen"
                    },
                    {
                        "name": "Weifeng Jiang"
                    },
                    {
                        "name": "Chengwei Qin"
                    },
                    {
                        "name": "Ishaan Singh Rawal"
                    },
                    {
                        "name": "Cheston Tan"
                    },
                    {
                        "name": "Dongkyu Choi"
                    },
                    {
                        "name": "Bo Xiong"
                    },
                    {
                        "name": "Bo Ai"
                    }
                ],
                "author_detail": {
                    "name": "Bo Ai"
                },
                "author": "Bo Ai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15903v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15903v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03359v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03359v1",
                "updated": "2024-12-04T14:45:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    14,
                    45,
                    9,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T14:45:09Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    14,
                    45,
                    9,
                    2,
                    339,
                    0
                ],
                "title": "WiS Platform: Enhancing Evaluation of LLM-Based Multi-Agent Systems\n  Through Game-Based Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WiS Platform: Enhancing Evaluation of LLM-Based Multi-Agent Systems\n  Through Game-Based Analysis"
                },
                "summary": "Recent advancements in autonomous multi-agent systems (MAS) based on large\nlanguage models (LLMs) have enhanced the application scenarios and improved the\ncapability of LLMs to handle complex tasks. Despite demonstrating\neffectiveness, existing studies still evidently struggle to evaluate, analysis,\nand reproducibility of LLM-based MAS. In this paper, to facilitate the research\non LLM-based MAS, we introduce an open, scalable, and real-time updated\nplatform for accessing and analyzing the LLM-based MAS based on the games Who\nis Spy?\" (WiS). Our platform is featured with three main worths: (1) a unified\nmodel evaluate interface that supports models available on Hugging Face; (2)\nreal-time updated leaderboard for model evaluation; (3) a comprehensive\nevaluation covering game-winning rates, attacking, defense strategies, and\nreasoning of LLMs. To rigorously test WiS, we conduct extensive experiments\ncoverage of various open- and closed-source LLMs, we find that different agents\nexhibit distinct and intriguing behaviors in the game. The experimental results\ndemonstrate the effectiveness and efficiency of our platform in evaluating\nLLM-based MAS. Our platform and its documentation are publicly available at\n\\url{https://whoisspy.ai/}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in autonomous multi-agent systems (MAS) based on large\nlanguage models (LLMs) have enhanced the application scenarios and improved the\ncapability of LLMs to handle complex tasks. Despite demonstrating\neffectiveness, existing studies still evidently struggle to evaluate, analysis,\nand reproducibility of LLM-based MAS. In this paper, to facilitate the research\non LLM-based MAS, we introduce an open, scalable, and real-time updated\nplatform for accessing and analyzing the LLM-based MAS based on the games Who\nis Spy?\" (WiS). Our platform is featured with three main worths: (1) a unified\nmodel evaluate interface that supports models available on Hugging Face; (2)\nreal-time updated leaderboard for model evaluation; (3) a comprehensive\nevaluation covering game-winning rates, attacking, defense strategies, and\nreasoning of LLMs. To rigorously test WiS, we conduct extensive experiments\ncoverage of various open- and closed-source LLMs, we find that different agents\nexhibit distinct and intriguing behaviors in the game. The experimental results\ndemonstrate the effectiveness and efficiency of our platform in evaluating\nLLM-based MAS. Our platform and its documentation are publicly available at\n\\url{https://whoisspy.ai/}"
                },
                "authors": [
                    {
                        "name": "Chengwei Hu"
                    },
                    {
                        "name": "Jianhui Zheng"
                    },
                    {
                        "name": "Yancheng He"
                    },
                    {
                        "name": "Hangyu Guo"
                    },
                    {
                        "name": "Junguang Jiang"
                    },
                    {
                        "name": "Han Zhu"
                    },
                    {
                        "name": "Kai Sun"
                    },
                    {
                        "name": "Yuning Jiang"
                    },
                    {
                        "name": "Wenbo Su"
                    },
                    {
                        "name": "Bo Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zheng"
                },
                "author": "Bo Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03359v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03359v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.03644v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.03644v2",
                "updated": "2024-12-04T14:27:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    14,
                    27,
                    6,
                    2,
                    339,
                    0
                ],
                "published": "2024-05-06T17:07:28Z",
                "published_parsed": [
                    2024,
                    5,
                    6,
                    17,
                    7,
                    28,
                    0,
                    127,
                    0
                ],
                "title": "When LLMs Meet Cybersecurity: A Systematic Literature Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When LLMs Meet Cybersecurity: A Systematic Literature Review"
                },
                "summary": "The rapid development of large language models (LLMs) has opened new avenues\nacross various fields, including cybersecurity, which faces an evolving threat\nlandscape and demand for innovative technologies. Despite initial explorations\ninto the application of LLMs in cybersecurity, there is a lack of a\ncomprehensive overview of this research area. This paper addresses this gap by\nproviding a systematic literature review, covering the analysis of over 300\nworks, encompassing 25 LLMs and more than 10 downstream scenarios. Our\ncomprehensive overview addresses three key research questions: the construction\nof cybersecurity-oriented LLMs, the application of LLMs to various\ncybersecurity tasks, the challenges and further research in this area. This\nstudy aims to shed light on the extensive potential of LLMs in enhancing\ncybersecurity practices and serve as a valuable resource for applying LLMs in\nthis field. We also maintain and regularly update a list of practical guides on\nLLMs for cybersecurity at https://github.com/tmylla/Awesome-LLM4Cybersecurity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of large language models (LLMs) has opened new avenues\nacross various fields, including cybersecurity, which faces an evolving threat\nlandscape and demand for innovative technologies. Despite initial explorations\ninto the application of LLMs in cybersecurity, there is a lack of a\ncomprehensive overview of this research area. This paper addresses this gap by\nproviding a systematic literature review, covering the analysis of over 300\nworks, encompassing 25 LLMs and more than 10 downstream scenarios. Our\ncomprehensive overview addresses three key research questions: the construction\nof cybersecurity-oriented LLMs, the application of LLMs to various\ncybersecurity tasks, the challenges and further research in this area. This\nstudy aims to shed light on the extensive potential of LLMs in enhancing\ncybersecurity practices and serve as a valuable resource for applying LLMs in\nthis field. We also maintain and regularly update a list of practical guides on\nLLMs for cybersecurity at https://github.com/tmylla/Awesome-LLM4Cybersecurity."
                },
                "authors": [
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Haoyu Bu"
                    },
                    {
                        "name": "Hui Wen"
                    },
                    {
                        "name": "Yongji Liu"
                    },
                    {
                        "name": "Haiqiang Fei"
                    },
                    {
                        "name": "Rongrong Xi"
                    },
                    {
                        "name": "Lun Li"
                    },
                    {
                        "name": "Yun Yang"
                    },
                    {
                        "name": "Hongsong Zhu"
                    },
                    {
                        "name": "Dan Meng"
                    }
                ],
                "author_detail": {
                    "name": "Dan Meng"
                },
                "author": "Dan Meng",
                "arxiv_comment": "We have updated the related papers up to Aug 31st, with 50+ new\n  papers added",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.03644v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.03644v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03343v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03343v1",
                "updated": "2024-12-04T14:23:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    14,
                    23,
                    16,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T14:23:16Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    14,
                    23,
                    16,
                    2,
                    339,
                    0
                ],
                "title": "Improving Linguistic Diversity of Large Language Models with Possibility\n  Exploration Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Linguistic Diversity of Large Language Models with Possibility\n  Exploration Fine-Tuning"
                },
                "summary": "While Large Language Models (LLMs) have made significant strides in\nreplicating human-like abilities, there are concerns about a reduction in the\nlinguistic diversity of their outputs. This results in the homogenization of\nviewpoints and perspectives, as well as the underrepresentation of specific\ndemographic groups. Although several fine-tuning and prompting techniques have\nbeen suggested to tackle the issue, they are often tailored to specific tasks\nor come with a substantial increase in computational cost and latency. This\nmakes them challenging to apply to applications that demand very low latency,\nsuch as chatbots and virtual assistants. We propose Possibility Exploration\nFine-Tuning (PEFT), a task-agnostic framework that enhances the text diversity\nof LLMs without increasing latency or computational cost. Given the same\nprompt, models fine-tuned with PEFT can simultaneously generate multiple\ndiverse responses, each corresponding with a controllable possibility number.\nExperiments on dialogue and story generation tasks demonstrate that PEFT\nsignificantly enhances the diversity of LLM outputs, as evidenced by lower\nsimilarity between candidate responses. Since PEFT emphasizes semantic\ndiversity over lexical diversity, it can also notably reduce demographic bias\nin dialogue systems. The implementations and datasets are available in our\nrepository: https://github.com/mailong25/peft_diversity",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large Language Models (LLMs) have made significant strides in\nreplicating human-like abilities, there are concerns about a reduction in the\nlinguistic diversity of their outputs. This results in the homogenization of\nviewpoints and perspectives, as well as the underrepresentation of specific\ndemographic groups. Although several fine-tuning and prompting techniques have\nbeen suggested to tackle the issue, they are often tailored to specific tasks\nor come with a substantial increase in computational cost and latency. This\nmakes them challenging to apply to applications that demand very low latency,\nsuch as chatbots and virtual assistants. We propose Possibility Exploration\nFine-Tuning (PEFT), a task-agnostic framework that enhances the text diversity\nof LLMs without increasing latency or computational cost. Given the same\nprompt, models fine-tuned with PEFT can simultaneously generate multiple\ndiverse responses, each corresponding with a controllable possibility number.\nExperiments on dialogue and story generation tasks demonstrate that PEFT\nsignificantly enhances the diversity of LLM outputs, as evidenced by lower\nsimilarity between candidate responses. Since PEFT emphasizes semantic\ndiversity over lexical diversity, it can also notably reduce demographic bias\nin dialogue systems. The implementations and datasets are available in our\nrepository: https://github.com/mailong25/peft_diversity"
                },
                "authors": [
                    {
                        "name": "Long Mai"
                    },
                    {
                        "name": "Julie Carson-Berndsen"
                    }
                ],
                "author_detail": {
                    "name": "Julie Carson-Berndsen"
                },
                "author": "Julie Carson-Berndsen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03343v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03343v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03338v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03338v1",
                "updated": "2024-12-04T14:13:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    14,
                    13,
                    38,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T14:13:38Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    14,
                    13,
                    38,
                    2,
                    339,
                    0
                ],
                "title": "AI-Driven Day-to-Day Route Choice",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-Driven Day-to-Day Route Choice"
                },
                "summary": "Understanding travelers' route choices can help policymakers devise optimal\noperational and planning strategies for both normal and abnormal circumstances.\nHowever, existing choice modeling methods often rely on predefined assumptions\nand struggle to capture the dynamic and adaptive nature of travel behavior.\nRecently, Large Language Models (LLMs) have emerged as a promising alternative,\ndemonstrating remarkable ability to replicate human-like behaviors across\nvarious fields. Despite this potential, their capacity to accurately simulate\nhuman route choice behavior in transportation contexts remains doubtful. To\nsatisfy this curiosity, this paper investigates the potential of LLMs for route\nchoice modeling by introducing an LLM-empowered agent, \"LLMTraveler.\" This\nagent integrates an LLM as its core, equipped with a memory system that learns\nfrom past experiences and makes decisions by balancing retrieved data and\npersonality traits. The study systematically evaluates the LLMTraveler's\nability to replicate human-like decision-making through two stages: (1)\nanalyzing its route-switching behavior in single origin-destination (OD) pair\ncongestion game scenarios, where it demonstrates patterns align with laboratory\ndata but are not fully explained by traditional models, and (2) testing its\ncapacity to model day-to-day (DTD) adaptive learning behaviors on the Ortuzar\nand Willumsen (OW) network, producing results comparable to Multinomial Logit\n(MNL) and Reinforcement Learning (RL) models. These experiments demonstrate\nthat the framework can partially replicate human-like decision-making in route\nchoice while providing natural language explanations for its decisions. This\ncapability offers valuable insights for transportation policymaking, such as\nsimulating traveler responses to new policies or changes in the network.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding travelers' route choices can help policymakers devise optimal\noperational and planning strategies for both normal and abnormal circumstances.\nHowever, existing choice modeling methods often rely on predefined assumptions\nand struggle to capture the dynamic and adaptive nature of travel behavior.\nRecently, Large Language Models (LLMs) have emerged as a promising alternative,\ndemonstrating remarkable ability to replicate human-like behaviors across\nvarious fields. Despite this potential, their capacity to accurately simulate\nhuman route choice behavior in transportation contexts remains doubtful. To\nsatisfy this curiosity, this paper investigates the potential of LLMs for route\nchoice modeling by introducing an LLM-empowered agent, \"LLMTraveler.\" This\nagent integrates an LLM as its core, equipped with a memory system that learns\nfrom past experiences and makes decisions by balancing retrieved data and\npersonality traits. The study systematically evaluates the LLMTraveler's\nability to replicate human-like decision-making through two stages: (1)\nanalyzing its route-switching behavior in single origin-destination (OD) pair\ncongestion game scenarios, where it demonstrates patterns align with laboratory\ndata but are not fully explained by traditional models, and (2) testing its\ncapacity to model day-to-day (DTD) adaptive learning behaviors on the Ortuzar\nand Willumsen (OW) network, producing results comparable to Multinomial Logit\n(MNL) and Reinforcement Learning (RL) models. These experiments demonstrate\nthat the framework can partially replicate human-like decision-making in route\nchoice while providing natural language explanations for its decisions. This\ncapability offers valuable insights for transportation policymaking, such as\nsimulating traveler responses to new policies or changes in the network."
                },
                "authors": [
                    {
                        "name": "Leizhen Wang"
                    },
                    {
                        "name": "Peibo Duan"
                    },
                    {
                        "name": "Zhengbing He"
                    },
                    {
                        "name": "Cheng Lyu"
                    },
                    {
                        "name": "Xin Chen"
                    },
                    {
                        "name": "Nan Zheng"
                    },
                    {
                        "name": "Li Yao"
                    },
                    {
                        "name": "Zhenliang Ma"
                    }
                ],
                "author_detail": {
                    "name": "Zhenliang Ma"
                },
                "author": "Zhenliang Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03338v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03338v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14845v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14845v2",
                "updated": "2024-12-04T13:43:28Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    13,
                    43,
                    28,
                    2,
                    339,
                    0
                ],
                "published": "2024-08-27T07:56:35Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    7,
                    56,
                    35,
                    1,
                    240,
                    0
                ],
                "title": "AAVENUE: Detecting LLM Biases on NLU Tasks in AAVE via a Novel Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AAVENUE: Detecting LLM Biases on NLU Tasks in AAVE via a Novel Benchmark"
                },
                "summary": "Detecting biases in natural language understanding (NLU) for African American\nVernacular English (AAVE) is crucial to developing inclusive natural language\nprocessing (NLP) systems. To address dialect-induced performance discrepancies,\nwe introduce AAVENUE ({AAVE} {N}atural Language {U}nderstanding {E}valuation),\na benchmark for evaluating large language model (LLM) performance on NLU tasks\nin AAVE and Standard American English (SAE). AAVENUE builds upon and extends\nexisting benchmarks like VALUE, replacing deterministic syntactic and\nmorphological transformations with a more flexible methodology leveraging\nLLM-based translation with few-shot prompting, improving performance across our\nevaluation metrics when translating key tasks from the GLUE and SuperGLUE\nbenchmarks. We compare AAVENUE and VALUE translations using five popular LLMs\nand a comprehensive set of metrics including fluency, BARTScore, quality,\ncoherence, and understandability. Additionally, we recruit fluent AAVE speakers\nto validate our translations for authenticity. Our evaluations reveal that LLMs\nconsistently perform better on SAE tasks than AAVE-translated versions,\nunderscoring inherent biases and highlighting the need for more inclusive NLP\nmodels. We have open-sourced our source code on GitHub and created a website to\nshowcase our work at https://aavenue.live.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting biases in natural language understanding (NLU) for African American\nVernacular English (AAVE) is crucial to developing inclusive natural language\nprocessing (NLP) systems. To address dialect-induced performance discrepancies,\nwe introduce AAVENUE ({AAVE} {N}atural Language {U}nderstanding {E}valuation),\na benchmark for evaluating large language model (LLM) performance on NLU tasks\nin AAVE and Standard American English (SAE). AAVENUE builds upon and extends\nexisting benchmarks like VALUE, replacing deterministic syntactic and\nmorphological transformations with a more flexible methodology leveraging\nLLM-based translation with few-shot prompting, improving performance across our\nevaluation metrics when translating key tasks from the GLUE and SuperGLUE\nbenchmarks. We compare AAVENUE and VALUE translations using five popular LLMs\nand a comprehensive set of metrics including fluency, BARTScore, quality,\ncoherence, and understandability. Additionally, we recruit fluent AAVE speakers\nto validate our translations for authenticity. Our evaluations reveal that LLMs\nconsistently perform better on SAE tasks than AAVE-translated versions,\nunderscoring inherent biases and highlighting the need for more inclusive NLP\nmodels. We have open-sourced our source code on GitHub and created a website to\nshowcase our work at https://aavenue.live."
                },
                "authors": [
                    {
                        "name": "Abhay Gupta"
                    },
                    {
                        "name": "Philip Meng"
                    },
                    {
                        "name": "Ece Yurtseven"
                    },
                    {
                        "name": "Sean O'Brien"
                    },
                    {
                        "name": "Kevin Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Zhu"
                },
                "author": "Kevin Zhu",
                "arxiv_comment": "Published at NLP4PI @ EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14845v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14845v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.11268v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.11268v5",
                "updated": "2024-12-04T12:43:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    12,
                    43,
                    30,
                    2,
                    339,
                    0
                ],
                "published": "2023-09-20T12:51:13Z",
                "published_parsed": [
                    2023,
                    9,
                    20,
                    12,
                    51,
                    13,
                    2,
                    263,
                    0
                ],
                "title": "StructChart: On the Schema, Metric, and Augmentation for Visual Chart\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StructChart: On the Schema, Metric, and Augmentation for Visual Chart\n  Understanding"
                },
                "summary": "Charts are common in literature across various scientific fields, conveying\nrich information easily accessible to readers. Current chart-related tasks\nfocus on either chart perception that extracts information from the visual\ncharts, or chart reasoning given the extracted data, e.g. in a tabular form. In\nthis paper, we introduce StructChart, a novel framework that leverages\nStructured Triplet Representations (STR) to achieve a unified and\nlabel-efficient approach to chart perception and reasoning tasks, which is\ngenerally applicable to different downstream tasks, beyond the\nquestion-answering task as specifically studied in peer works. Specifically,\nStructChart first reformulates the chart data from the tubular form (linearized\nCSV) to STR, which can friendlily reduce the task gap between chart perception\nand reasoning. We then propose a Structuring Chart-oriented Representation\nMetric (SCRM) to quantitatively evaluate the chart perception task performance.\nTo augment the training, we further explore the potential of Large Language\nModels (LLMs) to enhance the diversity in both chart visual style and\nstatistical information. Extensive experiments on various chart-related tasks\ndemonstrate the effectiveness and potential of a unified chart\nperception-reasoning paradigm to push the frontier of chart understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Charts are common in literature across various scientific fields, conveying\nrich information easily accessible to readers. Current chart-related tasks\nfocus on either chart perception that extracts information from the visual\ncharts, or chart reasoning given the extracted data, e.g. in a tabular form. In\nthis paper, we introduce StructChart, a novel framework that leverages\nStructured Triplet Representations (STR) to achieve a unified and\nlabel-efficient approach to chart perception and reasoning tasks, which is\ngenerally applicable to different downstream tasks, beyond the\nquestion-answering task as specifically studied in peer works. Specifically,\nStructChart first reformulates the chart data from the tubular form (linearized\nCSV) to STR, which can friendlily reduce the task gap between chart perception\nand reasoning. We then propose a Structuring Chart-oriented Representation\nMetric (SCRM) to quantitatively evaluate the chart perception task performance.\nTo augment the training, we further explore the potential of Large Language\nModels (LLMs) to enhance the diversity in both chart visual style and\nstatistical information. Extensive experiments on various chart-related tasks\ndemonstrate the effectiveness and potential of a unified chart\nperception-reasoning paradigm to push the frontier of chart understanding."
                },
                "authors": [
                    {
                        "name": "Renqiu Xia"
                    },
                    {
                        "name": "Haoyang Peng"
                    },
                    {
                        "name": "Hancheng Ye"
                    },
                    {
                        "name": "Mingsheng Li"
                    },
                    {
                        "name": "Xiangchao Yan"
                    },
                    {
                        "name": "Peng Ye"
                    },
                    {
                        "name": "Botian Shi"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Junchi Yan"
                    },
                    {
                        "name": "Bo Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zhang"
                },
                "author": "Bo Zhang",
                "arxiv_comment": "All codes, models and SimChart9K data are available for downloading\n  at: https://github.com/UniModal4Reasoning/ChartVLM and\n  https://github.com/UniModal4Reasoning/SimChart9K",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.11268v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.11268v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01946v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01946v2",
                "updated": "2024-12-04T12:19:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    12,
                    19,
                    35,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-02T20:14:46Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    20,
                    14,
                    46,
                    0,
                    337,
                    0
                ],
                "title": "The Reality of AI and Biorisk",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Reality of AI and Biorisk"
                },
                "summary": "To accurately and confidently answer the question 'could an AI model or\nsystem increase biorisk', it is necessary to have both a sound theoretical\nthreat model for how AI models or systems could increase biorisk and a robust\nmethod for testing that threat model. This paper provides an analysis of\nexisting available research surrounding two AI and biorisk threat models: 1)\naccess to information and planning via large language models (LLMs), and 2) the\nuse of AI-enabled biological tools (BTs) in synthesizing novel biological\nartifacts. We find that existing studies around AI-related biorisk are nascent,\noften speculative in nature, or limited in terms of their methodological\nmaturity and transparency. The available literature suggests that current LLMs\nand BTs do not pose an immediate risk, and more work is needed to develop\nrigorous approaches to understanding how future models could increase biorisks.\nWe end with recommendations about how empirical work can be expanded to more\nprecisely target biorisk and ensure rigor and validity of findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To accurately and confidently answer the question 'could an AI model or\nsystem increase biorisk', it is necessary to have both a sound theoretical\nthreat model for how AI models or systems could increase biorisk and a robust\nmethod for testing that threat model. This paper provides an analysis of\nexisting available research surrounding two AI and biorisk threat models: 1)\naccess to information and planning via large language models (LLMs), and 2) the\nuse of AI-enabled biological tools (BTs) in synthesizing novel biological\nartifacts. We find that existing studies around AI-related biorisk are nascent,\noften speculative in nature, or limited in terms of their methodological\nmaturity and transparency. The available literature suggests that current LLMs\nand BTs do not pose an immediate risk, and more work is needed to develop\nrigorous approaches to understanding how future models could increase biorisks.\nWe end with recommendations about how empirical work can be expanded to more\nprecisely target biorisk and ensure rigor and validity of findings."
                },
                "authors": [
                    {
                        "name": "Aidan Peppin"
                    },
                    {
                        "name": "Anka Reuel"
                    },
                    {
                        "name": "Stephen Casper"
                    },
                    {
                        "name": "Elliot Jones"
                    },
                    {
                        "name": "Andrew Strait"
                    },
                    {
                        "name": "Usman Anwar"
                    },
                    {
                        "name": "Anurag Agrawal"
                    },
                    {
                        "name": "Sayash Kapoor"
                    },
                    {
                        "name": "Sanmi Koyejo"
                    },
                    {
                        "name": "Marie Pellat"
                    },
                    {
                        "name": "Rishi Bommasani"
                    },
                    {
                        "name": "Nick Frosst"
                    },
                    {
                        "name": "Sara Hooker"
                    }
                ],
                "author_detail": {
                    "name": "Sara Hooker"
                },
                "author": "Sara Hooker",
                "arxiv_comment": "Updated to correct author affiliations",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01946v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01946v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03253v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03253v1",
                "updated": "2024-12-04T11:52:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    11,
                    52,
                    3,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T11:52:03Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    11,
                    52,
                    3,
                    2,
                    339,
                    0
                ],
                "title": "Alignment at Pre-training! Towards Native Alignment for Arabic LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alignment at Pre-training! Towards Native Alignment for Arabic LLMs"
                },
                "summary": "The alignment of large language models (LLMs) is critical for developing\neffective and safe language models. Traditional approaches focus on aligning\nmodels during the instruction tuning or reinforcement learning stages, referred\nto in this paper as `post alignment'. We argue that alignment during the\npre-training phase, which we term `native alignment', warrants investigation.\nNative alignment aims to prevent unaligned content from the beginning, rather\nthan relying on post-hoc processing. This approach leverages extensively\naligned pre-training data to enhance the effectiveness and usability of\npre-trained models. Our study specifically explores the application of native\nalignment in the context of Arabic LLMs. We conduct comprehensive experiments\nand ablation studies to evaluate the impact of native alignment on model\nperformance and alignment stability. Additionally, we release open-source\nArabic LLMs that demonstrate state-of-the-art performance on various\nbenchmarks, providing significant benefits to the Arabic LLM community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The alignment of large language models (LLMs) is critical for developing\neffective and safe language models. Traditional approaches focus on aligning\nmodels during the instruction tuning or reinforcement learning stages, referred\nto in this paper as `post alignment'. We argue that alignment during the\npre-training phase, which we term `native alignment', warrants investigation.\nNative alignment aims to prevent unaligned content from the beginning, rather\nthan relying on post-hoc processing. This approach leverages extensively\naligned pre-training data to enhance the effectiveness and usability of\npre-trained models. Our study specifically explores the application of native\nalignment in the context of Arabic LLMs. We conduct comprehensive experiments\nand ablation studies to evaluate the impact of native alignment on model\nperformance and alignment stability. Additionally, we release open-source\nArabic LLMs that demonstrate state-of-the-art performance on various\nbenchmarks, providing significant benefits to the Arabic LLM community."
                },
                "authors": [
                    {
                        "name": "Juhao Liang"
                    },
                    {
                        "name": "Zhenyang Cai"
                    },
                    {
                        "name": "Jianqing Zhu"
                    },
                    {
                        "name": "Huang Huang"
                    },
                    {
                        "name": "Kewei Zong"
                    },
                    {
                        "name": "Bang An"
                    },
                    {
                        "name": "Mosen Alharthi"
                    },
                    {
                        "name": "Juncai He"
                    },
                    {
                        "name": "Lian Zhang"
                    },
                    {
                        "name": "Haizhou Li"
                    },
                    {
                        "name": "Benyou Wang"
                    },
                    {
                        "name": "Jinchao Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jinchao Xu"
                },
                "author": "Jinchao Xu",
                "arxiv_comment": "Accepted to NeurIPS 2024 main conference. see\n  https://github.com/FreedomIntelligence/AceGPT-v2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03253v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03253v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03250v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03250v1",
                "updated": "2024-12-04T11:49:22Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    11,
                    49,
                    22,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T11:49:22Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    11,
                    49,
                    22,
                    2,
                    339,
                    0
                ],
                "title": "Controlling the Mutation in Large Language Models for the Efficient\n  Evolution of Algorithms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Controlling the Mutation in Large Language Models for the Efficient\n  Evolution of Algorithms"
                },
                "summary": "The integration of Large Language Models (LLMs) with evolutionary computation\n(EC) has introduced a promising paradigm for automating the design of\nmetaheuristic algorithms. However, existing frameworks, such as the Large\nLanguage Model Evolutionary Algorithm (LLaMEA), often lack precise control over\nmutation mechanisms, leading to inefficiencies in solution space exploration\nand potentially suboptimal convergence. This paper introduces a novel approach\nto mutation control within LLM-driven evolutionary frameworks, inspired by\ntheory of genetic algorithms. Specifically, we propose dynamic mutation prompts\nthat adaptively regulate mutation rates, leveraging a heavy-tailed power-law\ndistribution to balance exploration and exploitation. Experiments using\nGPT-3.5-turbo and GPT-4o models demonstrate that GPT-3.5-turbo fails to adhere\nto the specific mutation instructions, while GPT-4o is able to adapt its\nmutation based on the prompt engineered dynamic prompts. Further experiments\nshow that the introduction of these dynamic rates can improve the convergence\nspeed and adaptability of LLaMEA, when using GPT-4o. This work sets the\nstarting point for better controlled LLM-based mutations in code optimization\ntasks, paving the way for further advancements in automated metaheuristic\ndesign.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Large Language Models (LLMs) with evolutionary computation\n(EC) has introduced a promising paradigm for automating the design of\nmetaheuristic algorithms. However, existing frameworks, such as the Large\nLanguage Model Evolutionary Algorithm (LLaMEA), often lack precise control over\nmutation mechanisms, leading to inefficiencies in solution space exploration\nand potentially suboptimal convergence. This paper introduces a novel approach\nto mutation control within LLM-driven evolutionary frameworks, inspired by\ntheory of genetic algorithms. Specifically, we propose dynamic mutation prompts\nthat adaptively regulate mutation rates, leveraging a heavy-tailed power-law\ndistribution to balance exploration and exploitation. Experiments using\nGPT-3.5-turbo and GPT-4o models demonstrate that GPT-3.5-turbo fails to adhere\nto the specific mutation instructions, while GPT-4o is able to adapt its\nmutation based on the prompt engineered dynamic prompts. Further experiments\nshow that the introduction of these dynamic rates can improve the convergence\nspeed and adaptability of LLaMEA, when using GPT-4o. This work sets the\nstarting point for better controlled LLM-based mutations in code optimization\ntasks, paving the way for further advancements in automated metaheuristic\ndesign."
                },
                "authors": [
                    {
                        "name": "Haoran Yin"
                    },
                    {
                        "name": "Anna V. Kononova"
                    },
                    {
                        "name": "Thomas Bck"
                    },
                    {
                        "name": "Niki van Stein"
                    }
                ],
                "author_detail": {
                    "name": "Niki van Stein"
                },
                "author": "Niki van Stein",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03250v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03250v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10083v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10083v3",
                "updated": "2024-12-04T11:49:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    11,
                    49,
                    4,
                    2,
                    339,
                    0
                ],
                "published": "2024-11-15T10:01:52Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    10,
                    1,
                    52,
                    4,
                    320,
                    0
                ],
                "title": "Xmodel-1.5: An 1B-scale Multilingual LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Xmodel-1.5: An 1B-scale Multilingual LLM"
                },
                "summary": "We introduce Xmodel-1.5, a 1-billion-parameter multilingual large language\nmodel pretrained on 2 trillion tokens, designed for balanced performance and\nscalability. Unlike most large models that use the BPE tokenizer, Xmodel-1.5\nemploys a custom unigram tokenizer with 65,280 tokens, optimizing both\nefficiency and accuracy. The model delivers competitive results across multiple\nlanguages, including Thai, Arabic, French, Chinese, and English, outperforming\nAlibaba's PolyLM-1.7B on respective evaluation datasets. Xmodel-1.5 excels in\nbenchmarks like mMMLU and PIQA, and achieves state-of-the-art results in Thai.\nTo support low-resource language research, we release Xdata_Thai, a\nThai-specific evaluation dataset featuring unique linguistic challenges such as\ngendered particles and idioms. While the model demonstrates strong performance,\nthere is still room for improvement in handling culturally specific nuances. We\nhope this work contributes to advancements in multilingual AI research. Models\nand code are publicly available on GitHub at\nhttps://github.com/XiaoduoAILab/XmodelLM-1.5",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Xmodel-1.5, a 1-billion-parameter multilingual large language\nmodel pretrained on 2 trillion tokens, designed for balanced performance and\nscalability. Unlike most large models that use the BPE tokenizer, Xmodel-1.5\nemploys a custom unigram tokenizer with 65,280 tokens, optimizing both\nefficiency and accuracy. The model delivers competitive results across multiple\nlanguages, including Thai, Arabic, French, Chinese, and English, outperforming\nAlibaba's PolyLM-1.7B on respective evaluation datasets. Xmodel-1.5 excels in\nbenchmarks like mMMLU and PIQA, and achieves state-of-the-art results in Thai.\nTo support low-resource language research, we release Xdata_Thai, a\nThai-specific evaluation dataset featuring unique linguistic challenges such as\ngendered particles and idioms. While the model demonstrates strong performance,\nthere is still room for improvement in handling culturally specific nuances. We\nhope this work contributes to advancements in multilingual AI research. Models\nand code are publicly available on GitHub at\nhttps://github.com/XiaoduoAILab/XmodelLM-1.5"
                },
                "authors": [
                    {
                        "name": "Wang Qun"
                    },
                    {
                        "name": "Liu Yang"
                    },
                    {
                        "name": "Lin Qingquan"
                    },
                    {
                        "name": "Jiang Ling"
                    }
                ],
                "author_detail": {
                    "name": "Jiang Ling"
                },
                "author": "Jiang Ling",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10083v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10083v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03248v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03248v1",
                "updated": "2024-12-04T11:47:57Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    11,
                    47,
                    57,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T11:47:57Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    11,
                    47,
                    57,
                    2,
                    339,
                    0
                ],
                "title": "AIM: Adaptive Inference of Multi-Modal LLMs via Token Merging and\n  Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AIM: Adaptive Inference of Multi-Modal LLMs via Token Merging and\n  Pruning"
                },
                "summary": "Large language models (LLMs) have enabled the creation of multi-modal LLMs\nthat exhibit strong comprehension of visual data such as images and videos.\nHowever, these models usually rely on extensive visual tokens from visual\nencoders, leading to high computational demands, which limits their\napplicability in resource-constrained environments and for long-context tasks.\nIn this work, we propose a training-free adaptive inference method for\nmulti-modal LLMs that can accommodate a broad range of efficiency requirements\nwith a minimum performance drop. Our method consists of a) iterative token\nmerging based on embedding similarity before LLMs, and b) progressive token\npruning within LLM layers based on multi-modal importance. With a minimalist\ndesign, our method can be applied to both video and image LLMs. Extensive\nexperiments on diverse video and image benchmarks demonstrate that, our method\nsubstantially reduces computation load (e.g., a $\\textbf{7-fold}$ reduction in\nFLOPs) while preserving the performance of video and image LLMs. Further, under\na similar computational cost, our method outperforms the state-of-the-art\nmethods in long video understanding (e.g., $\\textbf{+4.6}$ on MLVU).\nAdditionally, our in-depth analysis provides insights into token redundancy and\nLLM layer behaviors, offering guidance for future research in designing\nefficient multi-modal LLMs. Our code will be available at\nhttps://github.com/LaVi-Lab/AIM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have enabled the creation of multi-modal LLMs\nthat exhibit strong comprehension of visual data such as images and videos.\nHowever, these models usually rely on extensive visual tokens from visual\nencoders, leading to high computational demands, which limits their\napplicability in resource-constrained environments and for long-context tasks.\nIn this work, we propose a training-free adaptive inference method for\nmulti-modal LLMs that can accommodate a broad range of efficiency requirements\nwith a minimum performance drop. Our method consists of a) iterative token\nmerging based on embedding similarity before LLMs, and b) progressive token\npruning within LLM layers based on multi-modal importance. With a minimalist\ndesign, our method can be applied to both video and image LLMs. Extensive\nexperiments on diverse video and image benchmarks demonstrate that, our method\nsubstantially reduces computation load (e.g., a $\\textbf{7-fold}$ reduction in\nFLOPs) while preserving the performance of video and image LLMs. Further, under\na similar computational cost, our method outperforms the state-of-the-art\nmethods in long video understanding (e.g., $\\textbf{+4.6}$ on MLVU).\nAdditionally, our in-depth analysis provides insights into token redundancy and\nLLM layer behaviors, offering guidance for future research in designing\nefficient multi-modal LLMs. Our code will be available at\nhttps://github.com/LaVi-Lab/AIM."
                },
                "authors": [
                    {
                        "name": "Yiwu Zhong"
                    },
                    {
                        "name": "Zhuoming Liu"
                    },
                    {
                        "name": "Yin Li"
                    },
                    {
                        "name": "Liwei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Liwei Wang"
                },
                "author": "Liwei Wang",
                "arxiv_comment": "12 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03248v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03248v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02626v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02626v2",
                "updated": "2024-12-04T11:45:34Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    11,
                    45,
                    34,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-03T17:54:12Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    17,
                    54,
                    12,
                    1,
                    338,
                    0
                ],
                "title": "Time-Reversal Provides Unsupervised Feedback to LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-Reversal Provides Unsupervised Feedback to LLMs"
                },
                "summary": "Large Language Models (LLMs) are typically trained to predict in the forward\ndirection of time. However, recent works have shown that prompting these models\nto look back and critique their own generations can produce useful feedback.\nMotivated by this, we explore the question of whether LLMs can be empowered to\nthink (predict and score) backwards to provide unsupervised feedback that\ncomplements forward LLMs. Towards this, we introduce Time Reversed Language\nModels (TRLMs), which can score and generate queries when conditioned on\nresponses, effectively functioning in the reverse direction of time. Further,\nto effectively infer in the response to query direction, we pre-train and\nfine-tune a language model (TRLM-Ba) in the reverse token order from scratch.\nWe show empirically (and theoretically in a stylized setting) that\ntime-reversed models can indeed complement forward model predictions when used\nto score the query given response for re-ranking multiple forward generations.\nWe obtain up to 5\\% improvement on the widely used AlpacaEval Leaderboard over\nthe competent baseline of best-of-N re-ranking using self log-perplexity\nscores. We further show that TRLM scoring outperforms conventional forward\nscoring of response given query, resulting in significant gains in applications\nsuch as citation generation and passage retrieval. We next leverage the\ngenerative ability of TRLM to augment or provide unsupervised feedback to input\nsafety filters of LLMs, demonstrating a drastic reduction in false negative\nrate with negligible impact on false positive rates against several attacks\npublished on the popular JailbreakBench leaderboard.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are typically trained to predict in the forward\ndirection of time. However, recent works have shown that prompting these models\nto look back and critique their own generations can produce useful feedback.\nMotivated by this, we explore the question of whether LLMs can be empowered to\nthink (predict and score) backwards to provide unsupervised feedback that\ncomplements forward LLMs. Towards this, we introduce Time Reversed Language\nModels (TRLMs), which can score and generate queries when conditioned on\nresponses, effectively functioning in the reverse direction of time. Further,\nto effectively infer in the response to query direction, we pre-train and\nfine-tune a language model (TRLM-Ba) in the reverse token order from scratch.\nWe show empirically (and theoretically in a stylized setting) that\ntime-reversed models can indeed complement forward model predictions when used\nto score the query given response for re-ranking multiple forward generations.\nWe obtain up to 5\\% improvement on the widely used AlpacaEval Leaderboard over\nthe competent baseline of best-of-N re-ranking using self log-perplexity\nscores. We further show that TRLM scoring outperforms conventional forward\nscoring of response given query, resulting in significant gains in applications\nsuch as citation generation and passage retrieval. We next leverage the\ngenerative ability of TRLM to augment or provide unsupervised feedback to input\nsafety filters of LLMs, demonstrating a drastic reduction in false negative\nrate with negligible impact on false positive rates against several attacks\npublished on the popular JailbreakBench leaderboard."
                },
                "authors": [
                    {
                        "name": "Yerram Varun"
                    },
                    {
                        "name": "Rahul Madhavan"
                    },
                    {
                        "name": "Sravanti Addepalli"
                    },
                    {
                        "name": "Arun Suggala"
                    },
                    {
                        "name": "Karthikeyan Shanmugam"
                    },
                    {
                        "name": "Prateek Jain"
                    }
                ],
                "author_detail": {
                    "name": "Prateek Jain"
                },
                "author": "Prateek Jain",
                "arxiv_comment": "Accepted as a spotlight in NeurIPS 2024",
                "arxiv_journal_ref": "Varun, Y., Madhavan, R., Addepalli, S., Suggala, A., Shanmugam,\n  K., & Jain, P. Time-Reversal Provides Unsupervised Feedback to LLMs. In The\n  Thirty-Eighth Annual Conference on Neural Information Processing Systems\n  (NeurIPS), 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02626v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02626v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03235v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03235v1",
                "updated": "2024-12-04T11:36:37Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    11,
                    36,
                    37,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T11:36:37Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    11,
                    36,
                    37,
                    2,
                    339,
                    0
                ],
                "title": "Does Safety Training of LLMs Generalize to Semantically Related Natural\n  Prompts?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does Safety Training of LLMs Generalize to Semantically Related Natural\n  Prompts?"
                },
                "summary": "Large Language Models (LLMs) are known to be susceptible to crafted\nadversarial attacks or jailbreaks that lead to the generation of objectionable\ncontent despite being aligned to human preferences using safety fine-tuning\nmethods. While the large dimensionality of input token space makes it\ninevitable to find adversarial prompts that can jailbreak these models, we aim\nto evaluate whether safety fine-tuned LLMs are safe against natural prompts\nwhich are semantically related to toxic seed prompts that elicit safe responses\nafter alignment. We surprisingly find that popular aligned LLMs such as GPT-4\ncan be compromised using naive prompts that are NOT even crafted with an\nobjective of jailbreaking the model. Furthermore, we empirically show that\ngiven a seed prompt that elicits a toxic response from an unaligned model, one\ncan systematically generate several semantically related natural prompts that\ncan jailbreak aligned LLMs. Towards this, we propose a method of Response\nGuided Question Augmentation (ReG-QA) to evaluate the generalization of safety\naligned LLMs to natural prompts, that first generates several toxic answers\ngiven a seed question using an unaligned LLM (Q to A), and further leverages an\nLLM to generate questions that are likely to produce these answers (A to Q). We\ninterestingly find that safety fine-tuned LLMs such as GPT-4o are vulnerable to\nproducing natural jailbreak questions from unsafe content (without denial) and\ncan thus be used for the latter (A to Q) step. We obtain attack success rates\nthat are comparable to/ better than leading adversarial attack methods on the\nJailbreakBench leaderboard, while being significantly more stable against\ndefenses such as Smooth-LLM and Synonym Substitution, which are effective\nagainst existing all attacks on the leaderboard.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are known to be susceptible to crafted\nadversarial attacks or jailbreaks that lead to the generation of objectionable\ncontent despite being aligned to human preferences using safety fine-tuning\nmethods. While the large dimensionality of input token space makes it\ninevitable to find adversarial prompts that can jailbreak these models, we aim\nto evaluate whether safety fine-tuned LLMs are safe against natural prompts\nwhich are semantically related to toxic seed prompts that elicit safe responses\nafter alignment. We surprisingly find that popular aligned LLMs such as GPT-4\ncan be compromised using naive prompts that are NOT even crafted with an\nobjective of jailbreaking the model. Furthermore, we empirically show that\ngiven a seed prompt that elicits a toxic response from an unaligned model, one\ncan systematically generate several semantically related natural prompts that\ncan jailbreak aligned LLMs. Towards this, we propose a method of Response\nGuided Question Augmentation (ReG-QA) to evaluate the generalization of safety\naligned LLMs to natural prompts, that first generates several toxic answers\ngiven a seed question using an unaligned LLM (Q to A), and further leverages an\nLLM to generate questions that are likely to produce these answers (A to Q). We\ninterestingly find that safety fine-tuned LLMs such as GPT-4o are vulnerable to\nproducing natural jailbreak questions from unsafe content (without denial) and\ncan thus be used for the latter (A to Q) step. We obtain attack success rates\nthat are comparable to/ better than leading adversarial attack methods on the\nJailbreakBench leaderboard, while being significantly more stable against\ndefenses such as Smooth-LLM and Synonym Substitution, which are effective\nagainst existing all attacks on the leaderboard."
                },
                "authors": [
                    {
                        "name": "Sravanti Addepalli"
                    },
                    {
                        "name": "Yerram Varun"
                    },
                    {
                        "name": "Arun Suggala"
                    },
                    {
                        "name": "Karthikeyan Shanmugam"
                    },
                    {
                        "name": "Prateek Jain"
                    }
                ],
                "author_detail": {
                    "name": "Prateek Jain"
                },
                "author": "Prateek Jain",
                "arxiv_comment": "Accepted at the Safe Generative AI Workshop @ NeurIPS 2024",
                "arxiv_journal_ref": "Addepalli, S., Varun, Y., Suggala, A., Shanmugam, K. and Jain, P.,\n  Does Safety Training of LLMs Generalize to Semantically Related Natural\n  Prompts?. In Neurips Safe Generative AI Workshop 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03235v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03235v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03223v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03223v1",
                "updated": "2024-12-04T11:18:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    11,
                    18,
                    32,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T11:18:32Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    11,
                    18,
                    32,
                    2,
                    339,
                    0
                ],
                "title": "Linq-Embed-Mistral Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linq-Embed-Mistral Technical Report"
                },
                "summary": "This report explores the enhancement of text retrieval performance using\nadvanced data refinement techniques. We develop\nLinq-Embed-Mistral\\footnote{\\url{https://huggingface.co/Linq-AI-Research/Linq-Embed-Mistral}}\nby building on the E5-mistral and Mistral-7B-v0.1 models, focusing on\nsophisticated data crafting, data filtering, and negative mining methods, which\nare highly tailored to each task, applied to both existing benchmark dataset\nand highly tailored synthetic dataset generated via large language models\n(LLMs). Linq-Embed-Mistral excels in the MTEB benchmarks (as of May 29, 2024),\nachieving an average score of 68.2 across 56 datasets, and ranks 1st among all\nmodels for retrieval tasks on the MTEB leaderboard with a performance score of\n60.2. This performance underscores its superior capability in enhancing search\nprecision and reliability. Our contributions include advanced data refinement\nmethods that significantly improve model performance on benchmark and synthetic\ndatasets, techniques for homogeneous task ordering and mixed task fine-tuning\nto enhance model generalization and stability, and a streamlined evaluation\nprocess using 4-bit precision and a light retrieval evaluation set, which\naccelerates validation without sacrificing accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This report explores the enhancement of text retrieval performance using\nadvanced data refinement techniques. We develop\nLinq-Embed-Mistral\\footnote{\\url{https://huggingface.co/Linq-AI-Research/Linq-Embed-Mistral}}\nby building on the E5-mistral and Mistral-7B-v0.1 models, focusing on\nsophisticated data crafting, data filtering, and negative mining methods, which\nare highly tailored to each task, applied to both existing benchmark dataset\nand highly tailored synthetic dataset generated via large language models\n(LLMs). Linq-Embed-Mistral excels in the MTEB benchmarks (as of May 29, 2024),\nachieving an average score of 68.2 across 56 datasets, and ranks 1st among all\nmodels for retrieval tasks on the MTEB leaderboard with a performance score of\n60.2. This performance underscores its superior capability in enhancing search\nprecision and reliability. Our contributions include advanced data refinement\nmethods that significantly improve model performance on benchmark and synthetic\ndatasets, techniques for homogeneous task ordering and mixed task fine-tuning\nto enhance model generalization and stability, and a streamlined evaluation\nprocess using 4-bit precision and a light retrieval evaluation set, which\naccelerates validation without sacrificing accuracy."
                },
                "authors": [
                    {
                        "name": "Chanyeol Choi"
                    },
                    {
                        "name": "Junseong Kim"
                    },
                    {
                        "name": "Seolhwa Lee"
                    },
                    {
                        "name": "Jihoon Kwon"
                    },
                    {
                        "name": "Sangmo Gu"
                    },
                    {
                        "name": "Yejin Kim"
                    },
                    {
                        "name": "Minkyung Cho"
                    },
                    {
                        "name": "Jy-yong Sohn"
                    }
                ],
                "author_detail": {
                    "name": "Jy-yong Sohn"
                },
                "author": "Jy-yong Sohn",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03223v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03223v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03220v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03220v1",
                "updated": "2024-12-04T11:14:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    11,
                    14,
                    6,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T11:14:06Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    11,
                    14,
                    6,
                    2,
                    339,
                    0
                ],
                "title": "Survey of different Large Language Model Architectures: Trends,\n  Benchmarks, and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Survey of different Large Language Model Architectures: Trends,\n  Benchmarks, and Challenges"
                },
                "summary": "Large Language Models (LLMs) represent a class of deep learning models adept\nat understanding natural language and generating coherent responses to various\nprompts or queries. These models far exceed the complexity of conventional\nneural networks, often encompassing dozens of neural network layers and\ncontaining billions to trillions of parameters. They are typically trained on\nvast datasets, utilizing architectures based on transformer blocks. Present-day\nLLMs are multi-functional, capable of performing a range of tasks from text\ngeneration and language translation to question answering, as well as code\ngeneration and analysis. An advanced subset of these models, known as\nMultimodal Large Language Models (MLLMs), extends LLM capabilities to process\nand interpret multiple data modalities, including images, audio, and video.\nThis enhancement empowers MLLMs with capabilities like video editing, image\ncomprehension, and captioning for visual content. This survey provides a\ncomprehensive overview of the recent advancements in LLMs. We begin by tracing\nthe evolution of LLMs and subsequently delve into the advent and nuances of\nMLLMs. We analyze emerging state-of-the-art MLLMs, exploring their technical\nfeatures, strengths, and limitations. Additionally, we present a comparative\nanalysis of these models and discuss their challenges, potential limitations,\nand prospects for future development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) represent a class of deep learning models adept\nat understanding natural language and generating coherent responses to various\nprompts or queries. These models far exceed the complexity of conventional\nneural networks, often encompassing dozens of neural network layers and\ncontaining billions to trillions of parameters. They are typically trained on\nvast datasets, utilizing architectures based on transformer blocks. Present-day\nLLMs are multi-functional, capable of performing a range of tasks from text\ngeneration and language translation to question answering, as well as code\ngeneration and analysis. An advanced subset of these models, known as\nMultimodal Large Language Models (MLLMs), extends LLM capabilities to process\nand interpret multiple data modalities, including images, audio, and video.\nThis enhancement empowers MLLMs with capabilities like video editing, image\ncomprehension, and captioning for visual content. This survey provides a\ncomprehensive overview of the recent advancements in LLMs. We begin by tracing\nthe evolution of LLMs and subsequently delve into the advent and nuances of\nMLLMs. We analyze emerging state-of-the-art MLLMs, exploring their technical\nfeatures, strengths, and limitations. Additionally, we present a comparative\nanalysis of these models and discuss their challenges, potential limitations,\nand prospects for future development."
                },
                "authors": [
                    {
                        "name": "Minghao Shao"
                    },
                    {
                        "name": "Abdul Basit"
                    },
                    {
                        "name": "Ramesh Karri"
                    },
                    {
                        "name": "Muhammad Shafique"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Shafique"
                },
                "author": "Muhammad Shafique",
                "arxiv_doi": "10.1109/ACCESS.2024.3482107",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ACCESS.2024.3482107",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.03220v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03220v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03213v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03213v1",
                "updated": "2024-12-04T10:58:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    10,
                    58,
                    27,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T10:58:27Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    10,
                    58,
                    27,
                    2,
                    339,
                    0
                ],
                "title": "ClusterKV: Manipulating LLM KV Cache in Semantic Space for Recallable\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ClusterKV: Manipulating LLM KV Cache in Semantic Space for Recallable\n  Compression"
                },
                "summary": "Large Language Models (LLMs) have been widely deployed in a variety of\napplications, and the context length is rapidly increasing to handle tasks such\nas long-document QA and complex logical reasoning. However, long context poses\nsignificant challenges for inference efficiency, including high memory costs of\nkey-value (KV) cache and increased latency due to extensive memory accesses.\nRecent works have proposed compressing KV cache to approximate computation, but\nthese methods either evict tokens permanently, never recalling them for later\ninference, or recall previous tokens at the granularity of pages divided by\ntextual positions. Both approaches degrade the model accuracy and output\nquality. To achieve efficient and accurate recallable KV cache compression, we\nintroduce ClusterKV, which recalls tokens at the granularity of semantic\nclusters. We design and implement efficient algorithms and systems for\nclustering, selection, indexing and caching. Experiment results show that\nClusterKV attains negligible accuracy loss across various tasks with 32k\ncontext lengths, using only a 1k to 2k KV cache budget, and achieves up to a\n2$\\times$ speedup in latency and a 2.5$\\times$ improvement in decoding\nthroughput. Compared to SoTA recallable KV compression methods, ClusterKV\ndemonstrates higher model accuracy and output quality, while maintaining or\nexceeding inference efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been widely deployed in a variety of\napplications, and the context length is rapidly increasing to handle tasks such\nas long-document QA and complex logical reasoning. However, long context poses\nsignificant challenges for inference efficiency, including high memory costs of\nkey-value (KV) cache and increased latency due to extensive memory accesses.\nRecent works have proposed compressing KV cache to approximate computation, but\nthese methods either evict tokens permanently, never recalling them for later\ninference, or recall previous tokens at the granularity of pages divided by\ntextual positions. Both approaches degrade the model accuracy and output\nquality. To achieve efficient and accurate recallable KV cache compression, we\nintroduce ClusterKV, which recalls tokens at the granularity of semantic\nclusters. We design and implement efficient algorithms and systems for\nclustering, selection, indexing and caching. Experiment results show that\nClusterKV attains negligible accuracy loss across various tasks with 32k\ncontext lengths, using only a 1k to 2k KV cache budget, and achieves up to a\n2$\\times$ speedup in latency and a 2.5$\\times$ improvement in decoding\nthroughput. Compared to SoTA recallable KV compression methods, ClusterKV\ndemonstrates higher model accuracy and output quality, while maintaining or\nexceeding inference efficiency."
                },
                "authors": [
                    {
                        "name": "Guangda Liu"
                    },
                    {
                        "name": "Chengwei Li"
                    },
                    {
                        "name": "Jieru Zhao"
                    },
                    {
                        "name": "Chenqi Zhang"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03213v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03213v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16926v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16926v2",
                "updated": "2024-12-04T10:52:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    10,
                    52,
                    4,
                    2,
                    339,
                    0
                ],
                "published": "2024-10-22T11:57:32Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    11,
                    57,
                    32,
                    1,
                    296,
                    0
                ],
                "title": "Pyramid Vector Quantization for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pyramid Vector Quantization for LLMs"
                },
                "summary": "Recent works on compression of large language models (LLM) using quantization\nconsidered reparameterizing the architecture such that weights are distributed\non the sphere. This demonstratively improves the ability to quantize by\nincreasing the mathematical notion of coherence, resulting in fewer weight\noutliers without affecting the network output. In this work, we aim to further\nexploit this spherical geometry of the weights when performing quantization by\nconsidering Pyramid Vector Quantization (PVQ) for large language models.\nArranging points evenly on the sphere is notoriously difficult, especially in\nhigh dimensions, and in case approximate solutions exists, representing points\nexplicitly in a codebook is typically not feasible due to its additional memory\ncost. Instead, PVQ uses a fixed integer lattice on the sphere by projecting\npoints onto the 1-sphere, which allows for efficient encoding and decoding\nwithout requiring an explicit codebook in memory. To obtain a practical\nalgorithm, we propose to combine PVQ with scale quantization for which we\nderive theoretically optimal quantizations, under empirically verified\nassumptions. Further, we extend pyramid vector quantization to use Hessian\ninformation to minimize quantization error under expected feature activations,\ninstead of only relying on weight magnitudes. Experimentally, we achieves\nstate-of-the-art quantization performance with pareto-optimal trade-off between\nperformance and bits per weight and bits per activation, compared to compared\nmethods. On weight-only, we find that we can quantize a Llama-3 70B model to\n3.25 bits per weight and retain 98\\% accuracy on downstream tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent works on compression of large language models (LLM) using quantization\nconsidered reparameterizing the architecture such that weights are distributed\non the sphere. This demonstratively improves the ability to quantize by\nincreasing the mathematical notion of coherence, resulting in fewer weight\noutliers without affecting the network output. In this work, we aim to further\nexploit this spherical geometry of the weights when performing quantization by\nconsidering Pyramid Vector Quantization (PVQ) for large language models.\nArranging points evenly on the sphere is notoriously difficult, especially in\nhigh dimensions, and in case approximate solutions exists, representing points\nexplicitly in a codebook is typically not feasible due to its additional memory\ncost. Instead, PVQ uses a fixed integer lattice on the sphere by projecting\npoints onto the 1-sphere, which allows for efficient encoding and decoding\nwithout requiring an explicit codebook in memory. To obtain a practical\nalgorithm, we propose to combine PVQ with scale quantization for which we\nderive theoretically optimal quantizations, under empirically verified\nassumptions. Further, we extend pyramid vector quantization to use Hessian\ninformation to minimize quantization error under expected feature activations,\ninstead of only relying on weight magnitudes. Experimentally, we achieves\nstate-of-the-art quantization performance with pareto-optimal trade-off between\nperformance and bits per weight and bits per activation, compared to compared\nmethods. On weight-only, we find that we can quantize a Llama-3 70B model to\n3.25 bits per weight and retain 98\\% accuracy on downstream tasks."
                },
                "authors": [
                    {
                        "name": "Tycho F. A. van der Ouderaa"
                    },
                    {
                        "name": "Maximilian L. Croci"
                    },
                    {
                        "name": "Agrin Hilmkil"
                    },
                    {
                        "name": "James Hensman"
                    }
                ],
                "author_detail": {
                    "name": "James Hensman"
                },
                "author": "James Hensman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16926v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16926v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00850v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00850v2",
                "updated": "2024-12-04T10:45:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    10,
                    45,
                    41,
                    2,
                    339,
                    0
                ],
                "published": "2024-10-30T11:16:04Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    11,
                    16,
                    4,
                    2,
                    304,
                    0
                ],
                "title": "GWQ: Gradient-Aware Weight Quantization for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GWQ: Gradient-Aware Weight Quantization for Large Language Models"
                },
                "summary": "Large language models (LLMs) show impressive performance in solving complex\nlanguage tasks. However, its large number of parameters present significant\nchallenges for the deployment and application of the model on edge devices.\nCompressing large language models to low bits can enable them to run on\nresource-constrained devices, often leading to performance degradation. To\naddress this problem, we propose gradient-aware weight quantization (GWQ), the\nfirst quantization approach for low-bit weight quantization that leverages\ngradients to localize outliers, requiring only a minimal amount of calibration\ndata for outlier detection. GWQ retains the weights corresponding to the top 1%\noutliers preferentially at FP16 precision, while the remaining non-outlier\nweights are stored in a low-bit format. GWQ found experimentally that utilizing\nthe sensitive weights in the gradient localization model is more scientific\ncompared to utilizing the sensitive weights in the Hessian matrix localization\nmodel. Compared to current quantization methods, GWQ can be applied to multiple\nlanguage models and achieves lower PPL on the WikiText2 and C4 dataset. In the\nzero-shot task, GWQ quantized models have higher accuracy compared to other\nquantization methods. GWQ is also suitable for multimodal model quantization,\nand the quantized Qwen-VL family model is more accurate than other methods.\nZero-shot target detection task dataset RefCOCO outperforms the current\nstat-of-the-arts method SPQR. GWQ achieves 1.2 times inference speedup in\ncomparison to the original model, and effectively reduces the inference memory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) show impressive performance in solving complex\nlanguage tasks. However, its large number of parameters present significant\nchallenges for the deployment and application of the model on edge devices.\nCompressing large language models to low bits can enable them to run on\nresource-constrained devices, often leading to performance degradation. To\naddress this problem, we propose gradient-aware weight quantization (GWQ), the\nfirst quantization approach for low-bit weight quantization that leverages\ngradients to localize outliers, requiring only a minimal amount of calibration\ndata for outlier detection. GWQ retains the weights corresponding to the top 1%\noutliers preferentially at FP16 precision, while the remaining non-outlier\nweights are stored in a low-bit format. GWQ found experimentally that utilizing\nthe sensitive weights in the gradient localization model is more scientific\ncompared to utilizing the sensitive weights in the Hessian matrix localization\nmodel. Compared to current quantization methods, GWQ can be applied to multiple\nlanguage models and achieves lower PPL on the WikiText2 and C4 dataset. In the\nzero-shot task, GWQ quantized models have higher accuracy compared to other\nquantization methods. GWQ is also suitable for multimodal model quantization,\nand the quantized Qwen-VL family model is more accurate than other methods.\nZero-shot target detection task dataset RefCOCO outperforms the current\nstat-of-the-arts method SPQR. GWQ achieves 1.2 times inference speedup in\ncomparison to the original model, and effectively reduces the inference memory."
                },
                "authors": [
                    {
                        "name": "Yihua Shao"
                    },
                    {
                        "name": "Siyu Liang"
                    },
                    {
                        "name": "Zijian Ling"
                    },
                    {
                        "name": "Minxi Yan"
                    },
                    {
                        "name": "Haiyang Liu"
                    },
                    {
                        "name": "Siyu Chen"
                    },
                    {
                        "name": "Ziyang Yan"
                    },
                    {
                        "name": "Chenyu Zhang"
                    },
                    {
                        "name": "Haotong Qin"
                    },
                    {
                        "name": "Michele Magno"
                    },
                    {
                        "name": "Yang Yang"
                    },
                    {
                        "name": "Zhen Lei"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Jingcai Guo"
                    },
                    {
                        "name": "Ling Shao"
                    },
                    {
                        "name": "Hao Tang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Tang"
                },
                "author": "Hao Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00850v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00850v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03205v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03205v1",
                "updated": "2024-12-04T10:44:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    10,
                    44,
                    50,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T10:44:50Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    10,
                    44,
                    50,
                    2,
                    339,
                    0
                ],
                "title": "U-MATH: A University-Level Benchmark for Evaluating Mathematical Skills\n  in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "U-MATH: A University-Level Benchmark for Evaluating Mathematical Skills\n  in LLMs"
                },
                "summary": "The current evaluation of mathematical skills in LLMs is limited, as existing\nbenchmarks are either relatively small, primarily focus on elementary and\nhigh-school problems, or lack diversity in topics. Additionally, the inclusion\nof visual elements in tasks remains largely under-explored.\n  To address these gaps, we introduce U-MATH, a novel benchmark of 1,100\nunpublished open-ended university-level problems sourced from teaching\nmaterials. It is balanced across six core subjects, with 20% of multimodal\nproblems. Given the open-ended nature of U-MATH problems, we employ an LLM to\njudge the correctness of generated solutions. To this end, we release\n$\\mu$-MATH, a dataset to evaluate the LLMs' capabilities in judging solutions.\n  The evaluation of general domain, math-specific, and multimodal LLMs\nhighlights the challenges presented by U-MATH. Our findings reveal that LLMs\nachieve a maximum accuracy of only 63% on text-based tasks, with even lower 45%\non visual problems. The solution assessment proves challenging for LLMs, with\nthe best LLM judge having an F1-score of 80% on $\\mu$-MATH.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current evaluation of mathematical skills in LLMs is limited, as existing\nbenchmarks are either relatively small, primarily focus on elementary and\nhigh-school problems, or lack diversity in topics. Additionally, the inclusion\nof visual elements in tasks remains largely under-explored.\n  To address these gaps, we introduce U-MATH, a novel benchmark of 1,100\nunpublished open-ended university-level problems sourced from teaching\nmaterials. It is balanced across six core subjects, with 20% of multimodal\nproblems. Given the open-ended nature of U-MATH problems, we employ an LLM to\njudge the correctness of generated solutions. To this end, we release\n$\\mu$-MATH, a dataset to evaluate the LLMs' capabilities in judging solutions.\n  The evaluation of general domain, math-specific, and multimodal LLMs\nhighlights the challenges presented by U-MATH. Our findings reveal that LLMs\nachieve a maximum accuracy of only 63% on text-based tasks, with even lower 45%\non visual problems. The solution assessment proves challenging for LLMs, with\nthe best LLM judge having an F1-score of 80% on $\\mu$-MATH."
                },
                "authors": [
                    {
                        "name": "Konstantin Chernyshev"
                    },
                    {
                        "name": "Vitaliy Polshkov"
                    },
                    {
                        "name": "Ekaterina Artemova"
                    },
                    {
                        "name": "Alex Myasnikov"
                    },
                    {
                        "name": "Vlad Stepanov"
                    },
                    {
                        "name": "Alexei Miasnikov"
                    },
                    {
                        "name": "Sergei Tilga"
                    }
                ],
                "author_detail": {
                    "name": "Sergei Tilga"
                },
                "author": "Sergei Tilga",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03205v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03205v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03201v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03201v1",
                "updated": "2024-12-04T10:41:01Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    10,
                    41,
                    1,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T10:41:01Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    10,
                    41,
                    1,
                    2,
                    339,
                    0
                ],
                "title": "TrustOps: Continuously Building Trustworthy Software",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TrustOps: Continuously Building Trustworthy Software"
                },
                "summary": "Software services play a crucial role in daily life, with automated actions\ndetermining access to resources and information. Trusting service providers to\nperform these actions fairly and accurately is essential, yet challenging for\nusers to verify. Even with publicly available codebases, the rapid pace of\ndevelopment and the complexity of modern deployments hinder the understanding\nand evaluation of service actions, including for experts. Hence, current trust\nmodels rely heavily on the assumption that service providers follow best\npractices and adhere to laws and regulations, which is increasingly impractical\nand risky, leading to undetected flaws and data leaks.\n  In this paper, we argue that gathering verifiable evidence during software\ndevelopment and operations is needed for creating a new trust model. Therefore,\nwe present TrustOps, an approach for continuously collecting verifiable\nevidence in all phases of the software life cycle, relying on and combining\nalready existing tools and trust-enhancing technologies to do so. For this, we\nintroduce the adaptable core principles of TrustOps and provide a roadmap for\nfuture research and development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software services play a crucial role in daily life, with automated actions\ndetermining access to resources and information. Trusting service providers to\nperform these actions fairly and accurately is essential, yet challenging for\nusers to verify. Even with publicly available codebases, the rapid pace of\ndevelopment and the complexity of modern deployments hinder the understanding\nand evaluation of service actions, including for experts. Hence, current trust\nmodels rely heavily on the assumption that service providers follow best\npractices and adhere to laws and regulations, which is increasingly impractical\nand risky, leading to undetected flaws and data leaks.\n  In this paper, we argue that gathering verifiable evidence during software\ndevelopment and operations is needed for creating a new trust model. Therefore,\nwe present TrustOps, an approach for continuously collecting verifiable\nevidence in all phases of the software life cycle, relying on and combining\nalready existing tools and trust-enhancing technologies to do so. For this, we\nintroduce the adaptable core principles of TrustOps and provide a roadmap for\nfuture research and development."
                },
                "authors": [
                    {
                        "name": "Eduardo Brito"
                    },
                    {
                        "name": "Fernando Castillo"
                    },
                    {
                        "name": "Pille Pullonen-Raudvere"
                    },
                    {
                        "name": "Sebastian Werner"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Werner"
                },
                "author": "Sebastian Werner",
                "arxiv_comment": "To be published in International Conference on Enterprise Design,\n  Operations, and Computing 2024 (EDOC 2024), 15 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03201v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03201v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18053v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18053v3",
                "updated": "2024-12-04T10:35:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    10,
                    35,
                    51,
                    2,
                    339,
                    0
                ],
                "published": "2024-09-26T16:58:04Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    16,
                    58,
                    4,
                    3,
                    270,
                    0
                ],
                "title": "DualAD: Dual-Layer Planning for Reasoning in Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DualAD: Dual-Layer Planning for Reasoning in Autonomous Driving"
                },
                "summary": "We present a novel autonomous driving framework, DualAD, designed to imitate\nhuman reasoning during driving. DualAD comprises two layers: a rule-based\nmotion planner at the bottom layer that handles routine driving tasks requiring\nminimal reasoning, and an upper layer featuring a rule-based text encoder that\nconverts driving scenarios from absolute states into text description. This\ntext is then processed by a large language model (LLM) to make driving\ndecisions. The upper layer intervenes in the bottom layer's decisions when\npotential danger is detected, mimicking human reasoning in critical situations.\nClosed-loop experiments demonstrate that DualAD, using a zero-shot pre-trained\nmodel, significantly outperforms rule-based motion planners that lack reasoning\nabilities. Our experiments also highlight the effectiveness of the text\nencoder, which considerably enhances the model's scenario understanding.\nAdditionally, the integrated DualAD model improves with stronger LLMs,\nindicating the framework's potential for further enhancement. Code and\nbenchmarks are available at github.com/TUM-AVS/DualAD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel autonomous driving framework, DualAD, designed to imitate\nhuman reasoning during driving. DualAD comprises two layers: a rule-based\nmotion planner at the bottom layer that handles routine driving tasks requiring\nminimal reasoning, and an upper layer featuring a rule-based text encoder that\nconverts driving scenarios from absolute states into text description. This\ntext is then processed by a large language model (LLM) to make driving\ndecisions. The upper layer intervenes in the bottom layer's decisions when\npotential danger is detected, mimicking human reasoning in critical situations.\nClosed-loop experiments demonstrate that DualAD, using a zero-shot pre-trained\nmodel, significantly outperforms rule-based motion planners that lack reasoning\nabilities. Our experiments also highlight the effectiveness of the text\nencoder, which considerably enhances the model's scenario understanding.\nAdditionally, the integrated DualAD model improves with stronger LLMs,\nindicating the framework's potential for further enhancement. Code and\nbenchmarks are available at github.com/TUM-AVS/DualAD."
                },
                "authors": [
                    {
                        "name": "Dingrui Wang"
                    },
                    {
                        "name": "Marc Kaufeld"
                    },
                    {
                        "name": "Johannes Betz"
                    }
                ],
                "author_detail": {
                    "name": "Johannes Betz"
                },
                "author": "Johannes Betz",
                "arxiv_comment": "Autonomous Driving, Large Language Models (LLMs), Human Reasoning,\n  Critical Scenario",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18053v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18053v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.08004v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.08004v2",
                "updated": "2024-12-04T10:35:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    10,
                    35,
                    25,
                    2,
                    339,
                    0
                ],
                "published": "2024-03-12T18:12:50Z",
                "published_parsed": [
                    2024,
                    3,
                    12,
                    18,
                    12,
                    50,
                    1,
                    72,
                    0
                ],
                "title": "Leveraging LLMs for On-the-Fly Instruction Guided Image Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging LLMs for On-the-Fly Instruction Guided Image Editing"
                },
                "summary": "The combination of language processing and image processing keeps attracting\nincreased interest given recent impressive advances that leverage the combined\nstrengths of both domains of research. Among these advances, the task of\nediting an image on the basis solely of a natural language instruction stands\nout as a most challenging endeavour. While recent approaches for this task\nresort, in one way or other, to some form of preliminary preparation, training\nor fine-tuning, this paper explores a novel approach: We propose a\npreparation-free method that permits instruction-guided image editing on the\nfly. This approach is organized along three steps properly orchestrated that\nresort to image captioning and DDIM inversion, followed by obtaining the edit\ndirection embedding, followed by image editing proper. While dispensing with\npreliminary preparation, our approach demonstrates to be effective and\ncompetitive, outperforming recent, state of the art models for this task when\nevaluated on the MAGICBRUSH dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The combination of language processing and image processing keeps attracting\nincreased interest given recent impressive advances that leverage the combined\nstrengths of both domains of research. Among these advances, the task of\nediting an image on the basis solely of a natural language instruction stands\nout as a most challenging endeavour. While recent approaches for this task\nresort, in one way or other, to some form of preliminary preparation, training\nor fine-tuning, this paper explores a novel approach: We propose a\npreparation-free method that permits instruction-guided image editing on the\nfly. This approach is organized along three steps properly orchestrated that\nresort to image captioning and DDIM inversion, followed by obtaining the edit\ndirection embedding, followed by image editing proper. While dispensing with\npreliminary preparation, our approach demonstrates to be effective and\ncompetitive, outperforming recent, state of the art models for this task when\nevaluated on the MAGICBRUSH dataset."
                },
                "authors": [
                    {
                        "name": "Rodrigo Santos"
                    },
                    {
                        "name": "Joo Silva"
                    },
                    {
                        "name": "Antnio Branco"
                    }
                ],
                "author_detail": {
                    "name": "Antnio Branco"
                },
                "author": "Antnio Branco",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.08004v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.08004v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.06209v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.06209v3",
                "updated": "2024-12-04T10:33:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    10,
                    33,
                    18,
                    2,
                    339,
                    0
                ],
                "published": "2024-04-09T10:58:21Z",
                "published_parsed": [
                    2024,
                    4,
                    9,
                    10,
                    58,
                    21,
                    1,
                    100,
                    0
                ],
                "title": "Elephants Never Forget: Memorization and Learning of Tabular Data in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Elephants Never Forget: Memorization and Learning of Tabular Data in\n  Large Language Models"
                },
                "summary": "While many have shown how Large Language Models (LLMs) can be applied to a\ndiverse set of tasks, the critical issues of data contamination and\nmemorization are often glossed over. In this work, we address this concern for\ntabular data. Specifically, we introduce a variety of different techniques to\nassess whether a language model has seen a tabular dataset during training.\nThis investigation reveals that LLMs have memorized many popular tabular\ndatasets verbatim. We then compare the few-shot learning performance of LLMs on\ndatasets that were seen during training to the performance on datasets released\nafter training. We find that LLMs perform better on datasets seen during\ntraining, indicating that memorization leads to overfitting. At the same time,\nLLMs show non-trivial performance on novel datasets and are surprisingly robust\nto data transformations. We then investigate the in-context statistical\nlearning abilities of LLMs. While LLMs are significantly better than random at\nsolving statistical classification problems, the sample efficiency of few-shot\nlearning lags behind traditional statistical learning algorithms, especially as\nthe dimension of the problem increases. This suggests that much of the observed\nfew-shot performance on novel real-world datasets is due to the LLM's world\nknowledge. Overall, our results highlight the importance of testing whether an\nLLM has seen an evaluation dataset during pre-training. We release the\nhttps://github.com/interpretml/LLM-Tabular-Memorization-Checker Python package\nto test LLMs for memorization of tabular datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While many have shown how Large Language Models (LLMs) can be applied to a\ndiverse set of tasks, the critical issues of data contamination and\nmemorization are often glossed over. In this work, we address this concern for\ntabular data. Specifically, we introduce a variety of different techniques to\nassess whether a language model has seen a tabular dataset during training.\nThis investigation reveals that LLMs have memorized many popular tabular\ndatasets verbatim. We then compare the few-shot learning performance of LLMs on\ndatasets that were seen during training to the performance on datasets released\nafter training. We find that LLMs perform better on datasets seen during\ntraining, indicating that memorization leads to overfitting. At the same time,\nLLMs show non-trivial performance on novel datasets and are surprisingly robust\nto data transformations. We then investigate the in-context statistical\nlearning abilities of LLMs. While LLMs are significantly better than random at\nsolving statistical classification problems, the sample efficiency of few-shot\nlearning lags behind traditional statistical learning algorithms, especially as\nthe dimension of the problem increases. This suggests that much of the observed\nfew-shot performance on novel real-world datasets is due to the LLM's world\nknowledge. Overall, our results highlight the importance of testing whether an\nLLM has seen an evaluation dataset during pre-training. We release the\nhttps://github.com/interpretml/LLM-Tabular-Memorization-Checker Python package\nto test LLMs for memorization of tabular datasets."
                },
                "authors": [
                    {
                        "name": "Sebastian Bordt"
                    },
                    {
                        "name": "Harsha Nori"
                    },
                    {
                        "name": "Vanessa Rodrigues"
                    },
                    {
                        "name": "Besmira Nushi"
                    },
                    {
                        "name": "Rich Caruana"
                    }
                ],
                "author_detail": {
                    "name": "Rich Caruana"
                },
                "author": "Rich Caruana",
                "arxiv_comment": "COLM camera ready, fix typo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.06209v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.06209v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03188v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03188v1",
                "updated": "2024-12-04T10:20:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    10,
                    20,
                    21,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T10:20:21Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    10,
                    20,
                    21,
                    2,
                    339,
                    0
                ],
                "title": "Semi-decentralized Training of Spatio-Temporal Graph Neural Networks for\n  Traffic Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semi-decentralized Training of Spatio-Temporal Graph Neural Networks for\n  Traffic Prediction"
                },
                "summary": "In smart mobility, large networks of geographically distributed sensors\nproduce vast amounts of high-frequency spatio-temporal data that must be\nprocessed in real time to avoid major disruptions. Traditional centralized\napproaches are increasingly unsuitable to this task, as they struggle to scale\nwith expanding sensor networks, and reliability issues in central components\ncan easily affect the whole deployment. To address these challenges, we explore\nand adapt semi-decentralized training techniques for Spatio-Temporal Graph\nNeural Networks (ST-GNNs) in smart mobility domain. We implement a simulation\nframework where sensors are grouped by proximity into multiple cloudlets, each\nhandling a subgraph of the traffic graph, fetching node features from other\ncloudlets to train its own local ST-GNN model, and exchanging model updates\nwith other cloudlets to ensure consistency, enhancing scalability and removing\nreliance on a centralized aggregator. We perform extensive comparative\nevaluation of four different ST-GNN training setups -- centralized, traditional\nFL, server-free FL, and Gossip Learning -- on large-scale traffic datasets, the\nMETR-LA and PeMS-BAY datasets, for short-, mid-, and long-term vehicle speed\npredictions. Experimental results show that semi-decentralized setups are\ncomparable to centralized approaches in performance metrics, while offering\nadvantages in terms of scalability and fault tolerance. In addition, we\nhighlight often overlooked issues in existing literature for distributed\nST-GNNs, such as the variation in model performance across different\ngeographical areas due to region-specific traffic patterns, and the significant\ncommunication overhead and computational costs that arise from the large\nreceptive field of GNNs, leading to substantial data transfers and increased\ncomputation of partial embeddings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In smart mobility, large networks of geographically distributed sensors\nproduce vast amounts of high-frequency spatio-temporal data that must be\nprocessed in real time to avoid major disruptions. Traditional centralized\napproaches are increasingly unsuitable to this task, as they struggle to scale\nwith expanding sensor networks, and reliability issues in central components\ncan easily affect the whole deployment. To address these challenges, we explore\nand adapt semi-decentralized training techniques for Spatio-Temporal Graph\nNeural Networks (ST-GNNs) in smart mobility domain. We implement a simulation\nframework where sensors are grouped by proximity into multiple cloudlets, each\nhandling a subgraph of the traffic graph, fetching node features from other\ncloudlets to train its own local ST-GNN model, and exchanging model updates\nwith other cloudlets to ensure consistency, enhancing scalability and removing\nreliance on a centralized aggregator. We perform extensive comparative\nevaluation of four different ST-GNN training setups -- centralized, traditional\nFL, server-free FL, and Gossip Learning -- on large-scale traffic datasets, the\nMETR-LA and PeMS-BAY datasets, for short-, mid-, and long-term vehicle speed\npredictions. Experimental results show that semi-decentralized setups are\ncomparable to centralized approaches in performance metrics, while offering\nadvantages in terms of scalability and fault tolerance. In addition, we\nhighlight often overlooked issues in existing literature for distributed\nST-GNNs, such as the variation in model performance across different\ngeographical areas due to region-specific traffic patterns, and the significant\ncommunication overhead and computational costs that arise from the large\nreceptive field of GNNs, leading to substantial data transfers and increased\ncomputation of partial embeddings."
                },
                "authors": [
                    {
                        "name": "Ivan Kralj"
                    },
                    {
                        "name": "Lodovico Giaretta"
                    },
                    {
                        "name": "Gordan Jei"
                    },
                    {
                        "name": "Ivana Podnar arko"
                    },
                    {
                        "name": "arnas Girdzijauskas"
                    }
                ],
                "author_detail": {
                    "name": "arnas Girdzijauskas"
                },
                "author": "arnas Girdzijauskas",
                "arxiv_comment": "8 pages, 4 figures, 3 tables, conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03188v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03188v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03187v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03187v1",
                "updated": "2024-12-04T10:15:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    10,
                    15,
                    12,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T10:15:12Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    10,
                    15,
                    12,
                    2,
                    339,
                    0
                ],
                "title": "Weighted-Reward Preference Optimization for Implicit Model Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Weighted-Reward Preference Optimization for Implicit Model Fusion"
                },
                "summary": "While fusing heterogeneous open-source LLMs with varying architectures and\nsizes can potentially integrate the strengths of different models, existing\nfusion methods face significant challenges, such as vocabulary alignment and\nmerging distribution matrices. These procedures are not only complex but also\nprone to introducing noise and errors. In this paper, we propose an implicit\nfusion method, Weighted-Reward Preference Optimization (WRPO), which leverages\npreference optimization between the source LLMs and the target LLM to transfer\ntheir capabilities effectively. WRPO eliminates the need for vocabulary\nalignment and matrix fusion and can be efficiently scaled to accommodate\nvarious LLMs. To address distributional deviations between the source and\ntarget LLMs, WRPO introduces a progressive adaptation strategy that gradually\nshifts reliance on preferred examples from the target LLM to the source LLMs.\nExtensive experiments on the MT-Bench, AlpacaEval-2, and Arena-Hard benchmarks\ndemonstrate that WRPO consistently outperforms existing knowledge fusion\nmethods and various fine-tuning baselines. When applied to LLaMA3-8B-Instruct\nas the target model, WRPO achieves a length-controlled win rate of 55.9%\nagainst GPT-4-Preview-1106 on AlpacaEval-2 and a win rate of 46.2% against\nGPT-4-0314 on Arena-Hard. Our code is available at\n\\url{https://github.com/SLIT-AI/WRPO}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While fusing heterogeneous open-source LLMs with varying architectures and\nsizes can potentially integrate the strengths of different models, existing\nfusion methods face significant challenges, such as vocabulary alignment and\nmerging distribution matrices. These procedures are not only complex but also\nprone to introducing noise and errors. In this paper, we propose an implicit\nfusion method, Weighted-Reward Preference Optimization (WRPO), which leverages\npreference optimization between the source LLMs and the target LLM to transfer\ntheir capabilities effectively. WRPO eliminates the need for vocabulary\nalignment and matrix fusion and can be efficiently scaled to accommodate\nvarious LLMs. To address distributional deviations between the source and\ntarget LLMs, WRPO introduces a progressive adaptation strategy that gradually\nshifts reliance on preferred examples from the target LLM to the source LLMs.\nExtensive experiments on the MT-Bench, AlpacaEval-2, and Arena-Hard benchmarks\ndemonstrate that WRPO consistently outperforms existing knowledge fusion\nmethods and various fine-tuning baselines. When applied to LLaMA3-8B-Instruct\nas the target model, WRPO achieves a length-controlled win rate of 55.9%\nagainst GPT-4-Preview-1106 on AlpacaEval-2 and a win rate of 46.2% against\nGPT-4-0314 on Arena-Hard. Our code is available at\n\\url{https://github.com/SLIT-AI/WRPO}."
                },
                "authors": [
                    {
                        "name": "Ziyi Yang"
                    },
                    {
                        "name": "Fanqi Wan"
                    },
                    {
                        "name": "Longguang Zhong"
                    },
                    {
                        "name": "Tianyuan Shi"
                    },
                    {
                        "name": "Xiaojun Quan"
                    }
                ],
                "author_detail": {
                    "name": "Xiaojun Quan"
                },
                "author": "Xiaojun Quan",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03187v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03187v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15017v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15017v4",
                "updated": "2024-12-04T09:54:59Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    9,
                    54,
                    59,
                    2,
                    339,
                    0
                ],
                "published": "2024-07-22T06:15:59Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    6,
                    15,
                    59,
                    0,
                    204,
                    0
                ],
                "title": "Knowledge Mechanisms in Large Language Models: A Survey and Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Mechanisms in Large Language Models: A Survey and Perspective"
                },
                "summary": "Understanding knowledge mechanisms in Large Language Models (LLMs) is crucial\nfor advancing towards trustworthy AGI. This paper reviews knowledge mechanism\nanalysis from a novel taxonomy including knowledge utilization and evolution.\nKnowledge utilization delves into the mechanism of memorization, comprehension\nand application, and creation. Knowledge evolution focuses on the dynamic\nprogression of knowledge within individual and group LLMs. Moreover, we discuss\nwhat knowledge LLMs have learned, the reasons for the fragility of parametric\nknowledge, and the potential dark knowledge (hypothesis) that will be\nchallenging to address. We hope this work can help understand knowledge in LLMs\nand provide insights for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding knowledge mechanisms in Large Language Models (LLMs) is crucial\nfor advancing towards trustworthy AGI. This paper reviews knowledge mechanism\nanalysis from a novel taxonomy including knowledge utilization and evolution.\nKnowledge utilization delves into the mechanism of memorization, comprehension\nand application, and creation. Knowledge evolution focuses on the dynamic\nprogression of knowledge within individual and group LLMs. Moreover, we discuss\nwhat knowledge LLMs have learned, the reasons for the fragility of parametric\nknowledge, and the potential dark knowledge (hypothesis) that will be\nchallenging to address. We hope this work can help understand knowledge in LLMs\nand provide insights for future research."
                },
                "authors": [
                    {
                        "name": "Mengru Wang"
                    },
                    {
                        "name": "Yunzhi Yao"
                    },
                    {
                        "name": "Ziwen Xu"
                    },
                    {
                        "name": "Shuofei Qiao"
                    },
                    {
                        "name": "Shumin Deng"
                    },
                    {
                        "name": "Peng Wang"
                    },
                    {
                        "name": "Xiang Chen"
                    },
                    {
                        "name": "Jia-Chen Gu"
                    },
                    {
                        "name": "Yong Jiang"
                    },
                    {
                        "name": "Pengjun Xie"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Huajun Chen"
                    },
                    {
                        "name": "Ningyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ningyu Zhang"
                },
                "author": "Ningyu Zhang",
                "arxiv_comment": "EMNLP 2024 Findings; 39 pages (v4)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15017v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15017v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01289v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01289v2",
                "updated": "2024-12-04T09:51:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    9,
                    51,
                    16,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-02T09:02:28Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    9,
                    2,
                    28,
                    0,
                    337,
                    0
                ],
                "title": "Enhancing Perception Capabilities of Multimodal LLMs with Training-Free\n  Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Perception Capabilities of Multimodal LLMs with Training-Free\n  Fusion"
                },
                "summary": "Multimodal LLMs (MLLMs) equip language models with visual capabilities by\naligning vision encoders with language models. Existing methods to enhance the\nvisual perception of MLLMs often involve designing more powerful vision\nencoders, which requires exploring a vast design space and re-aligning each\npotential encoder with the language model, resulting in prohibitively high\ntraining costs. In this paper, we introduce VisionFuse, a novel integration\nframework that efficiently utilizes multiple vision encoders from off-the-shelf\nMLLMs to enhance visual perception without requiring additional training. Our\napproach is motivated by the observation that different MLLMs tend to focus on\ndistinct regions given the same query and image. Moreover, we find that the\nfeature distributions of vision encoders within an MLLM family, a group of\nMLLMs sharing the same pretrained LLM, are highly aligned. Building on these\ninsights, VisionFuse enriches the visual context by concatenating the tokens\ngenerated by the vision encoders of selected MLLMs within a family. By merging\nthe parameters of language models from these MLLMs, VisionFuse allows a single\nlanguage model to align with various vision encoders, significantly reducing\ndeployment overhead. We conduct comprehensive evaluations across multiple\nmultimodal benchmarks using various MLLM combinations, demonstrating\nsubstantial improvements in multimodal tasks. Notably, when integrating\nMiniGemini-8B and SLIME-8B, VisionFuse achieves an average performance increase\nof over 4%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal LLMs (MLLMs) equip language models with visual capabilities by\naligning vision encoders with language models. Existing methods to enhance the\nvisual perception of MLLMs often involve designing more powerful vision\nencoders, which requires exploring a vast design space and re-aligning each\npotential encoder with the language model, resulting in prohibitively high\ntraining costs. In this paper, we introduce VisionFuse, a novel integration\nframework that efficiently utilizes multiple vision encoders from off-the-shelf\nMLLMs to enhance visual perception without requiring additional training. Our\napproach is motivated by the observation that different MLLMs tend to focus on\ndistinct regions given the same query and image. Moreover, we find that the\nfeature distributions of vision encoders within an MLLM family, a group of\nMLLMs sharing the same pretrained LLM, are highly aligned. Building on these\ninsights, VisionFuse enriches the visual context by concatenating the tokens\ngenerated by the vision encoders of selected MLLMs within a family. By merging\nthe parameters of language models from these MLLMs, VisionFuse allows a single\nlanguage model to align with various vision encoders, significantly reducing\ndeployment overhead. We conduct comprehensive evaluations across multiple\nmultimodal benchmarks using various MLLM combinations, demonstrating\nsubstantial improvements in multimodal tasks. Notably, when integrating\nMiniGemini-8B and SLIME-8B, VisionFuse achieves an average performance increase\nof over 4%."
                },
                "authors": [
                    {
                        "name": "Zhuokun Chen"
                    },
                    {
                        "name": "Jinwu Hu"
                    },
                    {
                        "name": "Zeshuai Deng"
                    },
                    {
                        "name": "Yufeng Wang"
                    },
                    {
                        "name": "Bohan Zhuang"
                    },
                    {
                        "name": "Mingkui Tan"
                    }
                ],
                "author_detail": {
                    "name": "Mingkui Tan"
                },
                "author": "Mingkui Tan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01289v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01289v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03162v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03162v1",
                "updated": "2024-12-04T09:39:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    9,
                    39,
                    56,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T09:39:56Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    9,
                    39,
                    56,
                    2,
                    339,
                    0
                ],
                "title": "LLM-Twin: A Generated-Persona Approach for Survey Pre-Testing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Twin: A Generated-Persona Approach for Survey Pre-Testing"
                },
                "summary": "Surveys are widely used in social sciences to understand human behavior, but\ntheir implementation often involves iterative adjustments that demand\nsignificant effort and resources. To this end, researchers have increasingly\nturned to large language models (LLMs) to simulate human behavior. While\nexisting studies have focused on distributional similarities, individual-level\ncomparisons remain underexplored. Building upon prior work, we investigate\nwhether providing LLMs with respondents' prior information can replicate both\nstatistical distributions and individual decision-making patterns using Partial\nLeast Squares Structural Equation Modeling (PLS-SEM), a well-established causal\nanalysis method. We also introduce the concept of the LLM-Twin, user personas\ngenerated by supplying respondent-specific information to the LLM. By comparing\nresponses generated by the LLM-Twin with actual individual survey responses, we\nassess its effectiveness in replicating individual-level outcomes. Our findings\nshow that: (1) PLS-SEM analysis shows LLM-generated responses align with human\nresponses, (2) LLMs, when provided with respondent-specific information, are\ncapable of reproducing individual human responses, and (3) LLM-Twin responses\nclosely follow human responses at the individual level. These findings\nhighlight the potential of LLMs as a complementary tool for pre-testing surveys\nand optimizing research design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surveys are widely used in social sciences to understand human behavior, but\ntheir implementation often involves iterative adjustments that demand\nsignificant effort and resources. To this end, researchers have increasingly\nturned to large language models (LLMs) to simulate human behavior. While\nexisting studies have focused on distributional similarities, individual-level\ncomparisons remain underexplored. Building upon prior work, we investigate\nwhether providing LLMs with respondents' prior information can replicate both\nstatistical distributions and individual decision-making patterns using Partial\nLeast Squares Structural Equation Modeling (PLS-SEM), a well-established causal\nanalysis method. We also introduce the concept of the LLM-Twin, user personas\ngenerated by supplying respondent-specific information to the LLM. By comparing\nresponses generated by the LLM-Twin with actual individual survey responses, we\nassess its effectiveness in replicating individual-level outcomes. Our findings\nshow that: (1) PLS-SEM analysis shows LLM-generated responses align with human\nresponses, (2) LLMs, when provided with respondent-specific information, are\ncapable of reproducing individual human responses, and (3) LLM-Twin responses\nclosely follow human responses at the individual level. These findings\nhighlight the potential of LLMs as a complementary tool for pre-testing surveys\nand optimizing research design."
                },
                "authors": [
                    {
                        "name": "Sunwoong Kim"
                    },
                    {
                        "name": "Jongho Jeong"
                    },
                    {
                        "name": "Jin Soo Han"
                    },
                    {
                        "name": "Donghyuk Shin"
                    }
                ],
                "author_detail": {
                    "name": "Donghyuk Shin"
                },
                "author": "Donghyuk Shin",
                "arxiv_comment": "11 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03162v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03162v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03160v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03160v1",
                "updated": "2024-12-04T09:38:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    9,
                    38,
                    11,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T09:38:11Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    9,
                    38,
                    11,
                    2,
                    339,
                    0
                ],
                "title": "Byte BPE Tokenization as an Inverse string Homomorphism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Byte BPE Tokenization as an Inverse string Homomorphism"
                },
                "summary": "Tokenization is an important preprocessing step in the training and inference\nof large language models (LLMs). While there has been extensive research on the\nexpressive power of the neural achitectures used in LLMs, the impact of\ntokenization has not been well understood. In this work, we demonstrate that\ntokenization, irrespective of the algorithm used, acts as an inverse\nhomomorphism between strings and tokens. This suggests that the character space\nof the source language and the token space of the tokenized language are\nhomomorphic, preserving the structural properties of the source language.\nAdditionally, we explore the concept of proper tokenization, which refers to an\nunambiguous tokenization returned from the tokenizer. Our analysis reveals that\nthe expressiveness of neural architectures in recognizing context-free\nlanguages is not affected by tokenization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tokenization is an important preprocessing step in the training and inference\nof large language models (LLMs). While there has been extensive research on the\nexpressive power of the neural achitectures used in LLMs, the impact of\ntokenization has not been well understood. In this work, we demonstrate that\ntokenization, irrespective of the algorithm used, acts as an inverse\nhomomorphism between strings and tokens. This suggests that the character space\nof the source language and the token space of the tokenized language are\nhomomorphic, preserving the structural properties of the source language.\nAdditionally, we explore the concept of proper tokenization, which refers to an\nunambiguous tokenization returned from the tokenizer. Our analysis reveals that\nthe expressiveness of neural architectures in recognizing context-free\nlanguages is not affected by tokenization."
                },
                "authors": [
                    {
                        "name": "Saibo Geng"
                    },
                    {
                        "name": "Sankalp Gambhir"
                    },
                    {
                        "name": "Chris Wendler"
                    },
                    {
                        "name": "Robert West"
                    }
                ],
                "author_detail": {
                    "name": "Robert West"
                },
                "author": "Robert West",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03160v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03160v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00809v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00809v2",
                "updated": "2024-12-04T09:26:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    9,
                    26,
                    47,
                    2,
                    339,
                    0
                ],
                "published": "2024-10-23T16:16:15Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    16,
                    16,
                    15,
                    2,
                    297,
                    0
                ],
                "title": "Adaptive Dense Reward: Understanding the Gap Between Action and Reward\n  Space in Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Dense Reward: Understanding the Gap Between Action and Reward\n  Space in Alignment"
                },
                "summary": "Reinforcement Learning from Human Feedback (RLHF) has proven highly effective\nin aligning Large Language Models (LLMs) with human preferences. However, the\noriginal RLHF typically optimizes under an overall reward, which can lead to a\nsuboptimal learning process. This limitation stems from RLHF's lack of\nawareness regarding which specific tokens should be reinforced or suppressed.\nMoreover, conflicts in supervision can arise, for instance, when a chosen\nresponse includes erroneous tokens, while a rejected response contains accurate\nelements. To rectify these shortcomings, increasing dense reward methods, such\nas step-wise and token-wise RLHF, have been proposed. However, these existing\nmethods are limited to specific tasks (like mathematics). In this paper, we\npropose the ``Adaptive Message-wise RLHF'' method, which robustly applies to\nvarious tasks. By defining pivot tokens as key indicators, our approach\nadaptively identifies essential information and converts sequence-level\nsupervision into fine-grained, subsequence-level supervision. This aligns the\ndensity of rewards and action spaces more closely with the information density\nof the input. Experiments demonstrate that our method can be integrated into\nvarious training methods, significantly mitigating hallucinations and\ncatastrophic forgetting problems, while outperforming other methods on multiple\nevaluation metrics. Our method improves the success rate on adversarial samples\nby 10\\% compared to the sample-wise approach, and achieves a 1.3\\% improvement\non evaluation benchmarks such as MMLU, GSM8K, HumanEval, etc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning from Human Feedback (RLHF) has proven highly effective\nin aligning Large Language Models (LLMs) with human preferences. However, the\noriginal RLHF typically optimizes under an overall reward, which can lead to a\nsuboptimal learning process. This limitation stems from RLHF's lack of\nawareness regarding which specific tokens should be reinforced or suppressed.\nMoreover, conflicts in supervision can arise, for instance, when a chosen\nresponse includes erroneous tokens, while a rejected response contains accurate\nelements. To rectify these shortcomings, increasing dense reward methods, such\nas step-wise and token-wise RLHF, have been proposed. However, these existing\nmethods are limited to specific tasks (like mathematics). In this paper, we\npropose the ``Adaptive Message-wise RLHF'' method, which robustly applies to\nvarious tasks. By defining pivot tokens as key indicators, our approach\nadaptively identifies essential information and converts sequence-level\nsupervision into fine-grained, subsequence-level supervision. This aligns the\ndensity of rewards and action spaces more closely with the information density\nof the input. Experiments demonstrate that our method can be integrated into\nvarious training methods, significantly mitigating hallucinations and\ncatastrophic forgetting problems, while outperforming other methods on multiple\nevaluation metrics. Our method improves the success rate on adversarial samples\nby 10\\% compared to the sample-wise approach, and achieves a 1.3\\% improvement\non evaluation benchmarks such as MMLU, GSM8K, HumanEval, etc."
                },
                "authors": [
                    {
                        "name": "Yanshi Li"
                    },
                    {
                        "name": "Shaopan Xiong"
                    },
                    {
                        "name": "Gengru Chen"
                    },
                    {
                        "name": "Xiaoyang Li"
                    },
                    {
                        "name": "Yijia Luo"
                    },
                    {
                        "name": "Xingyao Zhang"
                    },
                    {
                        "name": "Yanhui Huang"
                    },
                    {
                        "name": "Xingyuan Bu"
                    },
                    {
                        "name": "Yingshui Tan"
                    },
                    {
                        "name": "Chun Yuan"
                    },
                    {
                        "name": "Jiamang Wang"
                    },
                    {
                        "name": "Wenbo Su"
                    },
                    {
                        "name": "Bo Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zheng"
                },
                "author": "Bo Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00809v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00809v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03151v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03151v1",
                "updated": "2024-12-04T09:18:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    9,
                    18,
                    54,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T09:18:54Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    9,
                    18,
                    54,
                    2,
                    339,
                    0
                ],
                "title": "Large Language Models show both individual and collective creativity\n  comparable to humans",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models show both individual and collective creativity\n  comparable to humans"
                },
                "summary": "Artificial intelligence has, so far, largely automated routine tasks, but\nwhat does it mean for the future of work if Large Language Models (LLMs) show\ncreativity comparable to humans? To measure the creativity of LLMs\nholistically, the current study uses 13 creative tasks spanning three domains.\nWe benchmark the LLMs against individual humans, and also take a novel approach\nby comparing them to the collective creativity of groups of humans. We find\nthat the best LLMs (Claude and GPT-4) rank in the 52nd percentile against\nhumans, and overall LLMs excel in divergent thinking and problem solving but\nlag in creative writing. When questioned 10 times, an LLM's collective\ncreativity is equivalent to 8-10 humans. When more responses are requested, two\nadditional responses of LLMs equal one extra human. Ultimately, LLMs, when\noptimally applied, may compete with a small group of humans in the future of\nwork.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial intelligence has, so far, largely automated routine tasks, but\nwhat does it mean for the future of work if Large Language Models (LLMs) show\ncreativity comparable to humans? To measure the creativity of LLMs\nholistically, the current study uses 13 creative tasks spanning three domains.\nWe benchmark the LLMs against individual humans, and also take a novel approach\nby comparing them to the collective creativity of groups of humans. We find\nthat the best LLMs (Claude and GPT-4) rank in the 52nd percentile against\nhumans, and overall LLMs excel in divergent thinking and problem solving but\nlag in creative writing. When questioned 10 times, an LLM's collective\ncreativity is equivalent to 8-10 humans. When more responses are requested, two\nadditional responses of LLMs equal one extra human. Ultimately, LLMs, when\noptimally applied, may compete with a small group of humans in the future of\nwork."
                },
                "authors": [
                    {
                        "name": "Luning Sun"
                    },
                    {
                        "name": "Yuzhuo Yuan"
                    },
                    {
                        "name": "Yuan Yao"
                    },
                    {
                        "name": "Yanyan Li"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Xing Xie"
                    },
                    {
                        "name": "Xiting Wang"
                    },
                    {
                        "name": "Fang Luo"
                    },
                    {
                        "name": "David Stillwell"
                    }
                ],
                "author_detail": {
                    "name": "David Stillwell"
                },
                "author": "David Stillwell",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03151v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03151v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03148v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03148v1",
                "updated": "2024-12-04T09:14:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    9,
                    14,
                    56,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T09:14:56Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    9,
                    14,
                    56,
                    2,
                    339,
                    0
                ],
                "title": "Fine-Grained Behavior Simulation with Role-Playing Large Language Model\n  on Social Media",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Grained Behavior Simulation with Role-Playing Large Language Model\n  on Social Media"
                },
                "summary": "Large language models (LLMs) have demonstrated impressive capabilities in\nrole-playing tasks. However, there is limited research on whether LLMs can\naccurately simulate user behavior in real-world scenarios, such as social\nmedia. This requires models to effectively analyze a user's history and\nsimulate their role. In this paper, we introduce \\textbf{FineRob}, a novel\nfine-grained behavior simulation dataset. We collect the complete behavioral\nhistory of 1,866 distinct users across three social media platforms. Each\nbehavior is decomposed into three fine-grained elements: object, type, and\ncontent, resulting in 78.6k QA records. Based on FineRob, we identify two\ndominant reasoning patterns in LLMs' behavior simulation processes and propose\nthe \\textbf{OM-CoT} fine-tuning method to enhance the capability. Through\ncomprehensive experiments, we conduct an in-depth analysis of key factors of\nbehavior simulation and also demonstrate the effectiveness of OM-CoT\napproach\\footnote{Code and dataset are available at\n\\url{https://github.com/linkseed18612254945/FineRob}}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated impressive capabilities in\nrole-playing tasks. However, there is limited research on whether LLMs can\naccurately simulate user behavior in real-world scenarios, such as social\nmedia. This requires models to effectively analyze a user's history and\nsimulate their role. In this paper, we introduce \\textbf{FineRob}, a novel\nfine-grained behavior simulation dataset. We collect the complete behavioral\nhistory of 1,866 distinct users across three social media platforms. Each\nbehavior is decomposed into three fine-grained elements: object, type, and\ncontent, resulting in 78.6k QA records. Based on FineRob, we identify two\ndominant reasoning patterns in LLMs' behavior simulation processes and propose\nthe \\textbf{OM-CoT} fine-tuning method to enhance the capability. Through\ncomprehensive experiments, we conduct an in-depth analysis of key factors of\nbehavior simulation and also demonstrate the effectiveness of OM-CoT\napproach\\footnote{Code and dataset are available at\n\\url{https://github.com/linkseed18612254945/FineRob}}"
                },
                "authors": [
                    {
                        "name": "Kun Li"
                    },
                    {
                        "name": "Chenwei Dai"
                    },
                    {
                        "name": "Wei Zhou"
                    },
                    {
                        "name": "Songlin Hu"
                    }
                ],
                "author_detail": {
                    "name": "Songlin Hu"
                },
                "author": "Songlin Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03148v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03148v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.16539v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.16539v4",
                "updated": "2024-12-04T08:56:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    56,
                    17,
                    2,
                    339,
                    0
                ],
                "published": "2024-03-25T08:31:14Z",
                "published_parsed": [
                    2024,
                    3,
                    25,
                    8,
                    31,
                    14,
                    0,
                    85,
                    0
                ],
                "title": "Data-Efficient 3D Visual Grounding via Order-Aware Referring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-Efficient 3D Visual Grounding via Order-Aware Referring"
                },
                "summary": "3D visual grounding aims to identify the target object within a 3D point\ncloud scene referred to by a natural language description. Previous works\nusually require significant data relating to point color and their descriptions\nto exploit the corresponding complicated verbo-visual relations. In our work,\nwe introduce Vigor, a novel Data-Efficient 3D Visual Grounding framework via\nOrder-aware Referring. Vigor leverages LLM to produce a desirable referential\norder from the input description for 3D visual grounding. With the proposed\nstacked object-referring blocks, the predicted anchor objects in the above\norder allow one to locate the target object progressively without supervision\non the identities of anchor objects or exact relations between anchor/target\nobjects. In addition, we present an order-aware warm-up training strategy,\nwhich augments referential orders for pre-training the visual grounding\nframework. This allows us to better capture the complex verbo-visual relations\nand benefit the desirable data-efficient learning scheme. Experimental results\non the NR3D and ScanRefer datasets demonstrate our superiority in low-resource\nscenarios. In particular, Vigor surpasses current state-of-the-art frameworks\nby 9.3% and 7.6% grounding accuracy under 1% data and 10% data settings on the\nNR3D dataset, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D visual grounding aims to identify the target object within a 3D point\ncloud scene referred to by a natural language description. Previous works\nusually require significant data relating to point color and their descriptions\nto exploit the corresponding complicated verbo-visual relations. In our work,\nwe introduce Vigor, a novel Data-Efficient 3D Visual Grounding framework via\nOrder-aware Referring. Vigor leverages LLM to produce a desirable referential\norder from the input description for 3D visual grounding. With the proposed\nstacked object-referring blocks, the predicted anchor objects in the above\norder allow one to locate the target object progressively without supervision\non the identities of anchor objects or exact relations between anchor/target\nobjects. In addition, we present an order-aware warm-up training strategy,\nwhich augments referential orders for pre-training the visual grounding\nframework. This allows us to better capture the complex verbo-visual relations\nand benefit the desirable data-efficient learning scheme. Experimental results\non the NR3D and ScanRefer datasets demonstrate our superiority in low-resource\nscenarios. In particular, Vigor surpasses current state-of-the-art frameworks\nby 9.3% and 7.6% grounding accuracy under 1% data and 10% data settings on the\nNR3D dataset, respectively."
                },
                "authors": [
                    {
                        "name": "Tung-Yu Wu"
                    },
                    {
                        "name": "Sheng-Yu Huang"
                    },
                    {
                        "name": "Yu-Chiang Frank Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Chiang Frank Wang"
                },
                "author": "Yu-Chiang Frank Wang",
                "arxiv_comment": "accepted to WACV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.16539v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.16539v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03131v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03131v1",
                "updated": "2024-12-04T08:51:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    51,
                    23,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T08:51:23Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    51,
                    23,
                    2,
                    339,
                    0
                ],
                "title": "Unifying KV Cache Compression for Large Language Models with LeanKV",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying KV Cache Compression for Large Language Models with LeanKV"
                },
                "summary": "Large language models (LLMs) demonstrate exceptional performance but incur\nhigh serving costs due to substantial memory demands, with the key-value (KV)\ncache being a primary bottleneck. Existing KV cache compression methods,\nincluding quantization and pruning, struggle with limitations such as uniform\ntreatment of keys and values and static memory allocation across attention\nheads. To address these challenges, we introduce LeanKV, a unified KV cache\ncompression framework that enhances LLM serving efficiency without compromising\naccuracy through three innovations: (1) Hetero-KV quantization, which stores\nkeys at a higher precision than values to reflect their greater impact on\nattention computations; (2) per-head dynamic sparsity, which allocates memory\nbased on token importance per head and per request; and (3) unified KV\ncompression, integrating mixed-precision quantization and selective pruning to\nenable a smooth tradeoff between model accuracy and memory efficiency. To\nefficiently support these techniques, LeanKV introduces systems optimizations\nincluding unified paging and on-GPU parallel memory management. Implemented on\nvLLM, LeanKV compresses the KV cache by $3.0\\times$ to $5.0\\times$ without\naccuracy loss and up to $11.0\\times$ with under 5% accuracy loss, enhancing\nthroughput by $1.9\\times$ to $2.5\\times$, and up to $6.9\\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate exceptional performance but incur\nhigh serving costs due to substantial memory demands, with the key-value (KV)\ncache being a primary bottleneck. Existing KV cache compression methods,\nincluding quantization and pruning, struggle with limitations such as uniform\ntreatment of keys and values and static memory allocation across attention\nheads. To address these challenges, we introduce LeanKV, a unified KV cache\ncompression framework that enhances LLM serving efficiency without compromising\naccuracy through three innovations: (1) Hetero-KV quantization, which stores\nkeys at a higher precision than values to reflect their greater impact on\nattention computations; (2) per-head dynamic sparsity, which allocates memory\nbased on token importance per head and per request; and (3) unified KV\ncompression, integrating mixed-precision quantization and selective pruning to\nenable a smooth tradeoff between model accuracy and memory efficiency. To\nefficiently support these techniques, LeanKV introduces systems optimizations\nincluding unified paging and on-GPU parallel memory management. Implemented on\nvLLM, LeanKV compresses the KV cache by $3.0\\times$ to $5.0\\times$ without\naccuracy loss and up to $11.0\\times$ with under 5% accuracy loss, enhancing\nthroughput by $1.9\\times$ to $2.5\\times$, and up to $6.9\\times$."
                },
                "authors": [
                    {
                        "name": "Yanqi Zhang"
                    },
                    {
                        "name": "Yuwei Hu"
                    },
                    {
                        "name": "Runyuan Zhao"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03131v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03131v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03123v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03123v1",
                "updated": "2024-12-04T08:43:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    43,
                    12,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T08:43:12Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    43,
                    12,
                    2,
                    339,
                    0
                ],
                "title": "Robust Multi-bit Text Watermark with LLM-based Paraphrasers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Multi-bit Text Watermark with LLM-based Paraphrasers"
                },
                "summary": "We propose an imperceptible multi-bit text watermark embedded by paraphrasing\nwith LLMs. We fine-tune a pair of LLM paraphrasers that are designed to behave\ndifferently so that their paraphrasing difference reflected in the text\nsemantics can be identified by a trained decoder. To embed our multi-bit\nwatermark, we use two paraphrasers alternatively to encode the pre-defined\nbinary code at the sentence level. Then we use a text classifier as the decoder\nto decode each bit of the watermark. Through extensive experiments, we show\nthat our watermarks can achieve over 99.99\\% detection AUC with small (1.1B)\ntext paraphrasers while keeping the semantic information of the original\nsentence. More importantly, our pipeline is robust under word substitution and\nsentence paraphrasing perturbations and generalizes well to\nout-of-distributional data. We also show the stealthiness of our watermark with\nLLM-based evaluation. We open-source the code:\nhttps://github.com/xiaojunxu/multi-bit-text-watermark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose an imperceptible multi-bit text watermark embedded by paraphrasing\nwith LLMs. We fine-tune a pair of LLM paraphrasers that are designed to behave\ndifferently so that their paraphrasing difference reflected in the text\nsemantics can be identified by a trained decoder. To embed our multi-bit\nwatermark, we use two paraphrasers alternatively to encode the pre-defined\nbinary code at the sentence level. Then we use a text classifier as the decoder\nto decode each bit of the watermark. Through extensive experiments, we show\nthat our watermarks can achieve over 99.99\\% detection AUC with small (1.1B)\ntext paraphrasers while keeping the semantic information of the original\nsentence. More importantly, our pipeline is robust under word substitution and\nsentence paraphrasing perturbations and generalizes well to\nout-of-distributional data. We also show the stealthiness of our watermark with\nLLM-based evaluation. We open-source the code:\nhttps://github.com/xiaojunxu/multi-bit-text-watermark."
                },
                "authors": [
                    {
                        "name": "Xiaojun Xu"
                    },
                    {
                        "name": "Jinghan Jia"
                    },
                    {
                        "name": "Yuanshun Yao"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Hang Li"
                    }
                ],
                "author_detail": {
                    "name": "Hang Li"
                },
                "author": "Hang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03123v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03123v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03121v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03121v1",
                "updated": "2024-12-04T08:40:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    40,
                    11,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T08:40:11Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    40,
                    11,
                    2,
                    339,
                    0
                ],
                "title": "Splats in Splats: Embedding Invisible 3D Watermark within Gaussian\n  Splatting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Splats in Splats: Embedding Invisible 3D Watermark within Gaussian\n  Splatting"
                },
                "summary": "3D Gaussian splatting (3DGS) has demonstrated impressive 3D reconstruction\nperformance with explicit scene representations. Given the widespread\napplication of 3DGS in 3D reconstruction and generation tasks, there is an\nurgent need to protect the copyright of 3DGS assets. However, existing\ncopyright protection techniques for 3DGS overlook the usability of 3D assets,\nposing challenges for practical deployment. Here we describe WaterGS, the first\n3DGS watermarking framework that embeds 3D content in 3DGS itself without\nmodifying any attributes of the vanilla 3DGS. To achieve this, we take a deep\ninsight into spherical harmonics (SH) and devise an importance-graded SH\ncoefficient encryption strategy to embed the hidden SH coefficients.\nFurthermore, we employ a convolutional autoencoder to establish a mapping\nbetween the original Gaussian primitives' opacity and the hidden Gaussian\nprimitives' opacity. Extensive experiments indicate that WaterGS significantly\noutperforms existing 3D steganography techniques, with 5.31% higher scene\nfidelity and 3X faster rendering speed, while ensuring security, robustness,\nand user experience. Codes and data will be released at\nhttps://water-gs.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D Gaussian splatting (3DGS) has demonstrated impressive 3D reconstruction\nperformance with explicit scene representations. Given the widespread\napplication of 3DGS in 3D reconstruction and generation tasks, there is an\nurgent need to protect the copyright of 3DGS assets. However, existing\ncopyright protection techniques for 3DGS overlook the usability of 3D assets,\nposing challenges for practical deployment. Here we describe WaterGS, the first\n3DGS watermarking framework that embeds 3D content in 3DGS itself without\nmodifying any attributes of the vanilla 3DGS. To achieve this, we take a deep\ninsight into spherical harmonics (SH) and devise an importance-graded SH\ncoefficient encryption strategy to embed the hidden SH coefficients.\nFurthermore, we employ a convolutional autoencoder to establish a mapping\nbetween the original Gaussian primitives' opacity and the hidden Gaussian\nprimitives' opacity. Extensive experiments indicate that WaterGS significantly\noutperforms existing 3D steganography techniques, with 5.31% higher scene\nfidelity and 3X faster rendering speed, while ensuring security, robustness,\nand user experience. Codes and data will be released at\nhttps://water-gs.github.io."
                },
                "authors": [
                    {
                        "name": "Yijia Guo"
                    },
                    {
                        "name": "Wenkai Huang"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Gaolei Li"
                    },
                    {
                        "name": "Hang Zhang"
                    },
                    {
                        "name": "Liwen Hu"
                    },
                    {
                        "name": "Jianhua Li"
                    },
                    {
                        "name": "Tiejun Huang"
                    },
                    {
                        "name": "Lei Ma"
                    }
                ],
                "author_detail": {
                    "name": "Lei Ma"
                },
                "author": "Lei Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03121v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03121v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.04566v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.04566v4",
                "updated": "2024-12-04T08:36:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    36,
                    13,
                    2,
                    339,
                    0
                ],
                "published": "2024-04-06T09:27:04Z",
                "published_parsed": [
                    2024,
                    4,
                    6,
                    9,
                    27,
                    4,
                    5,
                    97,
                    0
                ],
                "title": "Efficient and Green Large Language Models for Software Engineering:\n  Literature Review, Vision, and the Road Ahead",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient and Green Large Language Models for Software Engineering:\n  Literature Review, Vision, and the Road Ahead"
                },
                "summary": "Large Language Models (LLMs) have recently shown remarkable capabilities in\nvarious software engineering tasks, spurring the rapid growth of the Large\nLanguage Models for Software Engineering (LLM4SE) area. However, limited\nattention has been paid to developing efficient LLM4SE techniques that demand\nminimal computational cost, time, and memory resources, as well as green LLM4SE\nsolutions that reduce energy consumption, water usage, and carbon emissions.\n  This paper aims to redirect the focus of the research community towards the\nefficiency and greenness of LLM4SE, while also sharing potential research\ndirections to achieve this goal. It commences with a brief overview of the\nsignificance of LLM4SE and highlights the need for efficient and green LLM4SE\nsolutions. Subsequently, the paper presents a vision for a future where\nefficient and green LLM4SE revolutionizes the LLM-based software engineering\ntool landscape, benefiting various stakeholders, including industry, individual\npractitioners, and society. The paper then delineates a roadmap for future\nresearch, outlining specific research paths and potential solutions for the\nresearch community to pursue. While not intended to be a definitive guide, the\npaper aims to inspire further progress, with the ultimate goal of establishing\nefficient and green LLM4SE as a central element in the future of software\nengineering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have recently shown remarkable capabilities in\nvarious software engineering tasks, spurring the rapid growth of the Large\nLanguage Models for Software Engineering (LLM4SE) area. However, limited\nattention has been paid to developing efficient LLM4SE techniques that demand\nminimal computational cost, time, and memory resources, as well as green LLM4SE\nsolutions that reduce energy consumption, water usage, and carbon emissions.\n  This paper aims to redirect the focus of the research community towards the\nefficiency and greenness of LLM4SE, while also sharing potential research\ndirections to achieve this goal. It commences with a brief overview of the\nsignificance of LLM4SE and highlights the need for efficient and green LLM4SE\nsolutions. Subsequently, the paper presents a vision for a future where\nefficient and green LLM4SE revolutionizes the LLM-based software engineering\ntool landscape, benefiting various stakeholders, including industry, individual\npractitioners, and society. The paper then delineates a roadmap for future\nresearch, outlining specific research paths and potential solutions for the\nresearch community to pursue. While not intended to be a definitive guide, the\npaper aims to inspire further progress, with the ultimate goal of establishing\nefficient and green LLM4SE as a central element in the future of software\nengineering."
                },
                "authors": [
                    {
                        "name": "Jieke Shi"
                    },
                    {
                        "name": "Zhou Yang"
                    },
                    {
                        "name": "David Lo"
                    }
                ],
                "author_detail": {
                    "name": "David Lo"
                },
                "author": "David Lo",
                "arxiv_comment": "Accepted by ACM Transactions on Software Engineering and Methodology\n  (TOSEM), 23 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.04566v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.04566v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16730v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16730v3",
                "updated": "2024-12-04T08:21:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    21,
                    17,
                    2,
                    339,
                    0
                ],
                "published": "2024-11-23T09:32:44Z",
                "published_parsed": [
                    2024,
                    11,
                    23,
                    9,
                    32,
                    44,
                    5,
                    328,
                    0
                ],
                "title": "\"Moralized\" Multi-Step Jailbreak Prompts: Black-Box Testing of\n  Guardrails in Large Language Models for Verbal Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"Moralized\" Multi-Step Jailbreak Prompts: Black-Box Testing of\n  Guardrails in Large Language Models for Verbal Attacks"
                },
                "summary": "As the application of large language models continues to expand in various\nfields, it poses higher challenges to the effectiveness of identifying harmful\ncontent generation and guardrail mechanisms. This research aims to evaluate the\nguardrail effectiveness of GPT-4o, Grok-2 Beta, Llama 3.1 (405B), Gemini 1.5,\nand Claude 3.5 Sonnet through black-box testing of seemingly ethical multi-step\njailbreak prompts. It conducts ethical attacks by designing an identical\nmulti-step prompts that simulates the scenario of \"corporate middle managers\ncompeting for promotions.\" The data results show that the guardrails of the\nabove-mentioned LLMs were bypassed and the content of verbal attacks was\ngenerated. Claude 3.5 Sonnet's resistance to multi-step jailbreak prompts is\nmore obvious. To ensure objectivity, the experimental process, black box test\ncode, and enhanced guardrail code are uploaded to the GitHub repository:\nhttps://github.com/brucewang123456789/GeniusTrail.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the application of large language models continues to expand in various\nfields, it poses higher challenges to the effectiveness of identifying harmful\ncontent generation and guardrail mechanisms. This research aims to evaluate the\nguardrail effectiveness of GPT-4o, Grok-2 Beta, Llama 3.1 (405B), Gemini 1.5,\nand Claude 3.5 Sonnet through black-box testing of seemingly ethical multi-step\njailbreak prompts. It conducts ethical attacks by designing an identical\nmulti-step prompts that simulates the scenario of \"corporate middle managers\ncompeting for promotions.\" The data results show that the guardrails of the\nabove-mentioned LLMs were bypassed and the content of verbal attacks was\ngenerated. Claude 3.5 Sonnet's resistance to multi-step jailbreak prompts is\nmore obvious. To ensure objectivity, the experimental process, black box test\ncode, and enhanced guardrail code are uploaded to the GitHub repository:\nhttps://github.com/brucewang123456789/GeniusTrail.git."
                },
                "authors": [
                    {
                        "name": "Libo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Libo Wang"
                },
                "author": "Libo Wang",
                "arxiv_comment": "This paper has been submitted to Nature Machine Intelligence and\n  OpenReview preprints. It has 7 pages of text, 3 figures, and 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16730v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16730v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.06744v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.06744v2",
                "updated": "2024-12-04T08:16:01Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    16,
                    1,
                    2,
                    339,
                    0
                ],
                "published": "2024-04-10T05:10:05Z",
                "published_parsed": [
                    2024,
                    4,
                    10,
                    5,
                    10,
                    5,
                    2,
                    101,
                    0
                ],
                "title": "YOLO based Ocean Eddy Localization with AWS SageMaker",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "YOLO based Ocean Eddy Localization with AWS SageMaker"
                },
                "summary": "Ocean eddies play a significant role both on the sea surface and beneath it,\ncontributing to the sustainability of marine life dependent on oceanic\nbehaviors. Therefore, it is crucial to investigate ocean eddies to monitor\nchanges in the Earth, particularly in the oceans, and their impact on climate.\nThis study aims to pinpoint ocean eddies using AWS cloud services, specifically\nSageMaker. The primary objective is to detect small-scale (<20km) ocean eddies\nfrom satellite remote images and assess the feasibility of utilizing SageMaker,\nwhich offers tools for deploying AI applications. Moreover, this research not\nonly explores the deployment of cloud-based services for remote sensing of\nEarth data but also evaluates several YOLO (You Only Look Once) models using\nsingle and multi-GPU-based services in the cloud. Furthermore, this study\nunderscores the potential of these services, their limitations, challenges\nrelated to deployment and resource management, and their user-riendliness for\nEarth science projects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ocean eddies play a significant role both on the sea surface and beneath it,\ncontributing to the sustainability of marine life dependent on oceanic\nbehaviors. Therefore, it is crucial to investigate ocean eddies to monitor\nchanges in the Earth, particularly in the oceans, and their impact on climate.\nThis study aims to pinpoint ocean eddies using AWS cloud services, specifically\nSageMaker. The primary objective is to detect small-scale (<20km) ocean eddies\nfrom satellite remote images and assess the feasibility of utilizing SageMaker,\nwhich offers tools for deploying AI applications. Moreover, this research not\nonly explores the deployment of cloud-based services for remote sensing of\nEarth data but also evaluates several YOLO (You Only Look Once) models using\nsingle and multi-GPU-based services in the cloud. Furthermore, this study\nunderscores the potential of these services, their limitations, challenges\nrelated to deployment and resource management, and their user-riendliness for\nEarth science projects."
                },
                "authors": [
                    {
                        "name": "Seraj Al Mahmud Mostafa"
                    },
                    {
                        "name": "Jinbo Wang"
                    },
                    {
                        "name": "Benjamin Holt"
                    },
                    {
                        "name": "Jianwu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jianwu Wang"
                },
                "author": "Jianwu Wang",
                "arxiv_comment": "9 pages",
                "arxiv_journal_ref": "EEE Big Data 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.06744v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.06744v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16436v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16436v3",
                "updated": "2024-12-04T08:15:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    15,
                    35,
                    2,
                    339,
                    0
                ],
                "published": "2024-05-26T05:38:50Z",
                "published_parsed": [
                    2024,
                    5,
                    26,
                    5,
                    38,
                    50,
                    6,
                    147,
                    0
                ],
                "title": "Provably Mitigating Overoptimization in RLHF: Your SFT Loss is\n  Implicitly an Adversarial Regularizer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Provably Mitigating Overoptimization in RLHF: Your SFT Loss is\n  Implicitly an Adversarial Regularizer"
                },
                "summary": "Aligning generative models with human preference via RLHF typically suffers\nfrom overoptimization, where an imperfectly learned reward model can misguide\nthe generative model to output undesired responses. We investigate this problem\nin a principled manner by identifying the source of the misalignment as a form\nof distributional shift and uncertainty in learning human preferences. To\nmitigate overoptimization, we first propose a theoretical algorithm that\nchooses the best policy for an adversarially chosen reward model; one that\nsimultaneously minimizes the maximum likelihood estimation of the loss and a\nreward penalty term. Here, the reward penalty term is introduced to prevent the\npolicy from choosing actions with spurious high proxy rewards, resulting in\nprovable sample efficiency of the algorithm under a partial coverage style\ncondition. Moving from theory to practice, the proposed algorithm further\nenjoys an equivalent but surprisingly easy-to-implement reformulation. Using\nthe equivalence between reward models and the corresponding optimal policy, the\nalgorithm features a simple objective that combines: (i) a preference\noptimization loss that directly aligns the policy with human preference, and\n(ii) a supervised learning loss that explicitly imitates the policy with a\n(suitable) baseline distribution. In the context of aligning large language\nmodels (LLM), this objective fuses the direct preference optimization (DPO)\nloss with the supervised fine-tuning (SFT) loss to help mitigate the\noveroptimization towards undesired responses, for which we name the algorithm\nRegularized Preference Optimization (RPO). Experiments of aligning LLMs\ndemonstrate the improved performance of RPO compared with DPO baselines. Our\nwork sheds light on the interplay between preference optimization and SFT in\ntuning LLMs with both theoretical guarantees and empirical evidence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning generative models with human preference via RLHF typically suffers\nfrom overoptimization, where an imperfectly learned reward model can misguide\nthe generative model to output undesired responses. We investigate this problem\nin a principled manner by identifying the source of the misalignment as a form\nof distributional shift and uncertainty in learning human preferences. To\nmitigate overoptimization, we first propose a theoretical algorithm that\nchooses the best policy for an adversarially chosen reward model; one that\nsimultaneously minimizes the maximum likelihood estimation of the loss and a\nreward penalty term. Here, the reward penalty term is introduced to prevent the\npolicy from choosing actions with spurious high proxy rewards, resulting in\nprovable sample efficiency of the algorithm under a partial coverage style\ncondition. Moving from theory to practice, the proposed algorithm further\nenjoys an equivalent but surprisingly easy-to-implement reformulation. Using\nthe equivalence between reward models and the corresponding optimal policy, the\nalgorithm features a simple objective that combines: (i) a preference\noptimization loss that directly aligns the policy with human preference, and\n(ii) a supervised learning loss that explicitly imitates the policy with a\n(suitable) baseline distribution. In the context of aligning large language\nmodels (LLM), this objective fuses the direct preference optimization (DPO)\nloss with the supervised fine-tuning (SFT) loss to help mitigate the\noveroptimization towards undesired responses, for which we name the algorithm\nRegularized Preference Optimization (RPO). Experiments of aligning LLMs\ndemonstrate the improved performance of RPO compared with DPO baselines. Our\nwork sheds light on the interplay between preference optimization and SFT in\ntuning LLMs with both theoretical guarantees and empirical evidence."
                },
                "authors": [
                    {
                        "name": "Zhihan Liu"
                    },
                    {
                        "name": "Miao Lu"
                    },
                    {
                        "name": "Shenao Zhang"
                    },
                    {
                        "name": "Boyi Liu"
                    },
                    {
                        "name": "Hongyi Guo"
                    },
                    {
                        "name": "Yingxiang Yang"
                    },
                    {
                        "name": "Jose Blanchet"
                    },
                    {
                        "name": "Zhaoran Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhaoran Wang"
                },
                "author": "Zhaoran Wang",
                "arxiv_comment": "Accepted by The Thirty-Eighth Annual Conference on Neural Information\n  Processing Systems. 31 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.16436v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16436v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03107v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03107v1",
                "updated": "2024-12-04T08:13:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    13,
                    29,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T08:13:29Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    13,
                    29,
                    2,
                    339,
                    0
                ],
                "title": "CredID: Credible Multi-Bit Watermark for Large Language Models\n  Identification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CredID: Credible Multi-Bit Watermark for Large Language Models\n  Identification"
                },
                "summary": "Large Language Models (LLMs) are widely used in complex natural language\nprocessing tasks but raise privacy and security concerns due to the lack of\nidentity recognition. This paper proposes a multi-party credible watermarking\nframework (CredID) involving a trusted third party (TTP) and multiple LLM\nvendors to address these issues. In the watermark embedding stage, vendors\nrequest a seed from the TTP to generate watermarked text without sending the\nuser's prompt. In the extraction stage, the TTP coordinates each vendor to\nextract and verify the watermark from the text. This provides a credible\nwatermarking scheme while preserving vendor privacy. Furthermore, current\nwatermarking algorithms struggle with text quality, information capacity, and\nrobustness, making it challenging to meet the diverse identification needs of\nLLMs. Thus, we propose a novel multi-bit watermarking algorithm and an\nopen-source toolkit to facilitate research. Experiments show our CredID\nenhances watermark credibility and efficiency without compromising text\nquality. Additionally, we successfully utilized this framework to achieve\nhighly accurate identification among multiple LLM vendors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are widely used in complex natural language\nprocessing tasks but raise privacy and security concerns due to the lack of\nidentity recognition. This paper proposes a multi-party credible watermarking\nframework (CredID) involving a trusted third party (TTP) and multiple LLM\nvendors to address these issues. In the watermark embedding stage, vendors\nrequest a seed from the TTP to generate watermarked text without sending the\nuser's prompt. In the extraction stage, the TTP coordinates each vendor to\nextract and verify the watermark from the text. This provides a credible\nwatermarking scheme while preserving vendor privacy. Furthermore, current\nwatermarking algorithms struggle with text quality, information capacity, and\nrobustness, making it challenging to meet the diverse identification needs of\nLLMs. Thus, we propose a novel multi-bit watermarking algorithm and an\nopen-source toolkit to facilitate research. Experiments show our CredID\nenhances watermark credibility and efficiency without compromising text\nquality. Additionally, we successfully utilized this framework to achieve\nhighly accurate identification among multiple LLM vendors."
                },
                "authors": [
                    {
                        "name": "Haoyu Jiang"
                    },
                    {
                        "name": "Xuhong Wang"
                    },
                    {
                        "name": "Ping Yi"
                    },
                    {
                        "name": "Shanzhe Lei"
                    },
                    {
                        "name": "Yilun Lin"
                    }
                ],
                "author_detail": {
                    "name": "Yilun Lin"
                },
                "author": "Yilun Lin",
                "arxiv_comment": "v1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03107v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03107v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03104v1",
                "updated": "2024-12-04T08:06:15Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    6,
                    15,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T08:06:15Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    6,
                    15,
                    2,
                    339,
                    0
                ],
                "title": "ChatTS: Aligning Time Series with LLMs via Synthetic Data for Enhanced\n  Understanding and Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChatTS: Aligning Time Series with LLMs via Synthetic Data for Enhanced\n  Understanding and Reasoning"
                },
                "summary": "Understanding time series is crucial for its application in real-world\nscenarios. Recently, large language models (LLMs) have been increasingly\napplied to time series tasks, leveraging their strong language capabilities to\nenhance various applications. However, research on multimodal LLMs (MLLMs) for\ntime series understanding and reasoning remains limited, primarily due to the\nscarcity of high-quality datasets that align time series with textual\ninformation. This paper introduces ChatTS, a novel MLLM designed for time\nseries analysis. ChatTS treats time series as a modality, similar to how vision\nMLLMs process images, enabling it to perform both understanding and reasoning\nwith time series. To address the scarcity of training data, we propose an\nattribute-based method for generating synthetic time series with detailed\nattribute descriptions. We further introduce Time Series Evol-Instruct, a novel\napproach that generates diverse time series Q&As, enhancing the model's\nreasoning capabilities. To the best of our knowledge, ChatTS is the first MLLM\nthat takes multivariate time series as input, which is fine-tuned exclusively\non synthetic datasets. We evaluate its performance using benchmark datasets\nwith real-world data, including six alignment tasks and four reasoning tasks.\nOur results show that ChatTS significantly outperforms existing vision-based\nMLLMs (e.g., GPT-4o) and text/agent-based LLMs, achieving a 46.0% improvement\nin alignment tasks and a 25.8% improvement in reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding time series is crucial for its application in real-world\nscenarios. Recently, large language models (LLMs) have been increasingly\napplied to time series tasks, leveraging their strong language capabilities to\nenhance various applications. However, research on multimodal LLMs (MLLMs) for\ntime series understanding and reasoning remains limited, primarily due to the\nscarcity of high-quality datasets that align time series with textual\ninformation. This paper introduces ChatTS, a novel MLLM designed for time\nseries analysis. ChatTS treats time series as a modality, similar to how vision\nMLLMs process images, enabling it to perform both understanding and reasoning\nwith time series. To address the scarcity of training data, we propose an\nattribute-based method for generating synthetic time series with detailed\nattribute descriptions. We further introduce Time Series Evol-Instruct, a novel\napproach that generates diverse time series Q&As, enhancing the model's\nreasoning capabilities. To the best of our knowledge, ChatTS is the first MLLM\nthat takes multivariate time series as input, which is fine-tuned exclusively\non synthetic datasets. We evaluate its performance using benchmark datasets\nwith real-world data, including six alignment tasks and four reasoning tasks.\nOur results show that ChatTS significantly outperforms existing vision-based\nMLLMs (e.g., GPT-4o) and text/agent-based LLMs, achieving a 46.0% improvement\nin alignment tasks and a 25.8% improvement in reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Zhe Xie"
                    },
                    {
                        "name": "Zeyan Li"
                    },
                    {
                        "name": "Xiao He"
                    },
                    {
                        "name": "Longlong Xu"
                    },
                    {
                        "name": "Xidao Wen"
                    },
                    {
                        "name": "Tieying Zhang"
                    },
                    {
                        "name": "Jianjun Chen"
                    },
                    {
                        "name": "Rui Shi"
                    },
                    {
                        "name": "Dan Pei"
                    }
                ],
                "author_detail": {
                    "name": "Dan Pei"
                },
                "author": "Dan Pei",
                "arxiv_comment": "14 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03096v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03096v1",
                "updated": "2024-12-04T07:50:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    7,
                    50,
                    17,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T07:50:17Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    7,
                    50,
                    17,
                    2,
                    339,
                    0
                ],
                "title": "TOOL-ED: Enhancing Empathetic Response Generation with the Tool Calling\n  Capability of LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TOOL-ED: Enhancing Empathetic Response Generation with the Tool Calling\n  Capability of LLM"
                },
                "summary": "Empathetic conversation is a crucial characteristic in daily conversations\nbetween individuals. Nowadays, Large Language models (LLMs) have shown\noutstanding performance in generating empathetic responses. Knowledge bases\nlike COMET can assist LLMs in mitigating illusions and enhancing the\nunderstanding of users' intentions and emotions. However, models remain heavily\nreliant on fixed knowledge bases and unrestricted incorporation of external\nknowledge can introduce noise. Tool learning is a flexible end-to-end approach\nthat assists LLMs in handling complex problems. In this paper, we propose\nEmotional Knowledge Tool Calling (EKTC) framework, which encapsulates the\ncommonsense knowledge bases as empathetic tools, enabling LLMs to integrate\nexternal knowledge flexibly through tool calling. In order to adapt the models\nto the new task, we construct a novel dataset TOOL-ED based on the\nEMPATHETICMPATHETIC DIALOGUE (ED) dataset. We validate EKTC on the ED dataset,\nand the experimental results demonstrate that our framework can enhance the\nability of LLMs to generate empathetic responses effectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empathetic conversation is a crucial characteristic in daily conversations\nbetween individuals. Nowadays, Large Language models (LLMs) have shown\noutstanding performance in generating empathetic responses. Knowledge bases\nlike COMET can assist LLMs in mitigating illusions and enhancing the\nunderstanding of users' intentions and emotions. However, models remain heavily\nreliant on fixed knowledge bases and unrestricted incorporation of external\nknowledge can introduce noise. Tool learning is a flexible end-to-end approach\nthat assists LLMs in handling complex problems. In this paper, we propose\nEmotional Knowledge Tool Calling (EKTC) framework, which encapsulates the\ncommonsense knowledge bases as empathetic tools, enabling LLMs to integrate\nexternal knowledge flexibly through tool calling. In order to adapt the models\nto the new task, we construct a novel dataset TOOL-ED based on the\nEMPATHETICMPATHETIC DIALOGUE (ED) dataset. We validate EKTC on the ED dataset,\nand the experimental results demonstrate that our framework can enhance the\nability of LLMs to generate empathetic responses effectively."
                },
                "authors": [
                    {
                        "name": "Huiying Cao"
                    },
                    {
                        "name": "Yiqun Zhang"
                    },
                    {
                        "name": "Shi Feng"
                    },
                    {
                        "name": "Xiaocui Yang"
                    },
                    {
                        "name": "Daling Wang"
                    },
                    {
                        "name": "Yifei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yifei Zhang"
                },
                "author": "Yifei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03096v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03096v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01544v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01544v3",
                "updated": "2024-12-04T07:47:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    7,
                    47,
                    11,
                    2,
                    339,
                    0
                ],
                "published": "2024-10-02T13:30:32Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    13,
                    30,
                    32,
                    2,
                    276,
                    0
                ],
                "title": "Boosting Weakly-Supervised Referring Image Segmentation via Progressive\n  Comprehension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Weakly-Supervised Referring Image Segmentation via Progressive\n  Comprehension"
                },
                "summary": "This paper explores the weakly-supervised referring image segmentation (WRIS)\nproblem, and focuses on a challenging setup where target localization is\nlearned directly from image-text pairs. We note that the input text description\ntypically already contains detailed information on how to localize the target\nobject, and we also observe that humans often follow a step-by-step\ncomprehension process (\\ie, progressively utilizing target-related attributes\nand relations as cues) to identify the target object. Hence, we propose a novel\nProgressive Comprehension Network (PCNet) to leverage target-related textual\ncues from the input description for progressively localizing the target object.\nSpecifically, we first use a Large Language Model (LLM) to decompose the input\ntext description into short phrases. These short phrases are taken as\ntarget-related cues and fed into a Conditional Referring Module (CRM) in\nmultiple stages, to allow updating the referring text embedding and enhance the\nresponse map for target localization in a multi-stage manner. Based on the CRM,\nwe then propose a Region-aware Shrinking (RaS) loss to constrain the visual\nlocalization to be conducted progressively in a coarse-to-fine manner across\ndifferent stages. Finally, we introduce an Instance-aware Disambiguation (IaD)\nloss to suppress instance localization ambiguity by differentiating overlapping\nresponse maps generated by different referring texts on the same image.\nExtensive experiments show that our method outperforms SOTA methods on three\ncommon benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the weakly-supervised referring image segmentation (WRIS)\nproblem, and focuses on a challenging setup where target localization is\nlearned directly from image-text pairs. We note that the input text description\ntypically already contains detailed information on how to localize the target\nobject, and we also observe that humans often follow a step-by-step\ncomprehension process (\\ie, progressively utilizing target-related attributes\nand relations as cues) to identify the target object. Hence, we propose a novel\nProgressive Comprehension Network (PCNet) to leverage target-related textual\ncues from the input description for progressively localizing the target object.\nSpecifically, we first use a Large Language Model (LLM) to decompose the input\ntext description into short phrases. These short phrases are taken as\ntarget-related cues and fed into a Conditional Referring Module (CRM) in\nmultiple stages, to allow updating the referring text embedding and enhance the\nresponse map for target localization in a multi-stage manner. Based on the CRM,\nwe then propose a Region-aware Shrinking (RaS) loss to constrain the visual\nlocalization to be conducted progressively in a coarse-to-fine manner across\ndifferent stages. Finally, we introduce an Instance-aware Disambiguation (IaD)\nloss to suppress instance localization ambiguity by differentiating overlapping\nresponse maps generated by different referring texts on the same image.\nExtensive experiments show that our method outperforms SOTA methods on three\ncommon benchmarks."
                },
                "authors": [
                    {
                        "name": "Zaiquan Yang"
                    },
                    {
                        "name": "Yuhao Liu"
                    },
                    {
                        "name": "Jiaying Lin"
                    },
                    {
                        "name": "Gerhard Hancke"
                    },
                    {
                        "name": "Rynson W. H. Lau"
                    }
                ],
                "author_detail": {
                    "name": "Rynson W. H. Lau"
                },
                "author": "Rynson W. H. Lau",
                "arxiv_comment": "Accepted to NeurIPS2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01544v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01544v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03092v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03092v1",
                "updated": "2024-12-04T07:44:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    7,
                    44,
                    35,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T07:44:35Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    7,
                    44,
                    35,
                    2,
                    339,
                    0
                ],
                "title": "Revolve: Optimizing AI Systems by Tracking Response Evolution in Textual\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revolve: Optimizing AI Systems by Tracking Response Evolution in Textual\n  Optimization"
                },
                "summary": "Recent advancements in large language models (LLMs) have significantly\nenhanced the ability of LLM-based systems to perform complex tasks through\nnatural language processing and tool interaction. However, optimizing these\nLLM-based systems for specific tasks remains challenging, often requiring\nmanual interventions like prompt engineering and hyperparameter tuning.\nExisting automatic optimization methods, such as textual feedback-based\ntechniques (e.g., TextGrad), tend to focus on immediate feedback, analogous to\nusing immediate derivatives in traditional numerical gradient descent. However,\nrelying solely on such feedback can be limited when the adjustments made in\nresponse to this feedback are either too small or fluctuate irregularly,\npotentially slowing down or even stalling the optimization process. To overcome\nthese challenges, more adaptive methods are needed, especially in situations\nwhere the system's response is evolving slowly or unpredictably. In this paper,\nwe introduce REVOLVE, an optimization method that tracks how \"R\"esponses\n\"EVOLVE\" across iterations in LLM systems. By focusing on the evolution of\nresponses over time, REVOLVE enables more stable and effective optimization by\nmaking thoughtful, progressive adjustments at each step. Experimental results\ndemonstrate that REVOLVE outperforms competitive baselines, achieving a 7.8%\nimprovement in prompt optimization, a 20.72% gain in solution refinement, and a\n29.17% increase in code optimization. Additionally, REVOLVE converges in fewer\niterations, resulting in significant computational savings. These advantages\nhighlight its adaptability and efficiency, positioning REVOLVE as a valuable\ntool for optimizing LLM-based systems and accelerating the development of\nnext-generation AI technologies. Code is available at:\nhttps://github.com/Peiyance/REVOLVE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have significantly\nenhanced the ability of LLM-based systems to perform complex tasks through\nnatural language processing and tool interaction. However, optimizing these\nLLM-based systems for specific tasks remains challenging, often requiring\nmanual interventions like prompt engineering and hyperparameter tuning.\nExisting automatic optimization methods, such as textual feedback-based\ntechniques (e.g., TextGrad), tend to focus on immediate feedback, analogous to\nusing immediate derivatives in traditional numerical gradient descent. However,\nrelying solely on such feedback can be limited when the adjustments made in\nresponse to this feedback are either too small or fluctuate irregularly,\npotentially slowing down or even stalling the optimization process. To overcome\nthese challenges, more adaptive methods are needed, especially in situations\nwhere the system's response is evolving slowly or unpredictably. In this paper,\nwe introduce REVOLVE, an optimization method that tracks how \"R\"esponses\n\"EVOLVE\" across iterations in LLM systems. By focusing on the evolution of\nresponses over time, REVOLVE enables more stable and effective optimization by\nmaking thoughtful, progressive adjustments at each step. Experimental results\ndemonstrate that REVOLVE outperforms competitive baselines, achieving a 7.8%\nimprovement in prompt optimization, a 20.72% gain in solution refinement, and a\n29.17% increase in code optimization. Additionally, REVOLVE converges in fewer\niterations, resulting in significant computational savings. These advantages\nhighlight its adaptability and efficiency, positioning REVOLVE as a valuable\ntool for optimizing LLM-based systems and accelerating the development of\nnext-generation AI technologies. Code is available at:\nhttps://github.com/Peiyance/REVOLVE."
                },
                "authors": [
                    {
                        "name": "Peiyan Zhang"
                    },
                    {
                        "name": "Haibo Jin"
                    },
                    {
                        "name": "Leyang Hu"
                    },
                    {
                        "name": "Xinnuo Li"
                    },
                    {
                        "name": "Liying Kang"
                    },
                    {
                        "name": "Man Luo"
                    },
                    {
                        "name": "Yangqiu Song"
                    },
                    {
                        "name": "Haohan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haohan Wang"
                },
                "author": "Haohan Wang",
                "arxiv_comment": "20 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03092v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03092v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.8",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03085v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03085v1",
                "updated": "2024-12-04T07:26:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    7,
                    26,
                    44,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T07:26:44Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    7,
                    26,
                    44,
                    2,
                    339,
                    0
                ],
                "title": "Mimir: Improving Video Diffusion Models for Precise Text Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mimir: Improving Video Diffusion Models for Precise Text Understanding"
                },
                "summary": "Text serves as the key control signal in video generation due to its\nnarrative nature. To render text descriptions into video clips, current video\ndiffusion models borrow features from text encoders yet struggle with limited\ntext comprehension. The recent success of large language models (LLMs)\nshowcases the power of decoder-only transformers, which offers three clear\nbenefits for text-to-video (T2V) generation, namely, precise text understanding\nresulting from the superior scalability, imagination beyond the input text\nenabled by next token prediction, and flexibility to prioritize user interests\nthrough instruction tuning. Nevertheless, the feature distribution gap emerging\nfrom the two different text modeling paradigms hinders the direct use of LLMs\nin established T2V models. This work addresses this challenge with Mimir, an\nend-to-end training framework featuring a carefully tailored token fuser to\nharmonize the outputs from text encoders and LLMs. Such a design allows the T2V\nmodel to fully leverage learned video priors while capitalizing on the\ntext-related capability of LLMs. Extensive quantitative and qualitative results\ndemonstrate the effectiveness of Mimir in generating high-quality videos with\nexcellent text comprehension, especially when processing short captions and\nmanaging shifting motions. Project page:\nhttps://lucaria-academy.github.io/Mimir/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text serves as the key control signal in video generation due to its\nnarrative nature. To render text descriptions into video clips, current video\ndiffusion models borrow features from text encoders yet struggle with limited\ntext comprehension. The recent success of large language models (LLMs)\nshowcases the power of decoder-only transformers, which offers three clear\nbenefits for text-to-video (T2V) generation, namely, precise text understanding\nresulting from the superior scalability, imagination beyond the input text\nenabled by next token prediction, and flexibility to prioritize user interests\nthrough instruction tuning. Nevertheless, the feature distribution gap emerging\nfrom the two different text modeling paradigms hinders the direct use of LLMs\nin established T2V models. This work addresses this challenge with Mimir, an\nend-to-end training framework featuring a carefully tailored token fuser to\nharmonize the outputs from text encoders and LLMs. Such a design allows the T2V\nmodel to fully leverage learned video priors while capitalizing on the\ntext-related capability of LLMs. Extensive quantitative and qualitative results\ndemonstrate the effectiveness of Mimir in generating high-quality videos with\nexcellent text comprehension, especially when processing short captions and\nmanaging shifting motions. Project page:\nhttps://lucaria-academy.github.io/Mimir/"
                },
                "authors": [
                    {
                        "name": "Shuai Tan"
                    },
                    {
                        "name": "Biao Gong"
                    },
                    {
                        "name": "Yutong Feng"
                    },
                    {
                        "name": "Kecheng Zheng"
                    },
                    {
                        "name": "Dandan Zheng"
                    },
                    {
                        "name": "Shuwei Shi"
                    },
                    {
                        "name": "Yujun Shen"
                    },
                    {
                        "name": "Jingdong Chen"
                    },
                    {
                        "name": "Ming Yang"
                    }
                ],
                "author_detail": {
                    "name": "Ming Yang"
                },
                "author": "Ming Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03085v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03085v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02538v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02538v2",
                "updated": "2024-12-04T07:11:07Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    7,
                    11,
                    7,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-03T16:32:19Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    16,
                    32,
                    19,
                    1,
                    338,
                    0
                ],
                "title": "On Privacy, Security, and Trustworthiness in Distributed Wireless Large\n  AI Models (WLAM)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Privacy, Security, and Trustworthiness in Distributed Wireless Large\n  AI Models (WLAM)"
                },
                "summary": "Combining wireless communication with large artificial intelligence (AI)\nmodels can open up a myriad of novel application scenarios. In sixth generation\n(6G) networks, ubiquitous communication and computing resources allow large AI\nmodels to serve democratic large AI models-related services to enable real-time\napplications like autonomous vehicles, smart cities, and Internet of Things\n(IoT) ecosystems. However, the security considerations and sustainable\ncommunication resources limit the deployment of large AI models over\ndistributed wireless networks. This paper provides a comprehensive overview of\nprivacy, security, and trustworthy for distributed wireless large AI model\n(WLAM). In particular, a detailed privacy and security are analysis for\ndistributed WLAM is fist revealed. The classifications and theoretical findings\nabout privacy and security in distributed WLAM are discussed. Then the\ntrustworthy and ethics for implementing distributed WLAM are described.\nFinally, the comprehensive applications of distributed WLAM are presented in\nthe context of electromagnetic signal processing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combining wireless communication with large artificial intelligence (AI)\nmodels can open up a myriad of novel application scenarios. In sixth generation\n(6G) networks, ubiquitous communication and computing resources allow large AI\nmodels to serve democratic large AI models-related services to enable real-time\napplications like autonomous vehicles, smart cities, and Internet of Things\n(IoT) ecosystems. However, the security considerations and sustainable\ncommunication resources limit the deployment of large AI models over\ndistributed wireless networks. This paper provides a comprehensive overview of\nprivacy, security, and trustworthy for distributed wireless large AI model\n(WLAM). In particular, a detailed privacy and security are analysis for\ndistributed WLAM is fist revealed. The classifications and theoretical findings\nabout privacy and security in distributed WLAM are discussed. Then the\ntrustworthy and ethics for implementing distributed WLAM are described.\nFinally, the comprehensive applications of distributed WLAM are presented in\nthe context of electromagnetic signal processing."
                },
                "authors": [
                    {
                        "name": "Zhaohui Yang"
                    },
                    {
                        "name": "Wei Xu"
                    },
                    {
                        "name": "Le Liang"
                    },
                    {
                        "name": "Yuanhao Cui"
                    },
                    {
                        "name": "Zhijin Qin"
                    },
                    {
                        "name": "Merouane Debbah"
                    }
                ],
                "author_detail": {
                    "name": "Merouane Debbah"
                },
                "author": "Merouane Debbah",
                "arxiv_comment": "12 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02538v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02538v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04422v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04422v6",
                "updated": "2024-12-04T07:03:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    7,
                    3,
                    49,
                    2,
                    339,
                    0
                ],
                "published": "2024-10-06T09:29:19Z",
                "published_parsed": [
                    2024,
                    10,
                    6,
                    9,
                    29,
                    19,
                    6,
                    280,
                    0
                ],
                "title": "Long-context Language Models Are Not Good At Retrieval Without Enough\n  Steps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context Language Models Are Not Good At Retrieval Without Enough\n  Steps"
                },
                "summary": "Long-context language models (LCLMs), characterized by their extensive\ncontext window, are becoming increasingly popular. However, despite they are\nnearly perfect at standard long-context retrieval, we find they are actually\nnot good at all of them. Specifically, we identify 2 basic cases,\n\"multi-matching retrieval,\" and \"logic-based retrieval\", which LLMs struggle to\nsolve under normal settings. Moreover, we find these cases can only be well\naddressed by specific CoT prompting, with enough reasoning steps. This finding\nreminds the developers and users of LCLMs that relying on LCLMs to directly\nperform even basic retrieval tasks may be unreliable, rather, a sufficiently\nlong reasoning process is necessary.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context language models (LCLMs), characterized by their extensive\ncontext window, are becoming increasingly popular. However, despite they are\nnearly perfect at standard long-context retrieval, we find they are actually\nnot good at all of them. Specifically, we identify 2 basic cases,\n\"multi-matching retrieval,\" and \"logic-based retrieval\", which LLMs struggle to\nsolve under normal settings. Moreover, we find these cases can only be well\naddressed by specific CoT prompting, with enough reasoning steps. This finding\nreminds the developers and users of LCLMs that relying on LCLMs to directly\nperform even basic retrieval tasks may be unreliable, rather, a sufficiently\nlong reasoning process is necessary."
                },
                "authors": [
                    {
                        "name": "Yijiong Yu"
                    },
                    {
                        "name": "Ma Xiufa"
                    },
                    {
                        "name": "Fang Jianwei"
                    },
                    {
                        "name": "Zhi Xu"
                    },
                    {
                        "name": "Su Guangyao"
                    },
                    {
                        "name": "Wang Jiancheng"
                    },
                    {
                        "name": "Yongfeng Huang"
                    },
                    {
                        "name": "Zhixiao Qi"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Weifeng Liu"
                    },
                    {
                        "name": "Ran Chen"
                    },
                    {
                        "name": "Ji Pei"
                    }
                ],
                "author_detail": {
                    "name": "Ji Pei"
                },
                "author": "Ji Pei",
                "arxiv_comment": "Our code is publicly available at\n  https://github.com/yuyijiong/hard_retrieval_for_llm and the datasets is at\n  https://huggingface.co/datasets/yuyijiong/difficult_retrieval",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04422v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04422v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03075v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03075v1",
                "updated": "2024-12-04T06:52:10Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    6,
                    52,
                    10,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T06:52:10Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    6,
                    52,
                    10,
                    2,
                    339,
                    0
                ],
                "title": "ASR-EC Benchmark: Evaluating Large Language Models on Chinese ASR Error\n  Correction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASR-EC Benchmark: Evaluating Large Language Models on Chinese ASR Error\n  Correction"
                },
                "summary": "Automatic speech Recognition (ASR) is a fundamental and important task in the\nfield of speech and natural language processing. It is an inherent building\nblock in many applications such as voice assistant, speech translation, etc.\nDespite the advancement of ASR technologies in recent years, it is still\ninevitable for modern ASR systems to have a substantial number of erroneous\nrecognition due to environmental noise, ambiguity, etc. Therefore, the error\ncorrection in ASR is crucial.\n  Motivated by this, this paper studies ASR error correction in the Chinese\nlanguage, which is one of the most popular languages and enjoys a large number\nof users in the world. We first create a benchmark dataset named \\emph{ASR-EC}\nthat contains a wide spectrum of ASR errors generated by industry-grade ASR\nsystems. To the best of our knowledge, it is the first Chinese ASR error\ncorrection benchmark. Then, inspired by the recent advances in \\emph{large\nlanguage models (LLMs)}, we investigate how to harness the power of LLMs to\ncorrect ASR errors. We apply LLMs to ASR error correction in three paradigms.\nThe first paradigm is prompting, which is further categorized as zero-shot,\nfew-shot, and multi-step. The second paradigm is finetuning, which finetunes\nLLMs with ASR error correction data. The third paradigm is multi-modal\naugmentation, which collectively utilizes the audio and ASR transcripts for\nerror correction. Extensive experiments reveal that prompting is not effective\nfor ASR error correction. Finetuning is effective only for a portion of LLMs.\nMulti-modal augmentation is the most effective method for error correction and\nachieves state-of-the-art performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic speech Recognition (ASR) is a fundamental and important task in the\nfield of speech and natural language processing. It is an inherent building\nblock in many applications such as voice assistant, speech translation, etc.\nDespite the advancement of ASR technologies in recent years, it is still\ninevitable for modern ASR systems to have a substantial number of erroneous\nrecognition due to environmental noise, ambiguity, etc. Therefore, the error\ncorrection in ASR is crucial.\n  Motivated by this, this paper studies ASR error correction in the Chinese\nlanguage, which is one of the most popular languages and enjoys a large number\nof users in the world. We first create a benchmark dataset named \\emph{ASR-EC}\nthat contains a wide spectrum of ASR errors generated by industry-grade ASR\nsystems. To the best of our knowledge, it is the first Chinese ASR error\ncorrection benchmark. Then, inspired by the recent advances in \\emph{large\nlanguage models (LLMs)}, we investigate how to harness the power of LLMs to\ncorrect ASR errors. We apply LLMs to ASR error correction in three paradigms.\nThe first paradigm is prompting, which is further categorized as zero-shot,\nfew-shot, and multi-step. The second paradigm is finetuning, which finetunes\nLLMs with ASR error correction data. The third paradigm is multi-modal\naugmentation, which collectively utilizes the audio and ASR transcripts for\nerror correction. Extensive experiments reveal that prompting is not effective\nfor ASR error correction. Finetuning is effective only for a portion of LLMs.\nMulti-modal augmentation is the most effective method for error correction and\nachieves state-of-the-art performance."
                },
                "authors": [
                    {
                        "name": "Victor Junqiu Wei"
                    },
                    {
                        "name": "Weicheng Wang"
                    },
                    {
                        "name": "Di Jiang"
                    },
                    {
                        "name": "Yuanfeng Song"
                    },
                    {
                        "name": "Lu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Lu Wang"
                },
                "author": "Lu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03075v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03075v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.17696v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.17696v4",
                "updated": "2024-12-04T06:33:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    6,
                    33,
                    55,
                    2,
                    339,
                    0
                ],
                "published": "2023-11-29T15:02:46Z",
                "published_parsed": [
                    2023,
                    11,
                    29,
                    15,
                    2,
                    46,
                    2,
                    333,
                    0
                ],
                "title": "How to Build an AI Tutor that Can Adapt to Any Course and Provide\n  Accurate Answers Using Large Language Model and Retrieval-Augmented\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to Build an AI Tutor that Can Adapt to Any Course and Provide\n  Accurate Answers Using Large Language Model and Retrieval-Augmented\n  Generation"
                },
                "summary": "This paper proposes a low-code solution to build an AI tutor that leverages\nadvanced AI techniques to provide accurate and contextually relevant responses\nin a personalized learning environment. The OpenAI Assistants API allows AI\nTutor to easily embed, store, retrieve, and manage files and chat history,\nenabling a low-code solution. Large Language Models (LLMs) and\nRetrieval-Augmented Generation (RAG) technology generate sophisticated answers\nbased on course-specific materials. The application efficiently organizes and\nretrieves relevant information through vector embedding and similarity-based\nretrieval algorithms. The AI Tutor prototype demonstrates its ability to\ngenerate relevant, accurate answers with source citations. It represents a\nsignificant advancement in technology-enhanced tutoring systems, democratizing\naccess to high-quality, customized educational support in higher education.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a low-code solution to build an AI tutor that leverages\nadvanced AI techniques to provide accurate and contextually relevant responses\nin a personalized learning environment. The OpenAI Assistants API allows AI\nTutor to easily embed, store, retrieve, and manage files and chat history,\nenabling a low-code solution. Large Language Models (LLMs) and\nRetrieval-Augmented Generation (RAG) technology generate sophisticated answers\nbased on course-specific materials. The application efficiently organizes and\nretrieves relevant information through vector embedding and similarity-based\nretrieval algorithms. The AI Tutor prototype demonstrates its ability to\ngenerate relevant, accurate answers with source citations. It represents a\nsignificant advancement in technology-enhanced tutoring systems, democratizing\naccess to high-quality, customized educational support in higher education."
                },
                "authors": [
                    {
                        "name": "Chenxi Dong"
                    },
                    {
                        "name": "Kan Chen"
                    },
                    {
                        "name": "Shupei Cheng"
                    },
                    {
                        "name": "Chujie Wen"
                    }
                ],
                "author_detail": {
                    "name": "Chujie Wen"
                },
                "author": "Chujie Wen",
                "arxiv_comment": "4 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.17696v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.17696v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12329v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12329v2",
                "updated": "2024-12-04T06:30:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    6,
                    30,
                    6,
                    2,
                    339,
                    0
                ],
                "published": "2024-06-18T06:54:05Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    6,
                    54,
                    5,
                    1,
                    170,
                    0
                ],
                "title": "Opt-Out: Investigating Entity-Level Unlearning for Large Language Models\n  via Optimal Transport",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Opt-Out: Investigating Entity-Level Unlearning for Large Language Models\n  via Optimal Transport"
                },
                "summary": "Instruction-following large language models (LLMs), such as ChatGPT, have\nbecome widely popular among everyday users. However, these models inadvertently\ndisclose private, sensitive information to their users, underscoring the need\nfor machine unlearning techniques to remove selective information from the\nmodels. While prior work has focused on forgetting small, random subsets of\ntraining data at the instance-level, we argue that real-world scenarios often\nrequire the removal of an entire user data, which may require a more careful\nmaneuver. In this study, we explore entity-level unlearning, which aims to\nerase all knowledge related to a target entity while preserving the remaining\nmodel capabilities. To address this, we introduce Opt-Out, an optimal\ntransport-based unlearning method that utilizes the Wasserstein distance from\nthe model's initial parameters to achieve more effective and fine-grained\nunlearning. We also present the first Entity-Level Unlearning Dataset (ELUDe)\ndesigned to evaluate entity-level unlearning. Our empirical results demonstrate\nthat Opt-Out surpasses existing methods, establishing a new standard for secure\nand adaptable LLMs that can accommodate user data removal requests without the\nneed for full retraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction-following large language models (LLMs), such as ChatGPT, have\nbecome widely popular among everyday users. However, these models inadvertently\ndisclose private, sensitive information to their users, underscoring the need\nfor machine unlearning techniques to remove selective information from the\nmodels. While prior work has focused on forgetting small, random subsets of\ntraining data at the instance-level, we argue that real-world scenarios often\nrequire the removal of an entire user data, which may require a more careful\nmaneuver. In this study, we explore entity-level unlearning, which aims to\nerase all knowledge related to a target entity while preserving the remaining\nmodel capabilities. To address this, we introduce Opt-Out, an optimal\ntransport-based unlearning method that utilizes the Wasserstein distance from\nthe model's initial parameters to achieve more effective and fine-grained\nunlearning. We also present the first Entity-Level Unlearning Dataset (ELUDe)\ndesigned to evaluate entity-level unlearning. Our empirical results demonstrate\nthat Opt-Out surpasses existing methods, establishing a new standard for secure\nand adaptable LLMs that can accommodate user data removal requests without the\nneed for full retraining."
                },
                "authors": [
                    {
                        "name": "Minseok Choi"
                    },
                    {
                        "name": "Daniel Rim"
                    },
                    {
                        "name": "Dohyun Lee"
                    },
                    {
                        "name": "Jaegul Choo"
                    }
                ],
                "author_detail": {
                    "name": "Jaegul Choo"
                },
                "author": "Jaegul Choo",
                "arxiv_comment": "17 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12329v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12329v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00721v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00721v2",
                "updated": "2024-12-04T06:23:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    6,
                    23,
                    40,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-01T08:07:01Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    8,
                    7,
                    1,
                    6,
                    336,
                    0
                ],
                "title": "A Comparative Study of LLM-based ASR and Whisper in Low Resource and\n  Code Switching Scenario",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comparative Study of LLM-based ASR and Whisper in Low Resource and\n  Code Switching Scenario"
                },
                "summary": "Large Language Models (LLMs) have showcased exceptional performance across\ndiverse NLP tasks, and their integration with speech encoder is rapidly\nemerging as a dominant trend in the Automatic Speech Recognition (ASR) field.\nPrevious works mainly concentrated on leveraging LLMs for speech recognition in\nEnglish and Chinese. However, their potential for addressing speech recognition\nchallenges in low resource settings remains underexplored. Hence, in this work,\nwe aim to explore the capability of LLMs in low resource ASR and\nMandarin-English code switching ASR. We also evaluate and compare the\nrecognition performance of LLM-based ASR systems against Whisper model.\nExtensive experiments demonstrate that LLM-based ASR yields a relative gain of\n12.8\\% over the Whisper model in low resource ASR while Whisper performs better\nin Mandarin-English code switching ASR. We hope that this study could shed\nlight on ASR for low resource scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have showcased exceptional performance across\ndiverse NLP tasks, and their integration with speech encoder is rapidly\nemerging as a dominant trend in the Automatic Speech Recognition (ASR) field.\nPrevious works mainly concentrated on leveraging LLMs for speech recognition in\nEnglish and Chinese. However, their potential for addressing speech recognition\nchallenges in low resource settings remains underexplored. Hence, in this work,\nwe aim to explore the capability of LLMs in low resource ASR and\nMandarin-English code switching ASR. We also evaluate and compare the\nrecognition performance of LLM-based ASR systems against Whisper model.\nExtensive experiments demonstrate that LLM-based ASR yields a relative gain of\n12.8\\% over the Whisper model in low resource ASR while Whisper performs better\nin Mandarin-English code switching ASR. We hope that this study could shed\nlight on ASR for low resource scenarios."
                },
                "authors": [
                    {
                        "name": "Zheshu Song"
                    },
                    {
                        "name": "Ziyang Ma"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Jianheng Zhuo"
                    },
                    {
                        "name": "Xie Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xie Chen"
                },
                "author": "Xie Chen",
                "arxiv_comment": "This work hasn't been finished yet",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00721v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00721v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03051v1",
                "updated": "2024-12-04T06:11:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    6,
                    11,
                    9,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T06:11:09Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    6,
                    11,
                    9,
                    2,
                    339,
                    0
                ],
                "title": "Less is More: A Stealthy and Efficient Adversarial Attack Method for\n  DRL-based Autonomous Driving Policies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Less is More: A Stealthy and Efficient Adversarial Attack Method for\n  DRL-based Autonomous Driving Policies"
                },
                "summary": "Despite significant advancements in deep reinforcement learning (DRL)-based\nautonomous driving policies, these policies still exhibit vulnerability to\nadversarial attacks. This vulnerability poses a formidable challenge to the\npractical deployment of these policies in autonomous driving. Designing\neffective adversarial attacks is an indispensable prerequisite for enhancing\nthe robustness of these policies. In view of this, we present a novel stealthy\nand efficient adversarial attack method for DRL-based autonomous driving\npolicies. Specifically, we introduce a DRL-based adversary designed to trigger\nsafety violations (e.g., collisions) by injecting adversarial samples at\ncritical moments. We model the attack as a mixed-integer optimization problem\nand formulate it as a Markov decision process. Then, we train the adversary to\nlearn the optimal policy for attacking at critical moments without domain\nknowledge. Furthermore, we introduce attack-related information and a\ntrajectory clipping method to enhance the learning capability of the adversary.\nFinally, we validate our method in an unprotected left-turn scenario across\ndifferent traffic densities. The experimental results show that our method\nachieves more than 90% collision rate within three attacks in most cases.\nFurthermore, our method achieves more than 130% improvement in attack\nefficiency compared to the unlimited attack method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite significant advancements in deep reinforcement learning (DRL)-based\nautonomous driving policies, these policies still exhibit vulnerability to\nadversarial attacks. This vulnerability poses a formidable challenge to the\npractical deployment of these policies in autonomous driving. Designing\neffective adversarial attacks is an indispensable prerequisite for enhancing\nthe robustness of these policies. In view of this, we present a novel stealthy\nand efficient adversarial attack method for DRL-based autonomous driving\npolicies. Specifically, we introduce a DRL-based adversary designed to trigger\nsafety violations (e.g., collisions) by injecting adversarial samples at\ncritical moments. We model the attack as a mixed-integer optimization problem\nand formulate it as a Markov decision process. Then, we train the adversary to\nlearn the optimal policy for attacking at critical moments without domain\nknowledge. Furthermore, we introduce attack-related information and a\ntrajectory clipping method to enhance the learning capability of the adversary.\nFinally, we validate our method in an unprotected left-turn scenario across\ndifferent traffic densities. The experimental results show that our method\nachieves more than 90% collision rate within three attacks in most cases.\nFurthermore, our method achieves more than 130% improvement in attack\nefficiency compared to the unlimited attack method."
                },
                "authors": [
                    {
                        "name": "Junchao Fan"
                    },
                    {
                        "name": "Xuyang Lei"
                    },
                    {
                        "name": "Xiaolin Chang"
                    },
                    {
                        "name": "Jelena Mii"
                    },
                    {
                        "name": "Vojislav B. Mii"
                    }
                ],
                "author_detail": {
                    "name": "Vojislav B. Mii"
                },
                "author": "Vojislav B. Mii",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10145v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10145v2",
                "updated": "2024-12-04T05:54:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    5,
                    54,
                    43,
                    2,
                    339,
                    0
                ],
                "published": "2024-11-15T12:39:02Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    12,
                    39,
                    2,
                    4,
                    320,
                    0
                ],
                "title": "An Effective Framework to Help Large Language Models Handle\n  Numeric-involved Long-context Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Effective Framework to Help Large Language Models Handle\n  Numeric-involved Long-context Tasks"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nhandling long texts and have almost perfect performance in traditional\nretrieval tasks. However, their performance significantly degrades when it\ncomes to numerical calculations in the long-context. Numeric-involved\nlong-context tasks typically cannot be addressed by current LLMs in normal\nsettings due to their inherent limitations in simultaneously handling complex\nand massive information. Some CoT like prompting methods can improve accuracy\nbut demands massive output tokens, which is costly and slow. To address this\nissue, we propose a workflow, which decompose a numeric-involved long-context\ntask into 4 low-level subtasks: judging, extracting and processing with code\nand conclusion. The former 2 subtasks is relatively simple, which allows us to\nuse smaller models for efficiently processing long context. When numerical\ncalculations are required, we use code generated by LLMs to avoid the\ndisadvantage of LLM not being good at calculations. The results in 2\nnumeric-involved long-context benchmarks demonstrate our workflow can not only\nimprove accuracy, but also significantly reduce the cost of API calls.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nhandling long texts and have almost perfect performance in traditional\nretrieval tasks. However, their performance significantly degrades when it\ncomes to numerical calculations in the long-context. Numeric-involved\nlong-context tasks typically cannot be addressed by current LLMs in normal\nsettings due to their inherent limitations in simultaneously handling complex\nand massive information. Some CoT like prompting methods can improve accuracy\nbut demands massive output tokens, which is costly and slow. To address this\nissue, we propose a workflow, which decompose a numeric-involved long-context\ntask into 4 low-level subtasks: judging, extracting and processing with code\nand conclusion. The former 2 subtasks is relatively simple, which allows us to\nuse smaller models for efficiently processing long context. When numerical\ncalculations are required, we use code generated by LLMs to avoid the\ndisadvantage of LLM not being good at calculations. The results in 2\nnumeric-involved long-context benchmarks demonstrate our workflow can not only\nimprove accuracy, but also significantly reduce the cost of API calls."
                },
                "authors": [
                    {
                        "name": "Yijiong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Yijiong Yu"
                },
                "author": "Yijiong Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10145v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10145v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15862v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15862v2",
                "updated": "2024-12-04T05:52:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    5,
                    52,
                    3,
                    2,
                    339,
                    0
                ],
                "published": "2024-11-24T14:38:59Z",
                "published_parsed": [
                    2024,
                    11,
                    24,
                    14,
                    38,
                    59,
                    6,
                    329,
                    0
                ],
                "title": "LLMs Do Not Think Step-by-step In Implicit Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs Do Not Think Step-by-step In Implicit Reasoning"
                },
                "summary": "It has been well-known that Chain-of-Thought can remarkably enhance LLMs'\nperformance on complex tasks. However, because it also introduces slower\ninference speeds and higher computational costs, many researches have attempted\nto use implicit CoT, which does not need LLMs to explicitly generate the\nintermediate steps. But there is still gap between their efficacy and typical\nexplicit CoT methods. This leaves us a doubt that, does implicit CoT really\nequal to explicit CoT? Therefore, in this study, we address this question\nthrough experiments. We probe the information of intermediate steps from the\nmodel's hidden states when it is performing implicit CoT. The results\nsurprisingly indicate that LLMs hardly think about intermediate steps,\nsuggesting they may just rely on experience rather than strict step-by-step\nreasoning. Moreover, we find LLMs' implicit reasoning capabilities are\nsusceptible and unstable, reaffirming the necessity of explicit CoT to\neffectively support complex tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It has been well-known that Chain-of-Thought can remarkably enhance LLMs'\nperformance on complex tasks. However, because it also introduces slower\ninference speeds and higher computational costs, many researches have attempted\nto use implicit CoT, which does not need LLMs to explicitly generate the\nintermediate steps. But there is still gap between their efficacy and typical\nexplicit CoT methods. This leaves us a doubt that, does implicit CoT really\nequal to explicit CoT? Therefore, in this study, we address this question\nthrough experiments. We probe the information of intermediate steps from the\nmodel's hidden states when it is performing implicit CoT. The results\nsurprisingly indicate that LLMs hardly think about intermediate steps,\nsuggesting they may just rely on experience rather than strict step-by-step\nreasoning. Moreover, we find LLMs' implicit reasoning capabilities are\nsusceptible and unstable, reaffirming the necessity of explicit CoT to\neffectively support complex tasks."
                },
                "authors": [
                    {
                        "name": "Yijiong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Yijiong Yu"
                },
                "author": "Yijiong Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15862v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15862v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03032v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03032v1",
                "updated": "2024-12-04T05:05:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    5,
                    5,
                    30,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T05:05:30Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    5,
                    5,
                    30,
                    2,
                    339,
                    0
                ],
                "title": "Edge System Design Using Containers and Unikernels for IoT Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge System Design Using Containers and Unikernels for IoT Applications"
                },
                "summary": "Edge computing is emerging as a key enabler of low-latency, high-efficiency\nprocessing for the Internet of Things (IoT) and other real-time applications.\nTo support these demands, containerization has gained traction in edge\ncomputing due to its lightweight virtualization and efficient resource\nmanagement. However, there is currently no established framework to leverage\nboth containers and unikernels on edge devices for optimized IoT deployments.\nThis paper proposes a hybrid edge system design that leverages container and\nunikernel technologies to optimize resource utilization based on application\ncomplexity. Containers are employed for resource-intensive applications, e.g.,\ncomputer vision, providing faster processing, flexibility, and ease of\ndeployment. In contrast, unikernels are used for lightweight applications,\noffering enhanced resource performance with minimal overhead. Our system design\nalso incorporates container orchestration to efficiently manage multiple\ninstances across the edge efficiently, ensuring scalability and reliability. We\ndemonstrate our hybrid approach's performance and efficiency advantages through\nreal-world computer vision and data science applications on ARM-powered edge\ndevice. Our results demonstrate that this hybrid approach improves resource\nutilization and reduces latency compared to traditional virtualized solutions.\nThis work provides insights into optimizing edge infrastructures, enabling more\nefficient and specialized deployment strategies for diverse application\nworkloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge computing is emerging as a key enabler of low-latency, high-efficiency\nprocessing for the Internet of Things (IoT) and other real-time applications.\nTo support these demands, containerization has gained traction in edge\ncomputing due to its lightweight virtualization and efficient resource\nmanagement. However, there is currently no established framework to leverage\nboth containers and unikernels on edge devices for optimized IoT deployments.\nThis paper proposes a hybrid edge system design that leverages container and\nunikernel technologies to optimize resource utilization based on application\ncomplexity. Containers are employed for resource-intensive applications, e.g.,\ncomputer vision, providing faster processing, flexibility, and ease of\ndeployment. In contrast, unikernels are used for lightweight applications,\noffering enhanced resource performance with minimal overhead. Our system design\nalso incorporates container orchestration to efficiently manage multiple\ninstances across the edge efficiently, ensuring scalability and reliability. We\ndemonstrate our hybrid approach's performance and efficiency advantages through\nreal-world computer vision and data science applications on ARM-powered edge\ndevice. Our results demonstrate that this hybrid approach improves resource\nutilization and reduces latency compared to traditional virtualized solutions.\nThis work provides insights into optimizing edge infrastructures, enabling more\nefficient and specialized deployment strategies for diverse application\nworkloads."
                },
                "authors": [
                    {
                        "name": "Shahidullah Kaiser"
                    },
                    {
                        "name": "Ali Saman Tosun"
                    },
                    {
                        "name": "Turgay Korkmaz"
                    }
                ],
                "author_detail": {
                    "name": "Turgay Korkmaz"
                },
                "author": "Turgay Korkmaz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03032v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03032v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05109v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05109v3",
                "updated": "2024-12-04T04:57:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    4,
                    57,
                    4,
                    2,
                    339,
                    0
                ],
                "published": "2024-08-09T14:59:36Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    14,
                    59,
                    36,
                    4,
                    222,
                    0
                ],
                "title": "A Survey of NL2SQL with Large Language Models: Where are we, and where\n  are we going?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of NL2SQL with Large Language Models: Where are we, and where\n  are we going?"
                },
                "summary": "Translating users' natural language queries (NL) into SQL queries (i.e.,\nNL2SQL, a.k.a., Text-to-SQL) can significantly reduce barriers to accessing\nrelational databases and support various commercial applications. The\nperformance of NL2SQL has been greatly enhanced with the emergence of Large\nLanguage Models (LLMs). In this survey, we provide a comprehensive review of\nNL2SQL techniques powered by LLMs, covering its entire lifecycle from the\nfollowing four aspects: (1) Model: NL2SQL translation techniques that tackle\nnot only NL ambiguity and under-specification, but also properly map NL with\ndatabase schema and instances; (2) Data: From the collection of training data,\ndata synthesis due to training data scarcity, to NL2SQL benchmarks; (3)\nEvaluation: Evaluating NL2SQL methods from multiple angles using different\nmetrics and granularities; and (4) Error Analysis: analyzing NL2SQL errors to\nfind the root cause and guiding NL2SQL models to evolve. Moreover, we provide a\nrule of thumb for developing NL2SQL solutions. Finally, we discuss the research\nchallenges and open problems of NL2SQL in the LLMs era.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Translating users' natural language queries (NL) into SQL queries (i.e.,\nNL2SQL, a.k.a., Text-to-SQL) can significantly reduce barriers to accessing\nrelational databases and support various commercial applications. The\nperformance of NL2SQL has been greatly enhanced with the emergence of Large\nLanguage Models (LLMs). In this survey, we provide a comprehensive review of\nNL2SQL techniques powered by LLMs, covering its entire lifecycle from the\nfollowing four aspects: (1) Model: NL2SQL translation techniques that tackle\nnot only NL ambiguity and under-specification, but also properly map NL with\ndatabase schema and instances; (2) Data: From the collection of training data,\ndata synthesis due to training data scarcity, to NL2SQL benchmarks; (3)\nEvaluation: Evaluating NL2SQL methods from multiple angles using different\nmetrics and granularities; and (4) Error Analysis: analyzing NL2SQL errors to\nfind the root cause and guiding NL2SQL models to evolve. Moreover, we provide a\nrule of thumb for developing NL2SQL solutions. Finally, we discuss the research\nchallenges and open problems of NL2SQL in the LLMs era."
                },
                "authors": [
                    {
                        "name": "Xinyu Liu"
                    },
                    {
                        "name": "Shuyu Shen"
                    },
                    {
                        "name": "Boyan Li"
                    },
                    {
                        "name": "Peixian Ma"
                    },
                    {
                        "name": "Runzhi Jiang"
                    },
                    {
                        "name": "Yuxin Zhang"
                    },
                    {
                        "name": "Ju Fan"
                    },
                    {
                        "name": "Guoliang Li"
                    },
                    {
                        "name": "Nan Tang"
                    },
                    {
                        "name": "Yuyu Luo"
                    }
                ],
                "author_detail": {
                    "name": "Yuyu Luo"
                },
                "author": "Yuyu Luo",
                "arxiv_comment": "20 pages, 11 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05109v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05109v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03030v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03030v1",
                "updated": "2024-12-04T04:56:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    4,
                    56,
                    27,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T04:56:27Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    4,
                    56,
                    27,
                    2,
                    339,
                    0
                ],
                "title": "Exploring the Viability of Unikernels for ARM-powered Edge Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Viability of Unikernels for ARM-powered Edge Computing"
                },
                "summary": "The rapid expansion of IoT devices and their real-time applications have\ndriven a growing need for edge computing. To meet this need, efficient and\nsecure solutions are required for running such applications on\nresource-constrained devices with limited power, CPU, and memory. Unikernel,\nwith its minimalistic design and application-specific approach, offers a\npromising alternative to traditional virtualization and container technologies\nin these environments. The existing research does not thoroughly examine the\nfeasibility of using unikernel for edge computing. This paper investigates the\npotential of unikernel for ARM-powered edge computing by evaluating the\nperformance and efficiency of three prominent unikernel systems such as OSv,\nNanos, and Unikraft against Docker container. We experiment with real-world\nedge computing applications and utilize key metrics such as boot time,\nexecution time, memory usage, CPU overhead, and network performance to\ndetermine how unikernel performs under the constraints of edge devices. Our\nfindings reveal the potential advantages of unikernel in terms of reduced\nresource consumption and faster startup times while highlighting areas where\nthey may need further optimization for edge deployment. This study provides\nvaluable insights for researchers and practitioners considering unikernel as a\nlightweight, efficient solution for edge computing on ARM architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid expansion of IoT devices and their real-time applications have\ndriven a growing need for edge computing. To meet this need, efficient and\nsecure solutions are required for running such applications on\nresource-constrained devices with limited power, CPU, and memory. Unikernel,\nwith its minimalistic design and application-specific approach, offers a\npromising alternative to traditional virtualization and container technologies\nin these environments. The existing research does not thoroughly examine the\nfeasibility of using unikernel for edge computing. This paper investigates the\npotential of unikernel for ARM-powered edge computing by evaluating the\nperformance and efficiency of three prominent unikernel systems such as OSv,\nNanos, and Unikraft against Docker container. We experiment with real-world\nedge computing applications and utilize key metrics such as boot time,\nexecution time, memory usage, CPU overhead, and network performance to\ndetermine how unikernel performs under the constraints of edge devices. Our\nfindings reveal the potential advantages of unikernel in terms of reduced\nresource consumption and faster startup times while highlighting areas where\nthey may need further optimization for edge deployment. This study provides\nvaluable insights for researchers and practitioners considering unikernel as a\nlightweight, efficient solution for edge computing on ARM architectures."
                },
                "authors": [
                    {
                        "name": "Shahidullah Kaiser"
                    },
                    {
                        "name": "Ali Saman Tosun"
                    },
                    {
                        "name": "Turgay Korkmaz"
                    }
                ],
                "author_detail": {
                    "name": "Turgay Korkmaz"
                },
                "author": "Turgay Korkmaz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03030v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03030v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03025v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03025v1",
                "updated": "2024-12-04T04:38:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    4,
                    38,
                    35,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T04:38:35Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    4,
                    38,
                    35,
                    2,
                    339,
                    0
                ],
                "title": "Human Variability vs. Machine Consistency: A Linguistic Analysis of\n  Texts Generated by Humans and Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human Variability vs. Machine Consistency: A Linguistic Analysis of\n  Texts Generated by Humans and Large Language Models"
                },
                "summary": "The rapid advancements in large language models (LLMs) have significantly\nimproved their ability to generate natural language, making texts generated by\nLLMs increasingly indistinguishable from human-written texts. Recent research\nhas predominantly focused on using LLMs to classify text as either\nhuman-written or machine-generated. In our study, we adopt a different approach\nby profiling texts spanning four domains based on 250 distinct linguistic\nfeatures. We select the M4 dataset from the Subtask B of SemEval 2024 Task 8.\nWe automatically calculate various linguistic features with the LFTK tool and\nadditionally measure the average syntactic depth, semantic similarity, and\nemotional content for each document. We then apply a two-dimensional PCA\nreduction to all the calculated features. Our analyses reveal significant\ndifferences between human-written texts and those generated by LLMs,\nparticularly in the variability of these features, which we find to be\nconsiderably higher in human-written texts. This discrepancy is especially\nevident in text genres with less rigid linguistic style constraints. Our\nfindings indicate that humans write texts that are less cognitively demanding,\nwith higher semantic content, and richer emotional content compared to texts\ngenerated by LLMs. These insights underscore the need for incorporating\nmeaningful linguistic features to enhance the understanding of textual outputs\nof LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancements in large language models (LLMs) have significantly\nimproved their ability to generate natural language, making texts generated by\nLLMs increasingly indistinguishable from human-written texts. Recent research\nhas predominantly focused on using LLMs to classify text as either\nhuman-written or machine-generated. In our study, we adopt a different approach\nby profiling texts spanning four domains based on 250 distinct linguistic\nfeatures. We select the M4 dataset from the Subtask B of SemEval 2024 Task 8.\nWe automatically calculate various linguistic features with the LFTK tool and\nadditionally measure the average syntactic depth, semantic similarity, and\nemotional content for each document. We then apply a two-dimensional PCA\nreduction to all the calculated features. Our analyses reveal significant\ndifferences between human-written texts and those generated by LLMs,\nparticularly in the variability of these features, which we find to be\nconsiderably higher in human-written texts. This discrepancy is especially\nevident in text genres with less rigid linguistic style constraints. Our\nfindings indicate that humans write texts that are less cognitively demanding,\nwith higher semantic content, and richer emotional content compared to texts\ngenerated by LLMs. These insights underscore the need for incorporating\nmeaningful linguistic features to enhance the understanding of textual outputs\nof LLMs."
                },
                "authors": [
                    {
                        "name": "Sergio E. Zanotto"
                    },
                    {
                        "name": "Segun Aroyehun"
                    }
                ],
                "author_detail": {
                    "name": "Segun Aroyehun"
                },
                "author": "Segun Aroyehun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03025v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03025v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08688v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08688v2",
                "updated": "2024-12-04T04:28:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    4,
                    28,
                    41,
                    2,
                    339,
                    0
                ],
                "published": "2024-10-11T10:21:42Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    10,
                    21,
                    42,
                    4,
                    285,
                    0
                ],
                "title": "Chain-of-Restoration: Multi-Task Image Restoration Models are Zero-Shot\n  Step-by-Step Universal Image Restorers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Restoration: Multi-Task Image Restoration Models are Zero-Shot\n  Step-by-Step Universal Image Restorers"
                },
                "summary": "Despite previous image restoration (IR) methods have often concentrated on\nisolated degradations, recent research has increasingly focused on addressing\ncomposite degradations involving a complex combination of multiple isolated\ndegradations. However, current IR methods for composite degradations require\nbuilding training data that contain an exponential number of possible\ndegradation combinations, which brings in a significant burden. To alleviate\nthis issue, this paper proposes a new task setting, i.e. Universal Image\nRestoration (UIR). Specifically, UIR doesn't require training on all the\ndegradation combinations but only on a set of degradation bases and then\nremoving any degradation that these bases can potentially compose in a\nzero-shot manner. Inspired by the Chain-of-Thought that prompts large language\nmodels (LLMs) to address problems step-by-step, we propose Chain-of-Restoration\n(CoR) mechanism, which instructs models to remove unknown composite\ndegradations step-by-step. By integrating a simple Degradation Discriminator\ninto pre-trained multi-task models, CoR facilitates the process where models\nremove one degradation basis per step, continuing this process until the image\nis fully restored from the unknown composite degradation. Extensive experiments\nshow that CoR can significantly improve model performance in removing composite\ndegradations, achieving comparable or better results than those\nstate-of-the-art (SoTA) methods trained on all degradations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite previous image restoration (IR) methods have often concentrated on\nisolated degradations, recent research has increasingly focused on addressing\ncomposite degradations involving a complex combination of multiple isolated\ndegradations. However, current IR methods for composite degradations require\nbuilding training data that contain an exponential number of possible\ndegradation combinations, which brings in a significant burden. To alleviate\nthis issue, this paper proposes a new task setting, i.e. Universal Image\nRestoration (UIR). Specifically, UIR doesn't require training on all the\ndegradation combinations but only on a set of degradation bases and then\nremoving any degradation that these bases can potentially compose in a\nzero-shot manner. Inspired by the Chain-of-Thought that prompts large language\nmodels (LLMs) to address problems step-by-step, we propose Chain-of-Restoration\n(CoR) mechanism, which instructs models to remove unknown composite\ndegradations step-by-step. By integrating a simple Degradation Discriminator\ninto pre-trained multi-task models, CoR facilitates the process where models\nremove one degradation basis per step, continuing this process until the image\nis fully restored from the unknown composite degradation. Extensive experiments\nshow that CoR can significantly improve model performance in removing composite\ndegradations, achieving comparable or better results than those\nstate-of-the-art (SoTA) methods trained on all degradations."
                },
                "authors": [
                    {
                        "name": "Jin Cao"
                    },
                    {
                        "name": "Deyu Meng"
                    },
                    {
                        "name": "Xiangyong Cao"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyong Cao"
                },
                "author": "Xiangyong Cao",
                "arxiv_comment": "code: https://github.com/toummHus/Chain-of-Restoration",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08688v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08688v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.15316v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.15316v4",
                "updated": "2024-12-04T04:08:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    4,
                    8,
                    49,
                    2,
                    339,
                    0
                ],
                "published": "2023-11-26T14:35:23Z",
                "published_parsed": [
                    2023,
                    11,
                    26,
                    14,
                    35,
                    23,
                    6,
                    330,
                    0
                ],
                "title": "Sibyl: Empowering Empathetic Dialogue Generation in Large Language\n  Models via Sensible and Visionary Commonsense Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sibyl: Empowering Empathetic Dialogue Generation in Large Language\n  Models via Sensible and Visionary Commonsense Inference"
                },
                "summary": "Recently, there has been a heightened interest in building chatbots based on\nLarge Language Models (LLMs) to emulate human-like qualities in multi-turn\nconversations. Despite having access to commonsense knowledge to better\nunderstand the psychological aspects and causality of dialogue context, even\nthese powerful LLMs struggle to achieve the goals of empathy and emotional\nsupport. Current commonsense knowledge derived from dialogue contexts is\ninherently limited and often fails to adequately anticipate the future course\nof a dialogue. This lack of foresight can mislead LLMs and hinder their ability\nto provide effective support. In response to this challenge, we present an\ninnovative framework named Sensible and Visionary Commonsense Knowledge\n(Sibyl). Designed to concentrate on the immediately succeeding dialogue, this\nparadigm equips LLMs with the capability to uncover the implicit requirements\nof the conversation, aiming to elicit more empathetic responses. Experimental\nresults demonstrate that incorporating our paradigm for acquiring commonsense\nknowledge into LLMs comprehensively enhances the quality of their responses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, there has been a heightened interest in building chatbots based on\nLarge Language Models (LLMs) to emulate human-like qualities in multi-turn\nconversations. Despite having access to commonsense knowledge to better\nunderstand the psychological aspects and causality of dialogue context, even\nthese powerful LLMs struggle to achieve the goals of empathy and emotional\nsupport. Current commonsense knowledge derived from dialogue contexts is\ninherently limited and often fails to adequately anticipate the future course\nof a dialogue. This lack of foresight can mislead LLMs and hinder their ability\nto provide effective support. In response to this challenge, we present an\ninnovative framework named Sensible and Visionary Commonsense Knowledge\n(Sibyl). Designed to concentrate on the immediately succeeding dialogue, this\nparadigm equips LLMs with the capability to uncover the implicit requirements\nof the conversation, aiming to elicit more empathetic responses. Experimental\nresults demonstrate that incorporating our paradigm for acquiring commonsense\nknowledge into LLMs comprehensively enhances the quality of their responses."
                },
                "authors": [
                    {
                        "name": "Lanrui Wang"
                    },
                    {
                        "name": "Jiangnan Li"
                    },
                    {
                        "name": "Chenxu Yang"
                    },
                    {
                        "name": "Zheng Lin"
                    },
                    {
                        "name": "Hongyin Tang"
                    },
                    {
                        "name": "Huan Liu"
                    },
                    {
                        "name": "Yanan Cao"
                    },
                    {
                        "name": "Jingang Wang"
                    },
                    {
                        "name": "Weiping Wang"
                    }
                ],
                "author_detail": {
                    "name": "Weiping Wang"
                },
                "author": "Weiping Wang",
                "arxiv_comment": "Accepted by COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.15316v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.15316v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01130v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01130v2",
                "updated": "2024-12-04T03:34:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    3,
                    34,
                    42,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-02T05:10:41Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    5,
                    10,
                    41,
                    0,
                    337,
                    0
                ],
                "title": "Enhancing Function-Calling Capabilities in LLMs: Strategies for Prompt\n  Formats, Data Integration, and Multilingual Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Function-Calling Capabilities in LLMs: Strategies for Prompt\n  Formats, Data Integration, and Multilingual Translation"
                },
                "summary": "Large language models (LLMs) have significantly advanced autonomous agents,\nparticularly in zero-shot tool usage, also known as function calling. This\nresearch delves into enhancing the function-calling capabilities of LLMs by\nexploring different approaches, including prompt formats for integrating\nfunction descriptions, blending function-calling and instruction-following\ndata, introducing a novel Decision Token for conditional prompts, leveraging\nchain-of-thought reasoning, and overcoming multilingual challenges with a\ntranslation pipeline. Our key findings and contributions are as follows: (1)\nInstruction-following data improves both function-calling accuracy and\nrelevance detection. (2) The use of the newly proposed Decision Token, combined\nwith synthetic non-function-call data, enhances relevance detection. (3) A\ntailored translation pipeline effectively overcomes multilingual limitations,\ndemonstrating significant improvements in Traditional Chinese. These insights\nhighlight the potential for improved function-calling capabilities and\nmultilingual applications in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have significantly advanced autonomous agents,\nparticularly in zero-shot tool usage, also known as function calling. This\nresearch delves into enhancing the function-calling capabilities of LLMs by\nexploring different approaches, including prompt formats for integrating\nfunction descriptions, blending function-calling and instruction-following\ndata, introducing a novel Decision Token for conditional prompts, leveraging\nchain-of-thought reasoning, and overcoming multilingual challenges with a\ntranslation pipeline. Our key findings and contributions are as follows: (1)\nInstruction-following data improves both function-calling accuracy and\nrelevance detection. (2) The use of the newly proposed Decision Token, combined\nwith synthetic non-function-call data, enhances relevance detection. (3) A\ntailored translation pipeline effectively overcomes multilingual limitations,\ndemonstrating significant improvements in Traditional Chinese. These insights\nhighlight the potential for improved function-calling capabilities and\nmultilingual applications in LLMs."
                },
                "authors": [
                    {
                        "name": "Yi-Chang Chen"
                    },
                    {
                        "name": "Po-Chun Hsu"
                    },
                    {
                        "name": "Chan-Jan Hsu"
                    },
                    {
                        "name": "Da-shan Shiu"
                    }
                ],
                "author_detail": {
                    "name": "Da-shan Shiu"
                },
                "author": "Da-shan Shiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01130v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01130v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16200v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16200v3",
                "updated": "2024-12-04T03:15:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    3,
                    15,
                    44,
                    2,
                    339,
                    0
                ],
                "published": "2024-08-29T01:42:38Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    1,
                    42,
                    38,
                    3,
                    242,
                    0
                ],
                "title": "PolarBEVDet: Exploring Polar Representation for Multi-View 3D Object\n  Detection in Bird's-Eye-View",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PolarBEVDet: Exploring Polar Representation for Multi-View 3D Object\n  Detection in Bird's-Eye-View"
                },
                "summary": "Recently, LSS-based multi-view 3D object detection provides an economical and\ndeployment-friendly solution for autonomous driving. However, all the existing\nLSS-based methods transform multi-view image features into a Cartesian\nBird's-Eye-View(BEV) representation, which does not take into account the\nnon-uniform image information distribution and hardly exploits the view\nsymmetry. In this paper, in order to adapt the image information distribution\nand preserve the view symmetry by regular convolution, we propose to employ the\npolar BEV representation to substitute the Cartesian BEV representation. To\nachieve this, we elaborately tailor three modules: a polar view transformer to\ngenerate the polar BEV representation, a polar temporal fusion module for\nfusing historical polar BEV features and a polar detection head to predict the\npolar-parameterized representation of the object. In addition, we design a 2D\nauxiliary detection head and a spatial attention enhancement module to improve\nthe quality of feature extraction in perspective view and BEV, respectively.\nFinally, we integrate the above improvements into a novel multi-view 3D object\ndetector, PolarBEVDet. Experiments on nuScenes show that PolarBEVDet achieves\nthe superior performance. The code is available at\nhttps://github.com/Yzichen/PolarBEVDet.git.(This work has been submitted to the\nIEEE for possible publication. Copyright may be transferred without notice,\nafter which this version may no longer be accessible)",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, LSS-based multi-view 3D object detection provides an economical and\ndeployment-friendly solution for autonomous driving. However, all the existing\nLSS-based methods transform multi-view image features into a Cartesian\nBird's-Eye-View(BEV) representation, which does not take into account the\nnon-uniform image information distribution and hardly exploits the view\nsymmetry. In this paper, in order to adapt the image information distribution\nand preserve the view symmetry by regular convolution, we propose to employ the\npolar BEV representation to substitute the Cartesian BEV representation. To\nachieve this, we elaborately tailor three modules: a polar view transformer to\ngenerate the polar BEV representation, a polar temporal fusion module for\nfusing historical polar BEV features and a polar detection head to predict the\npolar-parameterized representation of the object. In addition, we design a 2D\nauxiliary detection head and a spatial attention enhancement module to improve\nthe quality of feature extraction in perspective view and BEV, respectively.\nFinally, we integrate the above improvements into a novel multi-view 3D object\ndetector, PolarBEVDet. Experiments on nuScenes show that PolarBEVDet achieves\nthe superior performance. The code is available at\nhttps://github.com/Yzichen/PolarBEVDet.git.(This work has been submitted to the\nIEEE for possible publication. Copyright may be transferred without notice,\nafter which this version may no longer be accessible)"
                },
                "authors": [
                    {
                        "name": "Zichen Yu"
                    },
                    {
                        "name": "Quanli Liu"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Liyong Zhang"
                    },
                    {
                        "name": "Xiaoguang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoguang Zhao"
                },
                "author": "Xiaoguang Zhao",
                "arxiv_comment": "This work has been submitted to the IEEE for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16200v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16200v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02987v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02987v1",
                "updated": "2024-12-04T03:02:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    3,
                    2,
                    46,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T03:02:46Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    3,
                    2,
                    46,
                    2,
                    339,
                    0
                ],
                "title": "Advancing Conversational Psychotherapy: Integrating Privacy,\n  Dual-Memory, and Domain Expertise with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Conversational Psychotherapy: Integrating Privacy,\n  Dual-Memory, and Domain Expertise with Large Language Models"
                },
                "summary": "Mental health has increasingly become a global issue that reveals the\nlimitations of traditional conversational psychotherapy, constrained by\nlocation, time, expense, and privacy concerns. In response to these challenges,\nwe introduce SoulSpeak, a Large Language Model (LLM)-enabled chatbot designed\nto democratize access to psychotherapy. SoulSpeak improves upon the\ncapabilities of standard LLM-enabled chatbots by incorporating a novel\ndual-memory component that combines short-term and long-term context via\nRetrieval Augmented Generation (RAG) to offer personalized responses while\nensuring the preservation of user privacy and intimacy through a dedicated\nprivacy module. In addition, it leverages a counseling chat dataset of\ntherapist-client interactions and various prompting techniques to align the\ngenerated responses with psychotherapeutic methods. We introduce two fine-tuned\nBERT models to evaluate the system against existing LLMs and human therapists:\nthe Conversational Psychotherapy Preference Model (CPPM) to simulate human\npreference among responses and another to assess response relevance to user\ninput. CPPM is useful for training and evaluating psychotherapy-focused\nlanguage models independent from SoulSpeak, helping with the constrained\nresources available for psychotherapy. Furthermore, the effectiveness of the\ndual-memory component and the robustness of the privacy module are also\nexamined. Our findings highlight the potential and challenge of enhancing\nmental health care by offering an alternative that combines the expertise of\ntraditional therapy with the advantages of LLMs, providing a promising way to\naddress the accessibility and personalization gap in current mental health\nservices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mental health has increasingly become a global issue that reveals the\nlimitations of traditional conversational psychotherapy, constrained by\nlocation, time, expense, and privacy concerns. In response to these challenges,\nwe introduce SoulSpeak, a Large Language Model (LLM)-enabled chatbot designed\nto democratize access to psychotherapy. SoulSpeak improves upon the\ncapabilities of standard LLM-enabled chatbots by incorporating a novel\ndual-memory component that combines short-term and long-term context via\nRetrieval Augmented Generation (RAG) to offer personalized responses while\nensuring the preservation of user privacy and intimacy through a dedicated\nprivacy module. In addition, it leverages a counseling chat dataset of\ntherapist-client interactions and various prompting techniques to align the\ngenerated responses with psychotherapeutic methods. We introduce two fine-tuned\nBERT models to evaluate the system against existing LLMs and human therapists:\nthe Conversational Psychotherapy Preference Model (CPPM) to simulate human\npreference among responses and another to assess response relevance to user\ninput. CPPM is useful for training and evaluating psychotherapy-focused\nlanguage models independent from SoulSpeak, helping with the constrained\nresources available for psychotherapy. Furthermore, the effectiveness of the\ndual-memory component and the robustness of the privacy module are also\nexamined. Our findings highlight the potential and challenge of enhancing\nmental health care by offering an alternative that combines the expertise of\ntraditional therapy with the advantages of LLMs, providing a promising way to\naddress the accessibility and personalization gap in current mental health\nservices."
                },
                "authors": [
                    {
                        "name": "XiuYu Zhang"
                    },
                    {
                        "name": "Zening Luo"
                    }
                ],
                "author_detail": {
                    "name": "Zening Luo"
                },
                "author": "Zening Luo",
                "arxiv_comment": "Accepted as a Poster at Statistical Foundations of LLMs and\n  Foundation Models (NeurIPS 2024 Workshop)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02987v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02987v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02110v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02110v2",
                "updated": "2024-12-04T02:47:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    2,
                    47,
                    40,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-03T03:08:27Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    3,
                    8,
                    27,
                    1,
                    338,
                    0
                ],
                "title": "Retrofitting XoM for Stripped Binaries without Embedded Data Relocation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrofitting XoM for Stripped Binaries without Embedded Data Relocation"
                },
                "summary": "In this paper, we present PXoM, a practical technique to seamlessly retrofit\nXoM into stripped binaries on the x86-64 platform. As handling the mixture of\ncode and data is a well-known challenge for XoM, most existing methods require\nthe strict separation of code and data areas via either compile-time\ntransformation or binary patching, so that the unreadable permission can be\nsafely enforced at the granularity of memory pages. In contrast to previous\napproaches, we provide a fine-grained memory permission control mechanism to\nrestrict the read permission of code while allowing legitimate data reads\nwithin code pages. This novelty enables PXoM to harden stripped binaries but\nwithout resorting to error-prone embedded data relocation. We leverage Intel's\nhardware feature, Memory Protection Keys, to offer an efficient fine-grained\npermission control. We measure PXoM's performance with both micro- and\nmacro-benchmarks, and it only introduces negligible runtime overhead. Our\nsecurity evaluation shows that PXoM leaves adversaries with little wiggle room\nto harvest all of the required gadgets, suggesting PXoM is practical for\nreal-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present PXoM, a practical technique to seamlessly retrofit\nXoM into stripped binaries on the x86-64 platform. As handling the mixture of\ncode and data is a well-known challenge for XoM, most existing methods require\nthe strict separation of code and data areas via either compile-time\ntransformation or binary patching, so that the unreadable permission can be\nsafely enforced at the granularity of memory pages. In contrast to previous\napproaches, we provide a fine-grained memory permission control mechanism to\nrestrict the read permission of code while allowing legitimate data reads\nwithin code pages. This novelty enables PXoM to harden stripped binaries but\nwithout resorting to error-prone embedded data relocation. We leverage Intel's\nhardware feature, Memory Protection Keys, to offer an efficient fine-grained\npermission control. We measure PXoM's performance with both micro- and\nmacro-benchmarks, and it only introduces negligible runtime overhead. Our\nsecurity evaluation shows that PXoM leaves adversaries with little wiggle room\nto harvest all of the required gadgets, suggesting PXoM is practical for\nreal-world deployment."
                },
                "authors": [
                    {
                        "name": "Chenke Luo"
                    },
                    {
                        "name": "Jiang Ming"
                    },
                    {
                        "name": "Mengfei Xie"
                    },
                    {
                        "name": "Guojun Peng"
                    },
                    {
                        "name": "Jianming Fu"
                    }
                ],
                "author_detail": {
                    "name": "Jianming Fu"
                },
                "author": "Jianming Fu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02110v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02110v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08188v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08188v3",
                "updated": "2024-12-04T02:44:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    2,
                    44,
                    21,
                    2,
                    339,
                    0
                ],
                "published": "2024-08-15T14:46:13Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    14,
                    46,
                    13,
                    3,
                    228,
                    0
                ],
                "title": "Nl2Hltl2Plan: Scaling Up Natural Language Understanding for Multi-Robots\n  Through Hierarchical Temporal Logic Task Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nl2Hltl2Plan: Scaling Up Natural Language Understanding for Multi-Robots\n  Through Hierarchical Temporal Logic Task Representation"
                },
                "summary": "To enable non-experts to specify long-horizon, multi-robot collaborative\ntasks, language models are increasingly used to translate natural language\ncommands into formal specifications. However, because translation can occur in\nmultiple ways, such translations may lack accuracy or lead to inefficient\nmulti-robot planning. Our key insight is that concise hierarchical\nspecifications can simplify planning while remaining straightforward to derive\nfrom human instructions. We propose Nl2Hltl2Plan, a framework that translates\nnatural language commands into hierarchical Linear Temporal Logic (LTL) and\nsolves the corresponding planning problem. The translation involves two steps\nleveraging Large Language Models (LLMs). First, an LLM transforms instructions\ninto a Hierarchical Task Tree, capturing logical and temporal relations. Next,\na fine-tuned LLM converts sub-tasks into flat LTL formulas, which are\naggregated into hierarchical specifications, with the lowest level\ncorresponding to ordered robot actions. These specifications are then used with\noff-the-shelf planners. Our Nl2Hltl2Plan demonstrates the potential of LLMs in\nhierarchical reasoning for multi-robot task planning. Evaluations in simulation\nand real-world experiments with human participants show that Nl2Hltl2Plan\noutperforms existing methods, handling more complex instructions while\nachieving higher success rates and lower costs in task allocation and planning.\nAdditional details are available at https://nl2hltl2plan.github.io .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To enable non-experts to specify long-horizon, multi-robot collaborative\ntasks, language models are increasingly used to translate natural language\ncommands into formal specifications. However, because translation can occur in\nmultiple ways, such translations may lack accuracy or lead to inefficient\nmulti-robot planning. Our key insight is that concise hierarchical\nspecifications can simplify planning while remaining straightforward to derive\nfrom human instructions. We propose Nl2Hltl2Plan, a framework that translates\nnatural language commands into hierarchical Linear Temporal Logic (LTL) and\nsolves the corresponding planning problem. The translation involves two steps\nleveraging Large Language Models (LLMs). First, an LLM transforms instructions\ninto a Hierarchical Task Tree, capturing logical and temporal relations. Next,\na fine-tuned LLM converts sub-tasks into flat LTL formulas, which are\naggregated into hierarchical specifications, with the lowest level\ncorresponding to ordered robot actions. These specifications are then used with\noff-the-shelf planners. Our Nl2Hltl2Plan demonstrates the potential of LLMs in\nhierarchical reasoning for multi-robot task planning. Evaluations in simulation\nand real-world experiments with human participants show that Nl2Hltl2Plan\noutperforms existing methods, handling more complex instructions while\nachieving higher success rates and lower costs in task allocation and planning.\nAdditional details are available at https://nl2hltl2plan.github.io ."
                },
                "authors": [
                    {
                        "name": "Shaojun Xu"
                    },
                    {
                        "name": "Xusheng Luo"
                    },
                    {
                        "name": "Yutong Huang"
                    },
                    {
                        "name": "Letian Leng"
                    },
                    {
                        "name": "Ruixuan Liu"
                    },
                    {
                        "name": "Changliu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Changliu Liu"
                },
                "author": "Changliu Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08188v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08188v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01955v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01955v2",
                "updated": "2024-12-04T02:25:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    2,
                    25,
                    4,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-02T20:31:27Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    20,
                    31,
                    27,
                    0,
                    337,
                    0
                ],
                "title": "The use of large language models to enhance cancer clinical trial\n  educational materials",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of large language models to enhance cancer clinical trial\n  educational materials"
                },
                "summary": "Cancer clinical trials often face challenges in recruitment and engagement\ndue to a lack of participant-facing informational and educational resources.\nThis study investigated the potential of Large Language Models (LLMs),\nspecifically GPT4, in generating patient-friendly educational content from\nclinical trial informed consent forms. Using data from ClinicalTrials.gov, we\nemployed zero-shot learning for creating trial summaries and one-shot learning\nfor developing multiple-choice questions, evaluating their effectiveness\nthrough patient surveys and crowdsourced annotation. Results showed that\nGPT4-generated summaries were both readable and comprehensive, and may improve\npatients' understanding and interest in clinical trials. The multiple-choice\nquestions demonstrated high accuracy and agreement with crowdsourced\nannotators. For both resource types, hallucinations were identified that\nrequire ongoing human oversight. The findings demonstrate the potential of LLMs\n\"out-of-the-box\" to support the generation of clinical trial education\nmaterials with minimal trial-specific engineering, but implementation with a\nhuman-in-the-loop is still needed to avoid misinformation risks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cancer clinical trials often face challenges in recruitment and engagement\ndue to a lack of participant-facing informational and educational resources.\nThis study investigated the potential of Large Language Models (LLMs),\nspecifically GPT4, in generating patient-friendly educational content from\nclinical trial informed consent forms. Using data from ClinicalTrials.gov, we\nemployed zero-shot learning for creating trial summaries and one-shot learning\nfor developing multiple-choice questions, evaluating their effectiveness\nthrough patient surveys and crowdsourced annotation. Results showed that\nGPT4-generated summaries were both readable and comprehensive, and may improve\npatients' understanding and interest in clinical trials. The multiple-choice\nquestions demonstrated high accuracy and agreement with crowdsourced\nannotators. For both resource types, hallucinations were identified that\nrequire ongoing human oversight. The findings demonstrate the potential of LLMs\n\"out-of-the-box\" to support the generation of clinical trial education\nmaterials with minimal trial-specific engineering, but implementation with a\nhuman-in-the-loop is still needed to avoid misinformation risks."
                },
                "authors": [
                    {
                        "name": "Mingye Gao"
                    },
                    {
                        "name": "Aman Varshney"
                    },
                    {
                        "name": "Shan Chen"
                    },
                    {
                        "name": "Vikram Goddla"
                    },
                    {
                        "name": "Jack Gallifant"
                    },
                    {
                        "name": "Patrick Doyle"
                    },
                    {
                        "name": "Claire Novack"
                    },
                    {
                        "name": "Maeve Dillon-Martin"
                    },
                    {
                        "name": "Teresia Perkins"
                    },
                    {
                        "name": "Xinrong Correia"
                    },
                    {
                        "name": "Erik Duhaime"
                    },
                    {
                        "name": "Howard Isenstein"
                    },
                    {
                        "name": "Elad Sharon"
                    },
                    {
                        "name": "Lisa Soleymani Lehmann"
                    },
                    {
                        "name": "David Kozono"
                    },
                    {
                        "name": "Brian Anthony"
                    },
                    {
                        "name": "Dmitriy Dligach"
                    },
                    {
                        "name": "Danielle S. Bitterman"
                    }
                ],
                "author_detail": {
                    "name": "Danielle S. Bitterman"
                },
                "author": "Danielle S. Bitterman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01955v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01955v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02956v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02956v1",
                "updated": "2024-12-04T02:05:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    2,
                    5,
                    21,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T02:05:21Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    2,
                    5,
                    21,
                    2,
                    339,
                    0
                ],
                "title": "Curriculum-style Data Augmentation for LLM-based Metaphor Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Curriculum-style Data Augmentation for LLM-based Metaphor Detection"
                },
                "summary": "Recently, utilizing large language models (LLMs) for metaphor detection has\nachieved promising results. However, these methods heavily rely on the\ncapabilities of closed-source LLMs, which come with relatively high inference\ncosts and latency. To address this, we propose a method for metaphor detection\nby fine-tuning open-source LLMs, effectively reducing inference costs and\nlatency with a single inference step. Furthermore, metaphor detection suffers\nfrom a severe data scarcity problem, which hinders effective fine-tuning of\nLLMs. To tackle this, we introduce Curriculum-style Data Augmentation (CDA).\nSpecifically, before fine-tuning, we evaluate the training data to identify\ncorrectly predicted instances for fine-tuning, while incorrectly predicted\ninstances are used as seed data for data augmentation. This approach enables\nthe model to quickly learn simpler knowledge and progressively acquire more\ncomplex knowledge, thereby improving performance incrementally. Experimental\nresults demonstrate that our method achieves state-of-the-art performance\nacross all baselines. Additionally, we provide detailed ablation studies to\nvalidate the effectiveness of CDA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, utilizing large language models (LLMs) for metaphor detection has\nachieved promising results. However, these methods heavily rely on the\ncapabilities of closed-source LLMs, which come with relatively high inference\ncosts and latency. To address this, we propose a method for metaphor detection\nby fine-tuning open-source LLMs, effectively reducing inference costs and\nlatency with a single inference step. Furthermore, metaphor detection suffers\nfrom a severe data scarcity problem, which hinders effective fine-tuning of\nLLMs. To tackle this, we introduce Curriculum-style Data Augmentation (CDA).\nSpecifically, before fine-tuning, we evaluate the training data to identify\ncorrectly predicted instances for fine-tuning, while incorrectly predicted\ninstances are used as seed data for data augmentation. This approach enables\nthe model to quickly learn simpler knowledge and progressively acquire more\ncomplex knowledge, thereby improving performance incrementally. Experimental\nresults demonstrate that our method achieves state-of-the-art performance\nacross all baselines. Additionally, we provide detailed ablation studies to\nvalidate the effectiveness of CDA."
                },
                "authors": [
                    {
                        "name": "Kaidi Jia"
                    },
                    {
                        "name": "Yanxia Wu"
                    },
                    {
                        "name": "Rongsheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Rongsheng Li"
                },
                "author": "Rongsheng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02956v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02956v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02930v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02930v1",
                "updated": "2024-12-04T00:50:33Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    0,
                    50,
                    33,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T00:50:33Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    0,
                    50,
                    33,
                    2,
                    339,
                    0
                ],
                "title": "Video LLMs for Temporal Reasoning in Long Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video LLMs for Temporal Reasoning in Long Videos"
                },
                "summary": "This paper introduces TemporalVLM, a video large language model capable of\neffective temporal reasoning and fine-grained understanding in long videos. At\nthe core, our approach includes a visual encoder for mapping a long-term input\nvideo into features which are time-aware and contain both local and global\ncues. In particular, it first divides the input video into short-term clips,\nwhich are jointly encoded with their timestamps into time-sensitive local\nfeatures. Next, the local features are passed through a bidirectional long\nshort-term memory module for global feature aggregation. The extracted\ntime-aware and multi-level features are important for accurate temporal\nreasoning and fine-grained understanding in long videos. Moreover, to\nfacilitate the evaluation of TemporalVLM, we present a large-scale long video\ndataset of industry assembly processes, namely IndustryASM, which consists of\nvideos recorded on factory floors with actions and timestamps annotated by\nindustrial engineers for time and motion studies and temporal action\nsegmentation evaluation. Finally, extensive experiments on datasets of long\nvideos, including TimeIT and IndustryASM, show that TemporalVLM achieves\nsuperior performance than previous methods across temporal reasoning and\nfine-grained understanding tasks, namely dense video captioning, temporal video\ngrounding, video highlight detection, and temporal action segmentation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces TemporalVLM, a video large language model capable of\neffective temporal reasoning and fine-grained understanding in long videos. At\nthe core, our approach includes a visual encoder for mapping a long-term input\nvideo into features which are time-aware and contain both local and global\ncues. In particular, it first divides the input video into short-term clips,\nwhich are jointly encoded with their timestamps into time-sensitive local\nfeatures. Next, the local features are passed through a bidirectional long\nshort-term memory module for global feature aggregation. The extracted\ntime-aware and multi-level features are important for accurate temporal\nreasoning and fine-grained understanding in long videos. Moreover, to\nfacilitate the evaluation of TemporalVLM, we present a large-scale long video\ndataset of industry assembly processes, namely IndustryASM, which consists of\nvideos recorded on factory floors with actions and timestamps annotated by\nindustrial engineers for time and motion studies and temporal action\nsegmentation evaluation. Finally, extensive experiments on datasets of long\nvideos, including TimeIT and IndustryASM, show that TemporalVLM achieves\nsuperior performance than previous methods across temporal reasoning and\nfine-grained understanding tasks, namely dense video captioning, temporal video\ngrounding, video highlight detection, and temporal action segmentation."
                },
                "authors": [
                    {
                        "name": "Fawad Javed Fateh"
                    },
                    {
                        "name": "Umer Ahmed"
                    },
                    {
                        "name": "Hamza Khan"
                    },
                    {
                        "name": "M. Zeeshan Zia"
                    },
                    {
                        "name": "Quoc-Huy Tran"
                    }
                ],
                "author_detail": {
                    "name": "Quoc-Huy Tran"
                },
                "author": "Quoc-Huy Tran",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02930v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02930v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08474v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08474v3",
                "updated": "2024-12-04T00:43:57Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    0,
                    43,
                    57,
                    2,
                    339,
                    0
                ],
                "published": "2024-10-11T02:58:38Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    2,
                    58,
                    38,
                    4,
                    285,
                    0
                ],
                "title": "SPORTU: A Comprehensive Sports Understanding Benchmark for Multimodal\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPORTU: A Comprehensive Sports Understanding Benchmark for Multimodal\n  Large Language Models"
                },
                "summary": "Multimodal Large Language Models (MLLMs) are advancing the ability to reason\nabout complex sports scenarios by integrating textual and visual information.\nTo comprehensively evaluate their capabilities, we introduce SPORTU, a\nbenchmark designed to assess MLLMs across multi-level sports reasoning tasks.\nSPORTU comprises two key components: SPORTU-text, featuring 900 multiple-choice\nquestions with human-annotated explanations for rule comprehension and strategy\nunderstanding. This component focuses on testing models' ability to reason\nabout sports solely through question-answering (QA), without requiring visual\ninputs; SPORTU-video, consisting of 1,701 slow-motion video clips across 7\ndifferent sports and 12,048 QA pairs, designed to assess multi-level reasoning,\nfrom simple sports recognition to complex tasks like foul detection and rule\napplication. We evaluate four prevalent LLMs mainly utilizing few-shot learning\nparadigms supplemented by chain-of-thought (CoT) prompting on the SPORTU-text\npart. We evaluate four LLMs using few-shot learning and chain-of-thought (CoT)\nprompting on SPORTU-text. GPT-4o achieves the highest accuracy of 71%, but\nstill falls short of human-level performance, highlighting room for improvement\nin rule comprehension and reasoning. The evaluation for the SPORTU-video part\nincludes 7 proprietary and 6 open-source MLLMs. Experiments show that models\nfall short on hard tasks that require deep reasoning and rule-based\nunderstanding. Claude-3.5-Sonnet performs the best with only 52.6% accuracy on\nthe hard task, showing large room for improvement. We hope that SPORTU will\nserve as a critical step toward evaluating models' capabilities in sports\nunderstanding and reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) are advancing the ability to reason\nabout complex sports scenarios by integrating textual and visual information.\nTo comprehensively evaluate their capabilities, we introduce SPORTU, a\nbenchmark designed to assess MLLMs across multi-level sports reasoning tasks.\nSPORTU comprises two key components: SPORTU-text, featuring 900 multiple-choice\nquestions with human-annotated explanations for rule comprehension and strategy\nunderstanding. This component focuses on testing models' ability to reason\nabout sports solely through question-answering (QA), without requiring visual\ninputs; SPORTU-video, consisting of 1,701 slow-motion video clips across 7\ndifferent sports and 12,048 QA pairs, designed to assess multi-level reasoning,\nfrom simple sports recognition to complex tasks like foul detection and rule\napplication. We evaluate four prevalent LLMs mainly utilizing few-shot learning\nparadigms supplemented by chain-of-thought (CoT) prompting on the SPORTU-text\npart. We evaluate four LLMs using few-shot learning and chain-of-thought (CoT)\nprompting on SPORTU-text. GPT-4o achieves the highest accuracy of 71%, but\nstill falls short of human-level performance, highlighting room for improvement\nin rule comprehension and reasoning. The evaluation for the SPORTU-video part\nincludes 7 proprietary and 6 open-source MLLMs. Experiments show that models\nfall short on hard tasks that require deep reasoning and rule-based\nunderstanding. Claude-3.5-Sonnet performs the best with only 52.6% accuracy on\nthe hard task, showing large room for improvement. We hope that SPORTU will\nserve as a critical step toward evaluating models' capabilities in sports\nunderstanding and reasoning."
                },
                "authors": [
                    {
                        "name": "Haotian Xia"
                    },
                    {
                        "name": "Zhengbang Yang"
                    },
                    {
                        "name": "Junbo Zou"
                    },
                    {
                        "name": "Rhys Tracy"
                    },
                    {
                        "name": "Yuqing Wang"
                    },
                    {
                        "name": "Chi Lu"
                    },
                    {
                        "name": "Christopher Lai"
                    },
                    {
                        "name": "Yanjun He"
                    },
                    {
                        "name": "Xun Shao"
                    },
                    {
                        "name": "Zhuoqing Xie"
                    },
                    {
                        "name": "Yuan-fang Wang"
                    },
                    {
                        "name": "Weining Shen"
                    },
                    {
                        "name": "Hanjie Chen"
                    }
                ],
                "author_detail": {
                    "name": "Hanjie Chen"
                },
                "author": "Hanjie Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08474v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08474v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19094v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19094v5",
                "updated": "2024-12-04T00:40:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    0,
                    40,
                    25,
                    2,
                    339,
                    0
                ],
                "published": "2024-07-26T21:18:57Z",
                "published_parsed": [
                    2024,
                    7,
                    26,
                    21,
                    18,
                    57,
                    4,
                    208,
                    0
                ],
                "title": "Wonderful Team: Zero-Shot Physical Task Planning with Visual LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wonderful Team: Zero-Shot Physical Task Planning with Visual LLMs"
                },
                "summary": "We introduce Wonderful Team, a multi-agent Vision Large Language Model (VLLM)\nframework for executing high level robotic planning in a zero-shot regime. In\nour context, zero-shot high-level planning means that for a novel environment,\nwe provide a VLLM with an image of the robot's surroundings and a task\ndescription, and the VLLM outputs the sequence of actions necessary for the\nrobot to complete the task. Unlike previous methods for high-level visual\nplanning for robotic manipulation, our method uses VLLMs for the entire\nplanning process, enabling a more tightly integrated loop between perception,\ncontrol, and planning. As a result, Wonderful Team's performance on a\nreal-world semantic and physical planning tasks often exceeds methods that rely\non separate vision systems. For example, we see an average 40% success-rate\nimprovement on VimaBench over prior methods such as NLaP, an average 30%\nimprovement over Trajectory Generators on tasks from the Trajectory Generator\npaper including drawing and wiping a plate, and an average 70% improvement over\nTrajectory Generators on a new set of semantic reasoning tasks including\nenvironment re-arrangement with implicit linguistic constraints. We hope these\nresults highlight the rapid improvements of VLLMs in the past year, and\nmotivate the community to consider VLLMs as an option for some high-level\nrobotic planning problems in the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Wonderful Team, a multi-agent Vision Large Language Model (VLLM)\nframework for executing high level robotic planning in a zero-shot regime. In\nour context, zero-shot high-level planning means that for a novel environment,\nwe provide a VLLM with an image of the robot's surroundings and a task\ndescription, and the VLLM outputs the sequence of actions necessary for the\nrobot to complete the task. Unlike previous methods for high-level visual\nplanning for robotic manipulation, our method uses VLLMs for the entire\nplanning process, enabling a more tightly integrated loop between perception,\ncontrol, and planning. As a result, Wonderful Team's performance on a\nreal-world semantic and physical planning tasks often exceeds methods that rely\non separate vision systems. For example, we see an average 40% success-rate\nimprovement on VimaBench over prior methods such as NLaP, an average 30%\nimprovement over Trajectory Generators on tasks from the Trajectory Generator\npaper including drawing and wiping a plate, and an average 70% improvement over\nTrajectory Generators on a new set of semantic reasoning tasks including\nenvironment re-arrangement with implicit linguistic constraints. We hope these\nresults highlight the rapid improvements of VLLMs in the past year, and\nmotivate the community to consider VLLMs as an option for some high-level\nrobotic planning problems in the future."
                },
                "authors": [
                    {
                        "name": "Zidan Wang"
                    },
                    {
                        "name": "Rui Shen"
                    },
                    {
                        "name": "Bradly Stadie"
                    }
                ],
                "author_detail": {
                    "name": "Bradly Stadie"
                },
                "author": "Bradly Stadie",
                "arxiv_comment": "aka Wonderful Team",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19094v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19094v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12914v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12914v2",
                "updated": "2024-12-04T00:03:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    0,
                    3,
                    38,
                    2,
                    339,
                    0
                ],
                "published": "2024-09-19T17:10:34Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    17,
                    10,
                    34,
                    3,
                    263,
                    0
                ],
                "title": "Mitigating Unsafe Feedback with Learning Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Unsafe Feedback with Learning Constraints"
                },
                "summary": "While there has been progress towards aligning Large Language Models (LLMs)\nwith human values and ensuring safe behaviour at inference time, safety-guards\ncan easily be removed when fine-tuned on unsafe and harmful datasets.While this\nsetting has been treated extensively, another popular training paradigm,\nlearning from unsafe feedback with reinforcement learning, has previously been\nunexplored. This is concerning due to the widespread deployment of feedback\ncollection systems. We address this gap by providing an analysis of learning\nsettings where feedback is adversarial and noisy, i.e. that unsafe samples are\npreferred over safe ones despite model developers goal to maintain safety. We\nfind that safety-aligned LLMs easily explore unsafe action spaces through\ngenerating harmful text and optimize for adversarial reward indicating that\ncurrent safety guards are not enough to prevent learning from unsafe feedback.\nIn order to protect against this vulnerability, we adapt a number of both\n\"implict\" and \"explicit\" harmful fine-tuning defences to evaluate whether they\nare effective as learning constraints in an RL setting finding that no method\nis generally effective pointing to the need for more research in defences given\nthe widespread adoption of methods designed to learn from feedback. We end the\npaper with the observation that some defences work by performing \"harmless\nreward hacking\" for which we provide a theoretical explanation drawn from the\ntheory of Constrained Markov Decision Processes and provide some direction for\nfuture defence development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While there has been progress towards aligning Large Language Models (LLMs)\nwith human values and ensuring safe behaviour at inference time, safety-guards\ncan easily be removed when fine-tuned on unsafe and harmful datasets.While this\nsetting has been treated extensively, another popular training paradigm,\nlearning from unsafe feedback with reinforcement learning, has previously been\nunexplored. This is concerning due to the widespread deployment of feedback\ncollection systems. We address this gap by providing an analysis of learning\nsettings where feedback is adversarial and noisy, i.e. that unsafe samples are\npreferred over safe ones despite model developers goal to maintain safety. We\nfind that safety-aligned LLMs easily explore unsafe action spaces through\ngenerating harmful text and optimize for adversarial reward indicating that\ncurrent safety guards are not enough to prevent learning from unsafe feedback.\nIn order to protect against this vulnerability, we adapt a number of both\n\"implict\" and \"explicit\" harmful fine-tuning defences to evaluate whether they\nare effective as learning constraints in an RL setting finding that no method\nis generally effective pointing to the need for more research in defences given\nthe widespread adoption of methods designed to learn from feedback. We end the\npaper with the observation that some defences work by performing \"harmless\nreward hacking\" for which we provide a theoretical explanation drawn from the\ntheory of Constrained Markov Decision Processes and provide some direction for\nfuture defence development."
                },
                "authors": [
                    {
                        "name": "Domenic Rosati"
                    },
                    {
                        "name": "Giles Edkins"
                    },
                    {
                        "name": "Harsh Raj"
                    },
                    {
                        "name": "David Atanasov"
                    },
                    {
                        "name": "Subhabrata Majumdar"
                    },
                    {
                        "name": "Janarthanan Rajendran"
                    },
                    {
                        "name": "Frank Rudzicz"
                    },
                    {
                        "name": "Hassan Sajjad"
                    }
                ],
                "author_detail": {
                    "name": "Hassan Sajjad"
                },
                "author": "Hassan Sajjad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12914v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12914v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02915v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02915v1",
                "updated": "2024-12-03T23:58:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    23,
                    58,
                    35,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T23:58:35Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    23,
                    58,
                    35,
                    1,
                    338,
                    0
                ],
                "title": "Single-Cell Omics Arena: A Benchmark Study for Large Language Models on\n  Cell Type Annotation Using Single-Cell Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Single-Cell Omics Arena: A Benchmark Study for Large Language Models on\n  Cell Type Annotation Using Single-Cell Data"
                },
                "summary": "Over the past decade, the revolution in single-cell sequencing has enabled\nthe simultaneous molecular profiling of various modalities across thousands of\nindividual cells, allowing scientists to investigate the diverse functions of\ncomplex tissues and uncover underlying disease mechanisms. Among all the\nanalytical steps, assigning individual cells to specific types is fundamental\nfor understanding cellular heterogeneity. However, this process is usually\nlabor-intensive and requires extensive expert knowledge. Recent advances in\nlarge language models (LLMs) have demonstrated their ability to efficiently\nprocess and synthesize vast corpora of text to automatically extract essential\nbiological knowledge, such as marker genes, potentially promoting more\nefficient and automated cell type annotations. To thoroughly evaluate the\ncapability of modern instruction-tuned LLMs in automating the cell type\nidentification process, we introduce SOAR, a comprehensive benchmarking study\nof LLMs for cell type annotation tasks in single-cell genomics. Specifically,\nwe assess the performance of 8 instruction-tuned LLMs across 11 datasets,\nspanning multiple cell types and species. Our study explores the potential of\nLLMs to accurately classify and annotate cell types in single-cell RNA\nsequencing (scRNA-seq) data, while extending their application to multiomics\ndata through cross-modality translation. Additionally, we evaluate the\neffectiveness of chain-of-thought (CoT) prompting techniques in generating\ndetailed biological insights during the annotation process. The results\ndemonstrate that LLMs can provide robust interpretations of single-cell data\nwithout requiring additional fine-tuning, advancing the automation of cell type\nannotation in genomics research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Over the past decade, the revolution in single-cell sequencing has enabled\nthe simultaneous molecular profiling of various modalities across thousands of\nindividual cells, allowing scientists to investigate the diverse functions of\ncomplex tissues and uncover underlying disease mechanisms. Among all the\nanalytical steps, assigning individual cells to specific types is fundamental\nfor understanding cellular heterogeneity. However, this process is usually\nlabor-intensive and requires extensive expert knowledge. Recent advances in\nlarge language models (LLMs) have demonstrated their ability to efficiently\nprocess and synthesize vast corpora of text to automatically extract essential\nbiological knowledge, such as marker genes, potentially promoting more\nefficient and automated cell type annotations. To thoroughly evaluate the\ncapability of modern instruction-tuned LLMs in automating the cell type\nidentification process, we introduce SOAR, a comprehensive benchmarking study\nof LLMs for cell type annotation tasks in single-cell genomics. Specifically,\nwe assess the performance of 8 instruction-tuned LLMs across 11 datasets,\nspanning multiple cell types and species. Our study explores the potential of\nLLMs to accurately classify and annotate cell types in single-cell RNA\nsequencing (scRNA-seq) data, while extending their application to multiomics\ndata through cross-modality translation. Additionally, we evaluate the\neffectiveness of chain-of-thought (CoT) prompting techniques in generating\ndetailed biological insights during the annotation process. The results\ndemonstrate that LLMs can provide robust interpretations of single-cell data\nwithout requiring additional fine-tuning, advancing the automation of cell type\nannotation in genomics research."
                },
                "authors": [
                    {
                        "name": "Junhao Liu"
                    },
                    {
                        "name": "Siwei Xu"
                    },
                    {
                        "name": "Lei Zhang"
                    },
                    {
                        "name": "Jing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jing Zhang"
                },
                "author": "Jing Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02915v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02915v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.19318v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.19318v2",
                "updated": "2024-12-03T23:53:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    23,
                    53,
                    19,
                    1,
                    338,
                    0
                ],
                "published": "2024-04-30T07:38:08Z",
                "published_parsed": [
                    2024,
                    4,
                    30,
                    7,
                    38,
                    8,
                    1,
                    121,
                    0
                ],
                "title": "Enhancing Trust in LLM-Generated Code Summaries with Calibrated\n  Confidence Scores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Trust in LLM-Generated Code Summaries with Calibrated\n  Confidence Scores"
                },
                "summary": "A good summary can often be very useful during program comprehension. While a\nbrief, fluent, and relevant summary can be helpful, it does require significant\nhuman effort to produce. Often, good summaries are unavailable in software\nprojects, thus making maintenance more difficult. There has been a considerable\nbody of research into automated AI-based methods, using Large Language models\n(LLMs), to generate summaries of code; there also has been quite a bit work on\nways to measure the performance of such summarization methods, with special\nattention paid to how closely these AI-generated summaries resemble a summary a\nhuman might have produced. Measures such as BERTScore and BLEU have been\nsuggested and evaluated with human-subject studies.\n  However, LLM-produced summaries can be too long, irrelevant, etc: generally,\ntoo dissimilar to what a human might say. Given an LLM-produced code summary,\nhow can we judge if a summary is good enough? Given some input source code, and\nan LLM-generated summary, existing approaches can help judge brevity, fluency\nand relevance; however, it's difficult to gauge whether an LLM-produced summary\nsufficiently resembles what a human might produce, without a \"golden\"\nhuman-produced summary to compare against. We study this resemblance question\nas a calibration problem: given just the summary from an LLM, can we compute a\nconfidence measure, that provides a reliable indication of whether the summary\nsufficiently resembles what a human would have produced in this situation? We\nexamine this question using several LLMs, for several languages, and in several\ndifferent settings. Our investigation suggests approaches to provide reliable\npredictions of the likelihood that an LLM-generated summary would sufficiently\nresemble a summary a human might write for the same code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A good summary can often be very useful during program comprehension. While a\nbrief, fluent, and relevant summary can be helpful, it does require significant\nhuman effort to produce. Often, good summaries are unavailable in software\nprojects, thus making maintenance more difficult. There has been a considerable\nbody of research into automated AI-based methods, using Large Language models\n(LLMs), to generate summaries of code; there also has been quite a bit work on\nways to measure the performance of such summarization methods, with special\nattention paid to how closely these AI-generated summaries resemble a summary a\nhuman might have produced. Measures such as BERTScore and BLEU have been\nsuggested and evaluated with human-subject studies.\n  However, LLM-produced summaries can be too long, irrelevant, etc: generally,\ntoo dissimilar to what a human might say. Given an LLM-produced code summary,\nhow can we judge if a summary is good enough? Given some input source code, and\nan LLM-generated summary, existing approaches can help judge brevity, fluency\nand relevance; however, it's difficult to gauge whether an LLM-produced summary\nsufficiently resembles what a human might produce, without a \"golden\"\nhuman-produced summary to compare against. We study this resemblance question\nas a calibration problem: given just the summary from an LLM, can we compute a\nconfidence measure, that provides a reliable indication of whether the summary\nsufficiently resembles what a human would have produced in this situation? We\nexamine this question using several LLMs, for several languages, and in several\ndifferent settings. Our investigation suggests approaches to provide reliable\npredictions of the likelihood that an LLM-generated summary would sufficiently\nresemble a summary a human might write for the same code."
                },
                "authors": [
                    {
                        "name": "Yuvraj Virk"
                    },
                    {
                        "name": "Premkumar Devanbu"
                    },
                    {
                        "name": "Toufique Ahmed"
                    }
                ],
                "author_detail": {
                    "name": "Toufique Ahmed"
                },
                "author": "Toufique Ahmed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.19318v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.19318v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02906v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02906v1",
                "updated": "2024-12-03T23:19:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    23,
                    19,
                    40,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T23:19:40Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    23,
                    19,
                    40,
                    1,
                    338,
                    0
                ],
                "title": "Does Few-Shot Learning Help LLM Performance in Code Synthesis?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does Few-Shot Learning Help LLM Performance in Code Synthesis?"
                },
                "summary": "Large language models (LLMs) have made significant strides at code generation\nthrough improved model design, training, and chain-of-thought. However,\nprompt-level optimizations remain an important yet under-explored aspect of\nLLMs for coding. This work focuses on the few-shot examples present in most\ncode generation prompts, offering a systematic study on whether few-shot\nexamples improve LLM's coding capabilities, which few-shot examples have the\nlargest impact, and how to select impactful examples. Our work offers 2\napproaches for selecting few-shot examples, a model-free method,\nCODEEXEMPLAR-FREE, and a model-based method, CODEEXEMPLAR-BASED. The 2 methods\noffer a trade-off between improved performance and reliance on training data\nand interpretability. Both methods significantly improve CodeLlama's coding\nability across the popular HumanEval+ coding benchmark. In summary, our work\nprovides valuable insights into how to pick few-shot examples in code\ngeneration prompts to improve LLM code generation capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have made significant strides at code generation\nthrough improved model design, training, and chain-of-thought. However,\nprompt-level optimizations remain an important yet under-explored aspect of\nLLMs for coding. This work focuses on the few-shot examples present in most\ncode generation prompts, offering a systematic study on whether few-shot\nexamples improve LLM's coding capabilities, which few-shot examples have the\nlargest impact, and how to select impactful examples. Our work offers 2\napproaches for selecting few-shot examples, a model-free method,\nCODEEXEMPLAR-FREE, and a model-based method, CODEEXEMPLAR-BASED. The 2 methods\noffer a trade-off between improved performance and reliance on training data\nand interpretability. Both methods significantly improve CodeLlama's coding\nability across the popular HumanEval+ coding benchmark. In summary, our work\nprovides valuable insights into how to pick few-shot examples in code\ngeneration prompts to improve LLM code generation capabilities."
                },
                "authors": [
                    {
                        "name": "Derek Xu"
                    },
                    {
                        "name": "Tong Xie"
                    },
                    {
                        "name": "Botao Xia"
                    },
                    {
                        "name": "Haoyu Li"
                    },
                    {
                        "name": "Yunsheng Bai"
                    },
                    {
                        "name": "Yizhou Sun"
                    },
                    {
                        "name": "Wei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wang"
                },
                "author": "Wei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02906v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02906v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02904v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02904v1",
                "updated": "2024-12-03T23:14:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    23,
                    14,
                    47,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T23:14:47Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    23,
                    14,
                    47,
                    1,
                    338,
                    0
                ],
                "title": "Enhancing Trust in Large Language Models with Uncertainty-Aware\n  Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Trust in Large Language Models with Uncertainty-Aware\n  Fine-Tuning"
                },
                "summary": "Large language models (LLMs) have revolutionized the field of natural\nlanguage processing with their impressive reasoning and question-answering\ncapabilities. However, these models are sometimes prone to generating\ncredible-sounding but incorrect information, a phenomenon known as LLM\nhallucinations. Reliable uncertainty estimation in LLMs is essential for\nfostering trust in their generated responses and serves as a critical tool for\nthe detection and prevention of erroneous or hallucinated outputs. To achieve\nreliable and well-calibrated uncertainty quantification in open-ended and\nfree-form natural language generation, we propose an uncertainty-aware\nfine-tuning approach for LLMs. This approach enhances the model's ability to\nprovide reliable uncertainty estimates without compromising accuracy, thereby\nguiding them to produce more trustworthy responses. We introduce a novel\nuncertainty-aware causal language modeling loss function, grounded in the\nprinciples of decision theory. Through rigorous evaluation on multiple\nfree-form question-answering datasets and models, we demonstrate that our\nuncertainty-aware fine-tuning approach yields better calibrated uncertainty\nestimates in natural language generation tasks than fine-tuning with the\nstandard causal language modeling loss. Furthermore, the experimental results\nshow that the proposed method significantly improves the model's ability to\ndetect hallucinations and identify out-of-domain prompts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have revolutionized the field of natural\nlanguage processing with their impressive reasoning and question-answering\ncapabilities. However, these models are sometimes prone to generating\ncredible-sounding but incorrect information, a phenomenon known as LLM\nhallucinations. Reliable uncertainty estimation in LLMs is essential for\nfostering trust in their generated responses and serves as a critical tool for\nthe detection and prevention of erroneous or hallucinated outputs. To achieve\nreliable and well-calibrated uncertainty quantification in open-ended and\nfree-form natural language generation, we propose an uncertainty-aware\nfine-tuning approach for LLMs. This approach enhances the model's ability to\nprovide reliable uncertainty estimates without compromising accuracy, thereby\nguiding them to produce more trustworthy responses. We introduce a novel\nuncertainty-aware causal language modeling loss function, grounded in the\nprinciples of decision theory. Through rigorous evaluation on multiple\nfree-form question-answering datasets and models, we demonstrate that our\nuncertainty-aware fine-tuning approach yields better calibrated uncertainty\nestimates in natural language generation tasks than fine-tuning with the\nstandard causal language modeling loss. Furthermore, the experimental results\nshow that the proposed method significantly improves the model's ability to\ndetect hallucinations and identify out-of-domain prompts."
                },
                "authors": [
                    {
                        "name": "Ranganath Krishnan"
                    },
                    {
                        "name": "Piyush Khanna"
                    },
                    {
                        "name": "Omesh Tickoo"
                    }
                ],
                "author_detail": {
                    "name": "Omesh Tickoo"
                },
                "author": "Omesh Tickoo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02904v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02904v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02897v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02897v1",
                "updated": "2024-12-03T23:01:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    23,
                    1,
                    21,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T23:01:21Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    23,
                    1,
                    21,
                    1,
                    338,
                    0
                ],
                "title": "MLD-EA: Check and Complete Narrative Coherence by Introducing Emotions\n  and Actions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MLD-EA: Check and Complete Narrative Coherence by Introducing Emotions\n  and Actions"
                },
                "summary": "Narrative understanding and story generation are critical challenges in\nnatural language processing (NLP), with much of the existing research focused\non summarization and question-answering tasks. While previous studies have\nexplored predicting plot endings and generating extended narratives, they often\nneglect the logical coherence within stories, leaving a significant gap in the\nfield. To address this, we introduce the Missing Logic Detector by Emotion and\nAction (MLD-EA) model, which leverages large language models (LLMs) to identify\nnarrative gaps and generate coherent sentences that integrate seamlessly with\nthe story's emotional and logical flow. The experimental results demonstrate\nthat the MLD-EA model enhances narrative understanding and story generation,\nhighlighting LLMs' potential as effective logic checkers in story writing with\nlogical coherence and emotional consistency. This work fills a gap in NLP\nresearch and advances border goals of creating more sophisticated and reliable\nstory-generation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Narrative understanding and story generation are critical challenges in\nnatural language processing (NLP), with much of the existing research focused\non summarization and question-answering tasks. While previous studies have\nexplored predicting plot endings and generating extended narratives, they often\nneglect the logical coherence within stories, leaving a significant gap in the\nfield. To address this, we introduce the Missing Logic Detector by Emotion and\nAction (MLD-EA) model, which leverages large language models (LLMs) to identify\nnarrative gaps and generate coherent sentences that integrate seamlessly with\nthe story's emotional and logical flow. The experimental results demonstrate\nthat the MLD-EA model enhances narrative understanding and story generation,\nhighlighting LLMs' potential as effective logic checkers in story writing with\nlogical coherence and emotional consistency. This work fills a gap in NLP\nresearch and advances border goals of creating more sophisticated and reliable\nstory-generation systems."
                },
                "authors": [
                    {
                        "name": "Jinming Zhang"
                    },
                    {
                        "name": "Yunfei Long"
                    }
                ],
                "author_detail": {
                    "name": "Yunfei Long"
                },
                "author": "Yunfei Long",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02897v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02897v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02883v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02883v1",
                "updated": "2024-12-03T22:38:05Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    22,
                    38,
                    5,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T22:38:05Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    22,
                    38,
                    5,
                    1,
                    338,
                    0
                ],
                "title": "TDD-Bench Verified: Can LLMs Generate Tests for Issues Before They Get\n  Resolved?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TDD-Bench Verified: Can LLMs Generate Tests for Issues Before They Get\n  Resolved?"
                },
                "summary": "Test-driven development (TDD) is the practice of writing tests first and\ncoding later, and the proponents of TDD expound its numerous benefits. For\ninstance, given an issue on a source code repository, tests can clarify the\ndesired behavior among stake-holders before anyone writes code for the\nagreed-upon fix. Although there has been a lot of work on automated test\ngeneration for the practice \"write code first, test later\", there has been\nlittle such automation for TDD. Ideally, tests for TDD should be fail-to-pass\n(i.e., fail before the issue is resolved and pass after) and have good adequacy\nwith respect to covering the code changed during issue resolution. This paper\nintroduces TDD-Bench Verified, a high-quality benchmark suite of 449 issues\nmined from real-world GitHub code repositories. The benchmark's evaluation\nharness runs only relevant tests in isolation for simple yet accurate coverage\nmeasurements, and the benchmark's dataset is filtered both by human judges and\nby execution in the harness. This paper also presents Auto-TDD, an LLM-based\nsolution that takes as input an issue description and a codebase (prior to\nissue resolution) and returns as output a test that can be used to validate the\nchanges made for resolving the issue. Our evaluation shows that Auto-TDD yields\na better fail-to-pass rate than the strongest prior work while also yielding\nhigh coverage adequacy. Overall, we hope that this work helps make developers\nmore productive at resolving issues while simultaneously leading to more robust\nfixes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-driven development (TDD) is the practice of writing tests first and\ncoding later, and the proponents of TDD expound its numerous benefits. For\ninstance, given an issue on a source code repository, tests can clarify the\ndesired behavior among stake-holders before anyone writes code for the\nagreed-upon fix. Although there has been a lot of work on automated test\ngeneration for the practice \"write code first, test later\", there has been\nlittle such automation for TDD. Ideally, tests for TDD should be fail-to-pass\n(i.e., fail before the issue is resolved and pass after) and have good adequacy\nwith respect to covering the code changed during issue resolution. This paper\nintroduces TDD-Bench Verified, a high-quality benchmark suite of 449 issues\nmined from real-world GitHub code repositories. The benchmark's evaluation\nharness runs only relevant tests in isolation for simple yet accurate coverage\nmeasurements, and the benchmark's dataset is filtered both by human judges and\nby execution in the harness. This paper also presents Auto-TDD, an LLM-based\nsolution that takes as input an issue description and a codebase (prior to\nissue resolution) and returns as output a test that can be used to validate the\nchanges made for resolving the issue. Our evaluation shows that Auto-TDD yields\na better fail-to-pass rate than the strongest prior work while also yielding\nhigh coverage adequacy. Overall, we hope that this work helps make developers\nmore productive at resolving issues while simultaneously leading to more robust\nfixes."
                },
                "authors": [
                    {
                        "name": "Toufique Ahmed"
                    },
                    {
                        "name": "Martin Hirzel"
                    },
                    {
                        "name": "Rangeet Pan"
                    },
                    {
                        "name": "Avraham Shinnar"
                    },
                    {
                        "name": "Saurabh Sinha"
                    }
                ],
                "author_detail": {
                    "name": "Saurabh Sinha"
                },
                "author": "Saurabh Sinha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02883v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02883v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14485v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14485v2",
                "updated": "2024-12-03T22:27:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    22,
                    27,
                    12,
                    1,
                    338,
                    0
                ],
                "published": "2024-11-20T02:49:18Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    2,
                    49,
                    18,
                    2,
                    325,
                    0
                ],
                "title": "Mediating Modes of Thought: LLM's for design scripting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mediating Modes of Thought: LLM's for design scripting"
                },
                "summary": "Architects adopt visual scripting and parametric design tools to explore more\nexpansive design spaces (Coates, 2010), refine their thinking about the\ngeometric logic of their design (Woodbury, 2010), and overcome conventional\nsoftware limitations (Burry, 2011). Despite two decades of effort to make\ndesign scripting more accessible, a disconnect between a designer's free ways\nof thinking and the rigidity of algorithms remains (Burry, 2011). Recent\ndevelopments in Large Language Models (LLMs) suggest this might soon change, as\nLLMs encode a general understanding of human context and exhibit the capacity\nto produce geometric logic. This project speculates that if LLMs can\neffectively mediate between user intent and algorithms, they become a powerful\ntool to make scripting in design more widespread and fun. We explore if such\nsystems can interpret natural language prompts to assemble geometric operations\nrelevant to computational design scripting. In the system, multiple layers of\nLLM agents are configured with specific context to infer the user intent and\nconstruct a sequential logic. Given a user's high-level text prompt, a\ngeometric description is created, distilled into a sequence of logic\noperations, and mapped to software-specific commands. The completed script is\nconstructed in the user's visual programming interface. The system succeeds in\ngenerating complete visual scripts up to a certain complexity but fails beyond\nthis complexity threshold. It shows how LLMs can make design scripting much\nmore aligned with human creativity and thought. Future research should explore\nconversational interactions, expand to multimodal inputs and outputs, and\nassess the performance of these tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Architects adopt visual scripting and parametric design tools to explore more\nexpansive design spaces (Coates, 2010), refine their thinking about the\ngeometric logic of their design (Woodbury, 2010), and overcome conventional\nsoftware limitations (Burry, 2011). Despite two decades of effort to make\ndesign scripting more accessible, a disconnect between a designer's free ways\nof thinking and the rigidity of algorithms remains (Burry, 2011). Recent\ndevelopments in Large Language Models (LLMs) suggest this might soon change, as\nLLMs encode a general understanding of human context and exhibit the capacity\nto produce geometric logic. This project speculates that if LLMs can\neffectively mediate between user intent and algorithms, they become a powerful\ntool to make scripting in design more widespread and fun. We explore if such\nsystems can interpret natural language prompts to assemble geometric operations\nrelevant to computational design scripting. In the system, multiple layers of\nLLM agents are configured with specific context to infer the user intent and\nconstruct a sequential logic. Given a user's high-level text prompt, a\ngeometric description is created, distilled into a sequence of logic\noperations, and mapped to software-specific commands. The completed script is\nconstructed in the user's visual programming interface. The system succeeds in\ngenerating complete visual scripts up to a certain complexity but fails beyond\nthis complexity threshold. It shows how LLMs can make design scripting much\nmore aligned with human creativity and thought. Future research should explore\nconversational interactions, expand to multimodal inputs and outputs, and\nassess the performance of these tools."
                },
                "authors": [
                    {
                        "name": "Moritz Rietschel"
                    },
                    {
                        "name": "Fang Guo"
                    },
                    {
                        "name": "Kyle Steinfeld"
                    }
                ],
                "author_detail": {
                    "name": "Kyle Steinfeld"
                },
                "author": "Kyle Steinfeld",
                "arxiv_comment": "Published at ACADIA 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14485v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14485v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]