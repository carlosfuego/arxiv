[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2408.11049v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11049v4",
                "updated": "2025-03-26T17:42:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    17,
                    42,
                    17,
                    2,
                    85,
                    0
                ],
                "published": "2024-08-20T17:57:31Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    57,
                    31,
                    1,
                    233,
                    0
                ],
                "title": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding"
                },
                "summary": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency losslessly, but the conventional wisdom suggests\nthat its efficacy is limited to small batch sizes. In MagicDec, we show that\nsurprisingly SD can achieve speedup even for a high throughput inference regime\nfor moderate to long sequences. More interestingly, an intelligent drafting\nstrategy can achieve better speedup with increasing batch size based on our\nrigorous analysis. MagicDec first identifies the bottleneck shifts with\nincreasing batch size and sequence length, and uses these insights to deploy SD\nmore effectively for high throughput inference. We leverage draft model with\nsparse KV cache to address the KV bottleneck, which scales with both sequence\nlength and batch size. Additionally, we propose a theoretical model to select\nthe optimal drafting strategy for maximum speedup. Our work highlights the\nbroad applicability of speculative decoding in long-context serving, as it can\nenhance throughput and reduce latency without compromising accuracy. For\nmoderate to long sequences, we demonstrate up to 2.51x speedup for Llama3.1-8B\nwhen serving batch sizes ranging from 32 to 256 on various types of hardware\nand tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency losslessly, but the conventional wisdom suggests\nthat its efficacy is limited to small batch sizes. In MagicDec, we show that\nsurprisingly SD can achieve speedup even for a high throughput inference regime\nfor moderate to long sequences. More interestingly, an intelligent drafting\nstrategy can achieve better speedup with increasing batch size based on our\nrigorous analysis. MagicDec first identifies the bottleneck shifts with\nincreasing batch size and sequence length, and uses these insights to deploy SD\nmore effectively for high throughput inference. We leverage draft model with\nsparse KV cache to address the KV bottleneck, which scales with both sequence\nlength and batch size. Additionally, we propose a theoretical model to select\nthe optimal drafting strategy for maximum speedup. Our work highlights the\nbroad applicability of speculative decoding in long-context serving, as it can\nenhance throughput and reduce latency without compromising accuracy. For\nmoderate to long sequences, we demonstrate up to 2.51x speedup for Llama3.1-8B\nwhen serving batch sizes ranging from 32 to 256 on various types of hardware\nand tasks."
                },
                "authors": [
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Jian Chen"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Vashisth Tiwari"
                    },
                    {
                        "name": "Ruihang Lai"
                    },
                    {
                        "name": "Jinyuan Shi"
                    },
                    {
                        "name": "Ian En-Hsu Yen"
                    },
                    {
                        "name": "Avner May"
                    },
                    {
                        "name": "Tianqi Chen"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11049v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11049v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16302v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16302v2",
                "updated": "2025-03-26T15:08:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    15,
                    8,
                    12,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-20T16:23:44Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    16,
                    23,
                    44,
                    3,
                    79,
                    0
                ],
                "title": "Unleashing Vecset Diffusion Model for Fast Shape Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing Vecset Diffusion Model for Fast Shape Generation"
                },
                "summary": "3D shape generation has greatly flourished through the development of\nso-called \"native\" 3D diffusion, particularly through the Vecset Diffusion\nModel (VDM). While recent advancements have shown promising results in\ngenerating high-resolution 3D shapes, VDM still struggles with high-speed\ngeneration. Challenges exist because of difficulties not only in accelerating\ndiffusion sampling but also VAE decoding in VDM, areas under-explored in\nprevious works. To address these challenges, we present FlashVDM, a systematic\nframework for accelerating both VAE and DiT in VDM. For DiT, FlashVDM enables\nflexible diffusion sampling with as few as 5 inference steps and comparable\nquality, which is made possible by stabilizing consistency distillation with\nour newly introduced Progressive Flow Distillation. For VAE, we introduce a\nlightning vecset decoder equipped with Adaptive KV Selection, Hierarchical\nVolume Decoding, and Efficient Network Design. By exploiting the locality of\nthe vecset and the sparsity of shape surface in the volume, our decoder\ndrastically lowers FLOPs, minimizing the overall decoding overhead. We apply\nFlashVDM to Hunyuan3D-2 to obtain Hunyuan3D-2 Turbo. Through systematic\nevaluation, we show that our model significantly outperforms existing fast 3D\ngeneration methods, achieving comparable performance to the state-of-the-art\nwhile reducing inference time by over 45x for reconstruction and 32x for\ngeneration. Code and models are available at\nhttps://github.com/Tencent/FlashVDM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D shape generation has greatly flourished through the development of\nso-called \"native\" 3D diffusion, particularly through the Vecset Diffusion\nModel (VDM). While recent advancements have shown promising results in\ngenerating high-resolution 3D shapes, VDM still struggles with high-speed\ngeneration. Challenges exist because of difficulties not only in accelerating\ndiffusion sampling but also VAE decoding in VDM, areas under-explored in\nprevious works. To address these challenges, we present FlashVDM, a systematic\nframework for accelerating both VAE and DiT in VDM. For DiT, FlashVDM enables\nflexible diffusion sampling with as few as 5 inference steps and comparable\nquality, which is made possible by stabilizing consistency distillation with\nour newly introduced Progressive Flow Distillation. For VAE, we introduce a\nlightning vecset decoder equipped with Adaptive KV Selection, Hierarchical\nVolume Decoding, and Efficient Network Design. By exploiting the locality of\nthe vecset and the sparsity of shape surface in the volume, our decoder\ndrastically lowers FLOPs, minimizing the overall decoding overhead. We apply\nFlashVDM to Hunyuan3D-2 to obtain Hunyuan3D-2 Turbo. Through systematic\nevaluation, we show that our model significantly outperforms existing fast 3D\ngeneration methods, achieving comparable performance to the state-of-the-art\nwhile reducing inference time by over 45x for reconstruction and 32x for\ngeneration. Code and models are available at\nhttps://github.com/Tencent/FlashVDM."
                },
                "authors": [
                    {
                        "name": "Zeqiang Lai"
                    },
                    {
                        "name": "Yunfei Zhao"
                    },
                    {
                        "name": "Zibo Zhao"
                    },
                    {
                        "name": "Haolin Liu"
                    },
                    {
                        "name": "Fuyun Wang"
                    },
                    {
                        "name": "Huiwen Shi"
                    },
                    {
                        "name": "Xianghui Yang"
                    },
                    {
                        "name": "Qingxiang Lin"
                    },
                    {
                        "name": "Jingwei Huang"
                    },
                    {
                        "name": "Yuhong Liu"
                    },
                    {
                        "name": "Jie Jiang"
                    },
                    {
                        "name": "Chunchao Guo"
                    },
                    {
                        "name": "Xiangyu Yue"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Yue"
                },
                "author": "Xiangyu Yue",
                "arxiv_comment": "Technical report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16302v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16302v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13996v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13996v3",
                "updated": "2025-03-26T13:59:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    13,
                    59,
                    53,
                    2,
                    85,
                    0
                ],
                "published": "2024-07-19T03:01:32Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    3,
                    1,
                    32,
                    4,
                    201,
                    0
                ],
                "title": "SGDRC: Software-Defined Dynamic Resource Control for Concurrent DNN\n  Inference on NVIDIA GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SGDRC: Software-Defined Dynamic Resource Control for Concurrent DNN\n  Inference on NVIDIA GPUs"
                },
                "summary": "Cloud service providers heavily colocate high-priority, latency-sensitive\n(LS), and low-priority, best-effort (BE) DNN inference services on the same GPU\nto improve resource utilization in data centers. Among the critical shared GPU\nresources, there has been very limited analysis on the dynamic allocation of\ncompute units and VRAM bandwidth, mainly for two reasons: (1) The native GPU\nresource management solutions are either hardware-specific, or unable to\ndynamically allocate resources to different tenants, or both; (2) NVIDIA\ndoesn't expose interfaces for VRAM bandwidth allocation, and the software stack\nand VRAM channel architectures are black-box, both of which limit the\nsoftware-level resource management. These drive prior work to design either\nconservative sharing policies detrimental to throughput, or static resource\npartitioning only applicable to a few GPU models.\n  To bridge this gap, this paper proposes SGDRC, a fully software-defined\ndynamic VRAM bandwidth and compute unit management solution for concurrent DNN\ninference services. SGDRC aims at guaranteeing service quality, maximizing the\noverall throughput, and providing general applicability to NVIDIA GPUs. SGDRC\nfirst reveals a general VRAM channel hash mapping architecture of NVIDIA GPUs\nthrough comprehensive reverse engineering and eliminates VRAM channel conflicts\nusing software-level cache coloring. SGDRC applies bimodal tensors and tidal SM\nmasking to dynamically allocate VRAM bandwidth and compute units, and guides\nthe allocation of resources based on offline profiling. We evaluate 11\nmainstream DNNs with real-world workloads on two NVIDIA GPUs. The results show\nthat compared with the state-of-the-art GPU sharing solutions, SGDRC achieves\nthe highest SLO attainment rates (99.0% on average), and improves overall\nthroughput by up to 1.47x and BE job throughput by up to 2.36x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud service providers heavily colocate high-priority, latency-sensitive\n(LS), and low-priority, best-effort (BE) DNN inference services on the same GPU\nto improve resource utilization in data centers. Among the critical shared GPU\nresources, there has been very limited analysis on the dynamic allocation of\ncompute units and VRAM bandwidth, mainly for two reasons: (1) The native GPU\nresource management solutions are either hardware-specific, or unable to\ndynamically allocate resources to different tenants, or both; (2) NVIDIA\ndoesn't expose interfaces for VRAM bandwidth allocation, and the software stack\nand VRAM channel architectures are black-box, both of which limit the\nsoftware-level resource management. These drive prior work to design either\nconservative sharing policies detrimental to throughput, or static resource\npartitioning only applicable to a few GPU models.\n  To bridge this gap, this paper proposes SGDRC, a fully software-defined\ndynamic VRAM bandwidth and compute unit management solution for concurrent DNN\ninference services. SGDRC aims at guaranteeing service quality, maximizing the\noverall throughput, and providing general applicability to NVIDIA GPUs. SGDRC\nfirst reveals a general VRAM channel hash mapping architecture of NVIDIA GPUs\nthrough comprehensive reverse engineering and eliminates VRAM channel conflicts\nusing software-level cache coloring. SGDRC applies bimodal tensors and tidal SM\nmasking to dynamically allocate VRAM bandwidth and compute units, and guides\nthe allocation of resources based on offline profiling. We evaluate 11\nmainstream DNNs with real-world workloads on two NVIDIA GPUs. The results show\nthat compared with the state-of-the-art GPU sharing solutions, SGDRC achieves\nthe highest SLO attainment rates (99.0% on average), and improves overall\nthroughput by up to 1.47x and BE job throughput by up to 2.36x."
                },
                "authors": [
                    {
                        "name": "Yongkang Zhang"
                    },
                    {
                        "name": "Haoxuan Yu"
                    },
                    {
                        "name": "Chenxia Han"
                    },
                    {
                        "name": "Cheng Wang"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Yunzhe Li"
                    },
                    {
                        "name": "Zhifeng Jiang"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Xiaowen Chu"
                    },
                    {
                        "name": "Huaicheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Huaicheng Li"
                },
                "author": "Huaicheng Li",
                "arxiv_doi": "10.1145/3710848.3710863",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3710848.3710863",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.13996v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13996v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "15 pages, 19 figures",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.9; I.2.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20481v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20481v1",
                "updated": "2025-03-26T12:10:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    12,
                    10,
                    53,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T12:10:53Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    12,
                    10,
                    53,
                    2,
                    85,
                    0
                ],
                "title": "Analyzing Modern NVIDIA GPU cores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analyzing Modern NVIDIA GPU cores"
                },
                "summary": "GPUs are the most popular platform for accelerating HPC workloads, such as\nartificial intelligence and science simulations. However, most\nmicroarchitectural research in academia relies on GPU core pipeline designs\nbased on architectures that are more than 15 years old.\n  This paper reverse engineers modern NVIDIA GPU cores, unveiling many key\naspects of its design and explaining how GPUs leverage hardware-compiler\ntechniques where the compiler guides hardware during execution. In particular,\nit reveals how the issue logic works including the policy of the issue\nscheduler, the structure of the register file and its associated cache, and\nmultiple features of the memory pipeline. Moreover, it analyses how a simple\ninstruction prefetcher based on a stream buffer fits well with modern NVIDIA\nGPUs and is likely to be used. Furthermore, we investigate the impact of the\nregister file cache and the number of register file read ports on both\nsimulation accuracy and performance.\n  By modeling all these new discovered microarchitectural details, we achieve\n18.24% lower mean absolute percentage error (MAPE) in execution cycles than\nprevious state-of-the-art simulators, resulting in an average of 13.98% MAPE\nwith respect to real hardware (NVIDIA RTX A6000). Also, we demonstrate that\nthis new model stands for other NVIDIA architectures, such as Turing. Finally,\nwe show that the software-based dependence management mechanism included in\nmodern NVIDIA GPUs outperforms a hardware mechanism based on scoreboards in\nterms of performance and area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPUs are the most popular platform for accelerating HPC workloads, such as\nartificial intelligence and science simulations. However, most\nmicroarchitectural research in academia relies on GPU core pipeline designs\nbased on architectures that are more than 15 years old.\n  This paper reverse engineers modern NVIDIA GPU cores, unveiling many key\naspects of its design and explaining how GPUs leverage hardware-compiler\ntechniques where the compiler guides hardware during execution. In particular,\nit reveals how the issue logic works including the policy of the issue\nscheduler, the structure of the register file and its associated cache, and\nmultiple features of the memory pipeline. Moreover, it analyses how a simple\ninstruction prefetcher based on a stream buffer fits well with modern NVIDIA\nGPUs and is likely to be used. Furthermore, we investigate the impact of the\nregister file cache and the number of register file read ports on both\nsimulation accuracy and performance.\n  By modeling all these new discovered microarchitectural details, we achieve\n18.24% lower mean absolute percentage error (MAPE) in execution cycles than\nprevious state-of-the-art simulators, resulting in an average of 13.98% MAPE\nwith respect to real hardware (NVIDIA RTX A6000). Also, we demonstrate that\nthis new model stands for other NVIDIA architectures, such as Turing. Finally,\nwe show that the software-based dependence management mechanism included in\nmodern NVIDIA GPUs outperforms a hardware mechanism based on scoreboards in\nterms of performance and area."
                },
                "authors": [
                    {
                        "name": "Rodrigo Huerta"
                    },
                    {
                        "name": "Mojtaba Abaie Shoushtary"
                    },
                    {
                        "name": "José-Lorenzo Cruz"
                    },
                    {
                        "name": "Antonio González"
                    }
                ],
                "author_detail": {
                    "name": "Antonio González"
                },
                "author": "Antonio González",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20481v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20481v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12150v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12150v2",
                "updated": "2025-03-26T11:08:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    11,
                    8,
                    20,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-15T14:13:23Z",
                "published_parsed": [
                    2025,
                    3,
                    15,
                    14,
                    13,
                    23,
                    5,
                    74,
                    0
                ],
                "title": "Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and\n  Generalizable Point Cloud Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and\n  Generalizable Point Cloud Analysis"
                },
                "summary": "This paper proposes a general solution to enable point cloud recognition\nmodels to handle distribution shifts at test time. Unlike prior methods, which\nrely heavily on training data (often inaccessible during online inference) and\nare limited to recognizing a fixed set of point cloud classes predefined during\ntraining, we explore a more practical and challenging scenario: adapting the\nmodel solely based on online test data to recognize both previously seen\nclasses and novel, unseen classes at test time. To this end, we develop\n\\textbf{Point-Cache}, a hierarchical cache model that captures essential clues\nof online test samples, particularly focusing on the global structure of point\nclouds and their local-part details. Point-Cache, which serves as a rich 3D\nknowledge base, is dynamically managed to prioritize the inclusion of\nhigh-quality samples. Designed as a plug-and-play module, our method can be\nflexibly integrated into large multimodal 3D models to support open-vocabulary\npoint cloud recognition. Notably, our solution operates with efficiency\ncomparable to zero-shot inference, as it is entirely training-free. Point-Cache\ndemonstrates substantial gains across 8 challenging benchmarks and 4\nrepresentative large 3D models, highlighting its effectiveness. Code is\navailable at https://github.com/auniquesun/Point-Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a general solution to enable point cloud recognition\nmodels to handle distribution shifts at test time. Unlike prior methods, which\nrely heavily on training data (often inaccessible during online inference) and\nare limited to recognizing a fixed set of point cloud classes predefined during\ntraining, we explore a more practical and challenging scenario: adapting the\nmodel solely based on online test data to recognize both previously seen\nclasses and novel, unseen classes at test time. To this end, we develop\n\\textbf{Point-Cache}, a hierarchical cache model that captures essential clues\nof online test samples, particularly focusing on the global structure of point\nclouds and their local-part details. Point-Cache, which serves as a rich 3D\nknowledge base, is dynamically managed to prioritize the inclusion of\nhigh-quality samples. Designed as a plug-and-play module, our method can be\nflexibly integrated into large multimodal 3D models to support open-vocabulary\npoint cloud recognition. Notably, our solution operates with efficiency\ncomparable to zero-shot inference, as it is entirely training-free. Point-Cache\ndemonstrates substantial gains across 8 challenging benchmarks and 4\nrepresentative large 3D models, highlighting its effectiveness. Code is\navailable at https://github.com/auniquesun/Point-Cache."
                },
                "authors": [
                    {
                        "name": "Hongyu Sun"
                    },
                    {
                        "name": "Qiuhong Ke"
                    },
                    {
                        "name": "Ming Cheng"
                    },
                    {
                        "name": "Yongcai Wang"
                    },
                    {
                        "name": "Deying Li"
                    },
                    {
                        "name": "Chenhui Gou"
                    },
                    {
                        "name": "Jianfei Cai"
                    }
                ],
                "author_detail": {
                    "name": "Jianfei Cai"
                },
                "author": "Jianfei Cai",
                "arxiv_comment": "Accepted by CVPR 2025; 24 pages, 14 figures, 18 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12150v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12150v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20174v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20174v1",
                "updated": "2025-03-26T02:58:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    2,
                    58,
                    41,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T02:58:41Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    2,
                    58,
                    41,
                    2,
                    85,
                    0
                ],
                "title": "Devil is in the Uniformity: Exploring Diverse Learners within\n  Transformer for Image Restoration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Devil is in the Uniformity: Exploring Diverse Learners within\n  Transformer for Image Restoration"
                },
                "summary": "Transformer-based approaches have gained significant attention in image\nrestoration, where the core component, i.e, Multi-Head Attention (MHA), plays a\ncrucial role in capturing diverse features and recovering high-quality results.\nIn MHA, heads perform attention calculation independently from uniform split\nsubspaces, and a redundancy issue is triggered to hinder the model from\nachieving satisfactory outputs. In this paper, we propose to improve MHA by\nexploring diverse learners and introducing various interactions between heads,\nwhich results in a Hierarchical multI-head atteNtion driven Transformer model,\ntermed HINT, for image restoration. HINT contains two modules, i.e., the\nHierarchical Multi-Head Attention (HMHA) and the Query-Key Cache Updating\n(QKCU) module, to address the redundancy problem that is rooted in vanilla MHA.\nSpecifically, HMHA extracts diverse contextual features by employing heads to\nlearn from subspaces of varying sizes and containing different information.\nMoreover, QKCU, comprising intra- and inter-layer schemes, further reduces the\nredundancy problem by facilitating enhanced interactions between attention\nheads within and across layers. Extensive experiments are conducted on 12\nbenchmarks across 5 image restoration tasks, including low-light enhancement,\ndehazing, desnowing, denoising, and deraining, to demonstrate the superiority\nof HINT. The source code is available in the supplementary materials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based approaches have gained significant attention in image\nrestoration, where the core component, i.e, Multi-Head Attention (MHA), plays a\ncrucial role in capturing diverse features and recovering high-quality results.\nIn MHA, heads perform attention calculation independently from uniform split\nsubspaces, and a redundancy issue is triggered to hinder the model from\nachieving satisfactory outputs. In this paper, we propose to improve MHA by\nexploring diverse learners and introducing various interactions between heads,\nwhich results in a Hierarchical multI-head atteNtion driven Transformer model,\ntermed HINT, for image restoration. HINT contains two modules, i.e., the\nHierarchical Multi-Head Attention (HMHA) and the Query-Key Cache Updating\n(QKCU) module, to address the redundancy problem that is rooted in vanilla MHA.\nSpecifically, HMHA extracts diverse contextual features by employing heads to\nlearn from subspaces of varying sizes and containing different information.\nMoreover, QKCU, comprising intra- and inter-layer schemes, further reduces the\nredundancy problem by facilitating enhanced interactions between attention\nheads within and across layers. Extensive experiments are conducted on 12\nbenchmarks across 5 image restoration tasks, including low-light enhancement,\ndehazing, desnowing, denoising, and deraining, to demonstrate the superiority\nof HINT. The source code is available in the supplementary materials."
                },
                "authors": [
                    {
                        "name": "Shihao Zhou"
                    },
                    {
                        "name": "Dayu Li"
                    },
                    {
                        "name": "Jinshan Pan"
                    },
                    {
                        "name": "Juncheng Zhou"
                    },
                    {
                        "name": "Jinglei Shi"
                    },
                    {
                        "name": "Jufeng Yang"
                    }
                ],
                "author_detail": {
                    "name": "Jufeng Yang"
                },
                "author": "Jufeng Yang",
                "arxiv_comment": "11 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20174v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20174v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17264v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17264v2",
                "updated": "2025-03-26T01:58:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    1,
                    58,
                    40,
                    2,
                    85,
                    0
                ],
                "published": "2024-09-25T18:21:05Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    18,
                    21,
                    5,
                    2,
                    269,
                    0
                ],
                "title": "Medha: Efficiently Serving Multi-Million Context Length LLM Inference\n  Requests Without Approximations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medha: Efficiently Serving Multi-Million Context Length LLM Inference\n  Requests Without Approximations"
                },
                "summary": "As large language models (LLMs) handle increasingly longer contexts, serving\ninference requests for context lengths in the range of millions of tokens\npresents unique challenges. While existing techniques are effective for\ntraining, they fail to address the unique challenges of inference, such as\nvarying prefill and decode phases and their associated latency constraints --\nlike Time to First Token (TTFT) and Time per Output Token (TPOT). Furthermore,\nno long-context inference solutions address head-of-line blocking today.\n  We present Medha, a system for efficient long-context LLM inference that\nintroduces three key innovations: adaptive chunking with slack-aware scheduling\nto prevent head-ofline blocking, Sequence Pipeline Parallelism (SPP) to reduce\nTTFT, and KV Cache Parallelism (KVP) to minimize TPOT. By combining these into\na novel 3D parallelism serving engine, Medha achieves unprecedented scale --\nsupporting contexts up to 10M tokens with production-grade latency. Our\nevaluation shows Medha reduces median latency by up to 30x compared to\nstate-of-the-art systems when serving a mix of short and long requests, while\nimproving throughput by upwards of 5x. This enables, for the first time,\nefficient long-context LLM inference at scale without compromising on shorter\nrequest latencies or system efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) handle increasingly longer contexts, serving\ninference requests for context lengths in the range of millions of tokens\npresents unique challenges. While existing techniques are effective for\ntraining, they fail to address the unique challenges of inference, such as\nvarying prefill and decode phases and their associated latency constraints --\nlike Time to First Token (TTFT) and Time per Output Token (TPOT). Furthermore,\nno long-context inference solutions address head-of-line blocking today.\n  We present Medha, a system for efficient long-context LLM inference that\nintroduces three key innovations: adaptive chunking with slack-aware scheduling\nto prevent head-ofline blocking, Sequence Pipeline Parallelism (SPP) to reduce\nTTFT, and KV Cache Parallelism (KVP) to minimize TPOT. By combining these into\na novel 3D parallelism serving engine, Medha achieves unprecedented scale --\nsupporting contexts up to 10M tokens with production-grade latency. Our\nevaluation shows Medha reduces median latency by up to 30x compared to\nstate-of-the-art systems when serving a mix of short and long requests, while\nimproving throughput by upwards of 5x. This enables, for the first time,\nefficient long-context LLM inference at scale without compromising on shorter\nrequest latencies or system efficiency."
                },
                "authors": [
                    {
                        "name": "Amey Agrawal"
                    },
                    {
                        "name": "Haoran Qiu"
                    },
                    {
                        "name": "Junda Chen"
                    },
                    {
                        "name": "Íñigo Goiri"
                    },
                    {
                        "name": "Ramachandran Ramjee"
                    },
                    {
                        "name": "Chaojie Zhang"
                    },
                    {
                        "name": "Alexey Tumanov"
                    },
                    {
                        "name": "Esha Choukse"
                    }
                ],
                "author_detail": {
                    "name": "Esha Choukse"
                },
                "author": "Esha Choukse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17264v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17264v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13509v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13509v2",
                "updated": "2025-03-25T17:56:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    17,
                    56,
                    1,
                    1,
                    84,
                    0
                ],
                "published": "2024-12-18T05:16:11Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    5,
                    16,
                    11,
                    2,
                    353,
                    0
                ],
                "title": "Visualizing the Invisible: A Generative AR System for Intuitive\n  Multi-Modal Sensor Data Presentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visualizing the Invisible: A Generative AR System for Intuitive\n  Multi-Modal Sensor Data Presentation"
                },
                "summary": "Understanding sensor data can be difficult for non-experts because of the\ncomplexity and different semantic meanings of sensor modalities. This leads to\na need for intuitive and effective methods to present sensor information.\nHowever, creating intuitive sensor data visualizations presents three key\nchallenges: the variability of sensor readings, gaps in domain comprehension,\nand the dynamic nature of sensor data. To address these issues, we propose\nVivar, a novel system that integrates multi-modal sensor data and presents 3D\nvolumetric content for AR visualization. In particular, we introduce a\ncross-modal embedding approach that maps sensor data into a pre-trained visual\nembedding space through barycentric interpolation. This approach accurately\nreflects value changes in multi-modal sensor information, ensuring that sensor\nvariations are properly shown in visualization outcomes. Vivar also\nincorporates sensor-aware AR scene generation using foundation models and 3D\nGaussian Splatting (3DGS) without requiring domain expertise. In addition,\nVivar leverages latent reuse and caching strategies to accelerate 2D and AR\ncontent generation, demonstrating 11x latency reduction without compromising\nquality. A user study involving over 503 participants, including domain\nexperts, demonstrates Vivar's effectiveness in accuracy, consistency, and\nreal-world applicability, paving the way for more intuitive sensor data\nvisualization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding sensor data can be difficult for non-experts because of the\ncomplexity and different semantic meanings of sensor modalities. This leads to\na need for intuitive and effective methods to present sensor information.\nHowever, creating intuitive sensor data visualizations presents three key\nchallenges: the variability of sensor readings, gaps in domain comprehension,\nand the dynamic nature of sensor data. To address these issues, we propose\nVivar, a novel system that integrates multi-modal sensor data and presents 3D\nvolumetric content for AR visualization. In particular, we introduce a\ncross-modal embedding approach that maps sensor data into a pre-trained visual\nembedding space through barycentric interpolation. This approach accurately\nreflects value changes in multi-modal sensor information, ensuring that sensor\nvariations are properly shown in visualization outcomes. Vivar also\nincorporates sensor-aware AR scene generation using foundation models and 3D\nGaussian Splatting (3DGS) without requiring domain expertise. In addition,\nVivar leverages latent reuse and caching strategies to accelerate 2D and AR\ncontent generation, demonstrating 11x latency reduction without compromising\nquality. A user study involving over 503 participants, including domain\nexperts, demonstrates Vivar's effectiveness in accuracy, consistency, and\nreal-world applicability, paving the way for more intuitive sensor data\nvisualization."
                },
                "authors": [
                    {
                        "name": "Yunqi Guo"
                    },
                    {
                        "name": "Kaiyuan Hou"
                    },
                    {
                        "name": "Heming Fu"
                    },
                    {
                        "name": "Hongkai Chen"
                    },
                    {
                        "name": "Zhenyu Yan"
                    },
                    {
                        "name": "Guoliang Xing"
                    },
                    {
                        "name": "Xiaofan Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaofan Jiang"
                },
                "author": "Xiaofan Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13509v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13509v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19950v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19950v1",
                "updated": "2025-03-25T16:24:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    16,
                    24,
                    45,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T16:24:45Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    16,
                    24,
                    45,
                    1,
                    84,
                    0
                ],
                "title": "LogQuant: Log-Distributed 2-Bit Quantization of KV Cache with Superior\n  Accuracy Preservation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LogQuant: Log-Distributed 2-Bit Quantization of KV Cache with Superior\n  Accuracy Preservation"
                },
                "summary": "We introduce LogQuant, a groundbreaking 2-bit quantization technique for KV\nCache in large language model (LLM) inference, delivering substantial memory\nsavings while preserving superior performance. Previous methods either assume\nthat later tokens are more important or attempt to predict important tokens\nbased on earlier attention patterns. Both approaches, however, can result in\nperformance bottlenecks or frequent mispredictions.\n  LogQuant takes a different approach. By applying a log-based filtering\nmechanism, it selectively compresses the KV Cache across the entire context,\nachieving better performance with the same or even reduced memory footprint\ncompared to existing methods. In benchmark tests, it enhances throughput by 25%\nand boosts batch size by 60% without increasing memory consumption. For\nchallenging tasks such as Math and Code Completion, LogQuant improves accuracy\nby 40% to 200% at the same compression ratio, outperforming comparable\ntechniques.LogQuant integrates effortlessly with popular inference frameworks\nlike Python's transformers library. Implementation can be available in\nhttps://github.com/Concyclics/LogQuantKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce LogQuant, a groundbreaking 2-bit quantization technique for KV\nCache in large language model (LLM) inference, delivering substantial memory\nsavings while preserving superior performance. Previous methods either assume\nthat later tokens are more important or attempt to predict important tokens\nbased on earlier attention patterns. Both approaches, however, can result in\nperformance bottlenecks or frequent mispredictions.\n  LogQuant takes a different approach. By applying a log-based filtering\nmechanism, it selectively compresses the KV Cache across the entire context,\nachieving better performance with the same or even reduced memory footprint\ncompared to existing methods. In benchmark tests, it enhances throughput by 25%\nand boosts batch size by 60% without increasing memory consumption. For\nchallenging tasks such as Math and Code Completion, LogQuant improves accuracy\nby 40% to 200% at the same compression ratio, outperforming comparable\ntechniques.LogQuant integrates effortlessly with popular inference frameworks\nlike Python's transformers library. Implementation can be available in\nhttps://github.com/Concyclics/LogQuantKV."
                },
                "authors": [
                    {
                        "name": "Han Chen"
                    },
                    {
                        "name": "Zicong Jiang"
                    },
                    {
                        "name": "Zining Zhang"
                    },
                    {
                        "name": "Bingsheng He"
                    },
                    {
                        "name": "Pingyi Luo"
                    },
                    {
                        "name": "Mian Lu"
                    },
                    {
                        "name": "Yuqiang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yuqiang Chen"
                },
                "author": "Yuqiang Chen",
                "arxiv_comment": "Accepted by ICLR 2025 Workshop on Sparsity in LLMs (SLLM)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19950v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19950v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19786v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19786v1",
                "updated": "2025-03-25T15:52:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    15,
                    52,
                    34,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T15:52:34Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    15,
                    52,
                    34,
                    1,
                    84,
                    0
                ],
                "title": "Gemma 3 Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gemma 3 Technical Report"
                },
                "summary": "We introduce Gemma 3, a multimodal addition to the Gemma family of\nlightweight open models, ranging in scale from 1 to 27 billion parameters. This\nversion introduces vision understanding abilities, a wider coverage of\nlanguages and longer context - at least 128K tokens. We also change the\narchitecture of the model to reduce the KV-cache memory that tends to explode\nwith long context. This is achieved by increasing the ratio of local to global\nattention layers, and keeping the span on local attention short. The Gemma 3\nmodels are trained with distillation and achieve superior performance to Gemma\n2 for both pre-trained and instruction finetuned versions. In particular, our\nnovel post-training recipe significantly improves the math, chat,\ninstruction-following and multilingual abilities, making Gemma3-4B-IT\ncompetitive with Gemma2-27B-IT and Gemma3-27B-IT comparable to Gemini-1.5-Pro\nacross benchmarks. We release all our models to the community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Gemma 3, a multimodal addition to the Gemma family of\nlightweight open models, ranging in scale from 1 to 27 billion parameters. This\nversion introduces vision understanding abilities, a wider coverage of\nlanguages and longer context - at least 128K tokens. We also change the\narchitecture of the model to reduce the KV-cache memory that tends to explode\nwith long context. This is achieved by increasing the ratio of local to global\nattention layers, and keeping the span on local attention short. The Gemma 3\nmodels are trained with distillation and achieve superior performance to Gemma\n2 for both pre-trained and instruction finetuned versions. In particular, our\nnovel post-training recipe significantly improves the math, chat,\ninstruction-following and multilingual abilities, making Gemma3-4B-IT\ncompetitive with Gemma2-27B-IT and Gemma3-27B-IT comparable to Gemini-1.5-Pro\nacross benchmarks. We release all our models to the community."
                },
                "authors": [
                    {
                        "name": "Gemma Team"
                    },
                    {
                        "name": "Aishwarya Kamath"
                    },
                    {
                        "name": "Johan Ferret"
                    },
                    {
                        "name": "Shreya Pathak"
                    },
                    {
                        "name": "Nino Vieillard"
                    },
                    {
                        "name": "Ramona Merhej"
                    },
                    {
                        "name": "Sarah Perrin"
                    },
                    {
                        "name": "Tatiana Matejovicova"
                    },
                    {
                        "name": "Alexandre Ramé"
                    },
                    {
                        "name": "Morgane Rivière"
                    },
                    {
                        "name": "Louis Rouillard"
                    },
                    {
                        "name": "Thomas Mesnard"
                    },
                    {
                        "name": "Geoffrey Cideron"
                    },
                    {
                        "name": "Jean-bastien Grill"
                    },
                    {
                        "name": "Sabela Ramos"
                    },
                    {
                        "name": "Edouard Yvinec"
                    },
                    {
                        "name": "Michelle Casbon"
                    },
                    {
                        "name": "Etienne Pot"
                    },
                    {
                        "name": "Ivo Penchev"
                    },
                    {
                        "name": "Gaël Liu"
                    },
                    {
                        "name": "Francesco Visin"
                    },
                    {
                        "name": "Kathleen Kenealy"
                    },
                    {
                        "name": "Lucas Beyer"
                    },
                    {
                        "name": "Xiaohai Zhai"
                    },
                    {
                        "name": "Anton Tsitsulin"
                    },
                    {
                        "name": "Robert Busa-Fekete"
                    },
                    {
                        "name": "Alex Feng"
                    },
                    {
                        "name": "Noveen Sachdeva"
                    },
                    {
                        "name": "Benjamin Coleman"
                    },
                    {
                        "name": "Yi Gao"
                    },
                    {
                        "name": "Basil Mustafa"
                    },
                    {
                        "name": "Iain Barr"
                    },
                    {
                        "name": "Emilio Parisotto"
                    },
                    {
                        "name": "David Tian"
                    },
                    {
                        "name": "Matan Eyal"
                    },
                    {
                        "name": "Colin Cherry"
                    },
                    {
                        "name": "Jan-Thorsten Peter"
                    },
                    {
                        "name": "Danila Sinopalnikov"
                    },
                    {
                        "name": "Surya Bhupatiraju"
                    },
                    {
                        "name": "Rishabh Agarwal"
                    },
                    {
                        "name": "Mehran Kazemi"
                    },
                    {
                        "name": "Dan Malkin"
                    },
                    {
                        "name": "Ravin Kumar"
                    },
                    {
                        "name": "David Vilar"
                    },
                    {
                        "name": "Idan Brusilovsky"
                    },
                    {
                        "name": "Jiaming Luo"
                    },
                    {
                        "name": "Andreas Steiner"
                    },
                    {
                        "name": "Abe Friesen"
                    },
                    {
                        "name": "Abhanshu Sharma"
                    },
                    {
                        "name": "Abheesht Sharma"
                    },
                    {
                        "name": "Adi Mayrav Gilady"
                    },
                    {
                        "name": "Adrian Goedeckemeyer"
                    },
                    {
                        "name": "Alaa Saade"
                    },
                    {
                        "name": "Alex Feng"
                    },
                    {
                        "name": "Alexander Kolesnikov"
                    },
                    {
                        "name": "Alexei Bendebury"
                    },
                    {
                        "name": "Alvin Abdagic"
                    },
                    {
                        "name": "Amit Vadi"
                    },
                    {
                        "name": "András György"
                    },
                    {
                        "name": "André Susano Pinto"
                    },
                    {
                        "name": "Anil Das"
                    },
                    {
                        "name": "Ankur Bapna"
                    },
                    {
                        "name": "Antoine Miech"
                    },
                    {
                        "name": "Antoine Yang"
                    },
                    {
                        "name": "Antonia Paterson"
                    },
                    {
                        "name": "Ashish Shenoy"
                    },
                    {
                        "name": "Ayan Chakrabarti"
                    },
                    {
                        "name": "Bilal Piot"
                    },
                    {
                        "name": "Bo Wu"
                    },
                    {
                        "name": "Bobak Shahriari"
                    },
                    {
                        "name": "Bryce Petrini"
                    },
                    {
                        "name": "Charlie Chen"
                    },
                    {
                        "name": "Charline Le Lan"
                    },
                    {
                        "name": "Christopher A. Choquette-Choo"
                    },
                    {
                        "name": "CJ Carey"
                    },
                    {
                        "name": "Cormac Brick"
                    },
                    {
                        "name": "Daniel Deutsch"
                    },
                    {
                        "name": "Danielle Eisenbud"
                    },
                    {
                        "name": "Dee Cattle"
                    },
                    {
                        "name": "Derek Cheng"
                    },
                    {
                        "name": "Dimitris Paparas"
                    },
                    {
                        "name": "Divyashree Shivakumar Sreepathihalli"
                    },
                    {
                        "name": "Doug Reid"
                    },
                    {
                        "name": "Dustin Tran"
                    },
                    {
                        "name": "Dustin Zelle"
                    },
                    {
                        "name": "Eric Noland"
                    },
                    {
                        "name": "Erwin Huizenga"
                    },
                    {
                        "name": "Eugene Kharitonov"
                    },
                    {
                        "name": "Frederick Liu"
                    },
                    {
                        "name": "Gagik Amirkhanyan"
                    },
                    {
                        "name": "Glenn Cameron"
                    },
                    {
                        "name": "Hadi Hashemi"
                    },
                    {
                        "name": "Hanna Klimczak-Plucińska"
                    },
                    {
                        "name": "Harman Singh"
                    },
                    {
                        "name": "Harsh Mehta"
                    },
                    {
                        "name": "Harshal Tushar Lehri"
                    },
                    {
                        "name": "Hussein Hazimeh"
                    },
                    {
                        "name": "Ian Ballantyne"
                    },
                    {
                        "name": "Idan Szpektor"
                    },
                    {
                        "name": "Ivan Nardini"
                    },
                    {
                        "name": "Jean Pouget-Abadie"
                    },
                    {
                        "name": "Jetha Chan"
                    },
                    {
                        "name": "Joe Stanton"
                    },
                    {
                        "name": "John Wieting"
                    },
                    {
                        "name": "Jonathan Lai"
                    },
                    {
                        "name": "Jordi Orbay"
                    },
                    {
                        "name": "Joseph Fernandez"
                    },
                    {
                        "name": "Josh Newlan"
                    },
                    {
                        "name": "Ju-yeong Ji"
                    },
                    {
                        "name": "Jyotinder Singh"
                    },
                    {
                        "name": "Kat Black"
                    },
                    {
                        "name": "Kathy Yu"
                    },
                    {
                        "name": "Kevin Hui"
                    },
                    {
                        "name": "Kiran Vodrahalli"
                    },
                    {
                        "name": "Klaus Greff"
                    },
                    {
                        "name": "Linhai Qiu"
                    },
                    {
                        "name": "Marcella Valentine"
                    },
                    {
                        "name": "Marina Coelho"
                    },
                    {
                        "name": "Marvin Ritter"
                    },
                    {
                        "name": "Matt Hoffman"
                    },
                    {
                        "name": "Matthew Watson"
                    },
                    {
                        "name": "Mayank Chaturvedi"
                    },
                    {
                        "name": "Michael Moynihan"
                    },
                    {
                        "name": "Min Ma"
                    },
                    {
                        "name": "Nabila Babar"
                    },
                    {
                        "name": "Natasha Noy"
                    },
                    {
                        "name": "Nathan Byrd"
                    },
                    {
                        "name": "Nick Roy"
                    },
                    {
                        "name": "Nikola Momchev"
                    },
                    {
                        "name": "Nilay Chauhan"
                    },
                    {
                        "name": "Noveen Sachdeva"
                    },
                    {
                        "name": "Oskar Bunyan"
                    },
                    {
                        "name": "Pankil Botarda"
                    },
                    {
                        "name": "Paul Caron"
                    },
                    {
                        "name": "Paul Kishan Rubenstein"
                    },
                    {
                        "name": "Phil Culliton"
                    },
                    {
                        "name": "Philipp Schmid"
                    },
                    {
                        "name": "Pier Giuseppe Sessa"
                    },
                    {
                        "name": "Pingmei Xu"
                    },
                    {
                        "name": "Piotr Stanczyk"
                    },
                    {
                        "name": "Pouya Tafti"
                    },
                    {
                        "name": "Rakesh Shivanna"
                    },
                    {
                        "name": "Renjie Wu"
                    },
                    {
                        "name": "Renke Pan"
                    },
                    {
                        "name": "Reza Rokni"
                    },
                    {
                        "name": "Rob Willoughby"
                    },
                    {
                        "name": "Rohith Vallu"
                    },
                    {
                        "name": "Ryan Mullins"
                    },
                    {
                        "name": "Sammy Jerome"
                    },
                    {
                        "name": "Sara Smoot"
                    },
                    {
                        "name": "Sertan Girgin"
                    },
                    {
                        "name": "Shariq Iqbal"
                    },
                    {
                        "name": "Shashir Reddy"
                    },
                    {
                        "name": "Shruti Sheth"
                    },
                    {
                        "name": "Siim Põder"
                    },
                    {
                        "name": "Sijal Bhatnagar"
                    },
                    {
                        "name": "Sindhu Raghuram Panyam"
                    },
                    {
                        "name": "Sivan Eiger"
                    },
                    {
                        "name": "Susan Zhang"
                    },
                    {
                        "name": "Tianqi Liu"
                    },
                    {
                        "name": "Trevor Yacovone"
                    },
                    {
                        "name": "Tyler Liechty"
                    },
                    {
                        "name": "Uday Kalra"
                    },
                    {
                        "name": "Utku Evci"
                    },
                    {
                        "name": "Vedant Misra"
                    },
                    {
                        "name": "Vincent Roseberry"
                    },
                    {
                        "name": "Vlad Feinberg"
                    },
                    {
                        "name": "Vlad Kolesnikov"
                    },
                    {
                        "name": "Woohyun Han"
                    },
                    {
                        "name": "Woosuk Kwon"
                    },
                    {
                        "name": "Xi Chen"
                    },
                    {
                        "name": "Yinlam Chow"
                    },
                    {
                        "name": "Yuvein Zhu"
                    },
                    {
                        "name": "Zichuan Wei"
                    },
                    {
                        "name": "Zoltan Egyed"
                    },
                    {
                        "name": "Victor Cotruta"
                    },
                    {
                        "name": "Minh Giang"
                    },
                    {
                        "name": "Phoebe Kirk"
                    },
                    {
                        "name": "Anand Rao"
                    },
                    {
                        "name": "Kat Black"
                    },
                    {
                        "name": "Nabila Babar"
                    },
                    {
                        "name": "Jessica Lo"
                    },
                    {
                        "name": "Erica Moreira"
                    },
                    {
                        "name": "Luiz Gustavo Martins"
                    },
                    {
                        "name": "Omar Sanseviero"
                    },
                    {
                        "name": "Lucas Gonzalez"
                    },
                    {
                        "name": "Zach Gleicher"
                    },
                    {
                        "name": "Tris Warkentin"
                    },
                    {
                        "name": "Vahab Mirrokni"
                    },
                    {
                        "name": "Evan Senter"
                    },
                    {
                        "name": "Eli Collins"
                    },
                    {
                        "name": "Joelle Barral"
                    },
                    {
                        "name": "Zoubin Ghahramani"
                    },
                    {
                        "name": "Raia Hadsell"
                    },
                    {
                        "name": "Yossi Matias"
                    },
                    {
                        "name": "D. Sculley"
                    },
                    {
                        "name": "Slav Petrov"
                    },
                    {
                        "name": "Noah Fiedel"
                    },
                    {
                        "name": "Noam Shazeer"
                    },
                    {
                        "name": "Oriol Vinyals"
                    },
                    {
                        "name": "Jeff Dean"
                    },
                    {
                        "name": "Demis Hassabis"
                    },
                    {
                        "name": "Koray Kavukcuoglu"
                    },
                    {
                        "name": "Clement Farabet"
                    },
                    {
                        "name": "Elena Buchatskaya"
                    },
                    {
                        "name": "Jean-Baptiste Alayrac"
                    },
                    {
                        "name": "Rohan Anil"
                    },
                    {
                        "name": "Dmitry"
                    },
                    {
                        "name": "Lepikhin"
                    },
                    {
                        "name": "Sebastian Borgeaud"
                    },
                    {
                        "name": "Olivier Bachem"
                    },
                    {
                        "name": "Armand Joulin"
                    },
                    {
                        "name": "Alek Andreev"
                    },
                    {
                        "name": "Cassidy Hardin"
                    },
                    {
                        "name": "Robert Dadashi"
                    },
                    {
                        "name": "Léonard Hussenot"
                    }
                ],
                "author_detail": {
                    "name": "Léonard Hussenot"
                },
                "author": "Léonard Hussenot",
                "arxiv_affiliation": "Dima",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19786v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19786v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19390v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19390v1",
                "updated": "2025-03-25T06:45:13Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    6,
                    45,
                    13,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T06:45:13Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    6,
                    45,
                    13,
                    1,
                    84,
                    0
                ],
                "title": "Integrating Prefetcher Selection with Dynamic Request Allocation\n  Improves Prefetching Efficiency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating Prefetcher Selection with Dynamic Request Allocation\n  Improves Prefetching Efficiency"
                },
                "summary": "Hardware prefetching plays a critical role in hiding the off-chip DRAM\nlatency. The complexity of applications results in a wide variety of memory\naccess patterns, prompting the development of numerous cache-prefetching\nalgorithms. Consequently, commercial processors often employ a hybrid of these\nalgorithms to enhance the overall prefetching performance. Nonetheless, since\nthese prefetchers share hardware resources, conflicts arising from competing\nprefetching requests can negate the benefits of hardware prefetching. Under\nsuch circumstances, several prefetcher selection algorithms have been proposed\nto mitigate conflicts between prefetchers. However, these prior solutions\nsuffer from two limitations. First, the input demand request allocation is\ninaccurate. Second, the prefetcher selection criteria are coarse-grained.\n  In this paper, we address both limitations by introducing an efficient and\nwidely applicable prefetcher selection algorithm--Alecto, which tailors the\ndemand requests for each prefetcher. Every demand request is first sent to\nAlecto to identify suitable prefetchers before being routed to prefetchers for\ntraining and prefetching. Our analysis shows that Alecto is adept at not only\nharmonizing prefetching accuracy, coverage, and timeliness but also\nsignificantly enhancing the utilization of the prefetcher table, which is vital\nfor temporal prefetching. Alecto outperforms the state-of-the-art RL-based\nprefetcher selection algorithm--Bandit by 2.76% in single-core, and 7.56% in\neight-core. For memory-intensive benchmarks, Alecto outperforms Bandit by\n5.25%. Alecto consistently delivers state-of-the-art performance in scheduling\nvarious types of cache prefetchers. In addition to the performance improvement,\nAlecto can reduce the energy consumption associated with accessing the\nprefetchers' table by 48%, while only adding less than 1 KB of storage\noverhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hardware prefetching plays a critical role in hiding the off-chip DRAM\nlatency. The complexity of applications results in a wide variety of memory\naccess patterns, prompting the development of numerous cache-prefetching\nalgorithms. Consequently, commercial processors often employ a hybrid of these\nalgorithms to enhance the overall prefetching performance. Nonetheless, since\nthese prefetchers share hardware resources, conflicts arising from competing\nprefetching requests can negate the benefits of hardware prefetching. Under\nsuch circumstances, several prefetcher selection algorithms have been proposed\nto mitigate conflicts between prefetchers. However, these prior solutions\nsuffer from two limitations. First, the input demand request allocation is\ninaccurate. Second, the prefetcher selection criteria are coarse-grained.\n  In this paper, we address both limitations by introducing an efficient and\nwidely applicable prefetcher selection algorithm--Alecto, which tailors the\ndemand requests for each prefetcher. Every demand request is first sent to\nAlecto to identify suitable prefetchers before being routed to prefetchers for\ntraining and prefetching. Our analysis shows that Alecto is adept at not only\nharmonizing prefetching accuracy, coverage, and timeliness but also\nsignificantly enhancing the utilization of the prefetcher table, which is vital\nfor temporal prefetching. Alecto outperforms the state-of-the-art RL-based\nprefetcher selection algorithm--Bandit by 2.76% in single-core, and 7.56% in\neight-core. For memory-intensive benchmarks, Alecto outperforms Bandit by\n5.25%. Alecto consistently delivers state-of-the-art performance in scheduling\nvarious types of cache prefetchers. In addition to the performance improvement,\nAlecto can reduce the energy consumption associated with accessing the\nprefetchers' table by 48%, while only adding less than 1 KB of storage\noverhead."
                },
                "authors": [
                    {
                        "name": "Mengming Li"
                    },
                    {
                        "name": "Qijun Zhang"
                    },
                    {
                        "name": "Yongqing Ren"
                    },
                    {
                        "name": "Zhiyao Xie"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyao Xie"
                },
                "author": "Zhiyao Xie",
                "arxiv_comment": "In 31th IEEE International Symposium on High-Performance Computer\n  Architecture (HPCA 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19390v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19390v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14882v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14882v2",
                "updated": "2025-03-24T23:47:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    23,
                    47,
                    51,
                    0,
                    83,
                    0
                ],
                "published": "2025-02-15T05:08:01Z",
                "published_parsed": [
                    2025,
                    2,
                    15,
                    5,
                    8,
                    1,
                    5,
                    46,
                    0
                ],
                "title": "CalibQuant: 1-Bit KV Cache Quantization for Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CalibQuant: 1-Bit KV Cache Quantization for Multimodal LLMs"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\nperformance across diverse applications. However, their computational overhead\nduring deployment remains a critical bottleneck. While Key-Value (KV) caching\neffectively trades memory for computation to enhance inference efficiency, the\ngrowing memory footprint from extensive KV caches significantly reduces\nthroughput and restricts prolonged deployment on memory-constrained GPU\ndevices. To address this challenge, we propose CalibQuant, a simple yet highly\neffective visual quantization strategy that drastically reduces both memory and\ncomputational overhead. Specifically, CalibQuant introduces an extreme 1-bit\nquantization scheme, complemented by novel post-scaling and calibration\ntechniques tailored to the intrinsic patterns of KV caches, thereby ensuring\nhigh efficiency without compromising model performance. Leveraging Triton for\nruntime optimization, we achieve a 10x throughput increase on InternVL models.\nOur method is designed to be plug-and-play, seamlessly integrating with various\nexisting MLLMs without requiring architectural changes. Extensive experiments\nconfirm that our approach significantly reduces memory usage while maintaining\ncomputational efficiency and preserving multimodal capabilities. Codes are\navailable at https://github.com/insuhan/calibquant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\nperformance across diverse applications. However, their computational overhead\nduring deployment remains a critical bottleneck. While Key-Value (KV) caching\neffectively trades memory for computation to enhance inference efficiency, the\ngrowing memory footprint from extensive KV caches significantly reduces\nthroughput and restricts prolonged deployment on memory-constrained GPU\ndevices. To address this challenge, we propose CalibQuant, a simple yet highly\neffective visual quantization strategy that drastically reduces both memory and\ncomputational overhead. Specifically, CalibQuant introduces an extreme 1-bit\nquantization scheme, complemented by novel post-scaling and calibration\ntechniques tailored to the intrinsic patterns of KV caches, thereby ensuring\nhigh efficiency without compromising model performance. Leveraging Triton for\nruntime optimization, we achieve a 10x throughput increase on InternVL models.\nOur method is designed to be plug-and-play, seamlessly integrating with various\nexisting MLLMs without requiring architectural changes. Extensive experiments\nconfirm that our approach significantly reduces memory usage while maintaining\ncomputational efficiency and preserving multimodal capabilities. Codes are\navailable at https://github.com/insuhan/calibquant."
                },
                "authors": [
                    {
                        "name": "Insu Han"
                    },
                    {
                        "name": "Zeliang Zhang"
                    },
                    {
                        "name": "Zhiyuan Wang"
                    },
                    {
                        "name": "Yifan Zhu"
                    },
                    {
                        "name": "Susan Liang"
                    },
                    {
                        "name": "Jiani Liu"
                    },
                    {
                        "name": "Haiting Lin"
                    },
                    {
                        "name": "Mingjie Zhao"
                    },
                    {
                        "name": "Chenliang Xu"
                    },
                    {
                        "name": "Kun Wan"
                    },
                    {
                        "name": "Wentian Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Wentian Zhao"
                },
                "author": "Wentian Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14882v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14882v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09859v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09859v2",
                "updated": "2025-03-24T21:27:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    21,
                    27,
                    53,
                    0,
                    83,
                    0
                ],
                "published": "2024-11-15T00:37:31Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    0,
                    37,
                    31,
                    4,
                    320,
                    0
                ],
                "title": "Skew-Symmetric Matrix Decompositions on Shared-Memory Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skew-Symmetric Matrix Decompositions on Shared-Memory Architectures"
                },
                "summary": "The factorization of skew-symmetric matrices is a critically understudied\narea of dense linear algebra, particularly in comparison to that of general and\nsymmetric matrices. While some algorithms can be adapted from the symmetric\ncase, the cost of algorithms can be reduced by exploiting skew-symmetry. This\nwork examines the factorization of a skew-symmetric matrix $X$ into its\n$LTL^\\mathrm{T}$ decomposition, where $L$ is unit lower triangular and $T$ is\ntridiagonal. This is also known as a triangular tridiagonalization. This\noperation is a means for computing the determinant of $X$ as the square of the\n(cheaply-computed) Pfaffian of the skew-symmetric tridiagonal matrix $T$ as\nwell as for solving systems of equations, across fields such as quantum\nelectronic structure and machine learning. Its application also often requires\npivoting in order to improve numerical stability. We compare and contrast\npreviously-published algorithms with those systematically derived using the\nFLAME methodology. Performant parallel CPU implementations are achieved by\nfusing operations at multiple levels in order to reduce memory traffic\noverhead. A key factor is the employment of new capabilities of the BLAS-like\nLibrary Instantiation Software (BLIS) framework, which now supports casting\nlevel-2 and level-3 BLAS-like operations by leveraging its gemm and other\nkernels, hierarchical parallelism, and cache blocking. A prototype, concise C++\nAPI facilitates the translation of correct-by-construction algorithms into\ncorrect code. Experiments verify that the resulting implementations greatly\nexceed the performance of previous work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The factorization of skew-symmetric matrices is a critically understudied\narea of dense linear algebra, particularly in comparison to that of general and\nsymmetric matrices. While some algorithms can be adapted from the symmetric\ncase, the cost of algorithms can be reduced by exploiting skew-symmetry. This\nwork examines the factorization of a skew-symmetric matrix $X$ into its\n$LTL^\\mathrm{T}$ decomposition, where $L$ is unit lower triangular and $T$ is\ntridiagonal. This is also known as a triangular tridiagonalization. This\noperation is a means for computing the determinant of $X$ as the square of the\n(cheaply-computed) Pfaffian of the skew-symmetric tridiagonal matrix $T$ as\nwell as for solving systems of equations, across fields such as quantum\nelectronic structure and machine learning. Its application also often requires\npivoting in order to improve numerical stability. We compare and contrast\npreviously-published algorithms with those systematically derived using the\nFLAME methodology. Performant parallel CPU implementations are achieved by\nfusing operations at multiple levels in order to reduce memory traffic\noverhead. A key factor is the employment of new capabilities of the BLAS-like\nLibrary Instantiation Software (BLIS) framework, which now supports casting\nlevel-2 and level-3 BLAS-like operations by leveraging its gemm and other\nkernels, hierarchical parallelism, and cache blocking. A prototype, concise C++\nAPI facilitates the translation of correct-by-construction algorithms into\ncorrect code. Experiments verify that the resulting implementations greatly\nexceed the performance of previous work."
                },
                "authors": [
                    {
                        "name": "Ishna Satyarth"
                    },
                    {
                        "name": "Chao Yin"
                    },
                    {
                        "name": "Devin A. Matthews"
                    },
                    {
                        "name": "Maggie Myers"
                    },
                    {
                        "name": "Robert van de Geijn"
                    },
                    {
                        "name": "RuQing G. Xu"
                    }
                ],
                "author_detail": {
                    "name": "RuQing G. Xu"
                },
                "author": "RuQing G. Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09859v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09859v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19145v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19145v1",
                "updated": "2025-03-24T21:00:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    21,
                    0,
                    37,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T21:00:37Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    21,
                    0,
                    37,
                    0,
                    83,
                    0
                ],
                "title": "Compositional Caching for Training-free Open-vocabulary Attribute\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compositional Caching for Training-free Open-vocabulary Attribute\n  Detection"
                },
                "summary": "Attribute detection is crucial for many computer vision tasks, as it enables\nsystems to describe properties such as color, texture, and material. Current\napproaches often rely on labor-intensive annotation processes which are\ninherently limited: objects can be described at an arbitrary level of detail\n(e.g., color vs. color shades), leading to ambiguities when the annotators are\nnot instructed carefully. Furthermore, they operate within a predefined set of\nattributes, reducing scalability and adaptability to unforeseen downstream\napplications. We present Compositional Caching (ComCa), a training-free method\nfor open-vocabulary attribute detection that overcomes these constraints. ComCa\nrequires only the list of target attributes and objects as input, using them to\npopulate an auxiliary cache of images by leveraging web-scale databases and\nLarge Language Models to determine attribute-object compatibility. To account\nfor the compositional nature of attributes, cache images receive soft attribute\nlabels. Those are aggregated at inference time based on the similarity between\nthe input and cache images, refining the predictions of underlying\nVision-Language Models (VLMs). Importantly, our approach is model-agnostic,\ncompatible with various VLMs. Experiments on public datasets demonstrate that\nComCa significantly outperforms zero-shot and cache-based baselines, competing\nwith recent training-based methods, proving that a carefully designed\ntraining-free approach can successfully address open-vocabulary attribute\ndetection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attribute detection is crucial for many computer vision tasks, as it enables\nsystems to describe properties such as color, texture, and material. Current\napproaches often rely on labor-intensive annotation processes which are\ninherently limited: objects can be described at an arbitrary level of detail\n(e.g., color vs. color shades), leading to ambiguities when the annotators are\nnot instructed carefully. Furthermore, they operate within a predefined set of\nattributes, reducing scalability and adaptability to unforeseen downstream\napplications. We present Compositional Caching (ComCa), a training-free method\nfor open-vocabulary attribute detection that overcomes these constraints. ComCa\nrequires only the list of target attributes and objects as input, using them to\npopulate an auxiliary cache of images by leveraging web-scale databases and\nLarge Language Models to determine attribute-object compatibility. To account\nfor the compositional nature of attributes, cache images receive soft attribute\nlabels. Those are aggregated at inference time based on the similarity between\nthe input and cache images, refining the predictions of underlying\nVision-Language Models (VLMs). Importantly, our approach is model-agnostic,\ncompatible with various VLMs. Experiments on public datasets demonstrate that\nComCa significantly outperforms zero-shot and cache-based baselines, competing\nwith recent training-based methods, proving that a carefully designed\ntraining-free approach can successfully address open-vocabulary attribute\ndetection."
                },
                "authors": [
                    {
                        "name": "Marco Garosi"
                    },
                    {
                        "name": "Alessandro Conti"
                    },
                    {
                        "name": "Gaowen Liu"
                    },
                    {
                        "name": "Elisa Ricci"
                    },
                    {
                        "name": "Massimiliano Mancini"
                    }
                ],
                "author_detail": {
                    "name": "Massimiliano Mancini"
                },
                "author": "Massimiliano Mancini",
                "arxiv_comment": "CVPR 2025. Project website at https://comca-attributes.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19145v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19145v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13773v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13773v2",
                "updated": "2025-03-24T18:50:09Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    18,
                    50,
                    9,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-17T23:38:29Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    23,
                    38,
                    29,
                    0,
                    76,
                    0
                ],
                "title": "Mitigating KV Cache Competition to Enhance User Experience in LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating KV Cache Competition to Enhance User Experience in LLM\n  Inference"
                },
                "summary": "In Large Language Model (LLM) serving, the KV-cache (KVC) bottleneck causes\nhigh tail Time-to-First-Token (TTFT) and Time-Between-Tokens (TBT), impairing\nuser experience, particularly in time-sensitive applications. However,\nsatisfying both TTFT and TBT service-level objectives (SLOs) is challenging. To\naddress this, we propose a system, named CacheOPT for mitigating KV Cache\ncompetition, based on key insights from our measurements, incorporating novel\ncomponents. First, it estimates a request's output length, bounding the\ndeviation with a high specified probability, adjusted based on the request\narrival rate. Second, it allocates the estimated KVC demand to a request, and\nreuses other requests' allocated KVC to avoid preemptions while reducing\nwaiting time. Third, it proactively allocates KVC before instead of at the time\na request exhausts its allocation and reserves KVC globally to prevent\npreemptions. Fourth, it chooses a request that has long TBT SLO, long job\nremaining time and short preemption time to preempt. Fifth, it selects the\nshortest-latency strategy between swapping and recomputation for preemptions.\nExperiments show that CacheOPT achieves up to 3.29$\\times$ and 2.83$\\times$\nlower tail TBT and tail TTFT, 47\\% and 53\\% higher TTFT and TBT SLO\nattainments, and supports up to 1.58$\\times$ higher request arrival rate than\nthe state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Large Language Model (LLM) serving, the KV-cache (KVC) bottleneck causes\nhigh tail Time-to-First-Token (TTFT) and Time-Between-Tokens (TBT), impairing\nuser experience, particularly in time-sensitive applications. However,\nsatisfying both TTFT and TBT service-level objectives (SLOs) is challenging. To\naddress this, we propose a system, named CacheOPT for mitigating KV Cache\ncompetition, based on key insights from our measurements, incorporating novel\ncomponents. First, it estimates a request's output length, bounding the\ndeviation with a high specified probability, adjusted based on the request\narrival rate. Second, it allocates the estimated KVC demand to a request, and\nreuses other requests' allocated KVC to avoid preemptions while reducing\nwaiting time. Third, it proactively allocates KVC before instead of at the time\na request exhausts its allocation and reserves KVC globally to prevent\npreemptions. Fourth, it chooses a request that has long TBT SLO, long job\nremaining time and short preemption time to preempt. Fifth, it selects the\nshortest-latency strategy between swapping and recomputation for preemptions.\nExperiments show that CacheOPT achieves up to 3.29$\\times$ and 2.83$\\times$\nlower tail TBT and tail TTFT, 47\\% and 53\\% higher TTFT and TBT SLO\nattainments, and supports up to 1.58$\\times$ higher request arrival rate than\nthe state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Haiying Shen"
                    },
                    {
                        "name": "Tanmoy Sen"
                    },
                    {
                        "name": "Masahiro Tanaka"
                    }
                ],
                "author_detail": {
                    "name": "Masahiro Tanaka"
                },
                "author": "Masahiro Tanaka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13773v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13773v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06364v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06364v2",
                "updated": "2025-03-24T18:16:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    18,
                    16,
                    58,
                    0,
                    83,
                    0
                ],
                "published": "2024-11-10T05:12:51Z",
                "published_parsed": [
                    2024,
                    11,
                    10,
                    5,
                    12,
                    51,
                    6,
                    315,
                    0
                ],
                "title": "EconoServe: Maximizing Multi-Resource Utilization with SLO Guarantees in\n  LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EconoServe: Maximizing Multi-Resource Utilization with SLO Guarantees in\n  LLM Serving"
                },
                "summary": "As Large Language Models (LLMs) continue to grow, reducing costs and\nalleviating GPU demands has become increasingly critical. However, existing\nschedulers primarily target either GPU compute or Key-Value Cache (KVC)\nutilization, failing to fully optimize both GPU compute and KVC usage during\neach iteration or guarantee timely KVC allocations when needed. To address\nthese challenges, we conducted a trace-based experimental analysis and made\ninsightful observations, leading to the design of a system called EconoServe.\nEconoServe maximizes multi-resource utilization while ensuring service-level\nobjective (SLO) guarantees in LLM serving. To enable adding prompts to a batch\nto maximize GPU utilization in each iteration, EconoServe maintains separate\nwaiting queues for prompt processing tasks (PTs) and generation tasks (GTs). It\nbatches GTs with the same predicted response lengths (RL) to save scheduling\ntime and allocates KVC space for the predicted RL to avoid KVC allocation\nfailures. It further has a novel KVC pipelining method, allowing sharing\nallocated but unused KVC space to enhance KVC utilization. In addition, it\nprioritizes queued requests that occupy more KVC to release KVC earlier and\nsatisfy request service-level-objective (SLO). Experimental results demonstrate\nthat EconoServe increases throughput by up to 4$\\times$ with the same level of\nlatency, generates up to 91\\% lower job completion time and up to 91\\% higher\nSLO satisfaction ratio compared to vLLM. It also reduces the number of GPUs\nused in DistServe by up to 78\\% while maintaining the same level of goodput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) continue to grow, reducing costs and\nalleviating GPU demands has become increasingly critical. However, existing\nschedulers primarily target either GPU compute or Key-Value Cache (KVC)\nutilization, failing to fully optimize both GPU compute and KVC usage during\neach iteration or guarantee timely KVC allocations when needed. To address\nthese challenges, we conducted a trace-based experimental analysis and made\ninsightful observations, leading to the design of a system called EconoServe.\nEconoServe maximizes multi-resource utilization while ensuring service-level\nobjective (SLO) guarantees in LLM serving. To enable adding prompts to a batch\nto maximize GPU utilization in each iteration, EconoServe maintains separate\nwaiting queues for prompt processing tasks (PTs) and generation tasks (GTs). It\nbatches GTs with the same predicted response lengths (RL) to save scheduling\ntime and allocates KVC space for the predicted RL to avoid KVC allocation\nfailures. It further has a novel KVC pipelining method, allowing sharing\nallocated but unused KVC space to enhance KVC utilization. In addition, it\nprioritizes queued requests that occupy more KVC to release KVC earlier and\nsatisfy request service-level-objective (SLO). Experimental results demonstrate\nthat EconoServe increases throughput by up to 4$\\times$ with the same level of\nlatency, generates up to 91\\% lower job completion time and up to 91\\% higher\nSLO satisfaction ratio compared to vLLM. It also reduces the number of GPUs\nused in DistServe by up to 78\\% while maintaining the same level of goodput."
                },
                "authors": [
                    {
                        "name": "Haiying Shen"
                    },
                    {
                        "name": "Tanmoy Sen"
                    }
                ],
                "author_detail": {
                    "name": "Tanmoy Sen"
                },
                "author": "Tanmoy Sen",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06364v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06364v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18893v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18893v1",
                "updated": "2025-03-24T17:06:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    17,
                    6,
                    37,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T17:06:37Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    17,
                    6,
                    37,
                    0,
                    83,
                    0
                ],
                "title": "xKV: Cross-Layer SVD for KV-Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "xKV: Cross-Layer SVD for KV-Cache Compression"
                },
                "summary": "Large Language Models (LLMs) with long context windows enable powerful\napplications but come at the cost of high memory consumption to store the Key\nand Value states (KV-Cache). Recent studies attempted to merge KV-cache from\nmultiple layers into shared representations, yet these approaches either\nrequire expensive pretraining or rely on assumptions of high per-token cosine\nsimilarity across layers which generally does not hold in practice. We find\nthat the dominant singular vectors are remarkably well-aligned across multiple\nlayers of the KV-Cache. Exploiting this insight, we propose xKV, a simple\npost-training method that applies Singular Value Decomposition (SVD) on the\nKV-Cache of grouped layers. xKV consolidates the KV-Cache of multiple layers\ninto a shared low-rank subspace, significantly reducing KV-Cache sizes. Through\nextensive evaluations on the RULER long-context benchmark with widely-used LLMs\n(e.g., Llama-3.1 and Qwen2.5), xKV achieves up to 6.8x higher compression rates\nthan state-of-the-art inter-layer technique while improving accuracy by 2.7%.\nMoreover, xKV is compatible with the emerging Multi-Head Latent Attention (MLA)\n(e.g., DeepSeek-Coder-V2), yielding a notable 3x compression rates on coding\ntasks without performance degradation. These results highlight xKV's strong\ncapability and versatility in addressing memory bottlenecks for long-context\nLLM inference. Our code is publicly available at:\nhttps://github.com/abdelfattah-lab/xKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) with long context windows enable powerful\napplications but come at the cost of high memory consumption to store the Key\nand Value states (KV-Cache). Recent studies attempted to merge KV-cache from\nmultiple layers into shared representations, yet these approaches either\nrequire expensive pretraining or rely on assumptions of high per-token cosine\nsimilarity across layers which generally does not hold in practice. We find\nthat the dominant singular vectors are remarkably well-aligned across multiple\nlayers of the KV-Cache. Exploiting this insight, we propose xKV, a simple\npost-training method that applies Singular Value Decomposition (SVD) on the\nKV-Cache of grouped layers. xKV consolidates the KV-Cache of multiple layers\ninto a shared low-rank subspace, significantly reducing KV-Cache sizes. Through\nextensive evaluations on the RULER long-context benchmark with widely-used LLMs\n(e.g., Llama-3.1 and Qwen2.5), xKV achieves up to 6.8x higher compression rates\nthan state-of-the-art inter-layer technique while improving accuracy by 2.7%.\nMoreover, xKV is compatible with the emerging Multi-Head Latent Attention (MLA)\n(e.g., DeepSeek-Coder-V2), yielding a notable 3x compression rates on coding\ntasks without performance degradation. These results highlight xKV's strong\ncapability and versatility in addressing memory bottlenecks for long-context\nLLM inference. Our code is publicly available at:\nhttps://github.com/abdelfattah-lab/xKV."
                },
                "authors": [
                    {
                        "name": "Chi-Chih Chang"
                    },
                    {
                        "name": "Chien-Yu Lin"
                    },
                    {
                        "name": "Yash Akhauri"
                    },
                    {
                        "name": "Wei-Cheng Lin"
                    },
                    {
                        "name": "Kai-Chiang Wu"
                    },
                    {
                        "name": "Luis Ceze"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed S. Abdelfattah"
                },
                "author": "Mohamed S. Abdelfattah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18893v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18893v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13064v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13064v2",
                "updated": "2025-03-24T16:47:48Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    47,
                    48,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-17T11:10:49Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    11,
                    10,
                    49,
                    0,
                    76,
                    0
                ],
                "title": "HERMES: High-Performance RISC-V Memory Hierarchy for ML Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HERMES: High-Performance RISC-V Memory Hierarchy for ML Workloads"
                },
                "summary": "The growth of machine learning (ML) workloads has underscored the importance\nof efficient memory hierarchies to address bandwidth, latency, and scalability\nchallenges. HERMES focuses on optimizing memory subsystems for RISC-V\narchitectures to meet the computational needs of ML models such as CNNs, RNNs,\nand Transformers. This project explores state-of-the-art techniques such as\nadvanced prefetching, tensor-aware caching, and hybrid memory models. The\ncornerstone of HERMES is the integration of shared L3 caches with fine-grained\ncoherence protocols equipped with specialized pathways to deep-learning\naccelerators such as Gemmini. Simulation tools like Gem5 and DRAMSim2 were used\nto evaluate baseline performance and scalability under representative ML\nworkloads. The findings of this study highlight the design choices, and the\nanticipated challenges, paving the way for low-latency scalable memory\noperations for ML applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growth of machine learning (ML) workloads has underscored the importance\nof efficient memory hierarchies to address bandwidth, latency, and scalability\nchallenges. HERMES focuses on optimizing memory subsystems for RISC-V\narchitectures to meet the computational needs of ML models such as CNNs, RNNs,\nand Transformers. This project explores state-of-the-art techniques such as\nadvanced prefetching, tensor-aware caching, and hybrid memory models. The\ncornerstone of HERMES is the integration of shared L3 caches with fine-grained\ncoherence protocols equipped with specialized pathways to deep-learning\naccelerators such as Gemmini. Simulation tools like Gem5 and DRAMSim2 were used\nto evaluate baseline performance and scalability under representative ML\nworkloads. The findings of this study highlight the design choices, and the\nanticipated challenges, paving the way for low-latency scalable memory\noperations for ML applications."
                },
                "authors": [
                    {
                        "name": "Pranav Suryadevara"
                    }
                ],
                "author_detail": {
                    "name": "Pranav Suryadevara"
                },
                "author": "Pranav Suryadevara",
                "arxiv_comment": "5 pages, 5 figures. Individual Project",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13064v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13064v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.3.2; C.1.3; C.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18869v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18869v1",
                "updated": "2025-03-24T16:44:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    44,
                    32,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T16:44:32Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    44,
                    32,
                    0,
                    83,
                    0
                ],
                "title": "Reimagining Memory Access for LLM Inference: Compression-Aware Memory\n  Controller Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reimagining Memory Access for LLM Inference: Compression-Aware Memory\n  Controller Design"
                },
                "summary": "The efficiency of Large Language Model~(LLM) inference is often constrained\nby substantial memory bandwidth and capacity demands. Existing techniques, such\nas pruning, quantization, and mixture of experts/depth, reduce memory capacity\nand/or bandwidth consumption at the cost of slight degradation in inference\nquality. This paper introduces a design solution that further alleviates memory\nbottlenecks by enhancing the on-chip memory controller in AI accelerators to\nachieve two main objectives: (1) significantly reducing memory capacity and\nbandwidth usage through lossless block compression~(e.g., LZ4 and ZSTD) of\nmodel weights and key-value (KV) cache without compromising inference quality,\nand (2) enabling memory bandwidth and energy consumption to scale\nproportionally with context-dependent dynamic quantization. These goals are\naccomplished by equipping the on-chip memory controller with mechanisms to\nimprove fine-grained bit-level accessibility and compressibility of weights and\nKV cache through LLM-aware configuration of in-memory placement and\nrepresentation. Experimental results on publicly available LLMs demonstrate the\neffectiveness of this approach, showing memory footprint reductions of 25.2\\%\nfor model weights and 46.9\\% for KV cache. In addition, our hardware prototype\nat 4\\,GHz and 32 lanes (7\\,nm) achieves 8\\,TB/s throughput with a modest area\noverhead (under 3.8\\,mm\\(^2\\)), which underscores the viability of LLM-aware\nmemory control as a key to efficient large-scale inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficiency of Large Language Model~(LLM) inference is often constrained\nby substantial memory bandwidth and capacity demands. Existing techniques, such\nas pruning, quantization, and mixture of experts/depth, reduce memory capacity\nand/or bandwidth consumption at the cost of slight degradation in inference\nquality. This paper introduces a design solution that further alleviates memory\nbottlenecks by enhancing the on-chip memory controller in AI accelerators to\nachieve two main objectives: (1) significantly reducing memory capacity and\nbandwidth usage through lossless block compression~(e.g., LZ4 and ZSTD) of\nmodel weights and key-value (KV) cache without compromising inference quality,\nand (2) enabling memory bandwidth and energy consumption to scale\nproportionally with context-dependent dynamic quantization. These goals are\naccomplished by equipping the on-chip memory controller with mechanisms to\nimprove fine-grained bit-level accessibility and compressibility of weights and\nKV cache through LLM-aware configuration of in-memory placement and\nrepresentation. Experimental results on publicly available LLMs demonstrate the\neffectiveness of this approach, showing memory footprint reductions of 25.2\\%\nfor model weights and 46.9\\% for KV cache. In addition, our hardware prototype\nat 4\\,GHz and 32 lanes (7\\,nm) achieves 8\\,TB/s throughput with a modest area\noverhead (under 3.8\\,mm\\(^2\\)), which underscores the viability of LLM-aware\nmemory control as a key to efficient large-scale inference."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Asad Ul Haq"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Yunhua Fang"
                    },
                    {
                        "name": "Zirak Burzin Engineer"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "arxiv_comment": "9 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18869v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18869v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18862v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18862v1",
                "updated": "2025-03-24T16:38:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    38,
                    31,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T16:38:31Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    38,
                    31,
                    0,
                    83,
                    0
                ],
                "title": "Exploring the Integration of Key-Value Attention Into Pure and Hybrid\n  Transformers for Semantic Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Integration of Key-Value Attention Into Pure and Hybrid\n  Transformers for Semantic Segmentation"
                },
                "summary": "While CNNs were long considered state of the art for image processing, the\nintroduction of Transformer architectures has challenged this position. While\nachieving excellent results in image classification and segmentation,\nTransformers remain inherently reliant on large training datasets and remain\ncomputationally expensive. A newly introduced Transformer derivative named KV\nTransformer shows promising results in synthetic, NLP, and image classification\ntasks, while reducing complexity and memory usage. This is especially conducive\nto use cases where local inference is required, such as medical screening\napplications. We endeavoured to further evaluate the merit of KV Transformers\non semantic segmentation tasks, specifically in the domain of medical imaging.\nBy directly comparing traditional and KV variants of the same base\narchitectures, we provide further insight into the practical tradeoffs of\nreduced model complexity. We observe a notable reduction in parameter count and\nmultiply accumulate operations, while achieving similar performance from most\nof the KV variant models when directly compared to their QKV implementation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While CNNs were long considered state of the art for image processing, the\nintroduction of Transformer architectures has challenged this position. While\nachieving excellent results in image classification and segmentation,\nTransformers remain inherently reliant on large training datasets and remain\ncomputationally expensive. A newly introduced Transformer derivative named KV\nTransformer shows promising results in synthetic, NLP, and image classification\ntasks, while reducing complexity and memory usage. This is especially conducive\nto use cases where local inference is required, such as medical screening\napplications. We endeavoured to further evaluate the merit of KV Transformers\non semantic segmentation tasks, specifically in the domain of medical imaging.\nBy directly comparing traditional and KV variants of the same base\narchitectures, we provide further insight into the practical tradeoffs of\nreduced model complexity. We observe a notable reduction in parameter count and\nmultiply accumulate operations, while achieving similar performance from most\nof the KV variant models when directly compared to their QKV implementation."
                },
                "authors": [
                    {
                        "name": "DeShin Hwa"
                    },
                    {
                        "name": "Tobias Holmes"
                    },
                    {
                        "name": "Klaus Drechsler"
                    }
                ],
                "author_detail": {
                    "name": "Klaus Drechsler"
                },
                "author": "Klaus Drechsler",
                "arxiv_doi": "10.1007/978-3-658-47422-5_71",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-658-47422-5_71",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.18862v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18862v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "6 pages, 3 figures, Preprint. Final version published in:\n  Bildverarbeitung f\\\"ur die Medizin 2025, Springer. DOI:\n  https://doi.org/10.1007/978-3-658-47422-5_71",
                "arxiv_journal_ref": "Bildverarbeitung f\\\"ur die Medizin 2025. BVM 2025. Informatik\n  aktuell. Springer Vieweg, Wiesbaden, pp 305-310",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18773v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18773v1",
                "updated": "2025-03-24T15:22:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    15,
                    22,
                    41,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T15:22:41Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    15,
                    22,
                    41,
                    0,
                    83,
                    0
                ],
                "title": "BitDecoding: Unlocking Tensor Cores for Long-Context LLMs Decoding with\n  Low-Bit KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BitDecoding: Unlocking Tensor Cores for Long-Context LLMs Decoding with\n  Low-Bit KV Cache"
                },
                "summary": "The growing adoption of long-context Large Language Models (LLMs) has\nintroduced significant memory and computational challenges in autoregressive\ndecoding due to the expanding Key-Value (KV) cache. KV cache quantization has\nemerged as a promising solution, with prior work showing that 4-bit or even\n2-bit quantization can maintain model accuracy while reducing memory costs.\nHowever, despite these benefits, preliminary implementations for the low-bit KV\ncache struggle to deliver the expected speedup due to quantization and\ndequantization overheads and the lack of Tensor Cores utilization. In this\nwork, we propose BitDecoding, a GPU-optimized framework that unlocks Tensor\nCores for efficient decoding with low-bit KV cache. Efficiently leveraging\nTensor Cores for low-bit KV cache is challenging due to the dynamic nature of\nKV cache generation at each decoding step. BitDecoding addresses these\nchallenges with a Tensor Cores-Centric BitFusion Scheme that ensures data\nlayout compatibility to enable high utilization of Tensor Cores. Additionally,\nBitDecoding incorporates a warp-efficient parallel decoding kernel and a\nfine-grained asynchronous pipeline, minimizing dequantization overhead and\nimproving computational efficiency. Experiments show that BitDecoding achieves\nup to 7.5x speedup on RTX 4090, 4.8x on A100, and 8.9x on H100, compared to\nFP16 FlashDecoding-v2. It also outperforms the state-of-the-art low-bit KV\ncache implementation (QServe) by up to 4.3x. On LLaMA-3.1-8B with a 128K\nsequence length, BitDecoding reduces single-batch decoding latency by 3x,\ndemonstrating its effectiveness in long-context generation scenarios. The code\nis available at https://github.com/DD-DuDa/BitDecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing adoption of long-context Large Language Models (LLMs) has\nintroduced significant memory and computational challenges in autoregressive\ndecoding due to the expanding Key-Value (KV) cache. KV cache quantization has\nemerged as a promising solution, with prior work showing that 4-bit or even\n2-bit quantization can maintain model accuracy while reducing memory costs.\nHowever, despite these benefits, preliminary implementations for the low-bit KV\ncache struggle to deliver the expected speedup due to quantization and\ndequantization overheads and the lack of Tensor Cores utilization. In this\nwork, we propose BitDecoding, a GPU-optimized framework that unlocks Tensor\nCores for efficient decoding with low-bit KV cache. Efficiently leveraging\nTensor Cores for low-bit KV cache is challenging due to the dynamic nature of\nKV cache generation at each decoding step. BitDecoding addresses these\nchallenges with a Tensor Cores-Centric BitFusion Scheme that ensures data\nlayout compatibility to enable high utilization of Tensor Cores. Additionally,\nBitDecoding incorporates a warp-efficient parallel decoding kernel and a\nfine-grained asynchronous pipeline, minimizing dequantization overhead and\nimproving computational efficiency. Experiments show that BitDecoding achieves\nup to 7.5x speedup on RTX 4090, 4.8x on A100, and 8.9x on H100, compared to\nFP16 FlashDecoding-v2. It also outperforms the state-of-the-art low-bit KV\ncache implementation (QServe) by up to 4.3x. On LLaMA-3.1-8B with a 128K\nsequence length, BitDecoding reduces single-batch decoding latency by 3x,\ndemonstrating its effectiveness in long-context generation scenarios. The code\nis available at https://github.com/DD-DuDa/BitDecoding."
                },
                "authors": [
                    {
                        "name": "Dayou Du"
                    },
                    {
                        "name": "Shijie Cao"
                    },
                    {
                        "name": "Jianyi Cheng"
                    },
                    {
                        "name": "Ting Cao"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18773v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18773v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05941v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05941v2",
                "updated": "2025-03-24T13:09:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    13,
                    9,
                    3,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-07T21:16:41Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    21,
                    16,
                    41,
                    4,
                    66,
                    0
                ],
                "title": "Choosing Augmentation Parameters in OSQP- A New Approach based on\n  Conjugate Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Choosing Augmentation Parameters in OSQP- A New Approach based on\n  Conjugate Directions"
                },
                "summary": "This work proposes a new method to select the augmentation parameters in the\noperator splitting quadratic program (OSQP) algorithm so as to reduce the\ncomputation time of overall algorithm. The selection is based upon the\ninformation of conjugate directions of the coefficient matrix of a linear\nsystem of equations present in the algorithm. This selection makes it possible\nto cache these conjugate directions, instead of computing them at each\niteration, resulting in faster computation of the solution of the linear system\nthus reducing the overall computation time. This reduction is demonstrated by a\nnumerical example.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work proposes a new method to select the augmentation parameters in the\noperator splitting quadratic program (OSQP) algorithm so as to reduce the\ncomputation time of overall algorithm. The selection is based upon the\ninformation of conjugate directions of the coefficient matrix of a linear\nsystem of equations present in the algorithm. This selection makes it possible\nto cache these conjugate directions, instead of computing them at each\niteration, resulting in faster computation of the solution of the linear system\nthus reducing the overall computation time. This reduction is demonstrated by a\nnumerical example."
                },
                "authors": [
                    {
                        "name": "Avinash Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Avinash Kumar"
                },
                "author": "Avinash Kumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05941v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05941v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18599v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18599v1",
                "updated": "2025-03-24T11:56:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    56,
                    50,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T11:56:50Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    56,
                    50,
                    0,
                    83,
                    0
                ],
                "title": "Oaken: Fast and Efficient LLM Serving with Online-Offline Hybrid KV\n  Cache Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Oaken: Fast and Efficient LLM Serving with Online-Offline Hybrid KV\n  Cache Quantization"
                },
                "summary": "Modern Large Language Model serving system batches multiple requests to\nachieve high throughput, while batching attention operations is challenging,\nrendering memory bandwidth a critical bottleneck. The community relies on\nhigh-end GPUs with multiple high-bandwidth memory channels. Unfortunately,\nHBM's high bandwidth often comes at the expense of limited memory capacity,\nwhich reduces core utilization and increases costs. Recent advancements\nenabling longer contexts for LLMs have substantially increased the key-value\ncache size, further intensifying the pressures on memory capacity. The\nliterature has explored KV cache quantization techniques, which commonly use\nlow bitwidth for most values, selectively using higher bitwidth for outlier\nvalues. While this approach helps achieve high accuracy and low bitwidth\nsimultaneously, it comes with the limitation that cost for online outlier\ndetection is excessively high, negating the advantages. We propose Oaken, an\nacceleration solution that achieves high accuracy and high performance\nsimultaneously through co-designing algorithm and hardware. To effectively find\na sweet spot in the accuracy-performance trade-off space of KV cache\nquantization, Oaken employs an online-offline hybrid approach, setting outlier\nthresholds offline, which are then used to determine the quantization scale\nonline. To translate the proposed algorithmic technique into tangible\nperformance gains, Oaken also comes with custom quantization engines and memory\nmanagement units that can be integrated with any LLM accelerators. We built an\nOaken accelerator on top of an LLM accelerator, LPU, and conducted a\ncomprehensive evaluation. Our experiments show that for a batch size of 256,\nOaken achieves up to 1.58x throughput improvement over NVIDIA A100 GPU,\nincurring a minimal accuracy loss of only 0.54\\% on average, compared to\nstate-of-the-art KV cache quantization techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Large Language Model serving system batches multiple requests to\nachieve high throughput, while batching attention operations is challenging,\nrendering memory bandwidth a critical bottleneck. The community relies on\nhigh-end GPUs with multiple high-bandwidth memory channels. Unfortunately,\nHBM's high bandwidth often comes at the expense of limited memory capacity,\nwhich reduces core utilization and increases costs. Recent advancements\nenabling longer contexts for LLMs have substantially increased the key-value\ncache size, further intensifying the pressures on memory capacity. The\nliterature has explored KV cache quantization techniques, which commonly use\nlow bitwidth for most values, selectively using higher bitwidth for outlier\nvalues. While this approach helps achieve high accuracy and low bitwidth\nsimultaneously, it comes with the limitation that cost for online outlier\ndetection is excessively high, negating the advantages. We propose Oaken, an\nacceleration solution that achieves high accuracy and high performance\nsimultaneously through co-designing algorithm and hardware. To effectively find\na sweet spot in the accuracy-performance trade-off space of KV cache\nquantization, Oaken employs an online-offline hybrid approach, setting outlier\nthresholds offline, which are then used to determine the quantization scale\nonline. To translate the proposed algorithmic technique into tangible\nperformance gains, Oaken also comes with custom quantization engines and memory\nmanagement units that can be integrated with any LLM accelerators. We built an\nOaken accelerator on top of an LLM accelerator, LPU, and conducted a\ncomprehensive evaluation. Our experiments show that for a batch size of 256,\nOaken achieves up to 1.58x throughput improvement over NVIDIA A100 GPU,\nincurring a minimal accuracy loss of only 0.54\\% on average, compared to\nstate-of-the-art KV cache quantization techniques."
                },
                "authors": [
                    {
                        "name": "Minsu Kim"
                    },
                    {
                        "name": "Seongmin Hong"
                    },
                    {
                        "name": "RyeoWook Ko"
                    },
                    {
                        "name": "Soongyu Choi"
                    },
                    {
                        "name": "Hunjong Lee"
                    },
                    {
                        "name": "Junsoo Kim"
                    },
                    {
                        "name": "Joo-Young Kim"
                    },
                    {
                        "name": "Jongse Park"
                    }
                ],
                "author_detail": {
                    "name": "Jongse Park"
                },
                "author": "Jongse Park",
                "arxiv_comment": "15 pages, 14 figures, and 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18599v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18599v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17333v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17333v2",
                "updated": "2025-03-24T11:00:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    0,
                    35,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-21T17:33:03Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    17,
                    33,
                    3,
                    4,
                    80,
                    0
                ],
                "title": "Register Dispersion: Reducing the Footprint of the Vector Register File\n  in Vector Engines of Low-Cost RISC-V CPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Register Dispersion: Reducing the Footprint of the Vector Register File\n  in Vector Engines of Low-Cost RISC-V CPUs"
                },
                "summary": "The deployment of Machine Learning (ML) applications at the edge on\nresource-constrained devices has accentuated the need for efficient ML\nprocessing on low-cost processors. While traditional CPUs provide programming\nflexibility, their general-purpose architecture often lacks the throughput\nrequired for complex ML models. The augmentation of a RISC-V processor with a\nvector unit can provide substantial data-level parallelism. However, increasing\nthe data-level parallelism supported by vector processing would make the Vector\nRegister File (VRF) a major area consumer in ultra low-cost processors, since\n32 vector registers are required for RISC-V Vector ISA compliance. This work\nleverages the insight that many ML vectorized kernels require a small number of\nactive vector registers, and proposes the use of a physically smaller VRF that\ndynamically caches only the vector registers currently accessed by the\napplication. This approach, called Register Dispersion, maps the architectural\nvector registers to a smaller set of physical registers. The proposed\nISA-compliant VRF is significantly smaller than a full-size VRF and operates\nlike a conventional cache, i.e., it only stores the most recently accessed\nvector registers. Essential registers remain readily accessible within the\ncompact VRF, while the others are offloaded to the cache/memory sub-system. The\ncompact VRF design is demonstrated to yield substantial area and power savings,\nas compared to using a full VRF, with no or minimal impact on performance. This\neffective trade-off renders the inclusion of vector units in low-cost\nprocessors feasible and practical.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of Machine Learning (ML) applications at the edge on\nresource-constrained devices has accentuated the need for efficient ML\nprocessing on low-cost processors. While traditional CPUs provide programming\nflexibility, their general-purpose architecture often lacks the throughput\nrequired for complex ML models. The augmentation of a RISC-V processor with a\nvector unit can provide substantial data-level parallelism. However, increasing\nthe data-level parallelism supported by vector processing would make the Vector\nRegister File (VRF) a major area consumer in ultra low-cost processors, since\n32 vector registers are required for RISC-V Vector ISA compliance. This work\nleverages the insight that many ML vectorized kernels require a small number of\nactive vector registers, and proposes the use of a physically smaller VRF that\ndynamically caches only the vector registers currently accessed by the\napplication. This approach, called Register Dispersion, maps the architectural\nvector registers to a smaller set of physical registers. The proposed\nISA-compliant VRF is significantly smaller than a full-size VRF and operates\nlike a conventional cache, i.e., it only stores the most recently accessed\nvector registers. Essential registers remain readily accessible within the\ncompact VRF, while the others are offloaded to the cache/memory sub-system. The\ncompact VRF design is demonstrated to yield substantial area and power savings,\nas compared to using a full VRF, with no or minimal impact on performance. This\neffective trade-off renders the inclusion of vector units in low-cost\nprocessors feasible and practical."
                },
                "authors": [
                    {
                        "name": "Vasileios Titopoulos"
                    },
                    {
                        "name": "George Alexakis"
                    },
                    {
                        "name": "Kosmas Alexandridis"
                    },
                    {
                        "name": "Chrysostomos Nicopoulos"
                    },
                    {
                        "name": "Giorgos Dimitrakopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Giorgos Dimitrakopoulos"
                },
                "author": "Giorgos Dimitrakopoulos",
                "arxiv_comment": "22nd ACM International Conference on Computing Frontiers (CF' 25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17333v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17333v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17038v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17038v2",
                "updated": "2025-03-24T07:29:28Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    7,
                    29,
                    28,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-21T10:48:35Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    10,
                    48,
                    35,
                    4,
                    80,
                    0
                ],
                "title": "Arm DynamIQ Shared Unit and Real-Time: An Empirical Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Arm DynamIQ Shared Unit and Real-Time: An Empirical Evaluation"
                },
                "summary": "The increasing complexity of embedded hardware platforms poses significant\nchallenges for real-time workloads. Architectural features such as Intel RDT,\nArm QoS, and Arm MPAM are either unavailable on commercial embedded platforms\nor designed primarily for server environments optimized for average-case\nperformance and might fail to deliver the expected real-time guarantees. Arm\nDynamIQ Shared Unit (DSU) includes isolation features-among others, hardware\nper-way cache partitioning-that can improve the real-time guarantees of complex\nembedded multicore systems and facilitate real-time analysis. However, the DSU\nalso targets average cases, and its real-time capabilities have not yet been\nevaluated. This paper presents the first comprehensive analysis of three\nreal-world deployments of the Arm DSU on Rockchip RK3568, Rockchip RK3588, and\nNVIDIA Orin platforms. We integrate support for the DSU at the operating system\nand hypervisor level and conduct a large-scale evaluation using both synthetic\nand real-world benchmarks with varying types and intensities of interference.\nOur results make extensive use of performance counters and indicate that,\nalthough effective, the quality of partitioning and isolation provided by the\nDSU depends on the type and the intensity of the interfering workloads. In\naddition, we uncover and analyze in detail the correlation between benchmarks\nand different types and intensities of interference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing complexity of embedded hardware platforms poses significant\nchallenges for real-time workloads. Architectural features such as Intel RDT,\nArm QoS, and Arm MPAM are either unavailable on commercial embedded platforms\nor designed primarily for server environments optimized for average-case\nperformance and might fail to deliver the expected real-time guarantees. Arm\nDynamIQ Shared Unit (DSU) includes isolation features-among others, hardware\nper-way cache partitioning-that can improve the real-time guarantees of complex\nembedded multicore systems and facilitate real-time analysis. However, the DSU\nalso targets average cases, and its real-time capabilities have not yet been\nevaluated. This paper presents the first comprehensive analysis of three\nreal-world deployments of the Arm DSU on Rockchip RK3568, Rockchip RK3588, and\nNVIDIA Orin platforms. We integrate support for the DSU at the operating system\nand hypervisor level and conduct a large-scale evaluation using both synthetic\nand real-world benchmarks with varying types and intensities of interference.\nOur results make extensive use of performance counters and indicate that,\nalthough effective, the quality of partitioning and isolation provided by the\nDSU depends on the type and the intensity of the interfering workloads. In\naddition, we uncover and analyze in detail the correlation between benchmarks\nand different types and intensities of interference."
                },
                "authors": [
                    {
                        "name": "Ashutosh Pradhan"
                    },
                    {
                        "name": "Daniele Ottaviano"
                    },
                    {
                        "name": "Yi Jiang"
                    },
                    {
                        "name": "Haozheng Huang"
                    },
                    {
                        "name": "Alexander Zuepke"
                    },
                    {
                        "name": "Andrea Bastoni"
                    },
                    {
                        "name": "Marco Caccamo"
                    }
                ],
                "author_detail": {
                    "name": "Marco Caccamo"
                },
                "author": "Marco Caccamo",
                "arxiv_comment": "Accepted for publication in the Proceedings of the 31st IEEE\n  Real-Time and Embedded Technology and Applications Symposium (RTAS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17038v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17038v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68M20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.3; C.4; D.4.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18334v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18334v1",
                "updated": "2025-03-24T04:32:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    4,
                    32,
                    35,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T04:32:35Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    4,
                    32,
                    35,
                    0,
                    83,
                    0
                ],
                "title": "Mitigating Cache Noise in Test-Time Adaptation for Large Vision-Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Cache Noise in Test-Time Adaptation for Large Vision-Language\n  Models"
                },
                "summary": "Test-time adaptation (TTA) of visual language models has recently attracted\nsignificant attention as a solution to the performance degradation caused by\ndistribution shifts in downstream tasks. However, existing cache-based TTA\nmethods have certain limitations. They mainly rely on the accuracy of cached\nfeature labels, and the presence of noisy pseudo-labels can cause these\nfeatures to deviate from their true distribution. This makes cache retrieval\nmethods based on similarity matching highly sensitive to outliers or extreme\nsamples. Moreover, current methods lack effective mechanisms to model class\ndistributions, which limits their ability to fully exploit the potential of\ncached information. To address these challenges, we introduce a comprehensive\nand reliable caching mechanism and propose a novel zero-shot TTA method called\n``Cache, Residual, Gaussian\" (CRG). This method not only employs learnable\nresidual parameters to better align positive and negative visual prototypes\nwith text prototypes, thereby optimizing the quality of cached features, but\nalso incorporates Gaussian Discriminant Analysis (GDA) to dynamically model\nintra-class feature distributions, further mitigating the impact of noisy\nfeatures. Experimental results on 13 benchmarks demonstrate that CRG\noutperforms state-of-the-art TTA methods, showcasing exceptional robustness and\nadaptability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time adaptation (TTA) of visual language models has recently attracted\nsignificant attention as a solution to the performance degradation caused by\ndistribution shifts in downstream tasks. However, existing cache-based TTA\nmethods have certain limitations. They mainly rely on the accuracy of cached\nfeature labels, and the presence of noisy pseudo-labels can cause these\nfeatures to deviate from their true distribution. This makes cache retrieval\nmethods based on similarity matching highly sensitive to outliers or extreme\nsamples. Moreover, current methods lack effective mechanisms to model class\ndistributions, which limits their ability to fully exploit the potential of\ncached information. To address these challenges, we introduce a comprehensive\nand reliable caching mechanism and propose a novel zero-shot TTA method called\n``Cache, Residual, Gaussian\" (CRG). This method not only employs learnable\nresidual parameters to better align positive and negative visual prototypes\nwith text prototypes, thereby optimizing the quality of cached features, but\nalso incorporates Gaussian Discriminant Analysis (GDA) to dynamically model\nintra-class feature distributions, further mitigating the impact of noisy\nfeatures. Experimental results on 13 benchmarks demonstrate that CRG\noutperforms state-of-the-art TTA methods, showcasing exceptional robustness and\nadaptability."
                },
                "authors": [
                    {
                        "name": "Haotian Zhai"
                    },
                    {
                        "name": "Xinyu Chen"
                    },
                    {
                        "name": "Can Zhang"
                    },
                    {
                        "name": "Tianming Sha"
                    },
                    {
                        "name": "Ruirui Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruirui Li"
                },
                "author": "Ruirui Li",
                "arxiv_comment": "Accepted by ICME 2025 and ICLR 2025 Workshop on Foundation Models in\n  the Wild",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18334v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18334v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16653v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16653v2",
                "updated": "2025-03-24T03:18:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    3,
                    18,
                    49,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-20T19:10:37Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    19,
                    10,
                    37,
                    3,
                    79,
                    0
                ],
                "title": "iFlame: Interleaving Full and Linear Attention for Efficient Mesh\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "iFlame: Interleaving Full and Linear Attention for Efficient Mesh\n  Generation"
                },
                "summary": "This paper propose iFlame, a novel transformer-based network architecture for\nmesh generation. While attention-based models have demonstrated remarkable\nperformance in mesh generation, their quadratic computational complexity limits\nscalability, particularly for high-resolution 3D data. Conversely, linear\nattention mechanisms offer lower computational costs but often struggle to\ncapture long-range dependencies, resulting in suboptimal outcomes. To address\nthis trade-off, we propose an interleaving autoregressive mesh generation\nframework that combines the efficiency of linear attention with the expressive\npower of full attention mechanisms. To further enhance efficiency and leverage\nthe inherent structure of mesh representations, we integrate this interleaving\napproach into an hourglass architecture, which significantly boosts efficiency.\nOur approach reduces training time while achieving performance comparable to\npure attention-based models. To improve inference efficiency, we implemented a\ncaching algorithm that almost doubles the speed and reduces the KV cache size\nby seven-eighths compared to the original Transformer. We evaluate our\nframework on ShapeNet and Objaverse, demonstrating its ability to generate\nhigh-quality 3D meshes efficiently. Our results indicate that the proposed\ninterleaving framework effectively balances computational efficiency and\ngenerative performance, making it a practical solution for mesh generation. The\ntraining takes only 2 days with 4 GPUs on 39k data with a maximum of 4k faces\non Objaverse.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper propose iFlame, a novel transformer-based network architecture for\nmesh generation. While attention-based models have demonstrated remarkable\nperformance in mesh generation, their quadratic computational complexity limits\nscalability, particularly for high-resolution 3D data. Conversely, linear\nattention mechanisms offer lower computational costs but often struggle to\ncapture long-range dependencies, resulting in suboptimal outcomes. To address\nthis trade-off, we propose an interleaving autoregressive mesh generation\nframework that combines the efficiency of linear attention with the expressive\npower of full attention mechanisms. To further enhance efficiency and leverage\nthe inherent structure of mesh representations, we integrate this interleaving\napproach into an hourglass architecture, which significantly boosts efficiency.\nOur approach reduces training time while achieving performance comparable to\npure attention-based models. To improve inference efficiency, we implemented a\ncaching algorithm that almost doubles the speed and reduces the KV cache size\nby seven-eighths compared to the original Transformer. We evaluate our\nframework on ShapeNet and Objaverse, demonstrating its ability to generate\nhigh-quality 3D meshes efficiently. Our results indicate that the proposed\ninterleaving framework effectively balances computational efficiency and\ngenerative performance, making it a practical solution for mesh generation. The\ntraining takes only 2 days with 4 GPUs on 39k data with a maximum of 4k faces\non Objaverse."
                },
                "authors": [
                    {
                        "name": "Hanxiao Wang"
                    },
                    {
                        "name": "Biao Zhang"
                    },
                    {
                        "name": "Weize Quan"
                    },
                    {
                        "name": "Dong-Ming Yan"
                    },
                    {
                        "name": "Peter Wonka"
                    }
                ],
                "author_detail": {
                    "name": "Peter Wonka"
                },
                "author": "Peter Wonka",
                "arxiv_comment": "Project website: https://wanghanxiao123.github.io/iFa/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16653v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16653v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18292v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18292v1",
                "updated": "2025-03-24T02:28:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    2,
                    28,
                    4,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T02:28:04Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    2,
                    28,
                    4,
                    0,
                    83,
                    0
                ],
                "title": "Jenga: Effective Memory Management for Serving LLM with Heterogeneity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jenga: Effective Memory Management for Serving LLM with Heterogeneity"
                },
                "summary": "Large language models (LLMs) are widely used but expensive to run, especially\nas inference workloads grow. To lower costs, maximizing the request batch size\nby managing GPU memory efficiently is crucial. While PagedAttention has\nrecently been proposed to improve the efficiency of memory management, we find\nthat the growing heterogeneity in the embeddings dimensions, attention, and\naccess patterns of modern LLM architectures introduces new challenges for\nmemory allocation.\n  In this paper, we present Jenga, a novel memory allocation framework for\nheterogeneous embeddings in LLMs. Jenga tackles two key challenges: (1)\nminimizing memory fragmentation when managing embeddings of different sizes,\nand (2) enabling flexible caching and eviction policies tailored to the\nspecific token-dependency patterns of various layers. Jenga employs a two-level\nmemory allocator, leveraging the least common multiple (LCM) of embedding sizes\nto optimize memory usage and providing APIs to express layer-specific caching\nlogic to enhance memory reuse.\n  We implemente Jenga on vLLM, a state-of-the-art LLM inference engine, and\nevaluate it with diverse LLMs, datasets, and GPU configurations. Evaluations\nshow that Jenga improves GPU memory utilization by up to 79.6%, and increases\nserving throughput by up to 4.92x (1.80x on average).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are widely used but expensive to run, especially\nas inference workloads grow. To lower costs, maximizing the request batch size\nby managing GPU memory efficiently is crucial. While PagedAttention has\nrecently been proposed to improve the efficiency of memory management, we find\nthat the growing heterogeneity in the embeddings dimensions, attention, and\naccess patterns of modern LLM architectures introduces new challenges for\nmemory allocation.\n  In this paper, we present Jenga, a novel memory allocation framework for\nheterogeneous embeddings in LLMs. Jenga tackles two key challenges: (1)\nminimizing memory fragmentation when managing embeddings of different sizes,\nand (2) enabling flexible caching and eviction policies tailored to the\nspecific token-dependency patterns of various layers. Jenga employs a two-level\nmemory allocator, leveraging the least common multiple (LCM) of embedding sizes\nto optimize memory usage and providing APIs to express layer-specific caching\nlogic to enhance memory reuse.\n  We implemente Jenga on vLLM, a state-of-the-art LLM inference engine, and\nevaluate it with diverse LLMs, datasets, and GPU configurations. Evaluations\nshow that Jenga improves GPU memory utilization by up to 79.6%, and increases\nserving throughput by up to 4.92x (1.80x on average)."
                },
                "authors": [
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Woosuk Kwon"
                    },
                    {
                        "name": "Xiangxi Mo"
                    },
                    {
                        "name": "Yufeng Wang"
                    },
                    {
                        "name": "Xiaoxuan Liu"
                    },
                    {
                        "name": "Kaichao You"
                    },
                    {
                        "name": "Zhuohan Li"
                    },
                    {
                        "name": "Mingsheng Long"
                    },
                    {
                        "name": "Jidong Zhai"
                    },
                    {
                        "name": "Joseph Gonzalez"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "arxiv_comment": "16 pages, 19 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18292v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18292v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20504v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20504v5",
                "updated": "2025-03-24T02:17:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    2,
                    17,
                    34,
                    0,
                    83,
                    0
                ],
                "published": "2024-12-29T15:42:24Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    15,
                    42,
                    24,
                    6,
                    364,
                    0
                ],
                "title": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding"
                },
                "summary": "Video Large Language Models (VideoLLMs) have made significant strides in\nvideo understanding but struggle with long videos due to the limitations of\ntheir backbone LLMs. Existing solutions rely on length extrapolation, which is\nmemory-constrained, or visual token compression, which primarily leverages\nlow-level temporal redundancy while overlooking the more effective high-level\nknowledge redundancy. To address this, we propose $\\textbf{ReTaKe}$, a\ntraining-free method with two novel modules DPSelect and PivotKV, to jointly\nreduce both temporal visual redundancy and knowledge redundancy for video\ncompression. To align with the way of human temporal perception, DPSelect\nidentifies keyframes based on inter-frame distance peaks. To leverage LLMs'\nlearned prior knowledge, PivotKV marks the keyframes as pivots and compress\nnon-pivot frames by pruning low-attention tokens in their KV cache. ReTaKe\nenables VideoLLMs to process 8 times longer frames (up to 2048), outperforming\nsimilar-sized models by 3-5% and even rivaling much larger ones on VideoMME,\nMLVU, LongVideoBench, and LVBench. Moreover, by overlapping compression\noperations with prefilling, ReTaKe introduces only ~10% prefilling latency\noverhead while reducing decoding latency by ~20%. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Large Language Models (VideoLLMs) have made significant strides in\nvideo understanding but struggle with long videos due to the limitations of\ntheir backbone LLMs. Existing solutions rely on length extrapolation, which is\nmemory-constrained, or visual token compression, which primarily leverages\nlow-level temporal redundancy while overlooking the more effective high-level\nknowledge redundancy. To address this, we propose $\\textbf{ReTaKe}$, a\ntraining-free method with two novel modules DPSelect and PivotKV, to jointly\nreduce both temporal visual redundancy and knowledge redundancy for video\ncompression. To align with the way of human temporal perception, DPSelect\nidentifies keyframes based on inter-frame distance peaks. To leverage LLMs'\nlearned prior knowledge, PivotKV marks the keyframes as pivots and compress\nnon-pivot frames by pruning low-attention tokens in their KV cache. ReTaKe\nenables VideoLLMs to process 8 times longer frames (up to 2048), outperforming\nsimilar-sized models by 3-5% and even rivaling much larger ones on VideoMME,\nMLVU, LongVideoBench, and LVBench. Moreover, by overlapping compression\noperations with prefilling, ReTaKe introduces only ~10% prefilling latency\noverhead while reducing decoding latency by ~20%. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe."
                },
                "authors": [
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Qingyi Si"
                    },
                    {
                        "name": "Jianlong Wu"
                    },
                    {
                        "name": "Shiyu Zhu"
                    },
                    {
                        "name": "Li Cao"
                    },
                    {
                        "name": "Liqiang Nie"
                    }
                ],
                "author_detail": {
                    "name": "Liqiang Nie"
                },
                "author": "Liqiang Nie",
                "arxiv_comment": "Rewrite the methods section. Add more ablation studies and results in\n  LongVideoBench. Update metadata",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20504v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20504v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18278v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18278v1",
                "updated": "2025-03-24T01:47:26Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    1,
                    47,
                    26,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T01:47:26Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    1,
                    47,
                    26,
                    0,
                    83,
                    0
                ],
                "title": "TopV: Compatible Token Pruning with Inference Time Optimization for Fast\n  and Low-Memory Multimodal Vision Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TopV: Compatible Token Pruning with Inference Time Optimization for Fast\n  and Low-Memory Multimodal Vision Language Model"
                },
                "summary": "Vision-Language Models (VLMs) demand substantial computational resources\nduring inference, largely due to the extensive visual input tokens for\nrepresenting visual information. Previous studies have noted that visual tokens\ntend to receive less attention than text tokens, suggesting their lower\nimportance during inference and potential for pruning. However, their methods\nencounter several challenges: reliance on greedy heuristic criteria for token\nimportance and incompatibility with FlashAttention and KV cache. To address\nthese issues, we introduce \\textbf{TopV}, a compatible \\textbf{TO}ken\n\\textbf{P}runing with inference Time Optimization for fast and low-memory\n\\textbf{V}LM, achieving efficient pruning without additional training or\nfine-tuning. Instead of relying on attention scores, we formulate token pruning\nas an optimization problem, accurately identifying important visual tokens\nwhile remaining compatible with FlashAttention. Additionally, since we only\nperform this pruning once during the prefilling stage, it effectively reduces\nKV cache size. Our optimization framework incorporates a visual-aware cost\nfunction considering factors such as Feature Similarity, Relative Spatial\nDistance, and Absolute Central Distance, to measure the importance of each\nsource visual token, enabling effective pruning of low-importance tokens.\nExtensive experiments demonstrate that our method outperforms previous token\npruning methods, validating the effectiveness and efficiency of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) demand substantial computational resources\nduring inference, largely due to the extensive visual input tokens for\nrepresenting visual information. Previous studies have noted that visual tokens\ntend to receive less attention than text tokens, suggesting their lower\nimportance during inference and potential for pruning. However, their methods\nencounter several challenges: reliance on greedy heuristic criteria for token\nimportance and incompatibility with FlashAttention and KV cache. To address\nthese issues, we introduce \\textbf{TopV}, a compatible \\textbf{TO}ken\n\\textbf{P}runing with inference Time Optimization for fast and low-memory\n\\textbf{V}LM, achieving efficient pruning without additional training or\nfine-tuning. Instead of relying on attention scores, we formulate token pruning\nas an optimization problem, accurately identifying important visual tokens\nwhile remaining compatible with FlashAttention. Additionally, since we only\nperform this pruning once during the prefilling stage, it effectively reduces\nKV cache size. Our optimization framework incorporates a visual-aware cost\nfunction considering factors such as Feature Similarity, Relative Spatial\nDistance, and Absolute Central Distance, to measure the importance of each\nsource visual token, enabling effective pruning of low-importance tokens.\nExtensive experiments demonstrate that our method outperforms previous token\npruning methods, validating the effectiveness and efficiency of our approach."
                },
                "authors": [
                    {
                        "name": "Cheng Yang"
                    },
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Jinqi Xiao"
                    },
                    {
                        "name": "Lingyi Huang"
                    },
                    {
                        "name": "Yu Gong"
                    },
                    {
                        "name": "Chendi Li"
                    },
                    {
                        "name": "Jinghua Yan"
                    },
                    {
                        "name": "Yu Bai"
                    },
                    {
                        "name": "Ponnuswamy Sadayappan"
                    },
                    {
                        "name": "Xia Hu"
                    },
                    {
                        "name": "Bo Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Bo Yuan"
                },
                "author": "Bo Yuan",
                "arxiv_comment": "Accepted by CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18278v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18278v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18265v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18265v1",
                "updated": "2025-03-24T01:15:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    1,
                    15,
                    43,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T01:15:43Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    1,
                    15,
                    43,
                    0,
                    83,
                    0
                ],
                "title": "Risk Management for Distributed Arbitrage Systems: Integrating\n  Artificial Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Risk Management for Distributed Arbitrage Systems: Integrating\n  Artificial Intelligence"
                },
                "summary": "Effective risk management solutions become absolutely crucial when financial\nmarkets embrace distributed technology and decentralized financing (DeFi). This\nstudy offers a thorough survey and comparative analysis of the integration of\nartificial intelligence (AI) in risk management for distributed arbitrage\nsystems. We examine several modern caching techniques namely in memory caching,\ndistributed caching, and proxy caching and their functions in enhancing\nperformance in decentralized settings. Through literature review we examine the\nutilization of AI techniques for alleviating risks related to market\nvolatility, liquidity challenges, operational failures, regulatory compliance,\nand security threats. This comparison research evaluates various case studies\nfrom prominent DeFi technologies, emphasizing critical performance metrics like\nlatency reduction, load balancing, and system resilience. Additionally, we\nexamine the problems and trade offs associated with these technologies,\nemphasizing their effects on consistency, scalability, and fault tolerance. By\nmeticulously analyzing real world applications, specifically centering on the\nAave platform as our principal case study, we illustrate how the purposeful\namalgamation of AI with contemporary caching methodologies has revolutionized\nrisk management in distributed arbitrage systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective risk management solutions become absolutely crucial when financial\nmarkets embrace distributed technology and decentralized financing (DeFi). This\nstudy offers a thorough survey and comparative analysis of the integration of\nartificial intelligence (AI) in risk management for distributed arbitrage\nsystems. We examine several modern caching techniques namely in memory caching,\ndistributed caching, and proxy caching and their functions in enhancing\nperformance in decentralized settings. Through literature review we examine the\nutilization of AI techniques for alleviating risks related to market\nvolatility, liquidity challenges, operational failures, regulatory compliance,\nand security threats. This comparison research evaluates various case studies\nfrom prominent DeFi technologies, emphasizing critical performance metrics like\nlatency reduction, load balancing, and system resilience. Additionally, we\nexamine the problems and trade offs associated with these technologies,\nemphasizing their effects on consistency, scalability, and fault tolerance. By\nmeticulously analyzing real world applications, specifically centering on the\nAave platform as our principal case study, we illustrate how the purposeful\namalgamation of AI with contemporary caching methodologies has revolutionized\nrisk management in distributed arbitrage systems."
                },
                "authors": [
                    {
                        "name": "Akaash Vishal Hazarika"
                    },
                    {
                        "name": "Mahak Shah"
                    },
                    {
                        "name": "Swapnil Patil"
                    },
                    {
                        "name": "Pradyumna Shukla"
                    }
                ],
                "author_detail": {
                    "name": "Pradyumna Shukla"
                },
                "author": "Pradyumna Shukla",
                "arxiv_comment": "International Conference on AI and Financial Innovation AIFI-2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18265v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18265v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11; G.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18191v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18191v1",
                "updated": "2025-03-23T20:18:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    23,
                    20,
                    18,
                    16,
                    6,
                    82,
                    0
                ],
                "published": "2025-03-23T20:18:16Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    20,
                    18,
                    16,
                    6,
                    82,
                    0
                ],
                "title": "Enabling the Write-Back Page Cache with Strong Consistency in\n  Distributed Userspace File Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling the Write-Back Page Cache with Strong Consistency in\n  Distributed Userspace File Systems"
                },
                "summary": "The large-scale, multi-tenant nature of cloud computing requires distributed\nfile systems that offer stability, adaptability, and compatibility. FUSE-based\ndistributed file systems have emerged as a popular solution for the cloud,\noffering fast deployment, fault isolation, and POSIX compliance. However,\nFUSE's performance limitations, particularly its inability to reconcile page\ncaching with strong consistency in distributed environments, remain a\npersistent problem. Existing approaches either sacrifice consistency for\nperformance or rely on inefficient caching, limiting their practicality.\n  To this end, we present DistFUSE, the first FUSE-based distributed file\nsystem that relies on a write-back kernel-based page cache for performance and\nprovides strong consistency. DistFUSE achieves this by offloading userspace\nlock management to the kernel driver, allowing coordinated access to the\nkernel's page cache across nodes. This design eliminates blind local cache\nupdates and ensures cluster-wide consistency without compromising performance.\nOur evaluation shows DistFUSE improves throughput by up to 75% compared to\nbaseline approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The large-scale, multi-tenant nature of cloud computing requires distributed\nfile systems that offer stability, adaptability, and compatibility. FUSE-based\ndistributed file systems have emerged as a popular solution for the cloud,\noffering fast deployment, fault isolation, and POSIX compliance. However,\nFUSE's performance limitations, particularly its inability to reconcile page\ncaching with strong consistency in distributed environments, remain a\npersistent problem. Existing approaches either sacrifice consistency for\nperformance or rely on inefficient caching, limiting their practicality.\n  To this end, we present DistFUSE, the first FUSE-based distributed file\nsystem that relies on a write-back kernel-based page cache for performance and\nprovides strong consistency. DistFUSE achieves this by offloading userspace\nlock management to the kernel driver, allowing coordinated access to the\nkernel's page cache across nodes. This design eliminates blind local cache\nupdates and ensures cluster-wide consistency without compromising performance.\nOur evaluation shows DistFUSE improves throughput by up to 75% compared to\nbaseline approaches."
                },
                "authors": [
                    {
                        "name": "Haoyu Li"
                    },
                    {
                        "name": "Jingkai Fu"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Windsor Hsu"
                    },
                    {
                        "name": "Asaf Cidon"
                    }
                ],
                "author_detail": {
                    "name": "Asaf Cidon"
                },
                "author": "Asaf Cidon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18191v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18191v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18030v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18030v1",
                "updated": "2025-03-23T11:07:24Z",
                "updated_parsed": [
                    2025,
                    3,
                    23,
                    11,
                    7,
                    24,
                    6,
                    82,
                    0
                ],
                "published": "2025-03-23T11:07:24Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    11,
                    7,
                    24,
                    6,
                    82,
                    0
                ],
                "title": "Formal Verification of Parameterized Systems based on Induction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formal Verification of Parameterized Systems based on Induction"
                },
                "summary": "Parameterized systems play a crucial role in the computer field, and their\nsecurity is of great significance. Formal verification of parameterized\nprotocols is especially challenging due to its \"parameterized\" feature, which\nbrings complexity and undecidability. Existing automated parameterized\nverification methods have limitations, such as facing difficulties in\nautomatically deriving parameterized invariants constrained by mixed Forall and\nExists quantifiers, or having challenges in completing the parameterized\nverification of large and complex protocols. This paper proposes a formal\nverification framework for parameterized systems based on induction, named\nwiseParaverifier. It starts from small concretizations of protocols, analyzes\ninductive counterexamples, and constructs counterexample formulas to guide the\nentire process of parameterized verification. It also presents a heuristic\nGeneralize method to quickly find auxiliary invariants, a method for promoting\ncomplex mixed quantifiers and merging parameterized invariants, and uses\nsymmetric reduction ideas to accelerate the verification process. Experimental\nresults show that wiseParaverifier can successfully complete automatic\ninductive verification on 7 cache coherence protocols and 10 distributed\nprotocols. It has strong verification capabilities and migration capabilities,\nand can provide concise and readable verification results, which is helpful for\nlearners to understand protocol behaviors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameterized systems play a crucial role in the computer field, and their\nsecurity is of great significance. Formal verification of parameterized\nprotocols is especially challenging due to its \"parameterized\" feature, which\nbrings complexity and undecidability. Existing automated parameterized\nverification methods have limitations, such as facing difficulties in\nautomatically deriving parameterized invariants constrained by mixed Forall and\nExists quantifiers, or having challenges in completing the parameterized\nverification of large and complex protocols. This paper proposes a formal\nverification framework for parameterized systems based on induction, named\nwiseParaverifier. It starts from small concretizations of protocols, analyzes\ninductive counterexamples, and constructs counterexample formulas to guide the\nentire process of parameterized verification. It also presents a heuristic\nGeneralize method to quickly find auxiliary invariants, a method for promoting\ncomplex mixed quantifiers and merging parameterized invariants, and uses\nsymmetric reduction ideas to accelerate the verification process. Experimental\nresults show that wiseParaverifier can successfully complete automatic\ninductive verification on 7 cache coherence protocols and 10 distributed\nprotocols. It has strong verification capabilities and migration capabilities,\nand can provide concise and readable verification results, which is helpful for\nlearners to understand protocol behaviors."
                },
                "authors": [
                    {
                        "name": "Jiaqi Xiu"
                    },
                    {
                        "name": "Yongjian Li"
                    }
                ],
                "author_detail": {
                    "name": "Yongjian Li"
                },
                "author": "Yongjian Li",
                "arxiv_comment": "9 pages,2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18030v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18030v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.10425v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.10425v2",
                "updated": "2025-03-23T06:14:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    23,
                    6,
                    14,
                    35,
                    6,
                    82,
                    0
                ],
                "published": "2023-12-16T11:40:49Z",
                "published_parsed": [
                    2023,
                    12,
                    16,
                    11,
                    40,
                    49,
                    5,
                    350,
                    0
                ],
                "title": "Knowledge Rumination for Client Utility Evaluation in Heterogeneous\n  Federated Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Rumination for Client Utility Evaluation in Heterogeneous\n  Federated Learning"
                },
                "summary": "Federated Learning (FL) allows several clients to cooperatively train machine\nlearning models without disclosing the raw data. In practical applications,\nasynchronous FL (AFL) can address the straggler effect compared to synchronous\nFL. However, Non-IID data and stale models pose significant challenges to AFL,\nas they can diminish the practicality of the global model and even lead to\ntraining failures. In this work, we propose a novel AFL framework called\nFederated Historical Learning (FedHist), which effectively addresses the\nchallenges posed by both Non-IID data and gradient staleness based on the\nconcept of knowledge rumination. FedHist enhances the stability of local\ngradients by performing weighted fusion with historical global gradients cached\non the server. Relying on hindsight, it assigns aggregation weights to each\nparticipant in a multi-dimensional manner during each communication round. To\nfurther enhance the efficiency and stability of the training process, we\nintroduce an intelligent $\\ell_2$-norm amplification scheme, which dynamically\nregulates the learning progress based on the $\\ell_2$-norms of the submitted\ngradients. Extensive experiments indicate FedHist outperforms state-of-the-art\nmethods in terms of convergence performance and test accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) allows several clients to cooperatively train machine\nlearning models without disclosing the raw data. In practical applications,\nasynchronous FL (AFL) can address the straggler effect compared to synchronous\nFL. However, Non-IID data and stale models pose significant challenges to AFL,\nas they can diminish the practicality of the global model and even lead to\ntraining failures. In this work, we propose a novel AFL framework called\nFederated Historical Learning (FedHist), which effectively addresses the\nchallenges posed by both Non-IID data and gradient staleness based on the\nconcept of knowledge rumination. FedHist enhances the stability of local\ngradients by performing weighted fusion with historical global gradients cached\non the server. Relying on hindsight, it assigns aggregation weights to each\nparticipant in a multi-dimensional manner during each communication round. To\nfurther enhance the efficiency and stability of the training process, we\nintroduce an intelligent $\\ell_2$-norm amplification scheme, which dynamically\nregulates the learning progress based on the $\\ell_2$-norms of the submitted\ngradients. Extensive experiments indicate FedHist outperforms state-of-the-art\nmethods in terms of convergence performance and test accuracy."
                },
                "authors": [
                    {
                        "name": "Xiaorui Jiang"
                    },
                    {
                        "name": "Yu Gao"
                    },
                    {
                        "name": "Hengwei Xu"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Yong Liao"
                    },
                    {
                        "name": "Pengyuan Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Pengyuan Zhou"
                },
                "author": "Pengyuan Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.10425v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.10425v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17922v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17922v1",
                "updated": "2025-03-23T03:36:52Z",
                "updated_parsed": [
                    2025,
                    3,
                    23,
                    3,
                    36,
                    52,
                    6,
                    82,
                    0
                ],
                "published": "2025-03-23T03:36:52Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    3,
                    36,
                    52,
                    6,
                    82,
                    0
                ],
                "title": "WindowKV: Task-Adaptive Group-Wise KV Cache Window Selection for\n  Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WindowKV: Task-Adaptive Group-Wise KV Cache Window Selection for\n  Efficient LLM Inference"
                },
                "summary": "With the advancements in long-context inference capabilities of large\nlanguage models (LLMs), the KV cache has become one of the foundational\ncomponents. However, its substantial GPU memory consumption makes KV cache\ncompression a key technique for enabling efficient LLM inference in industrial\nscenarios. While recent studies have focused on optimizing the memory occupied\nby the KV cache, they overlook two critical factors: preserving semantic\ncoherence and considering task-specific characteristic during compression. To\naddress these limitations, we propose a novel task-adaptive KV cache window\nselection method, WindowKV. WindowKV dynamically selects local semantic windows\nconsisting of consecutive tokens, according to task-specific characteristics,\nensuring the retained KV cache captures continuous, essential context.\nAdditionally, we introduce an intra-group layer KV cache indices sharing\nstrategy to reduce computational overhead, achieving a balance between\nperformance and efficiency. We rigorously evaluate WindowKV on the LongBench\nbenchmark, and the results demonstrate that it maintains a performance\ncomparable to full KV cache retention while using only 12% of the original KV\ncache, significantly reducing memory requirements. Furthermore, our method also\nachieves state-of-the-art results in the Needle-in-a-Haystack evaluation,\nhighlighting its effectiveness and robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advancements in long-context inference capabilities of large\nlanguage models (LLMs), the KV cache has become one of the foundational\ncomponents. However, its substantial GPU memory consumption makes KV cache\ncompression a key technique for enabling efficient LLM inference in industrial\nscenarios. While recent studies have focused on optimizing the memory occupied\nby the KV cache, they overlook two critical factors: preserving semantic\ncoherence and considering task-specific characteristic during compression. To\naddress these limitations, we propose a novel task-adaptive KV cache window\nselection method, WindowKV. WindowKV dynamically selects local semantic windows\nconsisting of consecutive tokens, according to task-specific characteristics,\nensuring the retained KV cache captures continuous, essential context.\nAdditionally, we introduce an intra-group layer KV cache indices sharing\nstrategy to reduce computational overhead, achieving a balance between\nperformance and efficiency. We rigorously evaluate WindowKV on the LongBench\nbenchmark, and the results demonstrate that it maintains a performance\ncomparable to full KV cache retention while using only 12% of the original KV\ncache, significantly reducing memory requirements. Furthermore, our method also\nachieves state-of-the-art results in the Needle-in-a-Haystack evaluation,\nhighlighting its effectiveness and robustness."
                },
                "authors": [
                    {
                        "name": "Youhui Zuo"
                    },
                    {
                        "name": "Sibo Wei"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Zhuorui Liu"
                    },
                    {
                        "name": "Wenpeng Lu"
                    },
                    {
                        "name": "Dawei Song"
                    }
                ],
                "author_detail": {
                    "name": "Dawei Song"
                },
                "author": "Dawei Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17922v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17922v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17913v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17913v1",
                "updated": "2025-03-23T03:20:25Z",
                "updated_parsed": [
                    2025,
                    3,
                    23,
                    3,
                    20,
                    25,
                    6,
                    82,
                    0
                ],
                "published": "2025-03-23T03:20:25Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    3,
                    20,
                    25,
                    6,
                    82,
                    0
                ],
                "title": "Cache-Aware Cooperative Multicast Beamforming in Dynamic\n  Satellite-Terrestrial Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-Aware Cooperative Multicast Beamforming in Dynamic\n  Satellite-Terrestrial Networks"
                },
                "summary": "With the burgeoning demand for data-intensive services, satellite-terrestrial\nnetworks (STNs) face increasing backhaul link congestion, deteriorating user\nquality of service (QoS), and escalating power consumption. Cache-aided STNs\nare acknowledged as a promising paradigm for accelerating content delivery to\nusers and alleviating the load of backhaul links. However, the dynamic nature\nof low earth orbit (LEO) satellites and the complex interference among\nsatellite beams and terrestrial base stations pose challenges in effectively\nmanaging limited edge resources. To address these issues, this paper proposes a\nmethod for dynamically scheduling caching and communication resources, aiming\nto reduce network costs in terms of transmission power consumption and backhaul\ntraffic, while meeting user QoS demands and resource constraints. We formulate\na mixed timescale problem to jointly optimize cache placement, LEO satellite\nbeam direction, and cooperative multicast beamforming among satellite beams and\nbase stations. To tackle this intricate problem, we propose a two-stage\nsolution framework, where the primary problem is decoupled into a short-term\ncontent delivery subproblem and a long-term cache placement subproblem. The\nformer subproblem is solved by designing an alternating optimization approach\nwith whale optimization and successive convex approximation methods according\nto the cache placement state, while cache content in STNs is updated using an\niterative algorithm that utilizes historical information. Simulation results\ndemonstrate the effectiveness of our proposed algorithms, showcasing their\nconvergence and significantly reducing transmission power consumption and\nbackhaul traffic by up to 52%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the burgeoning demand for data-intensive services, satellite-terrestrial\nnetworks (STNs) face increasing backhaul link congestion, deteriorating user\nquality of service (QoS), and escalating power consumption. Cache-aided STNs\nare acknowledged as a promising paradigm for accelerating content delivery to\nusers and alleviating the load of backhaul links. However, the dynamic nature\nof low earth orbit (LEO) satellites and the complex interference among\nsatellite beams and terrestrial base stations pose challenges in effectively\nmanaging limited edge resources. To address these issues, this paper proposes a\nmethod for dynamically scheduling caching and communication resources, aiming\nto reduce network costs in terms of transmission power consumption and backhaul\ntraffic, while meeting user QoS demands and resource constraints. We formulate\na mixed timescale problem to jointly optimize cache placement, LEO satellite\nbeam direction, and cooperative multicast beamforming among satellite beams and\nbase stations. To tackle this intricate problem, we propose a two-stage\nsolution framework, where the primary problem is decoupled into a short-term\ncontent delivery subproblem and a long-term cache placement subproblem. The\nformer subproblem is solved by designing an alternating optimization approach\nwith whale optimization and successive convex approximation methods according\nto the cache placement state, while cache content in STNs is updated using an\niterative algorithm that utilizes historical information. Simulation results\ndemonstrate the effectiveness of our proposed algorithms, showcasing their\nconvergence and significantly reducing transmission power consumption and\nbackhaul traffic by up to 52%."
                },
                "authors": [
                    {
                        "name": "Shuo Yuan"
                    },
                    {
                        "name": "Yaohua Sun"
                    },
                    {
                        "name": "Mugen Peng"
                    }
                ],
                "author_detail": {
                    "name": "Mugen Peng"
                },
                "author": "Mugen Peng",
                "arxiv_doi": "10.1109/TVT.2024.3463548",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TVT.2024.3463548",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.17913v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17913v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by IEEE Transactions on Vehicular Technology",
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17911v1",
                "updated": "2025-03-23T03:16:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    23,
                    3,
                    16,
                    50,
                    6,
                    82,
                    0
                ],
                "published": "2025-03-23T03:16:50Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    3,
                    16,
                    50,
                    6,
                    82,
                    0
                ],
                "title": "VSAG: An Optimized Search Framework for Graph-based Approximate Nearest\n  Neighbor Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VSAG: An Optimized Search Framework for Graph-based Approximate Nearest\n  Neighbor Search"
                },
                "summary": "Approximate nearest neighbor search (ANNS) is a fundamental problem in vector\ndatabases and AI infrastructures. Recent graph-based ANNS algorithms have\nachieved high search accuracy with practical efficiency. Despite the\nadvancements, these algorithms still face performance bottlenecks in\nproduction, due to the random memory access patterns of graph-based search and\nthe high computational overheads of vector distance. In addition, the\nperformance of a graph-based ANNS algorithm is highly sensitive to parameters,\nwhile selecting the optimal parameters is cost-prohibitive, e.g., manual tuning\nrequires repeatedly re-building the index.\n  This paper introduces VSAG, an open-source framework that aims to enhance the\nin production performance of graph-based ANNS algorithms. VSAG has been\ndeployed at scale in the services of Ant Group, and it incorporates three key\noptimizations: (i) efficient memory access: it reduces L3 cache misses with\npre-fetching and cache-friendly vector organization; (ii) automated parameter\ntuning: it automatically selects performance-optimal parameters without\nrequiring index rebuilding; (iii) efficient distance computation: it leverages\nmodern hardware, scalar quantization, and smartly switches to low-precision\nrepresentation to dramatically reduce the distance computation costs. We\nevaluate VSAG on real-world datasets. The experimental results show that VSAG\nachieves the state-of-the-art performance and provides up to 4x speedup over\nHNSWlib (an industry-standard library) while ensuring the same accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximate nearest neighbor search (ANNS) is a fundamental problem in vector\ndatabases and AI infrastructures. Recent graph-based ANNS algorithms have\nachieved high search accuracy with practical efficiency. Despite the\nadvancements, these algorithms still face performance bottlenecks in\nproduction, due to the random memory access patterns of graph-based search and\nthe high computational overheads of vector distance. In addition, the\nperformance of a graph-based ANNS algorithm is highly sensitive to parameters,\nwhile selecting the optimal parameters is cost-prohibitive, e.g., manual tuning\nrequires repeatedly re-building the index.\n  This paper introduces VSAG, an open-source framework that aims to enhance the\nin production performance of graph-based ANNS algorithms. VSAG has been\ndeployed at scale in the services of Ant Group, and it incorporates three key\noptimizations: (i) efficient memory access: it reduces L3 cache misses with\npre-fetching and cache-friendly vector organization; (ii) automated parameter\ntuning: it automatically selects performance-optimal parameters without\nrequiring index rebuilding; (iii) efficient distance computation: it leverages\nmodern hardware, scalar quantization, and smartly switches to low-precision\nrepresentation to dramatically reduce the distance computation costs. We\nevaluate VSAG on real-world datasets. The experimental results show that VSAG\nachieves the state-of-the-art performance and provides up to 4x speedup over\nHNSWlib (an industry-standard library) while ensuring the same accuracy."
                },
                "authors": [
                    {
                        "name": "Xiaoyao Zhong"
                    },
                    {
                        "name": "Haotian Li"
                    },
                    {
                        "name": "Jiabao Jin"
                    },
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Deming Chu"
                    },
                    {
                        "name": "Xiangyu Wang"
                    },
                    {
                        "name": "Zhitao Shen"
                    },
                    {
                        "name": "Wei Jia"
                    },
                    {
                        "name": "George Gu"
                    },
                    {
                        "name": "Yi Xie"
                    },
                    {
                        "name": "Xuemin Lin"
                    },
                    {
                        "name": "Heng Tao Shen"
                    },
                    {
                        "name": "Jingkuan Song"
                    },
                    {
                        "name": "Peng Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Peng Cheng"
                },
                "author": "Peng Cheng",
                "arxiv_comment": "16 pages, the report of open-source library VSAG\n  (https://github.com/antgroup/vsag)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17895v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17895v1",
                "updated": "2025-03-23T01:17:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    23,
                    1,
                    17,
                    8,
                    6,
                    82,
                    0
                ],
                "published": "2025-03-23T01:17:08Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    1,
                    17,
                    8,
                    6,
                    82,
                    0
                ],
                "title": "Orientation-Dependent \\b{eta}-Ga2O3 Heterojunction Diode with Atomic\n  Layer Deposition (ALD) Grown NiO",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Orientation-Dependent \\b{eta}-Ga2O3 Heterojunction Diode with Atomic\n  Layer Deposition (ALD) Grown NiO"
                },
                "summary": "This work reports the demonstration of ALD-deposited NiO/\\b{eta}-Ga2O3\nheterojunction diodes (HJDs) on low doped drift layer and highly doped (001) &\n(100) n+ substrates with experimental observation of a parallel-plane junction\nelectric field as high as 7.5 MV/cm, revealing a crystal orientation dependence\nin \\b{eta}-Ga2O3. We use a novel metalorganic precursor\nbis(1,4-di-tert-butyl-1,3-diazadienyl) (nickel Ni(tBu2DAD)2) with ozone (O3) to\ndeposit NiO. The NiO/\\b{eta}-Ga2O3 HJD on 7.7 {\\mu}m-thick HVPE-grown drift\nregion exhibited an on-state current density of ~20 A/cm2 at 5 V, ~10-8 A/cm2\nreverse leakage at low reverse bias(-5 V), and a rectifying ratio(Jon/Joff) of\n~109. The HJD broke down at ~2.2 kV reverse bias, corresponding to a ~3.4 MV/cm\nparallel-plane junction electric field, with a noise floor reverse leakage\n(10-8~10-6 A/cm2, nA) at 80% of the device catastrophic breakdown voltage. The\nNiO/\\b{eta}-Ga2O3 HJDs on n+ (001) & (100) highly-doped substrates exhibited\nbreakdown voltages at 12.5-16.0 V and 28.5-70.5 V, respectively, with extracted\ncritical electric field (EC) at 2.30-2.76 MV/cm, and 4.33-7.50 MV/cm, revealing\na substrate crystal orientation dependence on breakdown electric field for\n\\b{eta}-Ga2O3. The 7.5 MV/cm EC reported here is one of the highest\nparallel-plane junction electric fields reported in literature.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work reports the demonstration of ALD-deposited NiO/\\b{eta}-Ga2O3\nheterojunction diodes (HJDs) on low doped drift layer and highly doped (001) &\n(100) n+ substrates with experimental observation of a parallel-plane junction\nelectric field as high as 7.5 MV/cm, revealing a crystal orientation dependence\nin \\b{eta}-Ga2O3. We use a novel metalorganic precursor\nbis(1,4-di-tert-butyl-1,3-diazadienyl) (nickel Ni(tBu2DAD)2) with ozone (O3) to\ndeposit NiO. The NiO/\\b{eta}-Ga2O3 HJD on 7.7 {\\mu}m-thick HVPE-grown drift\nregion exhibited an on-state current density of ~20 A/cm2 at 5 V, ~10-8 A/cm2\nreverse leakage at low reverse bias(-5 V), and a rectifying ratio(Jon/Joff) of\n~109. The HJD broke down at ~2.2 kV reverse bias, corresponding to a ~3.4 MV/cm\nparallel-plane junction electric field, with a noise floor reverse leakage\n(10-8~10-6 A/cm2, nA) at 80% of the device catastrophic breakdown voltage. The\nNiO/\\b{eta}-Ga2O3 HJDs on n+ (001) & (100) highly-doped substrates exhibited\nbreakdown voltages at 12.5-16.0 V and 28.5-70.5 V, respectively, with extracted\ncritical electric field (EC) at 2.30-2.76 MV/cm, and 4.33-7.50 MV/cm, revealing\na substrate crystal orientation dependence on breakdown electric field for\n\\b{eta}-Ga2O3. The 7.5 MV/cm EC reported here is one of the highest\nparallel-plane junction electric fields reported in literature."
                },
                "authors": [
                    {
                        "name": "Yizheng Liu"
                    },
                    {
                        "name": "Shane M. W. Witsell"
                    },
                    {
                        "name": "John F. Conley"
                    },
                    {
                        "name": "Sriram Krishnamoorthy"
                    }
                ],
                "author_detail": {
                    "name": "Sriram Krishnamoorthy"
                },
                "author": "Sriram Krishnamoorthy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17895v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17895v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17603v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17603v1",
                "updated": "2025-03-22T01:17:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    22,
                    1,
                    17,
                    56,
                    5,
                    81,
                    0
                ],
                "published": "2025-03-22T01:17:56Z",
                "published_parsed": [
                    2025,
                    3,
                    22,
                    1,
                    17,
                    56,
                    5,
                    81,
                    0
                ],
                "title": "A Generative Caching System for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Generative Caching System for Large Language Models"
                },
                "summary": "Caching has the potential to be of significant benefit for accessing large\nlanguage models (LLMs) due to their high latencies which typically range from a\nsmall number of seconds to well over a minute. Furthermore, many LLMs charge\nmoney for queries; caching thus has a clear monetary benefit. This paper\npresents a new caching system for improving user experiences with LLMs. In\naddition to reducing both latencies and monetary costs for accessing LLMs, our\nsystem also provides important features that go beyond the performance benefits\ntypically associated with caches. A key feature we provide is generative\ncaching, wherein multiple cached responses can be synthesized to provide\nanswers to queries which have never been seen before. Our generative caches\nfunction as repositories of valuable information which can be mined and\nanalyzed. We also improve upon past semantic caching techniques by tailoring\nthe caching algorithms to optimally balance cost and latency reduction with the\nquality of responses provided. Performance tests indicate that our caches are\nconsiderably faster than GPTcache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching has the potential to be of significant benefit for accessing large\nlanguage models (LLMs) due to their high latencies which typically range from a\nsmall number of seconds to well over a minute. Furthermore, many LLMs charge\nmoney for queries; caching thus has a clear monetary benefit. This paper\npresents a new caching system for improving user experiences with LLMs. In\naddition to reducing both latencies and monetary costs for accessing LLMs, our\nsystem also provides important features that go beyond the performance benefits\ntypically associated with caches. A key feature we provide is generative\ncaching, wherein multiple cached responses can be synthesized to provide\nanswers to queries which have never been seen before. Our generative caches\nfunction as repositories of valuable information which can be mined and\nanalyzed. We also improve upon past semantic caching techniques by tailoring\nthe caching algorithms to optimally balance cost and latency reduction with the\nquality of responses provided. Performance tests indicate that our caches are\nconsiderably faster than GPTcache."
                },
                "authors": [
                    {
                        "name": "Arun Iyengar"
                    },
                    {
                        "name": "Ashish Kundu"
                    },
                    {
                        "name": "Ramana Kompella"
                    },
                    {
                        "name": "Sai Nandan Mamidi"
                    }
                ],
                "author_detail": {
                    "name": "Sai Nandan Mamidi"
                },
                "author": "Sai Nandan Mamidi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17603v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17603v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17602v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17602v1",
                "updated": "2025-03-22T01:16:24Z",
                "updated_parsed": [
                    2025,
                    3,
                    22,
                    1,
                    16,
                    24,
                    5,
                    81,
                    0
                ],
                "published": "2025-03-22T01:16:24Z",
                "published_parsed": [
                    2025,
                    3,
                    22,
                    1,
                    16,
                    24,
                    5,
                    81,
                    0
                ],
                "title": "Multiport Support for Vortex OpenGPU Memory Hierarchy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multiport Support for Vortex OpenGPU Memory Hierarchy"
                },
                "summary": "Modern day applications have grown in size and require more computational\npower. The rise of machine learning and AI increased the need for parallel\ncomputation, which has increased the need for GPGPUs. With the increasing\ndemand for computational power, GPGPUs' SIMT architecture has solved this with\nan increase in the number of threads and the number of cores in a GPU,\nincreasing the throughput of these processors to match the demand of the\napplications. However, this created a larger demand for the memory, making the\nmemory bandwidth a bottleneck. The introduction of High-Bandwidth Memory (HBM)\nwith its increased number of memory ports offers a potential solution for the\nGPU to exploit its memory parallelism to increase the memory bandwidth.\nHowever, effectively leveraging HBM's memory parallelism to maximize bandwidth\npresents a unique and complex challenge for GPU architectures on how to\ndistribute those ports among the streaming multiprocessors in the GPGPU. In\nthis work, we extend the Vortex OpenGPU microarchitecture to incorporate a\nmultiport memory hierarchy, spanning from the L1 cache to the last-level cache\n(LLC). In addition, we propose various arbitration strategies to optimize\nmemory transfers across the cache hierarchy. The results have shown that an\nincrease in memory ports increases IPC, achieving an average speedup of 2.34x\nwith 8 memory ports in the tested configuration while showing relatively small\narea overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern day applications have grown in size and require more computational\npower. The rise of machine learning and AI increased the need for parallel\ncomputation, which has increased the need for GPGPUs. With the increasing\ndemand for computational power, GPGPUs' SIMT architecture has solved this with\nan increase in the number of threads and the number of cores in a GPU,\nincreasing the throughput of these processors to match the demand of the\napplications. However, this created a larger demand for the memory, making the\nmemory bandwidth a bottleneck. The introduction of High-Bandwidth Memory (HBM)\nwith its increased number of memory ports offers a potential solution for the\nGPU to exploit its memory parallelism to increase the memory bandwidth.\nHowever, effectively leveraging HBM's memory parallelism to maximize bandwidth\npresents a unique and complex challenge for GPU architectures on how to\ndistribute those ports among the streaming multiprocessors in the GPGPU. In\nthis work, we extend the Vortex OpenGPU microarchitecture to incorporate a\nmultiport memory hierarchy, spanning from the L1 cache to the last-level cache\n(LLC). In addition, we propose various arbitration strategies to optimize\nmemory transfers across the cache hierarchy. The results have shown that an\nincrease in memory ports increases IPC, achieving an average speedup of 2.34x\nwith 8 memory ports in the tested configuration while showing relatively small\narea overhead."
                },
                "authors": [
                    {
                        "name": "Injae Shin"
                    },
                    {
                        "name": "Blaise Tine"
                    }
                ],
                "author_detail": {
                    "name": "Blaise Tine"
                },
                "author": "Blaise Tine",
                "arxiv_comment": "OSSMPIC2025, 1st workshop on Open Source Solutions for Massively\n  Parallel Integrated Circuits",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17602v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17602v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07578v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07578v2",
                "updated": "2025-03-21T21:10:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    21,
                    10,
                    2,
                    4,
                    80,
                    0
                ],
                "published": "2025-02-11T14:25:20Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    14,
                    25,
                    20,
                    1,
                    42,
                    0
                ],
                "title": "PIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language\n  Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language\n  Model Inference"
                },
                "summary": "Large Language Model (LLM) inference uses an autoregressive manner to\ngenerate one token at a time, which exhibits notably lower operational\nintensity compared to earlier Machine Learning (ML) models such as encoder-only\ntransformers and Convolutional Neural Networks. At the same time, LLMs possess\nlarge parameter sizes and use key-value caches to store context information.\nModern LLMs support context windows with up to 1 million tokens to generate\nversatile text, audio, and video content. A large key-value cache unique to\neach prompt requires a large memory capacity, limiting the inference batch\nsize. Both low operational intensity and limited batch size necessitate a high\nmemory bandwidth. However, contemporary hardware systems for ML model\ndeployment, such as GPUs and TPUs, are primarily optimized for compute\nthroughput. This mismatch challenges the efficient deployment of advanced LLMs\nand makes users pay for expensive compute resources that are poorly utilized\nfor the memory-bound LLM inference tasks.\n  We propose CENT, a CXL-ENabled GPU-Free sysTem for LLM inference, which\nharnesses CXL memory expansion capabilities to accommodate substantial LLM\nsizes, and utilizes near-bank processing units to deliver high memory\nbandwidth, eliminating the need for expensive GPUs. CENT exploits a scalable\nCXL network to support peer-to-peer and collective communication primitives\nacross CXL devices. We implement various parallelism strategies to distribute\nLLMs across these devices. Compared to GPU baselines with maximum supported\nbatch sizes and similar average power, CENT achieves 2.3$\\times$ higher\nthroughput and consumes 2.3$\\times$ less energy. CENT enhances the Total Cost\nof Ownership (TCO), generating 5.2$\\times$ more tokens per dollar than GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference uses an autoregressive manner to\ngenerate one token at a time, which exhibits notably lower operational\nintensity compared to earlier Machine Learning (ML) models such as encoder-only\ntransformers and Convolutional Neural Networks. At the same time, LLMs possess\nlarge parameter sizes and use key-value caches to store context information.\nModern LLMs support context windows with up to 1 million tokens to generate\nversatile text, audio, and video content. A large key-value cache unique to\neach prompt requires a large memory capacity, limiting the inference batch\nsize. Both low operational intensity and limited batch size necessitate a high\nmemory bandwidth. However, contemporary hardware systems for ML model\ndeployment, such as GPUs and TPUs, are primarily optimized for compute\nthroughput. This mismatch challenges the efficient deployment of advanced LLMs\nand makes users pay for expensive compute resources that are poorly utilized\nfor the memory-bound LLM inference tasks.\n  We propose CENT, a CXL-ENabled GPU-Free sysTem for LLM inference, which\nharnesses CXL memory expansion capabilities to accommodate substantial LLM\nsizes, and utilizes near-bank processing units to deliver high memory\nbandwidth, eliminating the need for expensive GPUs. CENT exploits a scalable\nCXL network to support peer-to-peer and collective communication primitives\nacross CXL devices. We implement various parallelism strategies to distribute\nLLMs across these devices. Compared to GPU baselines with maximum supported\nbatch sizes and similar average power, CENT achieves 2.3$\\times$ higher\nthroughput and consumes 2.3$\\times$ less energy. CENT enhances the Total Cost\nof Ownership (TCO), generating 5.2$\\times$ more tokens per dollar than GPUs."
                },
                "authors": [
                    {
                        "name": "Yufeng Gu"
                    },
                    {
                        "name": "Alireza Khadem"
                    },
                    {
                        "name": "Sumanth Umesh"
                    },
                    {
                        "name": "Ning Liang"
                    },
                    {
                        "name": "Xavier Servot"
                    },
                    {
                        "name": "Onur Mutlu"
                    },
                    {
                        "name": "Ravi Iyer"
                    },
                    {
                        "name": "Reetuparna Das"
                    }
                ],
                "author_detail": {
                    "name": "Reetuparna Das"
                },
                "author": "Reetuparna Das",
                "arxiv_doi": "10.1145/3676641.3716267",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676641.3716267",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.07578v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07578v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "In Proceedings of the 30th ACM International Conference on\n  Architectural Support for Programming Languages and Operating Systems, Volume\n  2 (ASPLOS'25)",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09003v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09003v2",
                "updated": "2025-03-21T19:26:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    19,
                    26,
                    12,
                    4,
                    80,
                    0
                ],
                "published": "2025-02-13T06:44:33Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    6,
                    44,
                    33,
                    3,
                    44,
                    0
                ],
                "title": "RoSTE: An Efficient Quantization-Aware Supervised Fine-Tuning Approach\n  for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoSTE: An Efficient Quantization-Aware Supervised Fine-Tuning Approach\n  for Large Language Models"
                },
                "summary": "Supervised fine-tuning is a standard method for adapting pre-trained large\nlanguage models (LLMs) to downstream tasks. Quantization has been recently\nstudied as a post-training technique for efficient LLM deployment. To obtain\nquantized fine-tuned LLMs, conventional pipelines would first fine-tune the\npre-trained models, followed by post-training quantization. This often yields\nsuboptimal performance as it fails to leverage the synergy between fine-tuning\nand quantization. To effectively realize low-bit quantization of weights,\nactivations, and KV caches in LLMs, we propose an algorithm named Rotated\nStraight-Through-Estimator (RoSTE), which combines quantization-aware\nsupervised fine-tuning (QA-SFT) with an adaptive rotation strategy that\nidentifies an effective rotation configuration to reduce activation outliers.\nWe provide theoretical insights on RoSTE by analyzing its prediction error when\napplied to an overparameterized least square quantized training problem. Our\nfindings reveal that the prediction error is directly proportional to the\nquantization error of the converged weights, which can be effectively managed\nthrough an optimized rotation configuration. Experiments on Pythia, Qwen and\nLlama models of different sizes demonstrate the effectiveness of RoSTE.\nCompared to existing post-SFT quantization baselines, our method consistently\nachieves superior performances across various tasks and different LLM\narchitectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supervised fine-tuning is a standard method for adapting pre-trained large\nlanguage models (LLMs) to downstream tasks. Quantization has been recently\nstudied as a post-training technique for efficient LLM deployment. To obtain\nquantized fine-tuned LLMs, conventional pipelines would first fine-tune the\npre-trained models, followed by post-training quantization. This often yields\nsuboptimal performance as it fails to leverage the synergy between fine-tuning\nand quantization. To effectively realize low-bit quantization of weights,\nactivations, and KV caches in LLMs, we propose an algorithm named Rotated\nStraight-Through-Estimator (RoSTE), which combines quantization-aware\nsupervised fine-tuning (QA-SFT) with an adaptive rotation strategy that\nidentifies an effective rotation configuration to reduce activation outliers.\nWe provide theoretical insights on RoSTE by analyzing its prediction error when\napplied to an overparameterized least square quantized training problem. Our\nfindings reveal that the prediction error is directly proportional to the\nquantization error of the converged weights, which can be effectively managed\nthrough an optimized rotation configuration. Experiments on Pythia, Qwen and\nLlama models of different sizes demonstrate the effectiveness of RoSTE.\nCompared to existing post-SFT quantization baselines, our method consistently\nachieves superior performances across various tasks and different LLM\narchitectures."
                },
                "authors": [
                    {
                        "name": "Quan Wei"
                    },
                    {
                        "name": "Chung-Yiu Yau"
                    },
                    {
                        "name": "Hoi-To Wai"
                    },
                    {
                        "name": "Yang Katie Zhao"
                    },
                    {
                        "name": "Dongyeop Kang"
                    },
                    {
                        "name": "Youngsuk Park"
                    },
                    {
                        "name": "Mingyi Hong"
                    }
                ],
                "author_detail": {
                    "name": "Mingyi Hong"
                },
                "author": "Mingyi Hong",
                "arxiv_comment": "20 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09003v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09003v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12444v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12444v3",
                "updated": "2025-03-21T15:52:39Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    15,
                    52,
                    39,
                    4,
                    80,
                    0
                ],
                "published": "2024-12-17T01:12:35Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    1,
                    12,
                    35,
                    1,
                    352,
                    0
                ],
                "title": "LazyDiT: Lazy Learning for the Acceleration of Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LazyDiT: Lazy Learning for the Acceleration of Diffusion Transformers"
                },
                "summary": "Diffusion Transformers have emerged as the preeminent models for a wide array\nof generative tasks, demonstrating superior performance and efficacy across\nvarious applications. The promising results come at the cost of slow inference,\nas each denoising step requires running the whole transformer model with a\nlarge amount of parameters. In this paper, we show that performing the full\ncomputation of the model at each diffusion step is unnecessary, as some\ncomputations can be skipped by lazily reusing the results of previous steps.\nFurthermore, we show that the lower bound of similarity between outputs at\nconsecutive steps is notably high, and this similarity can be linearly\napproximated using the inputs. To verify our demonstrations, we propose the\n\\textbf{LazyDiT}, a lazy learning framework that efficiently leverages cached\nresults from earlier steps to skip redundant computations. Specifically, we\nincorporate lazy learning layers into the model, effectively trained to\nmaximize laziness, enabling dynamic skipping of redundant computations.\nExperimental results show that LazyDiT outperforms the DDIM sampler across\nmultiple diffusion transformer models at various resolutions. Furthermore, we\nimplement our method on mobile devices, achieving better performance than DDIM\nwith similar latency. Code: https://github.com/shawnricecake/lazydit",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers have emerged as the preeminent models for a wide array\nof generative tasks, demonstrating superior performance and efficacy across\nvarious applications. The promising results come at the cost of slow inference,\nas each denoising step requires running the whole transformer model with a\nlarge amount of parameters. In this paper, we show that performing the full\ncomputation of the model at each diffusion step is unnecessary, as some\ncomputations can be skipped by lazily reusing the results of previous steps.\nFurthermore, we show that the lower bound of similarity between outputs at\nconsecutive steps is notably high, and this similarity can be linearly\napproximated using the inputs. To verify our demonstrations, we propose the\n\\textbf{LazyDiT}, a lazy learning framework that efficiently leverages cached\nresults from earlier steps to skip redundant computations. Specifically, we\nincorporate lazy learning layers into the model, effectively trained to\nmaximize laziness, enabling dynamic skipping of redundant computations.\nExperimental results show that LazyDiT outperforms the DDIM sampler across\nmultiple diffusion transformer models at various resolutions. Furthermore, we\nimplement our method on mobile devices, achieving better performance than DDIM\nwith similar latency. Code: https://github.com/shawnricecake/lazydit"
                },
                "authors": [
                    {
                        "name": "Xuan Shen"
                    },
                    {
                        "name": "Zhao Song"
                    },
                    {
                        "name": "Yufa Zhou"
                    },
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Yanyu Li"
                    },
                    {
                        "name": "Yifan Gong"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Hao Tan"
                    },
                    {
                        "name": "Jason Kuen"
                    },
                    {
                        "name": "Henghui Ding"
                    },
                    {
                        "name": "Zhihao Shu"
                    },
                    {
                        "name": "Wei Niu"
                    },
                    {
                        "name": "Pu Zhao"
                    },
                    {
                        "name": "Yanzhi Wang"
                    },
                    {
                        "name": "Jiuxiang Gu"
                    }
                ],
                "author_detail": {
                    "name": "Jiuxiang Gu"
                },
                "author": "Jiuxiang Gu",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12444v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12444v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15102v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15102v3",
                "updated": "2025-03-21T15:47:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    15,
                    47,
                    53,
                    4,
                    80,
                    0
                ],
                "published": "2024-11-22T18:06:14Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    18,
                    6,
                    14,
                    4,
                    327,
                    0
                ],
                "title": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution"
                },
                "summary": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods."
                },
                "authors": [
                    {
                        "name": "Fengyuan Liu"
                    },
                    {
                        "name": "Nikhil Kandpal"
                    },
                    {
                        "name": "Colin Raffel"
                    }
                ],
                "author_detail": {
                    "name": "Colin Raffel"
                },
                "author": "Colin Raffel",
                "arxiv_comment": "24 pages, 11 figures, ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15102v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15102v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00876v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00876v4",
                "updated": "2025-03-21T13:30:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    13,
                    30,
                    33,
                    4,
                    80,
                    0
                ],
                "published": "2024-12-01T16:32:31Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    16,
                    32,
                    31,
                    6,
                    336,
                    0
                ],
                "title": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava ."
                },
                "authors": [
                    {
                        "name": "Wenxuan Huang"
                    },
                    {
                        "name": "Zijie Zhai"
                    },
                    {
                        "name": "Yunhang Shen"
                    },
                    {
                        "name": "Shaosheng Cao"
                    },
                    {
                        "name": "Fei Zhao"
                    },
                    {
                        "name": "Xiangfeng Xu"
                    },
                    {
                        "name": "Zheyu Ye"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Shaohui Lin"
                    }
                ],
                "author_detail": {
                    "name": "Shaohui Lin"
                },
                "author": "Shaohui Lin",
                "arxiv_comment": "Accepted to ICLR 2025. Code is available at\n  https://github.com/Osilly/dynamic_llava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00876v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00876v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09398v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09398v3",
                "updated": "2025-03-21T12:51:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    12,
                    51,
                    15,
                    4,
                    80,
                    0
                ],
                "published": "2024-09-14T10:15:37Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    10,
                    15,
                    37,
                    5,
                    258,
                    0
                ],
                "title": "Language-Queried Target Sound Extraction Without Parallel Training Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-Queried Target Sound Extraction Without Parallel Training Data"
                },
                "summary": "Language-queried target sound extraction (TSE) aims to extract specific\nsounds from mixtures based on language queries. Traditional fully-supervised\ntraining schemes require extensively annotated parallel audio-text data, which\nare labor-intensive. We introduce a parallel-data-free training scheme,\nrequiring only unlabelled audio clips for TSE model training by utilizing the\ncontrastive language-audio pre-trained model (CLAP). In a vanilla\nparallel-data-free training stage, target audio is encoded using the\npre-trained CLAP audio encoder to form a condition embedding, while during\ntesting, user language queries are encoded by CLAP text encoder as the\ncondition embedding. This vanilla approach assumes perfect alignment between\ntext and audio embeddings, which is unrealistic. Two major challenges arise\nfrom training-testing mismatch: the persistent modality gap between text and\naudio and the risk of overfitting due to the exposure of rich acoustic details\nin target audio embedding during training. To address this, we propose a\nretrieval-augmented strategy. Specifically, we create an embedding cache using\naudio captions generated by a large language model (LLM). During training,\ntarget audio embeddings retrieve text embeddings from this cache to use as\ncondition embeddings, ensuring consistent modalities between training and\ntesting and eliminating information leakage. Extensive experiment results show\nthat our retrieval-augmented approach achieves consistent and notable\nperformance improvements over existing state-of-the-art with better\ngeneralizability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-queried target sound extraction (TSE) aims to extract specific\nsounds from mixtures based on language queries. Traditional fully-supervised\ntraining schemes require extensively annotated parallel audio-text data, which\nare labor-intensive. We introduce a parallel-data-free training scheme,\nrequiring only unlabelled audio clips for TSE model training by utilizing the\ncontrastive language-audio pre-trained model (CLAP). In a vanilla\nparallel-data-free training stage, target audio is encoded using the\npre-trained CLAP audio encoder to form a condition embedding, while during\ntesting, user language queries are encoded by CLAP text encoder as the\ncondition embedding. This vanilla approach assumes perfect alignment between\ntext and audio embeddings, which is unrealistic. Two major challenges arise\nfrom training-testing mismatch: the persistent modality gap between text and\naudio and the risk of overfitting due to the exposure of rich acoustic details\nin target audio embedding during training. To address this, we propose a\nretrieval-augmented strategy. Specifically, we create an embedding cache using\naudio captions generated by a large language model (LLM). During training,\ntarget audio embeddings retrieve text embeddings from this cache to use as\ncondition embeddings, ensuring consistent modalities between training and\ntesting and eliminating information leakage. Extensive experiment results show\nthat our retrieval-augmented approach achieves consistent and notable\nperformance improvements over existing state-of-the-art with better\ngeneralizability."
                },
                "authors": [
                    {
                        "name": "Hao Ma"
                    },
                    {
                        "name": "Zhiyuan Peng"
                    },
                    {
                        "name": "Xu Li"
                    },
                    {
                        "name": "Yukai Li"
                    },
                    {
                        "name": "Mingjie Shao"
                    },
                    {
                        "name": "Qiuqiang Kong"
                    },
                    {
                        "name": "Ju Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ju Liu"
                },
                "author": "Ju Liu",
                "arxiv_comment": "Accepted by ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09398v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09398v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16870v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16870v1",
                "updated": "2025-03-21T05:58:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    5,
                    58,
                    18,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-21T05:58:18Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    5,
                    58,
                    18,
                    4,
                    80,
                    0
                ],
                "title": "Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs"
                },
                "summary": "Knowledge distillation can be a cost-effective technique to distill knowledge\nin Large Language Models, if the teacher output logits can be pre-computed and\ncached. However, successfully applying this to pre-training remains largely\nunexplored. In this work, we prove that naive approaches for sparse knowledge\ndistillation such as caching Top-K probabilities, while intuitive, provide\nbiased estimates of teacher probability distribution to the student, resulting\nin suboptimal performance and calibration. We propose an\nimportance-sampling-based method `Random Sampling Knowledge Distillation',\nwhich provides unbiased estimates, preserves the gradient in expectation, and\nrequires storing significantly sparser logits. Our method enables faster\ntraining of student models with marginal overhead (<10%) compared to\ncross-entropy based training, while maintaining competitive performance\ncompared to full distillation, across a range of model sizes from 300M to 3B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge distillation can be a cost-effective technique to distill knowledge\nin Large Language Models, if the teacher output logits can be pre-computed and\ncached. However, successfully applying this to pre-training remains largely\nunexplored. In this work, we prove that naive approaches for sparse knowledge\ndistillation such as caching Top-K probabilities, while intuitive, provide\nbiased estimates of teacher probability distribution to the student, resulting\nin suboptimal performance and calibration. We propose an\nimportance-sampling-based method `Random Sampling Knowledge Distillation',\nwhich provides unbiased estimates, preserves the gradient in expectation, and\nrequires storing significantly sparser logits. Our method enables faster\ntraining of student models with marginal overhead (<10%) compared to\ncross-entropy based training, while maintaining competitive performance\ncompared to full distillation, across a range of model sizes from 300M to 3B."
                },
                "authors": [
                    {
                        "name": "Anshumann"
                    },
                    {
                        "name": "Mohd Abbas Zaidi"
                    },
                    {
                        "name": "Akhil Kedia"
                    },
                    {
                        "name": "Jinwoo Ahn"
                    },
                    {
                        "name": "Taehwak Kwon"
                    },
                    {
                        "name": "Kangwook Lee"
                    },
                    {
                        "name": "Haejun Lee"
                    },
                    {
                        "name": "Joohyung Lee"
                    }
                ],
                "author_detail": {
                    "name": "Joohyung Lee"
                },
                "author": "Joohyung Lee",
                "arxiv_comment": "Anshumann, Mohd Abbas Zaidi and Akhil Kedia have Equal Contribution",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16870v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16870v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16131v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16131v2",
                "updated": "2025-03-21T01:59:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    21,
                    1,
                    59,
                    12,
                    4,
                    80,
                    0
                ],
                "published": "2025-03-20T13:25:03Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    13,
                    25,
                    3,
                    3,
                    79,
                    0
                ],
                "title": "MKG-Rank: Enhancing Large Language Models with Knowledge Graph for\n  Multilingual Medical Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MKG-Rank: Enhancing Large Language Models with Knowledge Graph for\n  Multilingual Medical Question Answering"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable progress in medical\nquestion answering (QA), yet their effectiveness remains predominantly limited\nto English due to imbalanced multilingual training data and scarce medical\nresources for low-resource languages. To address this critical language gap in\nmedical QA, we propose Multilingual Knowledge Graph-based Retrieval Ranking\n(MKG-Rank), a knowledge graph-enhanced framework that enables English-centric\nLLMs to perform multilingual medical QA. Through a word-level translation\nmechanism, our framework efficiently integrates comprehensive English-centric\nmedical knowledge graphs into LLM reasoning at a low cost, mitigating\ncross-lingual semantic distortion and achieving precise medical QA across\nlanguage barriers. To enhance efficiency, we introduce caching and multi-angle\nranking strategies to optimize the retrieval process, significantly reducing\nresponse times and prioritizing relevant medical knowledge. Extensive\nevaluations on multilingual medical QA benchmarks across Chinese, Japanese,\nKorean, and Swahili demonstrate that MKG-Rank consistently outperforms\nzero-shot LLMs, achieving maximum 35.03% increase in accuracy, while\nmaintaining an average retrieval time of only 0.0009 seconds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable progress in medical\nquestion answering (QA), yet their effectiveness remains predominantly limited\nto English due to imbalanced multilingual training data and scarce medical\nresources for low-resource languages. To address this critical language gap in\nmedical QA, we propose Multilingual Knowledge Graph-based Retrieval Ranking\n(MKG-Rank), a knowledge graph-enhanced framework that enables English-centric\nLLMs to perform multilingual medical QA. Through a word-level translation\nmechanism, our framework efficiently integrates comprehensive English-centric\nmedical knowledge graphs into LLM reasoning at a low cost, mitigating\ncross-lingual semantic distortion and achieving precise medical QA across\nlanguage barriers. To enhance efficiency, we introduce caching and multi-angle\nranking strategies to optimize the retrieval process, significantly reducing\nresponse times and prioritizing relevant medical knowledge. Extensive\nevaluations on multilingual medical QA benchmarks across Chinese, Japanese,\nKorean, and Swahili demonstrate that MKG-Rank consistently outperforms\nzero-shot LLMs, achieving maximum 35.03% increase in accuracy, while\nmaintaining an average retrieval time of only 0.0009 seconds."
                },
                "authors": [
                    {
                        "name": "Feiyang Li"
                    },
                    {
                        "name": "Yingjian Chen"
                    },
                    {
                        "name": "Haoran Liu"
                    },
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Han Yuan"
                    },
                    {
                        "name": "Yuang Jiang"
                    },
                    {
                        "name": "Tianxiao Li"
                    },
                    {
                        "name": "Edison Marrese Taylor"
                    },
                    {
                        "name": "Hossein Rouhizadeh"
                    },
                    {
                        "name": "Yusuke Iwasawa"
                    },
                    {
                        "name": "Douglas Teodoro"
                    },
                    {
                        "name": "Yutaka Matsuo"
                    },
                    {
                        "name": "Irene Li"
                    }
                ],
                "author_detail": {
                    "name": "Irene Li"
                },
                "author": "Irene Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16131v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16131v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02430v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02430v3",
                "updated": "2025-03-20T21:49:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    21,
                    49,
                    15,
                    3,
                    79,
                    0
                ],
                "published": "2025-02-04T15:55:10Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    15,
                    55,
                    10,
                    1,
                    35,
                    0
                ],
                "title": "A Scalable Crawling Algorithm Utilizing Noisy Change-Indicating Signals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Scalable Crawling Algorithm Utilizing Noisy Change-Indicating Signals"
                },
                "summary": "Web refresh crawling is the problem of keeping a cache of web pages fresh,\nthat is, having the most recent copy available when a page is requested, given\na limited bandwidth available to the crawler. Under the assumption that the\nchange and request events, resp., to each web page follow independent Poisson\nprocesses, the optimal scheduling policy was derived by Azar et al. 2018. In\nthis paper, we study an extension of this problem where side information\nindicating content changes, such as various types of web pings, for example,\nsignals from sitemaps, content delivery networks, etc., is available.\nIncorporating such side information into the crawling policy is challenging,\nbecause (i) the signals can be noisy with false positive events and with\nmissing change events; and (ii) the crawler should achieve a fair performance\nover web pages regardless of the quality of the side information, which might\ndiffer from web page to web page. We propose a scalable crawling algorithm\nwhich (i) uses the noisy side information in an optimal way under mild\nassumptions; (ii) can be deployed without heavy centralized computation; (iii)\nis able to crawl web pages at a constant total rate without spikes in the total\nbandwidth usage over any time interval, and automatically adapt to the new\noptimal solution when the total bandwidth changes without centralized\ncomputation. Experiments clearly demonstrate the versatility of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Web refresh crawling is the problem of keeping a cache of web pages fresh,\nthat is, having the most recent copy available when a page is requested, given\na limited bandwidth available to the crawler. Under the assumption that the\nchange and request events, resp., to each web page follow independent Poisson\nprocesses, the optimal scheduling policy was derived by Azar et al. 2018. In\nthis paper, we study an extension of this problem where side information\nindicating content changes, such as various types of web pings, for example,\nsignals from sitemaps, content delivery networks, etc., is available.\nIncorporating such side information into the crawling policy is challenging,\nbecause (i) the signals can be noisy with false positive events and with\nmissing change events; and (ii) the crawler should achieve a fair performance\nover web pages regardless of the quality of the side information, which might\ndiffer from web page to web page. We propose a scalable crawling algorithm\nwhich (i) uses the noisy side information in an optimal way under mild\nassumptions; (ii) can be deployed without heavy centralized computation; (iii)\nis able to crawl web pages at a constant total rate without spikes in the total\nbandwidth usage over any time interval, and automatically adapt to the new\noptimal solution when the total bandwidth changes without centralized\ncomputation. Experiments clearly demonstrate the versatility of our approach."
                },
                "authors": [
                    {
                        "name": "Róbert Busa-Fekete"
                    },
                    {
                        "name": "Julian Zimmert"
                    },
                    {
                        "name": "András György"
                    },
                    {
                        "name": "Linhai Qiu"
                    },
                    {
                        "name": "Tzu-Wei Sung"
                    },
                    {
                        "name": "Hao Shen"
                    },
                    {
                        "name": "Hyomin Choi"
                    },
                    {
                        "name": "Sharmila Subramaniam"
                    },
                    {
                        "name": "Li Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Li Xiao"
                },
                "author": "Li Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02430v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02430v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16588v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16588v1",
                "updated": "2025-03-20T17:37:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    37,
                    15,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T17:37:15Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    37,
                    15,
                    3,
                    79,
                    0
                ],
                "title": "A Unified Framework for Quantitative Cache Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Framework for Quantitative Cache Analysis"
                },
                "summary": "In this work we unify two existing lines of work towards cache analysis for\nnon-LRU policies. To this end, we extend the notion of competitiveness to\nblock-wise competitiveness and systematically analyze the competitiveness and\nblock competitiveness of FIFO and MRU relative to LRU for arbitrary\nassociativities. We show how competitiveness and block competitiveness can be\nexploited in state-of-the-art WCET analysis based on the results of existing\npersistence analyses for LRU. Unlike prior work, our approach is applicable to\nmicroarchitectures that exhibit timing anomalies. We experimentally evaluate\nthe precision and cost of our approach on benchmarks from TACLeBench. The\nexperiments demonstrate that quantitative cache analysis for FIFO and MRU comes\nclose to the precision of LRU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work we unify two existing lines of work towards cache analysis for\nnon-LRU policies. To this end, we extend the notion of competitiveness to\nblock-wise competitiveness and systematically analyze the competitiveness and\nblock competitiveness of FIFO and MRU relative to LRU for arbitrary\nassociativities. We show how competitiveness and block competitiveness can be\nexploited in state-of-the-art WCET analysis based on the results of existing\npersistence analyses for LRU. Unlike prior work, our approach is applicable to\nmicroarchitectures that exhibit timing anomalies. We experimentally evaluate\nthe precision and cost of our approach on benchmarks from TACLeBench. The\nexperiments demonstrate that quantitative cache analysis for FIFO and MRU comes\nclose to the precision of LRU."
                },
                "authors": [
                    {
                        "name": "Sophie Kahlen"
                    },
                    {
                        "name": "Jan Reineke"
                    }
                ],
                "author_detail": {
                    "name": "Jan Reineke"
                },
                "author": "Jan Reineke",
                "arxiv_comment": "Extended version of RTAS 2025 paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16588v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16588v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.3.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16257v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16257v1",
                "updated": "2025-03-20T15:52:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    15,
                    52,
                    43,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T15:52:43Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    15,
                    52,
                    43,
                    3,
                    79,
                    0
                ],
                "title": "Plug-and-Play 1.x-Bit KV Cache Quantization for Video Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plug-and-Play 1.x-Bit KV Cache Quantization for Video Large Language\n  Models"
                },
                "summary": "Video large language models (VideoLLMs) have demonstrated the capability to\nprocess longer video inputs and enable complex reasoning and analysis. However,\ndue to the thousands of visual tokens from the video frames, key-value (KV)\ncache can significantly increase memory requirements, becoming a bottleneck for\ninference speed and memory usage. KV cache quantization is a widely used\napproach to address this problem. In this paper, we find that 2-bit KV\nquantization of VideoLLMs can hardly hurt the model performance, while the\nlimit of KV cache quantization in even lower bits has not been investigated. To\nbridge this gap, we introduce VidKV, a plug-and-play KV cache quantization\nmethod to compress the KV cache to lower than 2 bits. Specifically, (1) for\nkey, we propose a mixed-precision quantization strategy in the channel\ndimension, where we perform 2-bit quantization for anomalous channels and 1-bit\nquantization combined with FFT for normal channels; (2) for value, we implement\n1.58-bit quantization while selectively filtering semantically salient visual\ntokens for targeted preservation, for a better trade-off between precision and\nmodel performance. Importantly, our findings suggest that the value cache of\nVideoLLMs should be quantized in a per-channel fashion instead of the per-token\nfashion proposed by prior KV cache quantization works for LLMs. Empirically,\nextensive results with LLaVA-OV-7B and Qwen2.5-VL-7B on six benchmarks show\nthat VidKV effectively compresses the KV cache to 1.5-bit and 1.58-bit\nprecision with almost no performance drop compared to the FP16 counterparts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video large language models (VideoLLMs) have demonstrated the capability to\nprocess longer video inputs and enable complex reasoning and analysis. However,\ndue to the thousands of visual tokens from the video frames, key-value (KV)\ncache can significantly increase memory requirements, becoming a bottleneck for\ninference speed and memory usage. KV cache quantization is a widely used\napproach to address this problem. In this paper, we find that 2-bit KV\nquantization of VideoLLMs can hardly hurt the model performance, while the\nlimit of KV cache quantization in even lower bits has not been investigated. To\nbridge this gap, we introduce VidKV, a plug-and-play KV cache quantization\nmethod to compress the KV cache to lower than 2 bits. Specifically, (1) for\nkey, we propose a mixed-precision quantization strategy in the channel\ndimension, where we perform 2-bit quantization for anomalous channels and 1-bit\nquantization combined with FFT for normal channels; (2) for value, we implement\n1.58-bit quantization while selectively filtering semantically salient visual\ntokens for targeted preservation, for a better trade-off between precision and\nmodel performance. Importantly, our findings suggest that the value cache of\nVideoLLMs should be quantized in a per-channel fashion instead of the per-token\nfashion proposed by prior KV cache quantization works for LLMs. Empirically,\nextensive results with LLaVA-OV-7B and Qwen2.5-VL-7B on six benchmarks show\nthat VidKV effectively compresses the KV cache to 1.5-bit and 1.58-bit\nprecision with almost no performance drop compared to the FP16 counterparts."
                },
                "authors": [
                    {
                        "name": "Keda Tao"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Can Qin"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16257v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16257v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16163v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16163v1",
                "updated": "2025-03-20T14:01:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    14,
                    1,
                    56,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T14:01:56Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    14,
                    1,
                    56,
                    3,
                    79,
                    0
                ],
                "title": "SpeCache: Speculative Key-Value Caching for Efficient Generation of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpeCache: Speculative Key-Value Caching for Efficient Generation of LLMs"
                },
                "summary": "Transformer-based large language models (LLMs) have already achieved\nremarkable results on long-text tasks, but the limited GPU memory (VRAM)\nresources struggle to accommodate the linearly growing demand for key-value\n(KV) cache as the sequence length increases, which has become a bottleneck for\nthe application of LLMs on long sequences. Existing KV cache compression\nmethods include eviction, merging, or quantization of the KV cache to reduce\nits size. However, compression results in irreversible information forgetting,\npotentially affecting the accuracy of subsequent decoding. In this paper, we\npropose SpeCache, which takes full advantage of the large and easily expandable\nCPU memory to offload the complete KV cache, and dynamically fetches KV pairs\nback in each decoding step based on their importance measured by low-bit KV\ncache copy in VRAM. To avoid inference latency caused by CPU-GPU communication,\nSpeCache speculatively predicts the KV pairs that the next token might attend\nto, allowing us to prefetch them before the next decoding step which enables\nparallelization of prefetching and computation. Experiments on LongBench and\nNeedle-in-a-Haystack benchmarks verify that SpeCache effectively reduces VRAM\nusage while avoiding information forgetting for long sequences without\nre-training, even with a 10x high KV cache compression ratio.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) have already achieved\nremarkable results on long-text tasks, but the limited GPU memory (VRAM)\nresources struggle to accommodate the linearly growing demand for key-value\n(KV) cache as the sequence length increases, which has become a bottleneck for\nthe application of LLMs on long sequences. Existing KV cache compression\nmethods include eviction, merging, or quantization of the KV cache to reduce\nits size. However, compression results in irreversible information forgetting,\npotentially affecting the accuracy of subsequent decoding. In this paper, we\npropose SpeCache, which takes full advantage of the large and easily expandable\nCPU memory to offload the complete KV cache, and dynamically fetches KV pairs\nback in each decoding step based on their importance measured by low-bit KV\ncache copy in VRAM. To avoid inference latency caused by CPU-GPU communication,\nSpeCache speculatively predicts the KV pairs that the next token might attend\nto, allowing us to prefetch them before the next decoding step which enables\nparallelization of prefetching and computation. Experiments on LongBench and\nNeedle-in-a-Haystack benchmarks verify that SpeCache effectively reduces VRAM\nusage while avoiding information forgetting for long sequences without\nre-training, even with a 10x high KV cache compression ratio."
                },
                "authors": [
                    {
                        "name": "Shibo Jie"
                    },
                    {
                        "name": "Yehui Tang"
                    },
                    {
                        "name": "Kai Han"
                    },
                    {
                        "name": "Zhi-Hong Deng"
                    },
                    {
                        "name": "Jing Han"
                    }
                ],
                "author_detail": {
                    "name": "Jing Han"
                },
                "author": "Jing Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16163v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16163v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16112v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16112v1",
                "updated": "2025-03-20T13:00:36Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    13,
                    0,
                    36,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T13:00:36Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    13,
                    0,
                    36,
                    3,
                    79,
                    0
                ],
                "title": "PromptMobile: Efficient Promptus for Low Bandwidth Mobile Video\n  Streaming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PromptMobile: Efficient Promptus for Low Bandwidth Mobile Video\n  Streaming"
                },
                "summary": "Traditional video compression algorithms exhibit significant quality\ndegradation at extremely low bitrates. Promptus emerges as a new paradigm for\nvideo streaming, substantially cutting down the bandwidth essential for video\nstreaming. However, Promptus is computationally intensive and can not run in\nreal-time on mobile devices. This paper presents PromptMobile, an efficient\nacceleration framework tailored for on-device Promptus. Specifically, we\npropose (1) a two-stage efficient generation framework to reduce computational\ncost by 8.1x, (2) a fine-grained inter-frame caching to reduce redundant\ncomputations by 16.6\\%, (3) system-level optimizations to further enhance\nefficiency. The evaluations demonstrate that compared with the original\nPromptus, PromptMobile achieves a 13.6x increase in image generation speed.\nCompared with other streaming methods, PromptMobile achives an average LPIPS\nimprovement of 0.016 (compared with H.265), reducing 60\\% of severely distorted\nframes (compared to VQGAN).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional video compression algorithms exhibit significant quality\ndegradation at extremely low bitrates. Promptus emerges as a new paradigm for\nvideo streaming, substantially cutting down the bandwidth essential for video\nstreaming. However, Promptus is computationally intensive and can not run in\nreal-time on mobile devices. This paper presents PromptMobile, an efficient\nacceleration framework tailored for on-device Promptus. Specifically, we\npropose (1) a two-stage efficient generation framework to reduce computational\ncost by 8.1x, (2) a fine-grained inter-frame caching to reduce redundant\ncomputations by 16.6\\%, (3) system-level optimizations to further enhance\nefficiency. The evaluations demonstrate that compared with the original\nPromptus, PromptMobile achieves a 13.6x increase in image generation speed.\nCompared with other streaming methods, PromptMobile achives an average LPIPS\nimprovement of 0.016 (compared with H.265), reducing 60\\% of severely distorted\nframes (compared to VQGAN)."
                },
                "authors": [
                    {
                        "name": "Liming Liu"
                    },
                    {
                        "name": "Jiangkai Wu"
                    },
                    {
                        "name": "Haoyang Wang"
                    },
                    {
                        "name": "Peiheng Wang"
                    },
                    {
                        "name": "Xinggong Zhang"
                    },
                    {
                        "name": "Zongming Guo"
                    }
                ],
                "author_detail": {
                    "name": "Zongming Guo"
                },
                "author": "Zongming Guo",
                "arxiv_comment": "7 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16112v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16112v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15927v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15927v1",
                "updated": "2025-03-20T08:07:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    8,
                    7,
                    31,
                    3,
                    79,
                    0
                ],
                "published": "2025-03-20T08:07:31Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    8,
                    7,
                    31,
                    3,
                    79,
                    0
                ],
                "title": "BlockDance: Reuse Structurally Similar Spatio-Temporal Features to\n  Accelerate Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BlockDance: Reuse Structurally Similar Spatio-Temporal Features to\n  Accelerate Diffusion Transformers"
                },
                "summary": "Diffusion models have demonstrated impressive generation capabilities,\nparticularly with recent advancements leveraging transformer architectures to\nimprove both visual and artistic quality. However, Diffusion Transformers\n(DiTs) continue to encounter challenges related to low inference speed,\nprimarily due to the iterative denoising process. To address this issue, we\npropose BlockDance, a training-free approach that explores feature similarities\nat adjacent time steps to accelerate DiTs. Unlike previous feature-reuse\nmethods that lack tailored reuse strategies for features at different scales,\nBlockDance prioritizes the identification of the most structurally similar\nfeatures, referred to as Structurally Similar Spatio-Temporal (STSS) features.\nThese features are primarily located within the structure-focused blocks of the\ntransformer during the later stages of denoising. BlockDance caches and reuses\nthese highly similar features to mitigate redundant computation, thereby\naccelerating DiTs while maximizing consistency with the generated results of\nthe original model. Furthermore, considering the diversity of generated content\nand the varying distributions of redundant features, we introduce\nBlockDance-Ada, a lightweight decision-making network tailored for\ninstance-specific acceleration. BlockDance-Ada dynamically allocates resources\nand provides superior content quality. Both BlockDance and BlockDance-Ada have\nproven effective across various generation tasks and models, achieving\naccelerations between 25% and 50% while maintaining generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have demonstrated impressive generation capabilities,\nparticularly with recent advancements leveraging transformer architectures to\nimprove both visual and artistic quality. However, Diffusion Transformers\n(DiTs) continue to encounter challenges related to low inference speed,\nprimarily due to the iterative denoising process. To address this issue, we\npropose BlockDance, a training-free approach that explores feature similarities\nat adjacent time steps to accelerate DiTs. Unlike previous feature-reuse\nmethods that lack tailored reuse strategies for features at different scales,\nBlockDance prioritizes the identification of the most structurally similar\nfeatures, referred to as Structurally Similar Spatio-Temporal (STSS) features.\nThese features are primarily located within the structure-focused blocks of the\ntransformer during the later stages of denoising. BlockDance caches and reuses\nthese highly similar features to mitigate redundant computation, thereby\naccelerating DiTs while maximizing consistency with the generated results of\nthe original model. Furthermore, considering the diversity of generated content\nand the varying distributions of redundant features, we introduce\nBlockDance-Ada, a lightweight decision-making network tailored for\ninstance-specific acceleration. BlockDance-Ada dynamically allocates resources\nand provides superior content quality. Both BlockDance and BlockDance-Ada have\nproven effective across various generation tasks and models, achieving\naccelerations between 25% and 50% while maintaining generation quality."
                },
                "authors": [
                    {
                        "name": "Hui Zhang"
                    },
                    {
                        "name": "Tingwei Gao"
                    },
                    {
                        "name": "Jie Shao"
                    },
                    {
                        "name": "Zuxuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zuxuan Wu"
                },
                "author": "Zuxuan Wu",
                "arxiv_comment": "Accepted by CVPR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15927v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15927v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18921v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18921v2",
                "updated": "2025-03-20T05:23:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    20,
                    5,
                    23,
                    42,
                    3,
                    79,
                    0
                ],
                "published": "2024-07-09T13:47:05Z",
                "published_parsed": [
                    2024,
                    7,
                    9,
                    13,
                    47,
                    5,
                    1,
                    191,
                    0
                ],
                "title": "Mobile Edge Intelligence for Large Language Models: A Contemporary\n  Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile Edge Intelligence for Large Language Models: A Contemporary\n  Survey"
                },
                "summary": "On-device large language models (LLMs), referring to running LLMs on edge\ndevices, have raised considerable interest since they are more cost-effective,\nlatency-efficient, and privacy-preserving compared with the cloud paradigm.\nNonetheless, the performance of on-device LLMs is intrinsically constrained by\nresource limitations on edge devices. Sitting between cloud and on-device AI,\nmobile edge intelligence (MEI) presents a viable solution by provisioning AI\ncapabilities at the edge of mobile networks, enabling end users to offload\nheavy AI computation to capable edge servers nearby. This article provides a\ncontemporary survey on harnessing MEI for LLMs. We begin by illustrating\nseveral killer applications to demonstrate the urgent need for deploying LLMs\nat the network edge. Next, we present the preliminaries of LLMs and MEI,\nfollowed by resource-efficient LLM techniques. We then present an architectural\noverview of MEI for LLMs (MEI4LLM), outlining its core components and how it\nsupports the deployment of LLMs. Subsequently, we delve into various aspects of\nMEI4LLM, extensively covering edge LLM caching and delivery, edge LLM training,\nand edge LLM inference. Finally, we identify future research opportunities. We\nhope this article inspires researchers in the field to leverage mobile edge\ncomputing to facilitate LLM deployment, thereby unleashing the potential of\nLLMs across various privacy- and delay-sensitive applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On-device large language models (LLMs), referring to running LLMs on edge\ndevices, have raised considerable interest since they are more cost-effective,\nlatency-efficient, and privacy-preserving compared with the cloud paradigm.\nNonetheless, the performance of on-device LLMs is intrinsically constrained by\nresource limitations on edge devices. Sitting between cloud and on-device AI,\nmobile edge intelligence (MEI) presents a viable solution by provisioning AI\ncapabilities at the edge of mobile networks, enabling end users to offload\nheavy AI computation to capable edge servers nearby. This article provides a\ncontemporary survey on harnessing MEI for LLMs. We begin by illustrating\nseveral killer applications to demonstrate the urgent need for deploying LLMs\nat the network edge. Next, we present the preliminaries of LLMs and MEI,\nfollowed by resource-efficient LLM techniques. We then present an architectural\noverview of MEI for LLMs (MEI4LLM), outlining its core components and how it\nsupports the deployment of LLMs. Subsequently, we delve into various aspects of\nMEI4LLM, extensively covering edge LLM caching and delivery, edge LLM training,\nand edge LLM inference. Finally, we identify future research opportunities. We\nhope this article inspires researchers in the field to leverage mobile edge\ncomputing to facilitate LLM deployment, thereby unleashing the potential of\nLLMs across various privacy- and delay-sensitive applications."
                },
                "authors": [
                    {
                        "name": "Guanqiao Qu"
                    },
                    {
                        "name": "Qiyuan Chen"
                    },
                    {
                        "name": "Wei Wei"
                    },
                    {
                        "name": "Zheng Lin"
                    },
                    {
                        "name": "Xianhao Chen"
                    },
                    {
                        "name": "Kaibin Huang"
                    }
                ],
                "author_detail": {
                    "name": "Kaibin Huang"
                },
                "author": "Kaibin Huang",
                "arxiv_comment": "42 pages, 17 figures. This paper has been accepted by IEEE\n  Communications Surveys & Tutorials",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18921v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18921v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15908v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15908v2",
                "updated": "2025-03-19T10:19:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    10,
                    19,
                    30,
                    2,
                    78,
                    0
                ],
                "published": "2024-10-21T11:29:49Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    11,
                    29,
                    49,
                    0,
                    295,
                    0
                ],
                "title": "Formalising CXL Cache Coherence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formalising CXL Cache Coherence"
                },
                "summary": "We report our experience formally modelling and verifying CXL.cache, the\ninter-device cache coherence protocol of the Compute Express Link standard. We\nhave used the Isabelle proof assistant to create a formal model for CXL.cache\nbased on the prose English specification. This led to us identifying and\nproposing fixes to several problems we identified as unclear, ambiguous or\ninaccurate, some of which could lead to incoherence if left unfixed. Nearly all\nour issues and proposed fixes have been confirmed and tentatively accepted by\nthe CXL consortium for adoption, save for one which is still under discussion.\nTo validate the faithfulness of our model we performed scenario verification of\nessential restrictions such as \"Snoop-pushes-GO\", and produced a fully\nmechanised proof of a coherence property of the model. The considerable size of\nthis proof, comprising tens of thousands of lemmas, prompted us to develop new\nproof automation tools, which we have made available for other Isabelle users\nworking with similarly cumbersome proofs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report our experience formally modelling and verifying CXL.cache, the\ninter-device cache coherence protocol of the Compute Express Link standard. We\nhave used the Isabelle proof assistant to create a formal model for CXL.cache\nbased on the prose English specification. This led to us identifying and\nproposing fixes to several problems we identified as unclear, ambiguous or\ninaccurate, some of which could lead to incoherence if left unfixed. Nearly all\nour issues and proposed fixes have been confirmed and tentatively accepted by\nthe CXL consortium for adoption, save for one which is still under discussion.\nTo validate the faithfulness of our model we performed scenario verification of\nessential restrictions such as \"Snoop-pushes-GO\", and produced a fully\nmechanised proof of a coherence property of the model. The considerable size of\nthis proof, comprising tens of thousands of lemmas, prompted us to develop new\nproof automation tools, which we have made available for other Isabelle users\nworking with similarly cumbersome proofs."
                },
                "authors": [
                    {
                        "name": "Chengsong Tan"
                    },
                    {
                        "name": "Alastair F. Donaldson"
                    },
                    {
                        "name": "John Wickerson"
                    }
                ],
                "author_detail": {
                    "name": "John Wickerson"
                },
                "author": "John Wickerson",
                "arxiv_doi": "10.1145/3676641.3715999",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676641.3715999",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.15908v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15908v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14 pages",
                "arxiv_journal_ref": "Proceedings of the 30th ACM International Conference on\n  Architectural Support for Programming Languages and Operating Systems, ASPLOS\n  2025, Rotterdam, Netherlands",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68 (Primary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.1; F.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14881v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14881v1",
                "updated": "2025-03-19T04:18:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    4,
                    18,
                    57,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T04:18:57Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    4,
                    18,
                    57,
                    2,
                    78,
                    0
                ],
                "title": "Exploring the Limits of KV Cache Compression in Visual Autoregressive\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Limits of KV Cache Compression in Visual Autoregressive\n  Transformers"
                },
                "summary": "A fundamental challenge in Visual Autoregressive models is the substantial\nmemory overhead required during inference to store previously generated\nrepresentations. Despite various attempts to mitigate this issue through\ncompression techniques, prior works have not explicitly formalized the problem\nof KV-cache compression in this context. In this work, we take the first step\nin formally defining the KV-cache compression problem for Visual Autoregressive\ntransformers. We then establish a fundamental negative result, proving that any\nmechanism for sequential visual token generation under attention-based\narchitectures must use at least $\\Omega(n^2 d)$ memory, when $d = \\Omega(\\log\nn)$, where $n$ is the number of tokens generated and $d$ is the embedding\ndimensionality. This result demonstrates that achieving truly sub-quadratic\nmemory usage is impossible without additional structural constraints. Our proof\nis constructed via a reduction from a computational lower bound problem,\nleveraging randomized embedding techniques inspired by dimensionality reduction\nprinciples. Finally, we discuss how sparsity priors on visual representations\ncan influence memory efficiency, presenting both impossibility results and\npotential directions for mitigating memory overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A fundamental challenge in Visual Autoregressive models is the substantial\nmemory overhead required during inference to store previously generated\nrepresentations. Despite various attempts to mitigate this issue through\ncompression techniques, prior works have not explicitly formalized the problem\nof KV-cache compression in this context. In this work, we take the first step\nin formally defining the KV-cache compression problem for Visual Autoregressive\ntransformers. We then establish a fundamental negative result, proving that any\nmechanism for sequential visual token generation under attention-based\narchitectures must use at least $\\Omega(n^2 d)$ memory, when $d = \\Omega(\\log\nn)$, where $n$ is the number of tokens generated and $d$ is the embedding\ndimensionality. This result demonstrates that achieving truly sub-quadratic\nmemory usage is impossible without additional structural constraints. Our proof\nis constructed via a reduction from a computational lower bound problem,\nleveraging randomized embedding techniques inspired by dimensionality reduction\nprinciples. Finally, we discuss how sparsity priors on visual representations\ncan influence memory efficiency, presenting both impossibility results and\npotential directions for mitigating memory overhead."
                },
                "authors": [
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Xiaoyu Li"
                    },
                    {
                        "name": "Yekun Ke"
                    },
                    {
                        "name": "Yingyu Liang"
                    },
                    {
                        "name": "Zhenmei Shi"
                    },
                    {
                        "name": "Zhao Song"
                    }
                ],
                "author_detail": {
                    "name": "Zhao Song"
                },
                "author": "Zhao Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14881v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14881v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14805v1",
                "updated": "2025-03-19T00:30:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    19,
                    0,
                    30,
                    43,
                    2,
                    78,
                    0
                ],
                "published": "2025-03-19T00:30:43Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    0,
                    30,
                    43,
                    2,
                    78,
                    0
                ],
                "title": "Degradation of 2.4-kV $Ga_{2}O_{3}$ Schottky Barrier Diode at High\n  Temperatures up to 500 °C",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Degradation of 2.4-kV $Ga_{2}O_{3}$ Schottky Barrier Diode at High\n  Temperatures up to 500 °C"
                },
                "summary": "Ga2O3 Schottky barrier diodes featuring a field plate and a composite\nSiO2/SiNx dielectric layer beneath the field plate were fabricated, achieving a\nbreakdown voltage of 2.4 kV at room temperature. Electrical performance and\ndegradation were analyzed via I-V and C-V measurements from 25 {\\deg}C to 500\n{\\deg}C, revealing temperature-dependent transport, interface stability, and\ndevice stability. Upon returning to room temperature, the diodes exhibited\nnearly unchanged forward characteristics, while the breakdown voltage declined\nsignificantly from 2.4 kV to 700 V. This behavior indicates a\ntemperature-induced reduction in the barrier height. Detailed analysis revealed\nthat variable range hopping (VRH) dominated the leakage mechanism at moderate\ntemperatures, while thermal emission (TE) became increasingly significant at\ntemperatures exceeding 400 {\\deg}C.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ga2O3 Schottky barrier diodes featuring a field plate and a composite\nSiO2/SiNx dielectric layer beneath the field plate were fabricated, achieving a\nbreakdown voltage of 2.4 kV at room temperature. Electrical performance and\ndegradation were analyzed via I-V and C-V measurements from 25 {\\deg}C to 500\n{\\deg}C, revealing temperature-dependent transport, interface stability, and\ndevice stability. Upon returning to room temperature, the diodes exhibited\nnearly unchanged forward characteristics, while the breakdown voltage declined\nsignificantly from 2.4 kV to 700 V. This behavior indicates a\ntemperature-induced reduction in the barrier height. Detailed analysis revealed\nthat variable range hopping (VRH) dominated the leakage mechanism at moderate\ntemperatures, while thermal emission (TE) became increasingly significant at\ntemperatures exceeding 400 {\\deg}C."
                },
                "authors": [
                    {
                        "name": "Hunter Ellis"
                    },
                    {
                        "name": "Wei Jia"
                    },
                    {
                        "name": "Imteaz Rahaman"
                    },
                    {
                        "name": "Apostoli Hillas"
                    },
                    {
                        "name": "Botong Li"
                    },
                    {
                        "name": "Michael A. Scarpulla"
                    },
                    {
                        "name": "Berardi Sensale Rodriguez"
                    },
                    {
                        "name": "Kai Fu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Fu"
                },
                "author": "Kai Fu",
                "arxiv_comment": "7 Pages, 9 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14708v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14708v1",
                "updated": "2025-03-18T20:16:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    20,
                    16,
                    50,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T20:16:50Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    20,
                    16,
                    50,
                    1,
                    77,
                    0
                ],
                "title": "NeCTAr: A Heterogeneous RISC-V SoC for Language Model Inference in Intel\n  16",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NeCTAr: A Heterogeneous RISC-V SoC for Language Model Inference in Intel\n  16"
                },
                "summary": "This paper introduces NeCTAr (Near-Cache Transformer Accelerator), a 16nm\nheterogeneous multicore RISC-V SoC for sparse and dense machine learning\nkernels with both near-core and near-memory accelerators. A prototype chip runs\nat 400MHz at 0.85V and performs matrix-vector multiplications with 109 GOPs/W.\nThe effectiveness of the design is demonstrated by running inference on a\nsparse language model, ReLU-Llama.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces NeCTAr (Near-Cache Transformer Accelerator), a 16nm\nheterogeneous multicore RISC-V SoC for sparse and dense machine learning\nkernels with both near-core and near-memory accelerators. A prototype chip runs\nat 400MHz at 0.85V and performs matrix-vector multiplications with 109 GOPs/W.\nThe effectiveness of the design is demonstrated by running inference on a\nsparse language model, ReLU-Llama."
                },
                "authors": [
                    {
                        "name": "Viansa Schmulbach"
                    },
                    {
                        "name": "Jason Kim"
                    },
                    {
                        "name": "Ethan Gao"
                    },
                    {
                        "name": "Lucy Revina"
                    },
                    {
                        "name": "Nikhil Jha"
                    },
                    {
                        "name": "Ethan Wu"
                    },
                    {
                        "name": "Borivoje Nikolic"
                    }
                ],
                "author_detail": {
                    "name": "Borivoje Nikolic"
                },
                "author": "Borivoje Nikolic",
                "arxiv_doi": "10.1109/HCS61935.2024.10665203",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/HCS61935.2024.10665203",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.14708v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14708v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14647v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14647v1",
                "updated": "2025-03-18T18:52:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    18,
                    52,
                    3,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-18T18:52:03Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    18,
                    52,
                    3,
                    1,
                    77,
                    0
                ],
                "title": "Towards More Economical Context-Augmented LLM Generation by Reusing\n  Stored KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards More Economical Context-Augmented LLM Generation by Reusing\n  Stored KV Cache"
                },
                "summary": "Across large language model (LLM) applications, we observe an emerging trend\nfor reusing KV caches to save the prefill delays of processing repeated input\ntexts in different LLM inputs. This has led to a broad design space, including\ncolocating stored KV caches with (or close to) GPUs to various KV cache\ncompression. However, a key question remains unanswered: can these delay\nreductions also be economically favorable? Specifically, we ask whether a\ndeveloper can use public cloud services to store precomputed KV caches and\nreuse them to save delay without incurring more costs in terms of compute,\nstorage, and network. To answer this question, we propose an validated\nanalytical model for the cloud cost (in compute, storage, and network) of\nstoring and reusing KV caches based on various workload parameters, such as\nreuse frequency, generated text lengths, model sizes, etc. Preliminary results\nshow that KV cache reusing is able to save both delay and cloud cost across a\nrange of workloads with long context. And we call more efforts on building more\neconomical context augmented LLM by KV cache reusing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Across large language model (LLM) applications, we observe an emerging trend\nfor reusing KV caches to save the prefill delays of processing repeated input\ntexts in different LLM inputs. This has led to a broad design space, including\ncolocating stored KV caches with (or close to) GPUs to various KV cache\ncompression. However, a key question remains unanswered: can these delay\nreductions also be economically favorable? Specifically, we ask whether a\ndeveloper can use public cloud services to store precomputed KV caches and\nreuse them to save delay without incurring more costs in terms of compute,\nstorage, and network. To answer this question, we propose an validated\nanalytical model for the cloud cost (in compute, storage, and network) of\nstoring and reusing KV caches based on various workload parameters, such as\nreuse frequency, generated text lengths, model sizes, etc. Preliminary results\nshow that KV cache reusing is able to save both delay and cloud cost across a\nrange of workloads with long context. And we call more efforts on building more\neconomical context augmented LLM by KV cache reusing."
                },
                "authors": [
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14647v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14647v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08640v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08640v2",
                "updated": "2025-03-18T17:13:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    17,
                    13,
                    42,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-11T17:30:58Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    17,
                    30,
                    58,
                    1,
                    70,
                    0
                ],
                "title": "Efficient Many-Shot In-Context Learning with Dynamic Block-Sparse\n  Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Many-Shot In-Context Learning with Dynamic Block-Sparse\n  Attention"
                },
                "summary": "Many-shot in-context learning has recently shown promise as an alternative to\nfinetuning, with the major advantage that the same model can be served for\nmultiple tasks. However, this shifts the computational burden from\ntraining-time to inference-time, making deployment of many-shot ICL challenging\nto justify in-practice. This cost is further increased if a custom\ndemonstration set is retrieved for each inference example. We present Dynamic\nBlock-Sparse Attention, a training-free framework for retrieval-based many-shot\nin-context learning. By combining carefully designed block-sparse attention and\nretrieval of cached groups of demonstrations, we achieve comparable per-example\nlatency to finetuning while maintaining on average >95% of the best method's\naccuracy across strong ICL and finetuning baselines. We hope that this will\nfurther enable the deployment of many-shot ICL at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many-shot in-context learning has recently shown promise as an alternative to\nfinetuning, with the major advantage that the same model can be served for\nmultiple tasks. However, this shifts the computational burden from\ntraining-time to inference-time, making deployment of many-shot ICL challenging\nto justify in-practice. This cost is further increased if a custom\ndemonstration set is retrieved for each inference example. We present Dynamic\nBlock-Sparse Attention, a training-free framework for retrieval-based many-shot\nin-context learning. By combining carefully designed block-sparse attention and\nretrieval of cached groups of demonstrations, we achieve comparable per-example\nlatency to finetuning while maintaining on average >95% of the best method's\naccuracy across strong ICL and finetuning baselines. We hope that this will\nfurther enable the deployment of many-shot ICL at scale."
                },
                "authors": [
                    {
                        "name": "Emily Xiao"
                    },
                    {
                        "name": "Chin-Jou Li"
                    },
                    {
                        "name": "Yilin Zhang"
                    },
                    {
                        "name": "Graham Neubig"
                    },
                    {
                        "name": "Amanda Bertsch"
                    }
                ],
                "author_detail": {
                    "name": "Amanda Bertsch"
                },
                "author": "Amanda Bertsch",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08640v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08640v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09573v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09573v2",
                "updated": "2025-03-18T15:58:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    15,
                    58,
                    18,
                    1,
                    77,
                    0
                ],
                "published": "2025-03-12T17:43:40Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    17,
                    43,
                    40,
                    2,
                    71,
                    0
                ],
                "title": "Block Diffusion: Interpolating Between Autoregressive and Diffusion\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block Diffusion: Interpolating Between Autoregressive and Diffusion\n  Language Models"
                },
                "summary": "Diffusion language models offer unique benefits over autoregressive models\ndue to their potential for parallelized generation and controllability, yet\nthey lag in likelihood modeling and are limited to fixed-length generation. In\nthis work, we introduce a class of block diffusion language models that\ninterpolate between discrete denoising diffusion and autoregressive models.\nBlock diffusion overcomes key limitations of both approaches by supporting\nflexible-length generation and improving inference efficiency with KV caching\nand parallel token sampling. We propose a recipe for building effective block\ndiffusion models that includes an efficient training algorithm, estimators of\ngradient variance, and data-driven noise schedules to minimize the variance.\nBlock diffusion sets a new state-of-the-art performance among diffusion models\non language modeling benchmarks and enables generation of arbitrary-length\nsequences. We provide the code, along with the model weights and blog post on\nthe project page: https://m-arriola.com/bd3lms/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion language models offer unique benefits over autoregressive models\ndue to their potential for parallelized generation and controllability, yet\nthey lag in likelihood modeling and are limited to fixed-length generation. In\nthis work, we introduce a class of block diffusion language models that\ninterpolate between discrete denoising diffusion and autoregressive models.\nBlock diffusion overcomes key limitations of both approaches by supporting\nflexible-length generation and improving inference efficiency with KV caching\nand parallel token sampling. We propose a recipe for building effective block\ndiffusion models that includes an efficient training algorithm, estimators of\ngradient variance, and data-driven noise schedules to minimize the variance.\nBlock diffusion sets a new state-of-the-art performance among diffusion models\non language modeling benchmarks and enables generation of arbitrary-length\nsequences. We provide the code, along with the model weights and blog post on\nthe project page: https://m-arriola.com/bd3lms/"
                },
                "authors": [
                    {
                        "name": "Marianne Arriola"
                    },
                    {
                        "name": "Aaron Gokaslan"
                    },
                    {
                        "name": "Justin T Chiu"
                    },
                    {
                        "name": "Zhihan Yang"
                    },
                    {
                        "name": "Zhixuan Qi"
                    },
                    {
                        "name": "Jiaqi Han"
                    },
                    {
                        "name": "Subham Sekhar Sahoo"
                    },
                    {
                        "name": "Volodymyr Kuleshov"
                    }
                ],
                "author_detail": {
                    "name": "Volodymyr Kuleshov"
                },
                "author": "Volodymyr Kuleshov",
                "arxiv_comment": "ICLR 2025 Oral. We provide the code at\n  https://github.com/kuleshov-group/bd3lms",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09573v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09573v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18753v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18753v2",
                "updated": "2025-03-18T09:43:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    9,
                    43,
                    33,
                    1,
                    77,
                    0
                ],
                "published": "2024-07-26T14:08:53Z",
                "published_parsed": [
                    2024,
                    7,
                    26,
                    14,
                    8,
                    53,
                    4,
                    208,
                    0
                ],
                "title": "Suffixient Arrays: a New Efficient Suffix Array Compression Technique",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Suffixient Arrays: a New Efficient Suffix Array Compression Technique"
                },
                "summary": "The Suffix Array is a classic text index enabling on-line pattern matching\nqueries via simple binary search. The main drawback of the Suffix Array is that\nit takes linear space in the text's length, even if the text itself is\nextremely compressible. Several works in the literature showed that the Suffix\nArray can be compressed, but they all rely on complex succinct data structures\nwhich in practice tend to exhibit poor cache locality and thus significantly\nslow down queries. In this paper, we propose a new simple and very efficient\nsolution to this problem by presenting the \\emph{Suffixient Array}: a tiny\nsubset of the Suffix Array \\emph{sufficient} to locate on-line one pattern\noccurrence (in general, all its Maximal Exact Matches) via binary search,\nprovided that random access to the text is available. We prove that: (i) the\nSuffixient Array length $\\chi$ is a strong repetitiveness measure, (ii) unlike\nmost existing repetition-aware indexes such as the $r$-index, our new index is\nefficient in the I/O model, and (iii) Suffixient Arrays can be computed in\nlinear time and compressed working space. We show experimentally that, when\nusing well-established compressed random access data structures on repetitive\ncollections, the Suffixient Array $\\SuA$ is \\emph{simultaneously} (i) faster\nand orders of magnitude smaller than the Suffix Array $\\SA$ and (ii) smaller\nand \\emph{one to two orders of magnitude faster} than the $r$-index. With an\naverage pattern matching query time as low as 3.5 ns per character, our new\nindex gets very close to the ultimate lower bound: the RAM throughput of our\nworkstation (1.18 ns per character).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Suffix Array is a classic text index enabling on-line pattern matching\nqueries via simple binary search. The main drawback of the Suffix Array is that\nit takes linear space in the text's length, even if the text itself is\nextremely compressible. Several works in the literature showed that the Suffix\nArray can be compressed, but they all rely on complex succinct data structures\nwhich in practice tend to exhibit poor cache locality and thus significantly\nslow down queries. In this paper, we propose a new simple and very efficient\nsolution to this problem by presenting the \\emph{Suffixient Array}: a tiny\nsubset of the Suffix Array \\emph{sufficient} to locate on-line one pattern\noccurrence (in general, all its Maximal Exact Matches) via binary search,\nprovided that random access to the text is available. We prove that: (i) the\nSuffixient Array length $\\chi$ is a strong repetitiveness measure, (ii) unlike\nmost existing repetition-aware indexes such as the $r$-index, our new index is\nefficient in the I/O model, and (iii) Suffixient Arrays can be computed in\nlinear time and compressed working space. We show experimentally that, when\nusing well-established compressed random access data structures on repetitive\ncollections, the Suffixient Array $\\SuA$ is \\emph{simultaneously} (i) faster\nand orders of magnitude smaller than the Suffix Array $\\SA$ and (ii) smaller\nand \\emph{one to two orders of magnitude faster} than the $r$-index. With an\naverage pattern matching query time as low as 3.5 ns per character, our new\nindex gets very close to the ultimate lower bound: the RAM throughput of our\nworkstation (1.18 ns per character)."
                },
                "authors": [
                    {
                        "name": "Davide Cenzato"
                    },
                    {
                        "name": "Lore Depuydt"
                    },
                    {
                        "name": "Travis Gagie"
                    },
                    {
                        "name": "Sung-Hwan Kim"
                    },
                    {
                        "name": "Giovanni Manzini"
                    },
                    {
                        "name": "Francisco Olivares"
                    },
                    {
                        "name": "Nicola Prezza"
                    }
                ],
                "author_detail": {
                    "name": "Nicola Prezza"
                },
                "author": "Nicola Prezza",
                "arxiv_comment": "40 pages, 7 figure, 1 table and 7 pseudocodes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18753v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18753v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13145v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13145v2",
                "updated": "2025-03-18T07:02:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    7,
                    2,
                    33,
                    1,
                    77,
                    0
                ],
                "published": "2025-02-18T18:59:57Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    59,
                    57,
                    1,
                    49,
                    0
                ],
                "title": "Multimodal Mamba: Decoder-only Multimodal State Space Model via\n  Quadratic to Linear Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Mamba: Decoder-only Multimodal State Space Model via\n  Quadratic to Linear Distillation"
                },
                "summary": "Recent Multimodal Large Language Models (MLLMs) have achieved remarkable\nperformance but face deployment challenges due to their quadratic computational\ncomplexity, growing Key-Value cache requirements, and reliance on separate\nvision encoders. We propose mmMamba, a framework for developing\nlinear-complexity native multimodal state space models through progressive\ndistillation from existing MLLMs using moderate academic computational\nresources. Our approach enables the direct conversion of trained decoder-only\nMLLMs to linear-complexity architectures without requiring pre-trained\nRNN-based LLM or vision encoders. We propose an seeding strategy to carve Mamba\nfrom trained Transformer and a three-stage distillation recipe, which can\neffectively transfer the knowledge from Transformer to Mamba while preserving\nmultimodal capabilities. Our method also supports flexible hybrid architectures\nthat combine Transformer and Mamba layers for customizable\nefficiency-performance trade-offs. Distilled from the Transformer-based\ndecoder-only HoVLE, mmMamba-linear achieves competitive performance against\nexisting linear and quadratic-complexity VLMs, while mmMamba-hybrid further\nimproves performance significantly, approaching HoVLE's capabilities. At 103K\ntokens, mmMamba-linear demonstrates 20.6$\\times$ speedup and 75.8% GPU memory\nreduction compared to HoVLE, while mmMamba-hybrid achieves 13.5$\\times$ speedup\nand 60.2% memory savings. Code and models are released at\nhttps://github.com/hustvl/mmMamba",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Multimodal Large Language Models (MLLMs) have achieved remarkable\nperformance but face deployment challenges due to their quadratic computational\ncomplexity, growing Key-Value cache requirements, and reliance on separate\nvision encoders. We propose mmMamba, a framework for developing\nlinear-complexity native multimodal state space models through progressive\ndistillation from existing MLLMs using moderate academic computational\nresources. Our approach enables the direct conversion of trained decoder-only\nMLLMs to linear-complexity architectures without requiring pre-trained\nRNN-based LLM or vision encoders. We propose an seeding strategy to carve Mamba\nfrom trained Transformer and a three-stage distillation recipe, which can\neffectively transfer the knowledge from Transformer to Mamba while preserving\nmultimodal capabilities. Our method also supports flexible hybrid architectures\nthat combine Transformer and Mamba layers for customizable\nefficiency-performance trade-offs. Distilled from the Transformer-based\ndecoder-only HoVLE, mmMamba-linear achieves competitive performance against\nexisting linear and quadratic-complexity VLMs, while mmMamba-hybrid further\nimproves performance significantly, approaching HoVLE's capabilities. At 103K\ntokens, mmMamba-linear demonstrates 20.6$\\times$ speedup and 75.8% GPU memory\nreduction compared to HoVLE, while mmMamba-hybrid achieves 13.5$\\times$ speedup\nand 60.2% memory savings. Code and models are released at\nhttps://github.com/hustvl/mmMamba"
                },
                "authors": [
                    {
                        "name": "Bencheng Liao"
                    },
                    {
                        "name": "Hongyuan Tao"
                    },
                    {
                        "name": "Qian Zhang"
                    },
                    {
                        "name": "Tianheng Cheng"
                    },
                    {
                        "name": "Yingyue Li"
                    },
                    {
                        "name": "Haoran Yin"
                    },
                    {
                        "name": "Wenyu Liu"
                    },
                    {
                        "name": "Xinggang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinggang Wang"
                },
                "author": "Xinggang Wang",
                "arxiv_comment": "Code and model are available at https://github.com/hustvl/mmMamba",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13145v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13145v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19108v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19108v2",
                "updated": "2025-03-18T04:49:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    4,
                    49,
                    23,
                    1,
                    77,
                    0
                ],
                "published": "2024-11-28T12:50:05Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    12,
                    50,
                    5,
                    3,
                    333,
                    0
                ],
                "title": "Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model"
                },
                "summary": "As a fundamental backbone for video generation, diffusion models are\nchallenged by low inference speed due to the sequential nature of denoising.\nPrevious methods speed up the models by caching and reusing model outputs at\nuniformly selected timesteps. However, such a strategy neglects the fact that\ndifferences among model outputs are not uniform across timesteps, which hinders\nselecting the appropriate model outputs to cache, leading to a poor balance\nbetween inference efficiency and visual quality. In this study, we introduce\nTimestep Embedding Aware Cache (TeaCache), a training-free caching approach\nthat estimates and leverages the fluctuating differences among model outputs\nacross timesteps. Rather than directly using the time-consuming model outputs,\nTeaCache focuses on model inputs, which have a strong correlation with the\nmodeloutputs while incurring negligible computational cost. TeaCache first\nmodulates the noisy inputs using the timestep embeddings to ensure their\ndifferences better approximating those of model outputs. TeaCache then\nintroduces a rescaling strategy to refine the estimated differences and\nutilizes them to indicate output caching. Experiments show that TeaCache\nachieves up to 4.41x acceleration over Open-Sora-Plan with negligible (-0.07%\nVbench score) degradation of visual quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a fundamental backbone for video generation, diffusion models are\nchallenged by low inference speed due to the sequential nature of denoising.\nPrevious methods speed up the models by caching and reusing model outputs at\nuniformly selected timesteps. However, such a strategy neglects the fact that\ndifferences among model outputs are not uniform across timesteps, which hinders\nselecting the appropriate model outputs to cache, leading to a poor balance\nbetween inference efficiency and visual quality. In this study, we introduce\nTimestep Embedding Aware Cache (TeaCache), a training-free caching approach\nthat estimates and leverages the fluctuating differences among model outputs\nacross timesteps. Rather than directly using the time-consuming model outputs,\nTeaCache focuses on model inputs, which have a strong correlation with the\nmodeloutputs while incurring negligible computational cost. TeaCache first\nmodulates the noisy inputs using the timestep embeddings to ensure their\ndifferences better approximating those of model outputs. TeaCache then\nintroduces a rescaling strategy to refine the estimated differences and\nutilizes them to indicate output caching. Experiments show that TeaCache\nachieves up to 4.41x acceleration over Open-Sora-Plan with negligible (-0.07%\nVbench score) degradation of visual quality."
                },
                "authors": [
                    {
                        "name": "Feng Liu"
                    },
                    {
                        "name": "Shiwei Zhang"
                    },
                    {
                        "name": "Xiaofeng Wang"
                    },
                    {
                        "name": "Yujie Wei"
                    },
                    {
                        "name": "Haonan Qiu"
                    },
                    {
                        "name": "Yuzhong Zhao"
                    },
                    {
                        "name": "Yingya Zhang"
                    },
                    {
                        "name": "Qixiang Ye"
                    },
                    {
                        "name": "Fang Wan"
                    }
                ],
                "author_detail": {
                    "name": "Fang Wan"
                },
                "author": "Fang Wan",
                "arxiv_comment": "Accepted in CVPR 2025. Project: https://liewfeng.github.io/TeaCache",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19108v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19108v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10511v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10511v3",
                "updated": "2025-03-18T01:58:36Z",
                "updated_parsed": [
                    2025,
                    3,
                    18,
                    1,
                    58,
                    36,
                    1,
                    77,
                    0
                ],
                "published": "2024-06-15T05:28:55Z",
                "published_parsed": [
                    2024,
                    6,
                    15,
                    5,
                    28,
                    55,
                    5,
                    167,
                    0
                ],
                "title": "Efficient Hardware Accelerator Based on Medium Granularity Dataflow for\n  SpTRSV",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Hardware Accelerator Based on Medium Granularity Dataflow for\n  SpTRSV"
                },
                "summary": "Sparse triangular solve (SpTRSV) is widely used in various domains. Numerous\nstudies have been conducted using CPUs, GPUs, and specific hardware\naccelerators, where dataflows can be categorized into coarse and fine\ngranularity. Coarse dataflows offer good spatial locality but suffer from low\nparallelism, while fine dataflows provide high parallelism but disrupt the\nspatial structure, leading to increased nodes and poor data reuse. This paper\nproposes a novel hardware accelerator for SpTRSV or SpTRSV-like DAGs. The\naccelerator implements a medium granularity dataflow through hardware-software\ncodesign and achieves both excellent spatial locality and high parallelism.\nAdditionally, a partial sum caching mechanism is introduced to reduce the\nblocking frequency of processing elements (PEs), and a reordering algorithm of\nintra-node edges computation is developed to enhance data reuse. Experimental\nresults on 245 benchmarks with node counts reaching up to 85,392 demonstrate\nthat this work achieves average performance improvements of 7.0$\\times$ (up to\n27.8$\\times$) over CPUs and 5.8$\\times$ (up to 98.8$\\times$) over GPUs.\nCompared to the state-of-the-art technique (DPU-v2), this work shows a\n2.5$\\times$ (up to 5.9$\\times$) average performance improvement and 1.7$\\times$\n(up to 4.1$\\times$) average energy efficiency enhancement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse triangular solve (SpTRSV) is widely used in various domains. Numerous\nstudies have been conducted using CPUs, GPUs, and specific hardware\naccelerators, where dataflows can be categorized into coarse and fine\ngranularity. Coarse dataflows offer good spatial locality but suffer from low\nparallelism, while fine dataflows provide high parallelism but disrupt the\nspatial structure, leading to increased nodes and poor data reuse. This paper\nproposes a novel hardware accelerator for SpTRSV or SpTRSV-like DAGs. The\naccelerator implements a medium granularity dataflow through hardware-software\ncodesign and achieves both excellent spatial locality and high parallelism.\nAdditionally, a partial sum caching mechanism is introduced to reduce the\nblocking frequency of processing elements (PEs), and a reordering algorithm of\nintra-node edges computation is developed to enhance data reuse. Experimental\nresults on 245 benchmarks with node counts reaching up to 85,392 demonstrate\nthat this work achieves average performance improvements of 7.0$\\times$ (up to\n27.8$\\times$) over CPUs and 5.8$\\times$ (up to 98.8$\\times$) over GPUs.\nCompared to the state-of-the-art technique (DPU-v2), this work shows a\n2.5$\\times$ (up to 5.9$\\times$) average performance improvement and 1.7$\\times$\n(up to 4.1$\\times$) average energy efficiency enhancement."
                },
                "authors": [
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Xiaofeng Yang"
                    },
                    {
                        "name": "Shengli Lu"
                    }
                ],
                "author_detail": {
                    "name": "Shengli Lu"
                },
                "author": "Shengli Lu",
                "arxiv_doi": "10.1109/TVLSI.2024.3497166",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TVLSI.2024.3497166",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.10511v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10511v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "IEEE Trans. Very Large Scale Integr. (VLSI) Syst. 33 (2025)\n  807-820",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13737v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13737v1",
                "updated": "2025-03-17T21:47:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    21,
                    47,
                    43,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T21:47:43Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    21,
                    47,
                    43,
                    0,
                    76,
                    0
                ],
                "title": "AccelGen: Heterogeneous SLO-Guaranteed High-Throughput LLM Inference\n  Serving for Diverse Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AccelGen: Heterogeneous SLO-Guaranteed High-Throughput LLM Inference\n  Serving for Diverse Applications"
                },
                "summary": "In this paper, we consider a mixed-prompt scenario for a large language model\n(LLM) inference serving system that supports diverse applications with both\nshort prompts and long prompts and heterogeneous SLOs for iteration time. To\nimprove throughput when handling long prompts, previous research introduces a\nchunking method, but has not addressed heterogeneous SLOs. To address the\nlimitation, we propose AccelGen, a high-throughput LLM inference serving system\nwith heterogeneous SLO guarantees for diverse applications. AccelGen introduces\nfour core components: (1) SLO-guaranteed dynamic chunking, which dynamically\nadjusts chunk sizes to maximize GPU compute utilization while meeting\niteration-level SLOs; (2) Iteration-level SLO-based task prioritization, which\nprioritizes tight-SLO requests and batches requests with similar SLOs; (3)\nMulti-resource-aware batching, which selects queued requests to maximize the\nutilizations of both GPU compute resource and key-value cache (KVC).\nTrace-driven real experiments demonstrate that AccelGen achieves 1.42-11.21X\nhigher throughput, 1.43-13.71X higher goodput, 37-90% higher SLO attainment,\nand 1.61-12.22X lower response latency compared to the state-of-the-art\napproaches. It achieves performance near the Oracle, which optimally maximizes\ngoodput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we consider a mixed-prompt scenario for a large language model\n(LLM) inference serving system that supports diverse applications with both\nshort prompts and long prompts and heterogeneous SLOs for iteration time. To\nimprove throughput when handling long prompts, previous research introduces a\nchunking method, but has not addressed heterogeneous SLOs. To address the\nlimitation, we propose AccelGen, a high-throughput LLM inference serving system\nwith heterogeneous SLO guarantees for diverse applications. AccelGen introduces\nfour core components: (1) SLO-guaranteed dynamic chunking, which dynamically\nadjusts chunk sizes to maximize GPU compute utilization while meeting\niteration-level SLOs; (2) Iteration-level SLO-based task prioritization, which\nprioritizes tight-SLO requests and batches requests with similar SLOs; (3)\nMulti-resource-aware batching, which selects queued requests to maximize the\nutilizations of both GPU compute resource and key-value cache (KVC).\nTrace-driven real experiments demonstrate that AccelGen achieves 1.42-11.21X\nhigher throughput, 1.43-13.71X higher goodput, 37-90% higher SLO attainment,\nand 1.61-12.22X lower response latency compared to the state-of-the-art\napproaches. It achieves performance near the Oracle, which optimally maximizes\ngoodput."
                },
                "authors": [
                    {
                        "name": "Haiying Shen"
                    },
                    {
                        "name": "Tanmoy Sen"
                    }
                ],
                "author_detail": {
                    "name": "Tanmoy Sen"
                },
                "author": "Tanmoy Sen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13737v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13737v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13723v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13723v1",
                "updated": "2025-03-17T21:11:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    21,
                    11,
                    30,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T21:11:30Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    21,
                    11,
                    30,
                    0,
                    76,
                    0
                ],
                "title": "Fast Maximum Likelihood Positioning for a Staggered Layer Scintillation\n  PET Detector",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Maximum Likelihood Positioning for a Staggered Layer Scintillation\n  PET Detector"
                },
                "summary": "In this study, we propose a fast implementation of a Maximum Likelihood\nPositioning (MLP) algorithm to estimate the energy and identify the active\nscintillator pixel in staggered layer scintillation detectors for PET. The\nstaggered layer design with pixelated scintillators enables the determination\nof the gamma's depth of interaction and facilitates an iteration-free\nformulation of the MLP algorithm. The efficacy of the algorithm optimization\nwas tested on a scintillation detector block designed for an ultra-high field\nBrainPET 7T, comprising three scintillator pixel layers. The three layers\ncontain 24 x 24, 24 x 23 and 23 x 22 scintillator pixels, respectively, with a\npixel pitch of 2 mm in both directions and layer thicknesses of 9, 8 and 7 mm.\nCalibration measurements, in combination with an automated calibration script,\nwere used to obtain the expected counts of scintillation photons required in\nthe MLP algorithm. Using Single-Instruction-Multiple-Data parallelization,\nmulti-threading and optimized cache lines, a maximum processing speed of\napproximately 22.5 million singles per second was achieved on a platform with\nfour Intel Xeon Platinum 8168 CPUs and 60 threads, encompassing all required\nprocessing steps. The automatic calibration failed for 1 to 15 individual\nscintillator pixels in approximately 10 per cent of the 120 scintillation\ndetector blocks, necessitating manual correction. After applying the energy\ncorrection to the positioned single events, an energy resolution of of 12 +/- 2\nper cent FWHM was obtained for the entire scintillation block. This value is\nvery close to the energy resolutions measured for the individual scintillator\npixels, proving that the MLP accurately identifies the scintillating pixel and\nthat the energy correction method effectively compensates for the light\ncollection variations of the SiPM array.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we propose a fast implementation of a Maximum Likelihood\nPositioning (MLP) algorithm to estimate the energy and identify the active\nscintillator pixel in staggered layer scintillation detectors for PET. The\nstaggered layer design with pixelated scintillators enables the determination\nof the gamma's depth of interaction and facilitates an iteration-free\nformulation of the MLP algorithm. The efficacy of the algorithm optimization\nwas tested on a scintillation detector block designed for an ultra-high field\nBrainPET 7T, comprising three scintillator pixel layers. The three layers\ncontain 24 x 24, 24 x 23 and 23 x 22 scintillator pixels, respectively, with a\npixel pitch of 2 mm in both directions and layer thicknesses of 9, 8 and 7 mm.\nCalibration measurements, in combination with an automated calibration script,\nwere used to obtain the expected counts of scintillation photons required in\nthe MLP algorithm. Using Single-Instruction-Multiple-Data parallelization,\nmulti-threading and optimized cache lines, a maximum processing speed of\napproximately 22.5 million singles per second was achieved on a platform with\nfour Intel Xeon Platinum 8168 CPUs and 60 threads, encompassing all required\nprocessing steps. The automatic calibration failed for 1 to 15 individual\nscintillator pixels in approximately 10 per cent of the 120 scintillation\ndetector blocks, necessitating manual correction. After applying the energy\ncorrection to the positioned single events, an energy resolution of of 12 +/- 2\nper cent FWHM was obtained for the entire scintillation block. This value is\nvery close to the energy resolutions measured for the individual scintillator\npixels, proving that the MLP accurately identifies the scintillating pixel and\nthat the energy correction method effectively compensates for the light\ncollection variations of the SiPM array."
                },
                "authors": [
                    {
                        "name": "Christoph W. Lerche"
                    },
                    {
                        "name": "Wenwei Bi"
                    },
                    {
                        "name": "Mirjam Schoeneck"
                    },
                    {
                        "name": "Debora Niekaemper"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Elisabeth Pfaehler"
                    },
                    {
                        "name": "Lutz Tellmann"
                    },
                    {
                        "name": "Juergen J. Scheins"
                    },
                    {
                        "name": "N. Jon Shah"
                    }
                ],
                "author_detail": {
                    "name": "N. Jon Shah"
                },
                "arxiv_affiliation": "Department of Neurology RWTH Aachen University Aachen Germany",
                "author": "N. Jon Shah",
                "arxiv_comment": "20 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13723v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13723v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "92C55 (Primary) 94A08 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13873v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13873v2",
                "updated": "2025-03-17T20:31:46Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    20,
                    31,
                    46,
                    0,
                    76,
                    0
                ],
                "published": "2025-02-19T16:54:58Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    16,
                    54,
                    58,
                    2,
                    50,
                    0
                ],
                "title": "NVR: Vector Runahead on NPUs for Sparse Memory Access",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NVR: Vector Runahead on NPUs for Sparse Memory Access"
                },
                "summary": "Deep Neural Networks are increasingly leveraging sparsity to reduce the\nscaling up of model parameter size. However, reducing wall-clock time through\nsparsity and pruning remains challenging due to irregular memory access\npatterns, leading to frequent cache misses. In this paper, we present NPU\nVector Runahead (NVR), a prefetching mechanism tailored for NPUs to address\ncache miss problems in sparse DNN workloads. Rather than optimising memory\npatterns with high overhead and poor portability, NVR adapts runahead execution\nto the unique architecture of NPUs. NVR provides a general micro-architectural\nsolution for sparse DNN workloads without requiring compiler or algorithmic\nsupport, operating as a decoupled, speculative, lightweight hardware sub-thread\nalongside the NPU, with minimal hardware overhead (under 5%). NVR achieves an\naverage 90% reduction in cache misses compared to SOTA prefetching in\ngeneral-purpose processors, delivering 4x average speedup on sparse workloads\nversus NPUs without prefetching. Moreover, we investigate the advantages of\nincorporating a small cache (16KB) into the NPU combined with NVR. Our\nevaluation shows that expanding this modest cache delivers 5x higher\nperformance benefits than increasing the L2 cache size by the same amount.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Neural Networks are increasingly leveraging sparsity to reduce the\nscaling up of model parameter size. However, reducing wall-clock time through\nsparsity and pruning remains challenging due to irregular memory access\npatterns, leading to frequent cache misses. In this paper, we present NPU\nVector Runahead (NVR), a prefetching mechanism tailored for NPUs to address\ncache miss problems in sparse DNN workloads. Rather than optimising memory\npatterns with high overhead and poor portability, NVR adapts runahead execution\nto the unique architecture of NPUs. NVR provides a general micro-architectural\nsolution for sparse DNN workloads without requiring compiler or algorithmic\nsupport, operating as a decoupled, speculative, lightweight hardware sub-thread\nalongside the NPU, with minimal hardware overhead (under 5%). NVR achieves an\naverage 90% reduction in cache misses compared to SOTA prefetching in\ngeneral-purpose processors, delivering 4x average speedup on sparse workloads\nversus NPUs without prefetching. Moreover, we investigate the advantages of\nincorporating a small cache (16KB) into the NPU combined with NVR. Our\nevaluation shows that expanding this modest cache delivers 5x higher\nperformance benefits than increasing the L2 cache size by the same amount."
                },
                "authors": [
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Zhengpeng Zhao"
                    },
                    {
                        "name": "Jing Wang"
                    },
                    {
                        "name": "Yushu Du"
                    },
                    {
                        "name": "Yuan Cheng"
                    },
                    {
                        "name": "Bing Guo"
                    },
                    {
                        "name": "He Xiao"
                    },
                    {
                        "name": "Chenhao Ma"
                    },
                    {
                        "name": "Xiaomeng Han"
                    },
                    {
                        "name": "Dean You"
                    },
                    {
                        "name": "Jiapeng Guan"
                    },
                    {
                        "name": "Ran Wei"
                    },
                    {
                        "name": "Dawei Yang"
                    },
                    {
                        "name": "Zhe Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Zhe Jiang"
                },
                "author": "Zhe Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13873v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13873v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13679v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13679v1",
                "updated": "2025-03-17T19:32:26Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    19,
                    32,
                    26,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T19:32:26Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    19,
                    32,
                    26,
                    0,
                    76,
                    0
                ],
                "title": "PrETi: Predicting Execution Time in Early Stage with LLVM and Machine\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrETi: Predicting Execution Time in Early Stage with LLVM and Machine\n  Learning"
                },
                "summary": "We introduce preti, a novel framework for predicting software execution time\nduring the early stages of development. preti leverages an LLVM-based\nsimulation environment to extract timing-related runtime information, such as\nthe count of executed LLVM IR instructions. This information, combined with\nhistorical execution time data, is utilized to train machine learning models\nfor accurate time prediction. To further enhance prediction accuracy, our\napproach incorporates simulations of cache accesses and branch prediction. The\nevaluations on public benchmarks demonstrate that preti achieves an average\nAbsolute Percentage Error (APE) of 11.98\\%, surpassing state-of-the-art\nmethods. These results underscore the effectiveness and efficiency of preti as\na robust solution for early-stage timing analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce preti, a novel framework for predicting software execution time\nduring the early stages of development. preti leverages an LLVM-based\nsimulation environment to extract timing-related runtime information, such as\nthe count of executed LLVM IR instructions. This information, combined with\nhistorical execution time data, is utilized to train machine learning models\nfor accurate time prediction. To further enhance prediction accuracy, our\napproach incorporates simulations of cache accesses and branch prediction. The\nevaluations on public benchmarks demonstrate that preti achieves an average\nAbsolute Percentage Error (APE) of 11.98\\%, surpassing state-of-the-art\nmethods. These results underscore the effectiveness and efficiency of preti as\na robust solution for early-stage timing analysis."
                },
                "authors": [
                    {
                        "name": "Risheng Xu"
                    },
                    {
                        "name": "Philipp Sieweck"
                    },
                    {
                        "name": "Hermann von Hasseln"
                    },
                    {
                        "name": "Dirk Nowotka"
                    }
                ],
                "author_detail": {
                    "name": "Dirk Nowotka"
                },
                "author": "Dirk Nowotka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13679v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13679v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16525v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16525v1",
                "updated": "2025-03-17T16:43:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    16,
                    43,
                    35,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T16:43:35Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    16,
                    43,
                    35,
                    0,
                    76,
                    0
                ],
                "title": "KVShare: Semantic-Aware Key-Value Cache Sharing for Efficient Large\n  Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVShare: Semantic-Aware Key-Value Cache Sharing for Efficient Large\n  Language Model Inference"
                },
                "summary": "This paper presents KVShare, a multi-user Key-Value (KV) Cache sharing\ntechnology based on semantic similarity, designed to enhance the inference\nefficiency of Large Language Models (LLMs) and Multimodal Large Language Models\n(MLLMs). Addressing the limitations of existing prefix caching (strict text\nprefix matching) and semantic caching (loss of response diversity), KVShare\nachieves fine-grained KV cache reuse through semantic alignment algorithms and\ndifferential editing operations. Experiments on real-world user conversation\ndatasets demonstrate that KVShare improves KV cache hit rates by over 60%,\nwhile maintaining output quality comparable to full computation (no significant\ndegradation in BLEU and Rouge-L metrics). This approach effectively reduces GPU\nresource consumption and is applicable to scenarios with repetitive queries,\nsuch as healthcare and education.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents KVShare, a multi-user Key-Value (KV) Cache sharing\ntechnology based on semantic similarity, designed to enhance the inference\nefficiency of Large Language Models (LLMs) and Multimodal Large Language Models\n(MLLMs). Addressing the limitations of existing prefix caching (strict text\nprefix matching) and semantic caching (loss of response diversity), KVShare\nachieves fine-grained KV cache reuse through semantic alignment algorithms and\ndifferential editing operations. Experiments on real-world user conversation\ndatasets demonstrate that KVShare improves KV cache hit rates by over 60%,\nwhile maintaining output quality comparable to full computation (no significant\ndegradation in BLEU and Rouge-L metrics). This approach effectively reduces GPU\nresource consumption and is applicable to scenarios with repetitive queries,\nsuch as healthcare and education."
                },
                "authors": [
                    {
                        "name": "Huan Yang"
                    },
                    {
                        "name": "Renji Zhang"
                    },
                    {
                        "name": "Deyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Deyu Zhang"
                },
                "author": "Deyu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16525v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16525v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13275v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13275v1",
                "updated": "2025-03-17T15:27:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    27,
                    2,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T15:27:02Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    27,
                    2,
                    0,
                    76,
                    0
                ],
                "title": "Knowledge-Aware Iterative Retrieval for Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge-Aware Iterative Retrieval for Multi-Agent Systems"
                },
                "summary": "We introduce a novel large language model (LLM)-driven agent framework, which\niteratively refines queries and filters contextual evidence by leveraging\ndynamically evolving knowledge. A defining feature of the system is its\ndecoupling of external sources from an internal knowledge cache that is\nprogressively updated to guide both query generation and evidence selection.\nThis design mitigates bias-reinforcement loops and enables dynamic, trackable\nsearch exploration paths, thereby optimizing the trade-off between exploring\ndiverse information and maintaining accuracy through autonomous agent\ndecision-making. Our approach is evaluated on a broad range of open-domain\nquestion answering benchmarks, including multi-step tasks that mirror\nreal-world scenarios where integrating information from multiple sources is\ncritical, especially given the vulnerabilities of LLMs that lack explicit\nreasoning or planning capabilities. The results show that the proposed system\nnot only outperforms single-step baselines regardless of task difficulty but\nalso, compared to conventional iterative retrieval methods, demonstrates\npronounced advantages in complex tasks through precise evidence-based reasoning\nand enhanced efficiency. The proposed system supports both competitive and\ncollaborative sharing of updated context, enabling multi-agent extension. The\nbenefits of multi-agent configurations become especially prominent as task\ndifficulty increases. The number of convergence steps scales with task\ndifficulty, suggesting cost-effective scalability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a novel large language model (LLM)-driven agent framework, which\niteratively refines queries and filters contextual evidence by leveraging\ndynamically evolving knowledge. A defining feature of the system is its\ndecoupling of external sources from an internal knowledge cache that is\nprogressively updated to guide both query generation and evidence selection.\nThis design mitigates bias-reinforcement loops and enables dynamic, trackable\nsearch exploration paths, thereby optimizing the trade-off between exploring\ndiverse information and maintaining accuracy through autonomous agent\ndecision-making. Our approach is evaluated on a broad range of open-domain\nquestion answering benchmarks, including multi-step tasks that mirror\nreal-world scenarios where integrating information from multiple sources is\ncritical, especially given the vulnerabilities of LLMs that lack explicit\nreasoning or planning capabilities. The results show that the proposed system\nnot only outperforms single-step baselines regardless of task difficulty but\nalso, compared to conventional iterative retrieval methods, demonstrates\npronounced advantages in complex tasks through precise evidence-based reasoning\nand enhanced efficiency. The proposed system supports both competitive and\ncollaborative sharing of updated context, enabling multi-agent extension. The\nbenefits of multi-agent configurations become especially prominent as task\ndifficulty increases. The number of convergence steps scales with task\ndifficulty, suggesting cost-effective scalability."
                },
                "authors": [
                    {
                        "name": "Seyoung Song"
                    }
                ],
                "author_detail": {
                    "name": "Seyoung Song"
                },
                "author": "Seyoung Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13275v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13275v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0; I.2.7; I.2.11; H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12991v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12991v1",
                "updated": "2025-03-17T09:46:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    9,
                    46,
                    35,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T09:46:35Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    9,
                    46,
                    35,
                    0,
                    76,
                    0
                ],
                "title": "Tuning the CMS Coffea-casa facility for 200 Gbps Challenge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tuning the CMS Coffea-casa facility for 200 Gbps Challenge"
                },
                "summary": "As a part of the IRIS-HEP \"Analysis Grand Challenge\" activities, the\nCoffea-casa AF team executed a \"200 Gbps Challenge\". One of the goals of this\nchallenge was to provide a setup for execution of a test notebook-style\nanalysis on the facility that could process a 200 TB CMS NanoAOD dataset in 20\nminutes.\n  We describe the solutions we deployed at the facility to execute the\nchallenge tasks. The facility was configured to provide 2000+ cores for quick\nturn-around, low-latency analysis. To reach the highest event processing rates\nwe tested different scaling backends, both scaling over HTCondor and Kubernetes\nresources and using Dask and Taskvine schedulers. This configuration also\nallowed us to compare two different services for managing Dask clusters, Dask\nlabextention, and Dask Gateway server, under extreme conditions.\n  A robust set of XCache servers with a redirector were deployed in Kubernetes\nto cache the dataset to minimize wide-area network traffic. The XCache servers\nwere backed with solid-state NVME drives deployed within the Kubernetes cluster\nnodes. All data access was authenticated using scitokens and was transparent to\nthe user. To ensure we could track and measure data throughput precisely, we\nused our existing Prometheus monitoring stack to monitor the XCache pod\nthroughput on the Kubernetes network layer. Using the rate query across all of\nthe 8 XCache pods we were able to view a stacked cumulative graph of the total\nthroughput for each XCache. This monitoring setup allowed us to ensure uniform\ndata rates across all nodes while verifying we had reached the 200 Gbps\nbenchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a part of the IRIS-HEP \"Analysis Grand Challenge\" activities, the\nCoffea-casa AF team executed a \"200 Gbps Challenge\". One of the goals of this\nchallenge was to provide a setup for execution of a test notebook-style\nanalysis on the facility that could process a 200 TB CMS NanoAOD dataset in 20\nminutes.\n  We describe the solutions we deployed at the facility to execute the\nchallenge tasks. The facility was configured to provide 2000+ cores for quick\nturn-around, low-latency analysis. To reach the highest event processing rates\nwe tested different scaling backends, both scaling over HTCondor and Kubernetes\nresources and using Dask and Taskvine schedulers. This configuration also\nallowed us to compare two different services for managing Dask clusters, Dask\nlabextention, and Dask Gateway server, under extreme conditions.\n  A robust set of XCache servers with a redirector were deployed in Kubernetes\nto cache the dataset to minimize wide-area network traffic. The XCache servers\nwere backed with solid-state NVME drives deployed within the Kubernetes cluster\nnodes. All data access was authenticated using scitokens and was transparent to\nthe user. To ensure we could track and measure data throughput precisely, we\nused our existing Prometheus monitoring stack to monitor the XCache pod\nthroughput on the Kubernetes network layer. Using the rate query across all of\nthe 8 XCache pods we were able to view a stacked cumulative graph of the total\nthroughput for each XCache. This monitoring setup allowed us to ensure uniform\ndata rates across all nodes while verifying we had reached the 200 Gbps\nbenchmark."
                },
                "authors": [
                    {
                        "name": "Sam Albin"
                    },
                    {
                        "name": "Garhan Attebury"
                    },
                    {
                        "name": "Kenneth Bloom"
                    },
                    {
                        "name": "Brian Paul Bockelman"
                    },
                    {
                        "name": "Benjamin Tovar Lopez"
                    },
                    {
                        "name": "Carl Lundstedt"
                    },
                    {
                        "name": "Oksana Shadura"
                    },
                    {
                        "name": "John Thiltges"
                    },
                    {
                        "name": "Derek Weitzel"
                    },
                    {
                        "name": "Andrew Wightman"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Wightman"
                },
                "arxiv_affiliation": "University of Nebraska-Lincoln",
                "author": "Andrew Wightman",
                "arxiv_comment": "Draft submitted to EPJ journal (CHEP 2024 conference proceedings)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12991v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12991v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ex",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12988v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12988v1",
                "updated": "2025-03-17T09:44:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    9,
                    44,
                    17,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-17T09:44:17Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    9,
                    44,
                    17,
                    0,
                    76,
                    0
                ],
                "title": "ROMA: a Read-Only-Memory-based Accelerator for QLoRA-based On-Device LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ROMA: a Read-Only-Memory-based Accelerator for QLoRA-based On-Device LLM"
                },
                "summary": "As large language models (LLMs) demonstrate powerful capabilities, deploying\nthem on edge devices has become increasingly crucial, offering advantages in\nprivacy and real-time interaction. QLoRA has emerged as the standard approach\nfor on-device LLMs, leveraging quantized models to reduce memory and\ncomputational costs while utilizing LoRA for task-specific adaptability. In\nthis work, we propose ROMA, a QLoRA accelerator with a hybrid storage\narchitecture that uses ROM for quantized base models and SRAM for LoRA weights\nand KV cache. Our insight is that the quantized base model is stable and\nconverged, making it well-suited for ROM storage. Meanwhile, LoRA modules offer\nthe flexibility to adapt to new data without requiring updates to the base\nmodel. To further reduce the area cost of ROM, we introduce a novel B-ROM\ndesign and integrate it with the compute unit to form a fused cell for\nefficient use of chip resources. ROMA can effectively store both a 4-bit 3B and\na 2-bit 8B LLaMA model entirely on-chip, achieving a notable generation speed\nexceeding 20,000 tokens/s without requiring external memory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) demonstrate powerful capabilities, deploying\nthem on edge devices has become increasingly crucial, offering advantages in\nprivacy and real-time interaction. QLoRA has emerged as the standard approach\nfor on-device LLMs, leveraging quantized models to reduce memory and\ncomputational costs while utilizing LoRA for task-specific adaptability. In\nthis work, we propose ROMA, a QLoRA accelerator with a hybrid storage\narchitecture that uses ROM for quantized base models and SRAM for LoRA weights\nand KV cache. Our insight is that the quantized base model is stable and\nconverged, making it well-suited for ROM storage. Meanwhile, LoRA modules offer\nthe flexibility to adapt to new data without requiring updates to the base\nmodel. To further reduce the area cost of ROM, we introduce a novel B-ROM\ndesign and integrate it with the compute unit to form a fused cell for\nefficient use of chip resources. ROMA can effectively store both a 4-bit 3B and\na 2-bit 8B LLaMA model entirely on-chip, achieving a notable generation speed\nexceeding 20,000 tokens/s without requiring external memory."
                },
                "authors": [
                    {
                        "name": "Wenqiang Wang"
                    },
                    {
                        "name": "Yijia Zhang"
                    },
                    {
                        "name": "Zikai Zhang"
                    },
                    {
                        "name": "Guanting Huo"
                    },
                    {
                        "name": "Hao Liang"
                    },
                    {
                        "name": "Shijie Cao"
                    },
                    {
                        "name": "Ningyi Xu"
                    }
                ],
                "author_detail": {
                    "name": "Ningyi Xu"
                },
                "author": "Ningyi Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12988v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12988v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08407v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08407v2",
                "updated": "2025-03-17T03:30:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    17,
                    3,
                    30,
                    29,
                    0,
                    76,
                    0
                ],
                "published": "2025-03-11T13:10:41Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    13,
                    10,
                    41,
                    1,
                    70,
                    0
                ],
                "title": "WildSeg3D: Segment Any 3D Objects in the Wild from 2D Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WildSeg3D: Segment Any 3D Objects in the Wild from 2D Images"
                },
                "summary": "Recent advances in interactive 3D segmentation from 2D images have\ndemonstrated impressive performance. However, current models typically require\nextensive scene-specific training to accurately reconstruct and segment\nobjects, which limits their applicability in real-time scenarios. In this\npaper, we introduce WildSeg3D, an efficient approach that enables the\nsegmentation of arbitrary 3D objects across diverse environments using a\nfeed-forward mechanism. A key challenge of this feed-forward approach lies in\nthe accumulation of 3D alignment errors across multiple 2D views, which can\nlead to inaccurate 3D segmentation results. To address this issue, we propose\nDynamic Global Aligning (DGA), a technique that improves the accuracy of global\nmulti-view alignment by focusing on difficult-to-match 3D points across images,\nusing a dynamic adjustment function. Additionally, for real-time interactive\nsegmentation, we introduce Multi-view Group Mapping (MGM), a method that\nutilizes an object mask cache to integrate multi-view segmentations and respond\nrapidly to user prompts. WildSeg3D demonstrates robust generalization across\narbitrary scenes, thereby eliminating the need for scene-specific training.\nSpecifically, WildSeg3D not only attains the accuracy of state-of-the-art\n(SOTA) methods but also achieves a $40\\times$ speedup compared to existing SOTA\nmodels. Our code will be publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in interactive 3D segmentation from 2D images have\ndemonstrated impressive performance. However, current models typically require\nextensive scene-specific training to accurately reconstruct and segment\nobjects, which limits their applicability in real-time scenarios. In this\npaper, we introduce WildSeg3D, an efficient approach that enables the\nsegmentation of arbitrary 3D objects across diverse environments using a\nfeed-forward mechanism. A key challenge of this feed-forward approach lies in\nthe accumulation of 3D alignment errors across multiple 2D views, which can\nlead to inaccurate 3D segmentation results. To address this issue, we propose\nDynamic Global Aligning (DGA), a technique that improves the accuracy of global\nmulti-view alignment by focusing on difficult-to-match 3D points across images,\nusing a dynamic adjustment function. Additionally, for real-time interactive\nsegmentation, we introduce Multi-view Group Mapping (MGM), a method that\nutilizes an object mask cache to integrate multi-view segmentations and respond\nrapidly to user prompts. WildSeg3D demonstrates robust generalization across\narbitrary scenes, thereby eliminating the need for scene-specific training.\nSpecifically, WildSeg3D not only attains the accuracy of state-of-the-art\n(SOTA) methods but also achieves a $40\\times$ speedup compared to existing SOTA\nmodels. Our code will be publicly available."
                },
                "authors": [
                    {
                        "name": "Yansong Guo"
                    },
                    {
                        "name": "Jie Hu"
                    },
                    {
                        "name": "Yansong Qu"
                    },
                    {
                        "name": "Liujuan Cao"
                    }
                ],
                "author_detail": {
                    "name": "Liujuan Cao"
                },
                "author": "Liujuan Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08407v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08407v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12491v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12491v1",
                "updated": "2025-03-16T12:49:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    16,
                    12,
                    49,
                    44,
                    6,
                    75,
                    0
                ],
                "published": "2025-03-16T12:49:44Z",
                "published_parsed": [
                    2025,
                    3,
                    16,
                    12,
                    49,
                    44,
                    6,
                    75,
                    0
                ],
                "title": "CAKE: Cascading and Adaptive KV Cache Eviction with Layer Preferences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAKE: Cascading and Adaptive KV Cache Eviction with Layer Preferences"
                },
                "summary": "Large language models (LLMs) excel at processing long sequences, boosting\ndemand for key-value (KV) caching. While recent efforts to evict KV cache have\nalleviated the inference burden, they often fail to allocate resources\nrationally across layers with different attention patterns. In this paper, we\nintroduce Cascading and Adaptive KV cache Eviction (CAKE), a novel approach\nthat frames KV cache eviction as a \"cake-slicing problem.\" CAKE assesses\nlayer-specific preferences by considering attention dynamics in both spatial\nand temporal dimensions, allocates rational cache size for layers accordingly,\nand manages memory constraints in a cascading manner. This approach enables a\nglobal view of cache allocation, adaptively distributing resources across\ndiverse attention mechanisms while maintaining memory budgets. CAKE also\nemploys a new eviction indicator that considers the shifting importance of\ntokens over time, addressing limitations in existing methods that overlook\ntemporal dynamics. Comprehensive experiments on LongBench and NeedleBench show\nthat CAKE maintains model performance with only 3.2% of the KV cache and\nconsistently outperforms current baselines across various models and memory\nconstraints, particularly in low-memory settings. Additionally, CAKE achieves\nover 10x speedup in decoding latency compared to full cache when processing\ncontexts of 128K tokens with FlashAttention-2. Our code is available at\nhttps://github.com/antgroup/cakekv.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel at processing long sequences, boosting\ndemand for key-value (KV) caching. While recent efforts to evict KV cache have\nalleviated the inference burden, they often fail to allocate resources\nrationally across layers with different attention patterns. In this paper, we\nintroduce Cascading and Adaptive KV cache Eviction (CAKE), a novel approach\nthat frames KV cache eviction as a \"cake-slicing problem.\" CAKE assesses\nlayer-specific preferences by considering attention dynamics in both spatial\nand temporal dimensions, allocates rational cache size for layers accordingly,\nand manages memory constraints in a cascading manner. This approach enables a\nglobal view of cache allocation, adaptively distributing resources across\ndiverse attention mechanisms while maintaining memory budgets. CAKE also\nemploys a new eviction indicator that considers the shifting importance of\ntokens over time, addressing limitations in existing methods that overlook\ntemporal dynamics. Comprehensive experiments on LongBench and NeedleBench show\nthat CAKE maintains model performance with only 3.2% of the KV cache and\nconsistently outperforms current baselines across various models and memory\nconstraints, particularly in low-memory settings. Additionally, CAKE achieves\nover 10x speedup in decoding latency compared to full cache when processing\ncontexts of 128K tokens with FlashAttention-2. Our code is available at\nhttps://github.com/antgroup/cakekv."
                },
                "authors": [
                    {
                        "name": "Ziran Qin"
                    },
                    {
                        "name": "Yuchen Cao"
                    },
                    {
                        "name": "Mingbao Lin"
                    },
                    {
                        "name": "Wen Hu"
                    },
                    {
                        "name": "Shixuan Fan"
                    },
                    {
                        "name": "Ke Cheng"
                    },
                    {
                        "name": "Weiyao Lin"
                    },
                    {
                        "name": "Jianguo Li"
                    }
                ],
                "author_detail": {
                    "name": "Jianguo Li"
                },
                "author": "Jianguo Li",
                "arxiv_comment": "Accepted by ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12491v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12491v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12450v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12450v1",
                "updated": "2025-03-16T10:54:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    16,
                    10,
                    54,
                    59,
                    6,
                    75,
                    0
                ],
                "published": "2025-03-16T10:54:59Z",
                "published_parsed": [
                    2025,
                    3,
                    16,
                    10,
                    54,
                    59,
                    6,
                    75,
                    0
                ],
                "title": "LazyMAR: Accelerating Masked Autoregressive Models via Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LazyMAR: Accelerating Masked Autoregressive Models via Feature Caching"
                },
                "summary": "Masked Autoregressive (MAR) models have emerged as a promising approach in\nimage generation, expected to surpass traditional autoregressive models in\ncomputational efficiency by leveraging the capability of parallel decoding.\nHowever, their dependence on bidirectional self-attention inherently conflicts\nwith conventional KV caching mechanisms, creating unexpected computational\nbottlenecks that undermine their expected efficiency. To address this problem,\nthis paper studies the caching mechanism for MAR by leveraging two types of\nredundancy: Token Redundancy indicates that a large portion of tokens have very\nsimilar representations in the adjacent decoding steps, which allows us to\nfirst cache them in previous steps and then reuse them in the later steps.\nCondition Redundancy indicates that the difference between conditional and\nunconditional output in classifier-free guidance exhibits very similar values\nin adjacent steps. Based on these two redundancies, we propose LazyMAR, which\nintroduces two caching mechanisms to handle them one by one. LazyMAR is\ntraining-free and plug-and-play for all MAR models. Experimental results\ndemonstrate that our method achieves 2.83 times acceleration with almost no\ndrop in generation quality. Our codes will be released in\nhttps://github.com/feihongyan1/LazyMAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masked Autoregressive (MAR) models have emerged as a promising approach in\nimage generation, expected to surpass traditional autoregressive models in\ncomputational efficiency by leveraging the capability of parallel decoding.\nHowever, their dependence on bidirectional self-attention inherently conflicts\nwith conventional KV caching mechanisms, creating unexpected computational\nbottlenecks that undermine their expected efficiency. To address this problem,\nthis paper studies the caching mechanism for MAR by leveraging two types of\nredundancy: Token Redundancy indicates that a large portion of tokens have very\nsimilar representations in the adjacent decoding steps, which allows us to\nfirst cache them in previous steps and then reuse them in the later steps.\nCondition Redundancy indicates that the difference between conditional and\nunconditional output in classifier-free guidance exhibits very similar values\nin adjacent steps. Based on these two redundancies, we propose LazyMAR, which\nintroduces two caching mechanisms to handle them one by one. LazyMAR is\ntraining-free and plug-and-play for all MAR models. Experimental results\ndemonstrate that our method achieves 2.83 times acceleration with almost no\ndrop in generation quality. Our codes will be released in\nhttps://github.com/feihongyan1/LazyMAR."
                },
                "authors": [
                    {
                        "name": "Feihong Yan"
                    },
                    {
                        "name": "Qingyan Wei"
                    },
                    {
                        "name": "Jiayi Tang"
                    },
                    {
                        "name": "Jiajun Li"
                    },
                    {
                        "name": "Yulin Wang"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Huiqi Li"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "10 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12450v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12450v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11972v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11972v1",
                "updated": "2025-03-15T02:48:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    15,
                    2,
                    48,
                    27,
                    5,
                    74,
                    0
                ],
                "published": "2025-03-15T02:48:27Z",
                "published_parsed": [
                    2025,
                    3,
                    15,
                    2,
                    48,
                    27,
                    5,
                    74,
                    0
                ],
                "title": "MoDM: Efficient Serving for Image Generation via Mixture-of-Diffusion\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoDM: Efficient Serving for Image Generation via Mixture-of-Diffusion\n  Models"
                },
                "summary": "Diffusion-based text-to-image generation models trade latency for quality:\nsmall models are fast but generate lower-quality images, while large models\nproduce better images but are slow.\n  We present MoDM, a novel caching-based serving system for diffusion models\nthat dynamically balances latency and quality through a mixture of diffusion\nmodels. Unlike prior approaches that rely on model-specific internal features,\nMoDM caches final images, allowing seamless retrieval and reuse across multiple\ndiffusion model families.\n  This design enables adaptive serving by dynamically balancing latency and\nimage quality: using smaller models for cache-hit requests to reduce latency\nwhile reserving larger models for cache-miss requests to maintain quality.\nSmall model image quality is preserved using retrieved cached images.\n  We design a global monitor that optimally allocates GPU resources and\nbalances inference workload, ensuring high throughput while meeting\nservice-level objectives under varying request rates. Our evaluations show that\nMoDM significantly reduces average serving time by 2.5x while retaining image\nquality, making it a practical solution for scalable and resource-efficient\nmodel deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based text-to-image generation models trade latency for quality:\nsmall models are fast but generate lower-quality images, while large models\nproduce better images but are slow.\n  We present MoDM, a novel caching-based serving system for diffusion models\nthat dynamically balances latency and quality through a mixture of diffusion\nmodels. Unlike prior approaches that rely on model-specific internal features,\nMoDM caches final images, allowing seamless retrieval and reuse across multiple\ndiffusion model families.\n  This design enables adaptive serving by dynamically balancing latency and\nimage quality: using smaller models for cache-hit requests to reduce latency\nwhile reserving larger models for cache-miss requests to maintain quality.\nSmall model image quality is preserved using retrieved cached images.\n  We design a global monitor that optimally allocates GPU resources and\nbalances inference workload, ensuring high throughput while meeting\nservice-level objectives under varying request rates. Our evaluations show that\nMoDM significantly reduces average serving time by 2.5x while retaining image\nquality, making it a practical solution for scalable and resource-efficient\nmodel deployment."
                },
                "authors": [
                    {
                        "name": "Yuchen Xia"
                    },
                    {
                        "name": "Divyam Sharma"
                    },
                    {
                        "name": "Yichao Yuan"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Nishil Talati"
                    }
                ],
                "author_detail": {
                    "name": "Nishil Talati"
                },
                "author": "Nishil Talati",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11972v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11972v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11946v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11946v1",
                "updated": "2025-03-15T01:35:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    15,
                    1,
                    35,
                    53,
                    5,
                    74,
                    0
                ],
                "published": "2025-03-15T01:35:53Z",
                "published_parsed": [
                    2025,
                    3,
                    15,
                    1,
                    35,
                    53,
                    5,
                    74,
                    0
                ],
                "title": "CCRSat: A Collaborative Computation Reuse Framework for Satellite Edge\n  Computing Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CCRSat: A Collaborative Computation Reuse Framework for Satellite Edge\n  Computing Networks"
                },
                "summary": "In satellite computing applications, such as remote sensing, tasks often\ninvolve similar or identical input data, leading to the same processing\nresults. Computation reuse is an emerging paradigm that leverages the execution\nresults of previous tasks to enhance the utilization of computational\nresources. While this paradigm has been extensively studied in terrestrial\nnetworks with abundant computing and caching resources, such as named data\nnetworking (NDN), it is essential to develop a framework appropriate for\nresource-constrained satellite networks, which are expected to have longer task\ncompletion times. In this paper, we propose CCRSat, a collaborative computation\nreuse framework for satellite edge computing networks. CCRSat initially\nimplements local computation reuse on an independent satellite, utilizing a\nsatellite reuse state (SRS) to assess the efficiency of computation reuse.\nAdditionally, an inter-satellite computation reuse algorithm is introduced,\nwhich utilizes the collaborative sharing of similarity in previously processed\ndata among multiple satellites. The evaluation results tested on real-world\ndatasets demonstrate that, compared to comparative scenarios, our proposed\nCCRSat can significantly reduce task completion time by up to 62.1% and\ncomputational resource consumption by up to 28.8%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In satellite computing applications, such as remote sensing, tasks often\ninvolve similar or identical input data, leading to the same processing\nresults. Computation reuse is an emerging paradigm that leverages the execution\nresults of previous tasks to enhance the utilization of computational\nresources. While this paradigm has been extensively studied in terrestrial\nnetworks with abundant computing and caching resources, such as named data\nnetworking (NDN), it is essential to develop a framework appropriate for\nresource-constrained satellite networks, which are expected to have longer task\ncompletion times. In this paper, we propose CCRSat, a collaborative computation\nreuse framework for satellite edge computing networks. CCRSat initially\nimplements local computation reuse on an independent satellite, utilizing a\nsatellite reuse state (SRS) to assess the efficiency of computation reuse.\nAdditionally, an inter-satellite computation reuse algorithm is introduced,\nwhich utilizes the collaborative sharing of similarity in previously processed\ndata among multiple satellites. The evaluation results tested on real-world\ndatasets demonstrate that, compared to comparative scenarios, our proposed\nCCRSat can significantly reduce task completion time by up to 62.1% and\ncomputational resource consumption by up to 28.8%."
                },
                "authors": [
                    {
                        "name": "Ye Zhang"
                    },
                    {
                        "name": "Zhishu Shen"
                    },
                    {
                        "name": "Dawen Jiang"
                    },
                    {
                        "name": "Xiangrui Liu"
                    },
                    {
                        "name": "Qiushi Zheng"
                    },
                    {
                        "name": "Jiong Jin"
                    }
                ],
                "author_detail": {
                    "name": "Jiong Jin"
                },
                "author": "Jiong Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11946v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11946v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.06348v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.06348v2",
                "updated": "2025-03-15T00:49:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    15,
                    0,
                    49,
                    55,
                    5,
                    74,
                    0
                ],
                "published": "2024-03-11T00:30:25Z",
                "published_parsed": [
                    2024,
                    3,
                    11,
                    0,
                    30,
                    25,
                    0,
                    71,
                    0
                ],
                "title": "Accelerating Sparse Tensor Decomposition Using Adaptive Linearized\n  Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Sparse Tensor Decomposition Using Adaptive Linearized\n  Representation"
                },
                "summary": "High-dimensional sparse data emerge in many critical application domains such\nas healthcare and cybersecurity. To extract meaningful insights from massive\nvolumes of these multi-dimensional data, scientists employ unsupervised\nanalysis tools based on tensor decomposition (TD) methods. However, real-world\nsparse tensors exhibit highly irregular shapes and data distributions, which\npose significant challenges for making efficient use of modern parallel\nprocessors. This study breaks the prevailing assumption that compressing sparse\ntensors into coarse-grained structures or along a particular dimension/mode is\nmore efficient than keeping them in a fine-grained, mode-agnostic form. Our\nnovel sparse tensor representation, Adaptive Linearized Tensor Order (ALTO),\nencodes tensors in a compact format that can be easily streamed from memory and\nis amenable to both caching and parallel execution. In contrast to existing\ncompressed tensor formats, ALTO constructs one tensor copy that is agnostic to\nboth the mode orientation and the irregular distribution of nonzero elements.\nTo demonstrate the efficacy of ALTO, we propose a set of parallel TD algorithms\nthat exploit the inherent data reuse of tensor computations to substantially\nreduce synchronization overhead, decrease memory footprint, and improve\nparallel performance. Additionally, we characterize the major execution\nbottlenecks of TD methods on the latest Intel Xeon Scalable processors and\nintroduce dynamic adaptation heuristics to automatically select the best\nalgorithm based on the sparse tensor characteristics. Across a diverse set of\nreal-world data sets, ALTO outperforms the state-of-the-art approaches,\nachieving more than an order-of-magnitude speedup over the best mode-agnostic\nformats. Compared to the best mode-specific formats, ALTO achieves 5.1X\ngeometric mean speedup at a fraction (25%) of their storage costs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-dimensional sparse data emerge in many critical application domains such\nas healthcare and cybersecurity. To extract meaningful insights from massive\nvolumes of these multi-dimensional data, scientists employ unsupervised\nanalysis tools based on tensor decomposition (TD) methods. However, real-world\nsparse tensors exhibit highly irregular shapes and data distributions, which\npose significant challenges for making efficient use of modern parallel\nprocessors. This study breaks the prevailing assumption that compressing sparse\ntensors into coarse-grained structures or along a particular dimension/mode is\nmore efficient than keeping them in a fine-grained, mode-agnostic form. Our\nnovel sparse tensor representation, Adaptive Linearized Tensor Order (ALTO),\nencodes tensors in a compact format that can be easily streamed from memory and\nis amenable to both caching and parallel execution. In contrast to existing\ncompressed tensor formats, ALTO constructs one tensor copy that is agnostic to\nboth the mode orientation and the irregular distribution of nonzero elements.\nTo demonstrate the efficacy of ALTO, we propose a set of parallel TD algorithms\nthat exploit the inherent data reuse of tensor computations to substantially\nreduce synchronization overhead, decrease memory footprint, and improve\nparallel performance. Additionally, we characterize the major execution\nbottlenecks of TD methods on the latest Intel Xeon Scalable processors and\nintroduce dynamic adaptation heuristics to automatically select the best\nalgorithm based on the sparse tensor characteristics. Across a diverse set of\nreal-world data sets, ALTO outperforms the state-of-the-art approaches,\nachieving more than an order-of-magnitude speedup over the best mode-agnostic\nformats. Compared to the best mode-specific formats, ALTO achieves 5.1X\ngeometric mean speedup at a fraction (25%) of their storage costs."
                },
                "authors": [
                    {
                        "name": "Jan Laukemann"
                    },
                    {
                        "name": "Ahmed E. Helal"
                    },
                    {
                        "name": "S. Isaac Geronimo Anderson"
                    },
                    {
                        "name": "Fabio Checconi"
                    },
                    {
                        "name": "Yongseok Soh"
                    },
                    {
                        "name": "Jesmin Jahan Tithi"
                    },
                    {
                        "name": "Teresa Ranadive"
                    },
                    {
                        "name": "Brian J Gravelle"
                    },
                    {
                        "name": "Fabrizio Petrini"
                    },
                    {
                        "name": "Jee Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jee Choi"
                },
                "author": "Jee Choi",
                "arxiv_comment": "Accepted to TPDS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.06348v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.06348v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11816v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11816v1",
                "updated": "2025-03-14T19:02:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    19,
                    2,
                    16,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T19:02:16Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    19,
                    2,
                    16,
                    4,
                    73,
                    0
                ],
                "title": "Key, Value, Compress: A Systematic Exploration of KV Cache Compression\n  Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key, Value, Compress: A Systematic Exploration of KV Cache Compression\n  Techniques"
                },
                "summary": "Large language models (LLMs) have demonstrated exceptional capabilities in\ngenerating text, images, and video content. However, as context length grows,\nthe computational cost of attention increases quadratically with the number of\ntokens, presenting significant efficiency challenges. This paper presents an\nanalysis of various Key-Value (KV) cache compression strategies, offering a\ncomprehensive taxonomy that categorizes these methods by their underlying\nprinciples and implementation techniques. Furthermore, we evaluate their impact\non performance and inference latency, providing critical insights into their\neffectiveness. Our findings highlight the trade-offs involved in KV cache\ncompression and its influence on handling long-context scenarios, paving the\nway for more efficient LLM implementations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated exceptional capabilities in\ngenerating text, images, and video content. However, as context length grows,\nthe computational cost of attention increases quadratically with the number of\ntokens, presenting significant efficiency challenges. This paper presents an\nanalysis of various Key-Value (KV) cache compression strategies, offering a\ncomprehensive taxonomy that categorizes these methods by their underlying\nprinciples and implementation techniques. Furthermore, we evaluate their impact\non performance and inference latency, providing critical insights into their\neffectiveness. Our findings highlight the trade-offs involved in KV cache\ncompression and its influence on handling long-context scenarios, paving the\nway for more efficient LLM implementations."
                },
                "authors": [
                    {
                        "name": "Neusha Javidnia"
                    },
                    {
                        "name": "Bita Darvish Rouhani"
                    },
                    {
                        "name": "Farinaz Koushanfar"
                    }
                ],
                "author_detail": {
                    "name": "Farinaz Koushanfar"
                },
                "author": "Farinaz Koushanfar",
                "arxiv_comment": "Invited paper to IEEE Custom Integrated Circuits Conference (CICC)\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11816v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11816v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11750v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11750v1",
                "updated": "2025-03-14T17:57:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    17,
                    57,
                    42,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T17:57:42Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    17,
                    57,
                    42,
                    4,
                    73,
                    0
                ],
                "title": "Making Every Step Effective: Jailbreaking Large Vision-Language Models\n  Through Hierarchical KV Equalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Making Every Step Effective: Jailbreaking Large Vision-Language Models\n  Through Hierarchical KV Equalization"
                },
                "summary": "In the realm of large vision-language models (LVLMs), adversarial jailbreak\nattacks serve as a red-teaming approach to identify safety vulnerabilities of\nthese models and their associated defense mechanisms. However, we identify a\ncritical limitation: not every adversarial optimization step leads to a\npositive outcome, and indiscriminately accepting optimization results at each\nstep may reduce the overall attack success rate. To address this challenge, we\nintroduce HKVE (Hierarchical Key-Value Equalization), an innovative\njailbreaking framework that selectively accepts gradient optimization results\nbased on the distribution of attention scores across different layers, ensuring\nthat every optimization step positively contributes to the attack. Extensive\nexperiments demonstrate HKVE's significant effectiveness, achieving attack\nsuccess rates of 75.08% on MiniGPT4, 85.84% on LLaVA and 81.00% on Qwen-VL,\nsubstantially outperforming existing methods by margins of 20.43\\%, 21.01\\% and\n26.43\\% respectively. Furthermore, making every step effective not only leads\nto an increase in attack success rate but also allows for a reduction in the\nnumber of iterations, thereby lowering computational costs. Warning: This paper\ncontains potentially harmful example data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the realm of large vision-language models (LVLMs), adversarial jailbreak\nattacks serve as a red-teaming approach to identify safety vulnerabilities of\nthese models and their associated defense mechanisms. However, we identify a\ncritical limitation: not every adversarial optimization step leads to a\npositive outcome, and indiscriminately accepting optimization results at each\nstep may reduce the overall attack success rate. To address this challenge, we\nintroduce HKVE (Hierarchical Key-Value Equalization), an innovative\njailbreaking framework that selectively accepts gradient optimization results\nbased on the distribution of attention scores across different layers, ensuring\nthat every optimization step positively contributes to the attack. Extensive\nexperiments demonstrate HKVE's significant effectiveness, achieving attack\nsuccess rates of 75.08% on MiniGPT4, 85.84% on LLaVA and 81.00% on Qwen-VL,\nsubstantially outperforming existing methods by margins of 20.43\\%, 21.01\\% and\n26.43\\% respectively. Furthermore, making every step effective not only leads\nto an increase in attack success rate but also allows for a reduction in the\nnumber of iterations, thereby lowering computational costs. Warning: This paper\ncontains potentially harmful example data."
                },
                "authors": [
                    {
                        "name": "Shuyang Hao"
                    },
                    {
                        "name": "Yiwei Wang"
                    },
                    {
                        "name": "Bryan Hooi"
                    },
                    {
                        "name": "Jun Liu"
                    },
                    {
                        "name": "Muhao Chen"
                    },
                    {
                        "name": "Zi Huang"
                    },
                    {
                        "name": "Yujun Cai"
                    }
                ],
                "author_detail": {
                    "name": "Yujun Cai"
                },
                "author": "Yujun Cai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11750v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11750v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01066v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01066v2",
                "updated": "2025-03-14T16:57:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    16,
                    57,
                    12,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-03T00:14:34Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    0,
                    14,
                    34,
                    0,
                    62,
                    0
                ],
                "title": "Alchemist: Towards the Design of Efficient Online Continual Learning\n  System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alchemist: Towards the Design of Efficient Online Continual Learning\n  System"
                },
                "summary": "Continual learning has become a promising solution to refine large language\nmodels incrementally by leveraging user feedback. In particular, online\ncontinual learning - iteratively training the model with small batches of user\nfeedback - has demonstrated notable performance improvements. However, the\nexisting practice of separating training and serving processes forces the\nonline trainer to recompute the intermediate results already done during\nserving. Such redundant computations can account for 30%-42% of total training\ntime.\n  In this paper, we propose Alchemist, to the best of our knowledge, the first\nonline continual learning system that efficiently reuses serving activations to\nincrease training throughput. Alchemist introduces two key techniques: (1)\nrecording and storing activations and KV cache only during the prefill phase to\nminimize latency and memory overhead; and (2) smart activation offloading and\nhedging. Evaluations with inputs of varied token length sampled from ShareGPT\ndataset show that compared with a separate training cluster, Alchemist\nsignificantly increases training throughput by up to 1.72x, reduces up to 47%\nmemory usage during training, and supports up to 2x more training tokens - all\nwhile maintaining negligible impact on serving latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual learning has become a promising solution to refine large language\nmodels incrementally by leveraging user feedback. In particular, online\ncontinual learning - iteratively training the model with small batches of user\nfeedback - has demonstrated notable performance improvements. However, the\nexisting practice of separating training and serving processes forces the\nonline trainer to recompute the intermediate results already done during\nserving. Such redundant computations can account for 30%-42% of total training\ntime.\n  In this paper, we propose Alchemist, to the best of our knowledge, the first\nonline continual learning system that efficiently reuses serving activations to\nincrease training throughput. Alchemist introduces two key techniques: (1)\nrecording and storing activations and KV cache only during the prefill phase to\nminimize latency and memory overhead; and (2) smart activation offloading and\nhedging. Evaluations with inputs of varied token length sampled from ShareGPT\ndataset show that compared with a separate training cluster, Alchemist\nsignificantly increases training throughput by up to 1.72x, reduces up to 47%\nmemory usage during training, and supports up to 2x more training tokens - all\nwhile maintaining negligible impact on serving latency."
                },
                "authors": [
                    {
                        "name": "Yuyang Huang"
                    },
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Haryadi S. Gunawi"
                    },
                    {
                        "name": "Beibin Li"
                    },
                    {
                        "name": "Changho Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Changho Hwang"
                },
                "author": "Changho Hwang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01066v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01066v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11460v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11460v1",
                "updated": "2025-03-14T14:47:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    14,
                    47,
                    55,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T14:47:55Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    14,
                    47,
                    55,
                    4,
                    73,
                    0
                ],
                "title": "ARCAS: Adaptive Runtime System for Chiplet-Aware Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARCAS: Adaptive Runtime System for Chiplet-Aware Scheduling"
                },
                "summary": "The growing disparity between CPU core counts and available memory bandwidth\nhas intensified memory contention in servers. This particularly affects highly\nparallelizable applications, which must achieve efficient cache utilization to\nmaintain performance as CPU core counts grow. Optimizing cache utilization,\nhowever, is complex for recent chiplet-based CPUs, whose partitioned L3 caches\nlead to varying latencies and bandwidths, even within a single NUMA domain.\nClassical NUMA optimizations and task scheduling approaches unfortunately fail\nto address the performance issues of chiplet-based CPUs.\n  We describe Adaptive Runtime system for Chiplet-Aware Scheduling (ARCAS), a\nnew runtime system designed for chiplet-based CPUs. ARCAS combines\nchiplet-aware task scheduling heuristics, hardware-aware memory allocation, and\nfine-grained performance monitoring to optimize workload execution. It\nimplements a lightweight concurrency model that combines user-level thread\nfeatures-such as individual stacks, per-task scheduling, and state\nmanagement-with coroutine-like behavior, allowing tasks to suspend and resume\nexecution at defined points while efficiently managing task migration across\nchiplets. Our evaluation across diverse scenarios shows ARCAS's effectiveness\nfor optimizing the performance of memory-intensive parallel applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing disparity between CPU core counts and available memory bandwidth\nhas intensified memory contention in servers. This particularly affects highly\nparallelizable applications, which must achieve efficient cache utilization to\nmaintain performance as CPU core counts grow. Optimizing cache utilization,\nhowever, is complex for recent chiplet-based CPUs, whose partitioned L3 caches\nlead to varying latencies and bandwidths, even within a single NUMA domain.\nClassical NUMA optimizations and task scheduling approaches unfortunately fail\nto address the performance issues of chiplet-based CPUs.\n  We describe Adaptive Runtime system for Chiplet-Aware Scheduling (ARCAS), a\nnew runtime system designed for chiplet-based CPUs. ARCAS combines\nchiplet-aware task scheduling heuristics, hardware-aware memory allocation, and\nfine-grained performance monitoring to optimize workload execution. It\nimplements a lightweight concurrency model that combines user-level thread\nfeatures-such as individual stacks, per-task scheduling, and state\nmanagement-with coroutine-like behavior, allowing tasks to suspend and resume\nexecution at defined points while efficiently managing task migration across\nchiplets. Our evaluation across diverse scenarios shows ARCAS's effectiveness\nfor optimizing the performance of memory-intensive parallel applications."
                },
                "authors": [
                    {
                        "name": "Alessandro Fogli"
                    },
                    {
                        "name": "Bo Zhao"
                    },
                    {
                        "name": "Peter Pietzuch"
                    },
                    {
                        "name": "Jana Giceva"
                    }
                ],
                "author_detail": {
                    "name": "Jana Giceva"
                },
                "author": "Jana Giceva",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11460v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11460v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11426v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11426v1",
                "updated": "2025-03-14T14:14:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    14,
                    14,
                    5,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T14:14:05Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    14,
                    14,
                    5,
                    4,
                    73,
                    0
                ],
                "title": "Text Compression for Efficient Language Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text Compression for Efficient Language Generation"
                },
                "summary": "We challenge the prevailing assumption that LLMs must rely fully on sub-word\ntokens for high-quality text generation. To this end, we propose the\n\"Generative Pretrained Thoughtformer\" (GPTHF), a hierarchical transformer\nlanguage model capable of text generation by compressing text into sentence\nembeddings and employing a sentence attention mechanism. GPTHF retains GPT's\narchitecture, modifying only token interactions via dynamic sparse attention\nmasks.\n  Our experiments show that GPTHF achieves an up to an order of magnitude\nimprovement in FLOPs efficiency and a threefold increase in runtime speed\ncompared to equally-sized GPT models in the low-size regime. This is achieved\nthrough a unique generation method that caches and reuses sentence embeddings,\nallowing significant portions of the input to bypass large parts of the\nnetwork.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We challenge the prevailing assumption that LLMs must rely fully on sub-word\ntokens for high-quality text generation. To this end, we propose the\n\"Generative Pretrained Thoughtformer\" (GPTHF), a hierarchical transformer\nlanguage model capable of text generation by compressing text into sentence\nembeddings and employing a sentence attention mechanism. GPTHF retains GPT's\narchitecture, modifying only token interactions via dynamic sparse attention\nmasks.\n  Our experiments show that GPTHF achieves an up to an order of magnitude\nimprovement in FLOPs efficiency and a threefold increase in runtime speed\ncompared to equally-sized GPT models in the low-size regime. This is achieved\nthrough a unique generation method that caches and reuses sentence embeddings,\nallowing significant portions of the input to bypass large parts of the\nnetwork."
                },
                "authors": [
                    {
                        "name": "David Gu"
                    },
                    {
                        "name": "Peter Belcak"
                    },
                    {
                        "name": "Roger Wattenhofer"
                    }
                ],
                "author_detail": {
                    "name": "Roger Wattenhofer"
                },
                "author": "Roger Wattenhofer",
                "arxiv_comment": "accepted to NAACL SRW 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11426v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11426v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11132v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11132v1",
                "updated": "2025-03-14T06:49:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    49,
                    37,
                    4,
                    73,
                    0
                ],
                "published": "2025-03-14T06:49:37Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    49,
                    37,
                    4,
                    73,
                    0
                ],
                "title": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and\n  Extreme KV Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and\n  Extreme KV Compression"
                },
                "summary": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid (i.e., combination of regular attention and MLA\nlayers) or full MLA variant through lightweight post-training adaptation,\nbypassing the need for extensive pre-training. We demonstrate that leveraging\nthe dark knowledge of a well-trained model can enhance training accuracy and\nenable extreme KV cache compression in MLA without compromising model\nperformance. Our results show that using an 8B teacher model allows us to\ncompress the KV cache size of the Llama3.2-1B-Inst baseline by 6.4x while\npreserving 100% of its average score across multiple tasks on the LM Harness\nEvaluation benchmark. This is achieved with only 3.6B training tokens and about\n70 GPU hours on AMD MI300 GPUs, compared to the 370K GPU hours required for\npre-training the Llama3.2-1B model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid (i.e., combination of regular attention and MLA\nlayers) or full MLA variant through lightweight post-training adaptation,\nbypassing the need for extensive pre-training. We demonstrate that leveraging\nthe dark knowledge of a well-trained model can enhance training accuracy and\nenable extreme KV cache compression in MLA without compromising model\nperformance. Our results show that using an 8B teacher model allows us to\ncompress the KV cache size of the Llama3.2-1B-Inst baseline by 6.4x while\npreserving 100% of its average score across multiple tasks on the LM Harness\nEvaluation benchmark. This is achieved with only 3.6B training tokens and about\n70 GPU hours on AMD MI300 GPUs, compared to the 370K GPU hours required for\npre-training the Llama3.2-1B model."
                },
                "authors": [
                    {
                        "name": "Guihong Li"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    },
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Vikram Appia"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11132v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11132v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11108v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11108v2",
                "updated": "2025-03-27T07:02:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    7,
                    2,
                    19,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-14T06:01:42Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    1,
                    42,
                    4,
                    73,
                    0
                ],
                "title": "Time and Memory Trade-off of KV-Cache Compression in Tensor Transformer\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time and Memory Trade-off of KV-Cache Compression in Tensor Transformer\n  Decoding"
                },
                "summary": "The key-value (KV) cache in the tensor version of transformers presents a\nsignificant bottleneck during inference. While previous work analyzes the\nfundamental space complexity barriers in standard attention mechanisms [Haris\nand Onak, 2025], our work generalizes the space complexity barriers result to\ntensor attention version. Our theoretical contributions rely on a reduction\nfrom communication complexity and deduce the memory lower bound for\ntensor-structured attention mechanisms when $d = \\Omega(\\log n)$. Furthermore,\nwe introduce two types of tensor attention cache and present a trade-off\nbetween time and memory for two scenarios. Overall, our work provides a\ntheoretical foundation for us to understand the time-memory tradeoff of\nKV-Cache compression in tensor attention decoding and offers more perspectives\nin developing more memory-efficient tensor attention Transformer architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The key-value (KV) cache in the tensor version of transformers presents a\nsignificant bottleneck during inference. While previous work analyzes the\nfundamental space complexity barriers in standard attention mechanisms [Haris\nand Onak, 2025], our work generalizes the space complexity barriers result to\ntensor attention version. Our theoretical contributions rely on a reduction\nfrom communication complexity and deduce the memory lower bound for\ntensor-structured attention mechanisms when $d = \\Omega(\\log n)$. Furthermore,\nwe introduce two types of tensor attention cache and present a trade-off\nbetween time and memory for two scenarios. Overall, our work provides a\ntheoretical foundation for us to understand the time-memory tradeoff of\nKV-Cache compression in tensor attention decoding and offers more perspectives\nin developing more memory-efficient tensor attention Transformer architectures."
                },
                "authors": [
                    {
                        "name": "Yifang Chen"
                    },
                    {
                        "name": "Xiaoyu Li"
                    },
                    {
                        "name": "Yingyu Liang"
                    },
                    {
                        "name": "Zhenmei Shi"
                    },
                    {
                        "name": "Zhao Song"
                    },
                    {
                        "name": "Yu Tian"
                    }
                ],
                "author_detail": {
                    "name": "Yu Tian"
                },
                "author": "Yu Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11108v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11108v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10589v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10589v1",
                "updated": "2025-03-13T17:40:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    40,
                    7,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T17:40:07Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    40,
                    7,
                    3,
                    72,
                    0
                ],
                "title": "Long Context Tuning for Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long Context Tuning for Video Generation"
                },
                "summary": "Recent advances in video generation can produce realistic, minute-long\nsingle-shot videos with scalable diffusion transformers. However, real-world\nnarrative videos require multi-shot scenes with visual and dynamic consistency\nacross shots. In this work, we introduce Long Context Tuning (LCT), a training\nparadigm that expands the context window of pre-trained single-shot video\ndiffusion models to learn scene-level consistency directly from data. Our\nmethod expands full attention mechanisms from individual shots to encompass all\nshots within a scene, incorporating interleaved 3D position embedding and an\nasynchronous noise strategy, enabling both joint and auto-regressive shot\ngeneration without additional parameters. Models with bidirectional attention\nafter LCT can further be fine-tuned with context-causal attention, facilitating\nauto-regressive generation with efficient KV-cache. Experiments demonstrate\nsingle-shot models after LCT can produce coherent multi-shot scenes and exhibit\nemerging capabilities, including compositional generation and interactive shot\nextension, paving the way for more practical visual content creation. See\nhttps://guoyww.github.io/projects/long-context-video/ for more details.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in video generation can produce realistic, minute-long\nsingle-shot videos with scalable diffusion transformers. However, real-world\nnarrative videos require multi-shot scenes with visual and dynamic consistency\nacross shots. In this work, we introduce Long Context Tuning (LCT), a training\nparadigm that expands the context window of pre-trained single-shot video\ndiffusion models to learn scene-level consistency directly from data. Our\nmethod expands full attention mechanisms from individual shots to encompass all\nshots within a scene, incorporating interleaved 3D position embedding and an\nasynchronous noise strategy, enabling both joint and auto-regressive shot\ngeneration without additional parameters. Models with bidirectional attention\nafter LCT can further be fine-tuned with context-causal attention, facilitating\nauto-regressive generation with efficient KV-cache. Experiments demonstrate\nsingle-shot models after LCT can produce coherent multi-shot scenes and exhibit\nemerging capabilities, including compositional generation and interactive shot\nextension, paving the way for more practical visual content creation. See\nhttps://guoyww.github.io/projects/long-context-video/ for more details."
                },
                "authors": [
                    {
                        "name": "Yuwei Guo"
                    },
                    {
                        "name": "Ceyuan Yang"
                    },
                    {
                        "name": "Ziyan Yang"
                    },
                    {
                        "name": "Zhibei Ma"
                    },
                    {
                        "name": "Zhijie Lin"
                    },
                    {
                        "name": "Zhenheng Yang"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Lu Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Lu Jiang"
                },
                "author": "Lu Jiang",
                "arxiv_comment": "Project Page: https://guoyww.github.io/projects/long-context-video/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10589v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10589v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10568v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10568v1",
                "updated": "2025-03-13T17:19:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    19,
                    51,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T17:19:51Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    19,
                    51,
                    3,
                    72,
                    0
                ],
                "title": "Autoregressive Image Generation with Randomized Parallel Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Image Generation with Randomized Parallel Decoding"
                },
                "summary": "We introduce ARPG, a novel visual autoregressive model that enables\nrandomized parallel generation, addressing the inherent limitations of\nconventional raster-order approaches, which hinder inference efficiency and\nzero-shot generalization due to their sequential, predefined token generation\norder. Our key insight is that effective random-order modeling necessitates\nexplicit guidance for determining the position of the next predicted token. To\nthis end, we propose a novel guided decoding framework that decouples\npositional guidance from content representation, encoding them separately as\nqueries and key-value pairs. By directly incorporating this guidance into the\ncausal attention mechanism, our approach enables fully random-order training\nand generation, eliminating the need for bidirectional attention. Consequently,\nARPG readily generalizes to zero-shot tasks such as image inpainting,\noutpainting, and resolution expansion. Furthermore, it supports parallel\ninference by concurrently processing multiple queries using a shared KV cache.\nOn the ImageNet-1K 256 benchmark, our approach attains an FID of 1.94 with only\n64 sampling steps, achieving over a 20-fold increase in throughput while\nreducing memory consumption by over 75% compared to representative recent\nautoregressive models at a similar scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce ARPG, a novel visual autoregressive model that enables\nrandomized parallel generation, addressing the inherent limitations of\nconventional raster-order approaches, which hinder inference efficiency and\nzero-shot generalization due to their sequential, predefined token generation\norder. Our key insight is that effective random-order modeling necessitates\nexplicit guidance for determining the position of the next predicted token. To\nthis end, we propose a novel guided decoding framework that decouples\npositional guidance from content representation, encoding them separately as\nqueries and key-value pairs. By directly incorporating this guidance into the\ncausal attention mechanism, our approach enables fully random-order training\nand generation, eliminating the need for bidirectional attention. Consequently,\nARPG readily generalizes to zero-shot tasks such as image inpainting,\noutpainting, and resolution expansion. Furthermore, it supports parallel\ninference by concurrently processing multiple queries using a shared KV cache.\nOn the ImageNet-1K 256 benchmark, our approach attains an FID of 1.94 with only\n64 sampling steps, achieving over a 20-fold increase in throughput while\nreducing memory consumption by over 75% compared to representative recent\nautoregressive models at a similar scale."
                },
                "authors": [
                    {
                        "name": "Haopeng Li"
                    },
                    {
                        "name": "Jinyue Yang"
                    },
                    {
                        "name": "Guoqi Li"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10568v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10568v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07720v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07720v2",
                "updated": "2025-03-13T16:29:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    16,
                    29,
                    17,
                    3,
                    72,
                    0
                ],
                "published": "2024-12-10T18:13:20Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    13,
                    20,
                    1,
                    345,
                    0
                ],
                "title": "ACDiT: Interpolating Autoregressive Conditional Modeling and Diffusion\n  Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ACDiT: Interpolating Autoregressive Conditional Modeling and Diffusion\n  Transformer"
                },
                "summary": "We present ACDiT, a novel Autoregressive blockwise Conditional Diffusion\nTransformer, that innovatively combines autoregressive and diffusion paradigms\nfor modeling continuous visual information. By introducing a block-wise\nautoregressive unit, ACDiT offers a flexible interpolation between token-wise\nautoregression and full-sequence diffusion, bypassing the limitations of\ndiscrete tokenization. The generation of each block is formulated as a\nconditional diffusion process, conditioned on prior blocks. ACDiT is easy to\nimplement, as simple as creating a Skip-Causal Attention Mask (SCAM) on\nstandard diffusion transformer during training. During inference, the process\niterates between diffusion denoising and autoregressive decoding that can make\nfull use of KV-Cache. We show that ACDiT performs best among all autoregressive\nbaselines under similar model scales on image and video generation tasks. We\nalso demonstrate that benefiting from autoregressive modeling, pretrained ACDiT\ncan be transferred in visual understanding tasks despite being trained with the\ndiffusion objective. The analysis of the trade-off between autoregressive\nmodeling and diffusion demonstrates the potential of ACDiT to be used in\nlong-horizon visual generation tasks. We hope that ACDiT offers a novel\nperspective on visual autoregressive generation and unlocks new avenues for\nunified models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present ACDiT, a novel Autoregressive blockwise Conditional Diffusion\nTransformer, that innovatively combines autoregressive and diffusion paradigms\nfor modeling continuous visual information. By introducing a block-wise\nautoregressive unit, ACDiT offers a flexible interpolation between token-wise\nautoregression and full-sequence diffusion, bypassing the limitations of\ndiscrete tokenization. The generation of each block is formulated as a\nconditional diffusion process, conditioned on prior blocks. ACDiT is easy to\nimplement, as simple as creating a Skip-Causal Attention Mask (SCAM) on\nstandard diffusion transformer during training. During inference, the process\niterates between diffusion denoising and autoregressive decoding that can make\nfull use of KV-Cache. We show that ACDiT performs best among all autoregressive\nbaselines under similar model scales on image and video generation tasks. We\nalso demonstrate that benefiting from autoregressive modeling, pretrained ACDiT\ncan be transferred in visual understanding tasks despite being trained with the\ndiffusion objective. The analysis of the trade-off between autoregressive\nmodeling and diffusion demonstrates the potential of ACDiT to be used in\nlong-horizon visual generation tasks. We hope that ACDiT offers a novel\nperspective on visual autoregressive generation and unlocks new avenues for\nunified models."
                },
                "authors": [
                    {
                        "name": "Jinyi Hu"
                    },
                    {
                        "name": "Shengding Hu"
                    },
                    {
                        "name": "Yuxuan Song"
                    },
                    {
                        "name": "Yufei Huang"
                    },
                    {
                        "name": "Mingxuan Wang"
                    },
                    {
                        "name": "Hao Zhou"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Wei-Ying Ma"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07720v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07720v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10501v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10501v1",
                "updated": "2025-03-13T16:04:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    16,
                    4,
                    31,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T16:04:31Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    16,
                    4,
                    31,
                    3,
                    72,
                    0
                ],
                "title": "TokenCarve: Information-Preserving Visual Token Compression in\n  Multimodal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenCarve: Information-Preserving Visual Token Compression in\n  Multimodal Large Language Models"
                },
                "summary": "Multimodal Large Language Models (MLLMs) are becoming increasingly popular,\nwhile the high computational cost associated with multimodal data input,\nparticularly from visual tokens, poses a significant challenge. Existing\ntraining-based token compression methods improve inference efficiency but\nrequire costly retraining, while training-free methods struggle to maintain\nperformance when aggressively reducing token counts. In this study, we reveal\nthat the performance degradation of MLLM closely correlates with the\naccelerated loss of information in the attention output matrix. This insight\nintroduces a novel information-preserving perspective, making it possible to\nmaintain performance even under extreme token compression. Based on this\nfinding, we propose TokenCarve, a training-free, plug-and-play, two-stage token\ncompression framework. The first stage employs an\nInformation-Preservation-Guided Selection (IPGS) strategy to prune\nlow-information tokens, while the second stage further leverages IPGS to guide\ntoken merging, minimizing information loss. Extensive experiments on 11\ndatasets and 2 model variants demonstrate the effectiveness of TokenCarve. It\ncan even reduce the number of visual tokens to 22.2% of the original count,\nachieving a 1.23x speedup in inference, a 64% reduction in KV cache storage,\nand only a 1.54% drop in accuracy. Our code is available at\nhttps://github.com/ShawnTan86/TokenCarve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) are becoming increasingly popular,\nwhile the high computational cost associated with multimodal data input,\nparticularly from visual tokens, poses a significant challenge. Existing\ntraining-based token compression methods improve inference efficiency but\nrequire costly retraining, while training-free methods struggle to maintain\nperformance when aggressively reducing token counts. In this study, we reveal\nthat the performance degradation of MLLM closely correlates with the\naccelerated loss of information in the attention output matrix. This insight\nintroduces a novel information-preserving perspective, making it possible to\nmaintain performance even under extreme token compression. Based on this\nfinding, we propose TokenCarve, a training-free, plug-and-play, two-stage token\ncompression framework. The first stage employs an\nInformation-Preservation-Guided Selection (IPGS) strategy to prune\nlow-information tokens, while the second stage further leverages IPGS to guide\ntoken merging, minimizing information loss. Extensive experiments on 11\ndatasets and 2 model variants demonstrate the effectiveness of TokenCarve. It\ncan even reduce the number of visual tokens to 22.2% of the original count,\nachieving a 1.23x speedup in inference, a 64% reduction in KV cache storage,\nand only a 1.54% drop in accuracy. Our code is available at\nhttps://github.com/ShawnTan86/TokenCarve."
                },
                "authors": [
                    {
                        "name": "Xudong Tan"
                    },
                    {
                        "name": "Peng Ye"
                    },
                    {
                        "name": "Chongjun Tu"
                    },
                    {
                        "name": "Jianjian Cao"
                    },
                    {
                        "name": "Yaoxin Yang"
                    },
                    {
                        "name": "Lin Zhang"
                    },
                    {
                        "name": "Dongzhan Zhou"
                    },
                    {
                        "name": "Tao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tao Chen"
                },
                "author": "Tao Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10501v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10501v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10494v1",
                "updated": "2025-03-13T15:57:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    15,
                    57,
                    50,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T15:57:50Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    15,
                    57,
                    50,
                    3,
                    72,
                    0
                ],
                "title": "Source-primed Multi-turn Conversation Helps Large Language Models\n  Translate Documents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Source-primed Multi-turn Conversation Helps Large Language Models\n  Translate Documents"
                },
                "summary": "LLMs have paved the way for truly simple document-level machine translation,\nbut challenges such as omission errors remain. In this paper, we study a simple\nmethod for handling document-level machine translation, by leveraging previous\ncontexts in a multi-turn conversational manner. Specifically, by decomposing\ndocuments into segments and iteratively translating them while maintaining\nprevious turns, this method ensures coherent translations without additional\ntraining, and can fully re-use the KV cache of previous turns thus minimizing\ncomputational overhead. We further propose a `source-primed' method that first\nprovides the whole source document before multi-turn translation. We\nempirically show this multi-turn method outperforms both translating entire\ndocuments in a single turn and translating each segment independently according\nto multiple automatic metrics in representative LLMs, establishing a strong\nbaseline for document-level translation using LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs have paved the way for truly simple document-level machine translation,\nbut challenges such as omission errors remain. In this paper, we study a simple\nmethod for handling document-level machine translation, by leveraging previous\ncontexts in a multi-turn conversational manner. Specifically, by decomposing\ndocuments into segments and iteratively translating them while maintaining\nprevious turns, this method ensures coherent translations without additional\ntraining, and can fully re-use the KV cache of previous turns thus minimizing\ncomputational overhead. We further propose a `source-primed' method that first\nprovides the whole source document before multi-turn translation. We\nempirically show this multi-turn method outperforms both translating entire\ndocuments in a single turn and translating each segment independently according\nto multiple automatic metrics in representative LLMs, establishing a strong\nbaseline for document-level translation using LLMs."
                },
                "authors": [
                    {
                        "name": "Hanxu Hu"
                    },
                    {
                        "name": "Jannis Vamvas"
                    },
                    {
                        "name": "Rico Sennrich"
                    }
                ],
                "author_detail": {
                    "name": "Rico Sennrich"
                },
                "author": "Rico Sennrich",
                "arxiv_comment": "9 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10337v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10337v1",
                "updated": "2025-03-13T13:15:28Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    13,
                    15,
                    28,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T13:15:28Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    13,
                    15,
                    28,
                    3,
                    72,
                    0
                ],
                "title": "KV-Distill: Nearly Lossless Learnable Context Compression for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Distill: Nearly Lossless Learnable Context Compression for LLMs"
                },
                "summary": "Sequence-to-sequence tasks often benefit from long contexts, but the\nquadratic complexity of self-attention in standard Transformers renders this\nnon-trivial. During generation, temporary representations -stored in the\nso-called KV cache-account for a large portion of GPU memory usage and scale\nlinearly with context length. We introduce KV-Distill, a Transformer\ncompression framework that distills long context KV caches into significantly\nshorter representations in a question-independent fashion. KV-Distill can be\ntrained as a parameter-efficient adaptor for pretrained models, and enables the\ncompression of arbitrary spans of a context while preserving pre-trained model\ncapabilities. We treat a compressed-uncompressed cache as a student-teacher\npairing and apply a KL-type divergence to match the generated outputs.\nKV-Distill outperforms other compression techniques in worst-case extractive\ntasks and approaches uncompressed performance in long context question\nanswering and summarization, and it can be fine-tuned on domain-specific\ncontexts to reduce lengths by up to 99% while preserving downstream\nperformance. We demonstrate the generalizability of KV-Distill across various\nmodel sizes and architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequence-to-sequence tasks often benefit from long contexts, but the\nquadratic complexity of self-attention in standard Transformers renders this\nnon-trivial. During generation, temporary representations -stored in the\nso-called KV cache-account for a large portion of GPU memory usage and scale\nlinearly with context length. We introduce KV-Distill, a Transformer\ncompression framework that distills long context KV caches into significantly\nshorter representations in a question-independent fashion. KV-Distill can be\ntrained as a parameter-efficient adaptor for pretrained models, and enables the\ncompression of arbitrary spans of a context while preserving pre-trained model\ncapabilities. We treat a compressed-uncompressed cache as a student-teacher\npairing and apply a KL-type divergence to match the generated outputs.\nKV-Distill outperforms other compression techniques in worst-case extractive\ntasks and approaches uncompressed performance in long context question\nanswering and summarization, and it can be fine-tuned on domain-specific\ncontexts to reduce lengths by up to 99% while preserving downstream\nperformance. We demonstrate the generalizability of KV-Distill across various\nmodel sizes and architectures."
                },
                "authors": [
                    {
                        "name": "Vivek Chari"
                    },
                    {
                        "name": "Guanghui Qin"
                    },
                    {
                        "name": "Benjamin Van Durme"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Van Durme"
                },
                "author": "Benjamin Van Durme",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10337v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10337v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10270v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10270v1",
                "updated": "2025-03-13T11:26:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    11,
                    26,
                    45,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T11:26:45Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    11,
                    26,
                    45,
                    3,
                    72,
                    0
                ],
                "title": "EEdit : Rethinking the Spatial and Temporal Redundancy for Efficient\n  Image Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EEdit : Rethinking the Spatial and Temporal Redundancy for Efficient\n  Image Editing"
                },
                "summary": "Inversion-based image editing is rapidly gaining momentum while suffering\nfrom significant computation overhead, hindering its application in real-time\ninteractive scenarios. In this paper, we rethink that the redundancy in\ninversion-based image editing exists in both the spatial and temporal\ndimensions, such as the unnecessary computation in unedited regions and the\nredundancy in the inversion progress. To tackle these challenges, we propose a\npractical framework, named EEdit, to achieve efficient image editing.\nSpecifically, we introduce three techniques to solve them one by one. For\nspatial redundancy, spatial locality caching is introduced to compute the\nedited region and its neighboring regions while skipping the unedited regions,\nand token indexing preprocessing is designed to further accelerate the caching.\nFor temporal redundancy, inversion step skipping is proposed to reuse the\nlatent for efficient editing. Our experiments demonstrate an average of 2.46\n$\\times$ acceleration without performance drop in a wide range of editing tasks\nincluding prompt-guided image editing, dragging and image composition. Our\ncodes are available at https://github.com/yuriYanZeXuan/EEdit",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inversion-based image editing is rapidly gaining momentum while suffering\nfrom significant computation overhead, hindering its application in real-time\ninteractive scenarios. In this paper, we rethink that the redundancy in\ninversion-based image editing exists in both the spatial and temporal\ndimensions, such as the unnecessary computation in unedited regions and the\nredundancy in the inversion progress. To tackle these challenges, we propose a\npractical framework, named EEdit, to achieve efficient image editing.\nSpecifically, we introduce three techniques to solve them one by one. For\nspatial redundancy, spatial locality caching is introduced to compute the\nedited region and its neighboring regions while skipping the unedited regions,\nand token indexing preprocessing is designed to further accelerate the caching.\nFor temporal redundancy, inversion step skipping is proposed to reuse the\nlatent for efficient editing. Our experiments demonstrate an average of 2.46\n$\\times$ acceleration without performance drop in a wide range of editing tasks\nincluding prompt-guided image editing, dragging and image composition. Our\ncodes are available at https://github.com/yuriYanZeXuan/EEdit"
                },
                "authors": [
                    {
                        "name": "Zexuan Yan"
                    },
                    {
                        "name": "Yue Ma"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Wenteng Chen"
                    },
                    {
                        "name": "Qifeng Chen"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "17 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10270v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10270v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07752v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07752v3",
                "updated": "2025-03-13T11:14:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    11,
                    14,
                    49,
                    3,
                    72,
                    0
                ],
                "published": "2024-12-10T18:50:37Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    50,
                    37,
                    1,
                    345,
                    0
                ],
                "title": "FlashRNN: I/O-Aware Optimization of Traditional RNNs on modern hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashRNN: I/O-Aware Optimization of Traditional RNNs on modern hardware"
                },
                "summary": "While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: https://github.com/NX-AI/flashrnn",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: https://github.com/NX-AI/flashrnn"
                },
                "authors": [
                    {
                        "name": "Korbinian Pöppel"
                    },
                    {
                        "name": "Maximilian Beck"
                    },
                    {
                        "name": "Sepp Hochreiter"
                    }
                ],
                "author_detail": {
                    "name": "Sepp Hochreiter"
                },
                "author": "Sepp Hochreiter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07752v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07752v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10074v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10074v1",
                "updated": "2025-03-13T05:43:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    5,
                    43,
                    14,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T05:43:14Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    5,
                    43,
                    14,
                    3,
                    72,
                    0
                ],
                "title": "Demoting Security via Exploitation of Cache Demote Operation in Intel's\n  Latest ISA Extension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demoting Security via Exploitation of Cache Demote Operation in Intel's\n  Latest ISA Extension"
                },
                "summary": "ISA extensions are increasingly adopted to boost the performance of\nspecialized workloads without requiring an entire architectural redesign.\nHowever, these enhancements can inadvertently expose new attack surfaces in the\nmicroarchitecture. In this paper, we investigate Intel's recently introduced\ncldemote extension, which promotes efficient data sharing by transferring cache\nlines from upper-level caches to the Last Level Cache (LLC). Despite its\nperformance benefits, we uncover critical properties-unprivileged access,\ninter-cache state transition, and fault suppression-that render cldemote\nexploitable for microarchitectural attacks. We propose two new attack\nprimitives, Flush+Demote and Demote+Time, built on our analysis. Flush+Demote\nconstructs a covert channel with a bandwidth of 2.84 Mbps and a bit error rate\nof 0.018%, while Demote+Time derandomizes the kernel base address in 2.49 ms on\nLinux. Furthermore, we show that leveraging cldemote accelerates eviction set\nconstruction in non-inclusive LLC designs by obviating the need for helper\nthreads or extensive cache conflicts, thereby reducing construction time by 36%\nyet retaining comparable success rates. Finally, we examine how ISA extensions\ncontribute to broader microarchitectural attacks, identifying five key\nexploitable characteristics and categorizing four distinct attack types. We\nalso discuss potential countermeasures, highlighting the far-reaching security\nimplications of emerging ISA extensions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ISA extensions are increasingly adopted to boost the performance of\nspecialized workloads without requiring an entire architectural redesign.\nHowever, these enhancements can inadvertently expose new attack surfaces in the\nmicroarchitecture. In this paper, we investigate Intel's recently introduced\ncldemote extension, which promotes efficient data sharing by transferring cache\nlines from upper-level caches to the Last Level Cache (LLC). Despite its\nperformance benefits, we uncover critical properties-unprivileged access,\ninter-cache state transition, and fault suppression-that render cldemote\nexploitable for microarchitectural attacks. We propose two new attack\nprimitives, Flush+Demote and Demote+Time, built on our analysis. Flush+Demote\nconstructs a covert channel with a bandwidth of 2.84 Mbps and a bit error rate\nof 0.018%, while Demote+Time derandomizes the kernel base address in 2.49 ms on\nLinux. Furthermore, we show that leveraging cldemote accelerates eviction set\nconstruction in non-inclusive LLC designs by obviating the need for helper\nthreads or extensive cache conflicts, thereby reducing construction time by 36%\nyet retaining comparable success rates. Finally, we examine how ISA extensions\ncontribute to broader microarchitectural attacks, identifying five key\nexploitable characteristics and categorizing four distinct attack types. We\nalso discuss potential countermeasures, highlighting the far-reaching security\nimplications of emerging ISA extensions."
                },
                "authors": [
                    {
                        "name": "Taehun Kim"
                    },
                    {
                        "name": "Hyerean Jang"
                    },
                    {
                        "name": "Youngjoo Shin"
                    }
                ],
                "author_detail": {
                    "name": "Youngjoo Shin"
                },
                "author": "Youngjoo Shin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10074v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10074v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17599v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17599v2",
                "updated": "2025-03-13T04:04:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    4,
                    4,
                    8,
                    3,
                    72,
                    0
                ],
                "published": "2025-02-24T19:34:52Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    19,
                    34,
                    52,
                    0,
                    55,
                    0
                ],
                "title": "MEDA: Dynamic KV Cache Allocation for Efficient Multimodal Long-Context\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MEDA: Dynamic KV Cache Allocation for Efficient Multimodal Long-Context\n  Inference"
                },
                "summary": "Long-context Multimodal Large Language Models (MLLMs) that incorporate long\ntext-image and text-video modalities, demand substantial resources as their\nmultimodal Key-Value (KV) caches grow with increasing input lengths,\nchallenging inference efficiency. Existing methods for KV cache compression, in\nboth text-only and multimodal LLMs, have neglected attention density variations\nacross layers, thus often adopting uniform or progressive reduction strategies\nfor layer-wise cache allocation. In this work, we propose MEDA, a dynamic\nlayer-wise KV cache allocation method for efficient multimodal long-context\ninference. As its core, MEDA utilizes cross-modal attention entropy to\ndetermine the KV cache size at each MLLMs layer. Given the dynamically\nallocated KV cache size at each layer, MEDA also employs a KV pair selection\nscheme to identify which KV pairs to select and a KV pair merging strategy that\nmerges the selected and non-selected ones to preserve information from the\nentire context. MEDA achieves up to 72% KV cache memory reduction and 2.82\ntimes faster decoding speed, while maintaining or enhancing performance on\nvarious multimodal tasks in long-context settings, including multi-images and\nlong-video scenarios. Our code is released at\nhttps://github.com/AIoT-MLSys-Lab/MEDA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context Multimodal Large Language Models (MLLMs) that incorporate long\ntext-image and text-video modalities, demand substantial resources as their\nmultimodal Key-Value (KV) caches grow with increasing input lengths,\nchallenging inference efficiency. Existing methods for KV cache compression, in\nboth text-only and multimodal LLMs, have neglected attention density variations\nacross layers, thus often adopting uniform or progressive reduction strategies\nfor layer-wise cache allocation. In this work, we propose MEDA, a dynamic\nlayer-wise KV cache allocation method for efficient multimodal long-context\ninference. As its core, MEDA utilizes cross-modal attention entropy to\ndetermine the KV cache size at each MLLMs layer. Given the dynamically\nallocated KV cache size at each layer, MEDA also employs a KV pair selection\nscheme to identify which KV pairs to select and a KV pair merging strategy that\nmerges the selected and non-selected ones to preserve information from the\nentire context. MEDA achieves up to 72% KV cache memory reduction and 2.82\ntimes faster decoding speed, while maintaining or enhancing performance on\nvarious multimodal tasks in long-context settings, including multi-images and\nlong-video scenarios. Our code is released at\nhttps://github.com/AIoT-MLSys-Lab/MEDA."
                },
                "authors": [
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Hui Shen"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Che Liu"
                    },
                    {
                        "name": "Zheda Mai"
                    },
                    {
                        "name": "Mi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Mi Zhang"
                },
                "author": "Mi Zhang",
                "arxiv_comment": "NAACL 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17599v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17599v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10714v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10714v1",
                "updated": "2025-03-13T03:36:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    3,
                    36,
                    3,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-13T03:36:03Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    3,
                    36,
                    3,
                    3,
                    72,
                    0
                ],
                "title": "ZeroMerge: Parameter-Free KV Cache Compression for Memory-Efficient\n  Long-Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZeroMerge: Parameter-Free KV Cache Compression for Memory-Efficient\n  Long-Context LLMs"
                },
                "summary": "The linear growth of key-value (KV) cache memory and quadratic computational\ncomplexity pose significant bottlenecks for large language models (LLMs) in\nlong-context processing. While existing KV cache optimization methods address\nthese challenges through token pruning or feature merging, they often suffer\nfrom irreversible information loss or require costly parameter retraining. We\npropose ZeroMerge, a dynamic zero-shot compression framework that achieves\nefficient cache management through three key innovations: (1) Fine-grained\nmemory allocation guided by multi-dimensional token importance metrics at\nhead-level granularity, (2) A residual merging mechanism that preserves\ncritical context through compensated attention scoring, and (3) Parameter-free\nadaptation compatible with diverse LLM architectures without retraining.\nComprehensive evaluations across LLaMA-2 model demonstrate that ZeroMerge\nmaintains full-cache performance at 5\\% compression ratios while doubling\ninference throughput at 40K token lengths. The method effectively balances\nmemory efficiency, generation quality, and deployment flexibility, advancing\npractical long-context LLM applications. The code is available at\nhttps://github.com/SusCom-Lab/ZeroMerge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The linear growth of key-value (KV) cache memory and quadratic computational\ncomplexity pose significant bottlenecks for large language models (LLMs) in\nlong-context processing. While existing KV cache optimization methods address\nthese challenges through token pruning or feature merging, they often suffer\nfrom irreversible information loss or require costly parameter retraining. We\npropose ZeroMerge, a dynamic zero-shot compression framework that achieves\nefficient cache management through three key innovations: (1) Fine-grained\nmemory allocation guided by multi-dimensional token importance metrics at\nhead-level granularity, (2) A residual merging mechanism that preserves\ncritical context through compensated attention scoring, and (3) Parameter-free\nadaptation compatible with diverse LLM architectures without retraining.\nComprehensive evaluations across LLaMA-2 model demonstrate that ZeroMerge\nmaintains full-cache performance at 5\\% compression ratios while doubling\ninference throughput at 40K token lengths. The method effectively balances\nmemory efficiency, generation quality, and deployment flexibility, advancing\npractical long-context LLM applications. The code is available at\nhttps://github.com/SusCom-Lab/ZeroMerge."
                },
                "authors": [
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Pei Liu"
                    },
                    {
                        "name": "Guoming Tang"
                    }
                ],
                "author_detail": {
                    "name": "Guoming Tang"
                },
                "author": "Guoming Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10714v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10714v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13035v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13035v3",
                "updated": "2025-03-13T03:16:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    3,
                    16,
                    43,
                    3,
                    72,
                    0
                ],
                "published": "2024-06-18T20:01:51Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    20,
                    1,
                    51,
                    1,
                    170,
                    0
                ],
                "title": "D2O: Dynamic Discriminative Operations for Efficient Long-Context\n  Inference of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "D2O: Dynamic Discriminative Operations for Efficient Long-Context\n  Inference of Large Language Models"
                },
                "summary": "Generative inference in Large Language Models (LLMs) is impeded by the\ngrowing memory demands of Key-Value (KV) cache, especially for longer\nsequences. Traditional KV cache eviction strategies, which discard less\ncritical KV pairs based on attention scores, often degrade generation quality,\nleading to issues such as context loss or hallucinations. In this work, we\nintroduce Dynamic Discriminative Operations (D2O), a KV cache compression\nmethod that optimizes KV cache size dynamically and discriminatively at two\nlevels without fine-tuning, while preserving essential context. At layer level,\nD2O leverages the varying densities of attention weights between shallow and\ndeep layers to dynamically determine which layers should avoid excessive\neviction via a novel dynamic allocation strategy to minimize information loss.\nAt token level, D2O incorporates a compensation mechanism that maintains a\nsimilarity threshold to re-discriminate the importance of currently discarded\ntokens, determining whether they should be recalled and merged with similar\ntokens. We conduct experiments on various benchmarks and LLM architectures. Our\nresults show that D2O not only achieves significant memory savings and enhances\ninference throughput by more than 3$\\times$ but also maintains high-quality\nlong-text generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative inference in Large Language Models (LLMs) is impeded by the\ngrowing memory demands of Key-Value (KV) cache, especially for longer\nsequences. Traditional KV cache eviction strategies, which discard less\ncritical KV pairs based on attention scores, often degrade generation quality,\nleading to issues such as context loss or hallucinations. In this work, we\nintroduce Dynamic Discriminative Operations (D2O), a KV cache compression\nmethod that optimizes KV cache size dynamically and discriminatively at two\nlevels without fine-tuning, while preserving essential context. At layer level,\nD2O leverages the varying densities of attention weights between shallow and\ndeep layers to dynamically determine which layers should avoid excessive\neviction via a novel dynamic allocation strategy to minimize information loss.\nAt token level, D2O incorporates a compensation mechanism that maintains a\nsimilarity threshold to re-discriminate the importance of currently discarded\ntokens, determining whether they should be recalled and merged with similar\ntokens. We conduct experiments on various benchmarks and LLM architectures. Our\nresults show that D2O not only achieves significant memory savings and enhances\ninference throughput by more than 3$\\times$ but also maintains high-quality\nlong-text generation."
                },
                "authors": [
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Xinjian Wu"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Yi Xin"
                    },
                    {
                        "name": "Chaofan Tao"
                    },
                    {
                        "name": "Zhihong Zhu"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Siqi Luo"
                    },
                    {
                        "name": "Jing Xiong"
                    },
                    {
                        "name": "Longyue Wang"
                    },
                    {
                        "name": "Mi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Mi Zhang"
                },
                "author": "Mi Zhang",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.13035v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13035v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.14361v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.14361v3",
                "updated": "2025-03-12T18:14:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    18,
                    14,
                    21,
                    2,
                    71,
                    0
                ],
                "published": "2024-01-25T18:07:50Z",
                "published_parsed": [
                    2024,
                    1,
                    25,
                    18,
                    7,
                    50,
                    3,
                    25,
                    0
                ],
                "title": "MoE-Infinity: Efficient MoE Inference on Personal Machines with\n  Sparsity-Aware Expert Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoE-Infinity: Efficient MoE Inference on Personal Machines with\n  Sparsity-Aware Expert Cache"
                },
                "summary": "This paper presents MoE-Infinity, an efficient MoE inference system designed\nfor personal machines with limited GPU memory capacity. The key idea for\nMoE-Infinity is that on personal machines, which are often single-user\nenvironments, MoE-based LLMs typically operate with a batch size of one. In\nthis setting, MoE models exhibit a high degree of activation sparsity, meaning\na small number of experts are frequently reused in generating tokens during the\ndecode phase. Leveraging this idea, we design a sparsity-aware expert cache,\nwhich can trace the sparse activation of experts during inference and carefully\nselect the trace that represents the sparsity pattern. By analyzing these\nselected traces, MoE-Infinity guides the replacement and prefetching of the\nexpert cache, providing 3.1-16.7x per-token latency improvements over numerous\nstate-of-the-art systems, including vLLM, Ollama, DeepSpeed and BrainStorm\nacross various MoE models (DeepSeek and Mixtral) when handling different LLM\ntasks. MoE-Infinity's source code is publicly available at\nhttps://github.com/EfficientMoE/MoE-Infinity",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents MoE-Infinity, an efficient MoE inference system designed\nfor personal machines with limited GPU memory capacity. The key idea for\nMoE-Infinity is that on personal machines, which are often single-user\nenvironments, MoE-based LLMs typically operate with a batch size of one. In\nthis setting, MoE models exhibit a high degree of activation sparsity, meaning\na small number of experts are frequently reused in generating tokens during the\ndecode phase. Leveraging this idea, we design a sparsity-aware expert cache,\nwhich can trace the sparse activation of experts during inference and carefully\nselect the trace that represents the sparsity pattern. By analyzing these\nselected traces, MoE-Infinity guides the replacement and prefetching of the\nexpert cache, providing 3.1-16.7x per-token latency improvements over numerous\nstate-of-the-art systems, including vLLM, Ollama, DeepSpeed and BrainStorm\nacross various MoE models (DeepSeek and Mixtral) when handling different LLM\ntasks. MoE-Infinity's source code is publicly available at\nhttps://github.com/EfficientMoE/MoE-Infinity"
                },
                "authors": [
                    {
                        "name": "Leyang Xue"
                    },
                    {
                        "name": "Yao Fu"
                    },
                    {
                        "name": "Zhan Lu"
                    },
                    {
                        "name": "Luo Mai"
                    },
                    {
                        "name": "Mahesh Marina"
                    }
                ],
                "author_detail": {
                    "name": "Mahesh Marina"
                },
                "author": "Mahesh Marina",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.14361v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.14361v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2503.20786v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20786v1",
                "updated": "2025-03-26T17:59:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    17,
                    59,
                    56,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T17:59:56Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    17,
                    59,
                    56,
                    2,
                    85,
                    0
                ],
                "title": "Mobile-MMLU: A Mobile Intelligence Language Understanding Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile-MMLU: A Mobile Intelligence Language Understanding Benchmark"
                },
                "summary": "Rapid advancements in large language models (LLMs) have increased interest in\ndeploying them on mobile devices for on-device AI applications. Mobile users\ninteract differently with LLMs compared to desktop users, creating unique\nexpectations and data biases. Current benchmark datasets primarily target at\nserver and desktop environments, and there is a notable lack of extensive\ndatasets specifically designed for mobile contexts. Additionally, mobile\ndevices face strict limitations in storage and computing resources,\nconstraining model size and capabilities, thus requiring optimized efficiency\nand prioritized knowledge. To address these challenges, we introduce\nMobile-MMLU, a large-scale benchmark dataset tailored for mobile intelligence.\nIt consists of 16,186 questions across 80 mobile-related fields, designed to\nevaluate LLM performance in realistic mobile scenarios. A challenging subset,\nMobile-MMLU-Pro, provides advanced evaluation similar in size to MMLU-Pro but\nsignificantly more difficult than our standard full set. Both benchmarks use\nmultiple-choice, order-invariant questions focused on practical mobile\ninteractions, such as recipe suggestions, travel planning, and essential daily\ntasks. The dataset emphasizes critical mobile-specific metrics like inference\nlatency, energy consumption, memory usage, and response quality, offering\ncomprehensive insights into model performance under mobile constraints.\nMoreover, it prioritizes privacy and adaptability, assessing models' ability to\nperform on-device processing, maintain user privacy, and adapt to personalized\nusage patterns. Mobile-MMLU family offers a standardized framework for\ndeveloping and comparing mobile-optimized LLMs, enabling advancements in\nproductivity and decision-making within mobile computing environments. Our code\nand data are available at: https://github.com/VILA-Lab/Mobile-MMLU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid advancements in large language models (LLMs) have increased interest in\ndeploying them on mobile devices for on-device AI applications. Mobile users\ninteract differently with LLMs compared to desktop users, creating unique\nexpectations and data biases. Current benchmark datasets primarily target at\nserver and desktop environments, and there is a notable lack of extensive\ndatasets specifically designed for mobile contexts. Additionally, mobile\ndevices face strict limitations in storage and computing resources,\nconstraining model size and capabilities, thus requiring optimized efficiency\nand prioritized knowledge. To address these challenges, we introduce\nMobile-MMLU, a large-scale benchmark dataset tailored for mobile intelligence.\nIt consists of 16,186 questions across 80 mobile-related fields, designed to\nevaluate LLM performance in realistic mobile scenarios. A challenging subset,\nMobile-MMLU-Pro, provides advanced evaluation similar in size to MMLU-Pro but\nsignificantly more difficult than our standard full set. Both benchmarks use\nmultiple-choice, order-invariant questions focused on practical mobile\ninteractions, such as recipe suggestions, travel planning, and essential daily\ntasks. The dataset emphasizes critical mobile-specific metrics like inference\nlatency, energy consumption, memory usage, and response quality, offering\ncomprehensive insights into model performance under mobile constraints.\nMoreover, it prioritizes privacy and adaptability, assessing models' ability to\nperform on-device processing, maintain user privacy, and adapt to personalized\nusage patterns. Mobile-MMLU family offers a standardized framework for\ndeveloping and comparing mobile-optimized LLMs, enabling advancements in\nproductivity and decision-making within mobile computing environments. Our code\nand data are available at: https://github.com/VILA-Lab/Mobile-MMLU."
                },
                "authors": [
                    {
                        "name": "Sondos Mahmoud Bsharat"
                    },
                    {
                        "name": "Mukul Ranjan"
                    },
                    {
                        "name": "Aidar Myrzakhan"
                    },
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Bowei Guo"
                    },
                    {
                        "name": "Shengkun Tang"
                    },
                    {
                        "name": "Zhuang Liu"
                    },
                    {
                        "name": "Yuanzhi Li"
                    },
                    {
                        "name": "Zhiqiang Shen"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Shen"
                },
                "author": "Zhiqiang Shen",
                "arxiv_comment": "An order-invariant and mobile-centric benchmark. Code and data are\n  available at: https://github.com/VILA-Lab/Mobile-MMLU",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20786v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20786v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20783v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20783v1",
                "updated": "2025-03-26T17:59:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    17,
                    59,
                    14,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T17:59:14Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    17,
                    59,
                    14,
                    2,
                    85,
                    0
                ],
                "title": "Understanding R1-Zero-Like Training: A Critical Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding R1-Zero-Like Training: A Critical Perspective"
                },
                "summary": "DeepSeek-R1-Zero has shown that reinforcement learning (RL) at scale can\ndirectly enhance the reasoning capabilities of LLMs without supervised\nfine-tuning. In this work, we critically examine R1-Zero-like training by\nanalyzing its two core components: base models and RL. We investigate a wide\nrange of base models, including DeepSeek-V3-Base, to understand how pretraining\ncharacteristics influence RL performance. Our analysis reveals that\nDeepSeek-V3-Base already exhibit ''Aha moment'', while Qwen2.5 base models\ndemonstrate strong reasoning capabilities even without prompt templates,\nsuggesting potential pretraining biases. Additionally, we identify an\noptimization bias in Group Relative Policy Optimization (GRPO), which\nartificially increases response length (especially for incorrect outputs)\nduring training. To address this, we introduce Dr. GRPO, an unbiased\noptimization method that improves token efficiency while maintaining reasoning\nperformance. Leveraging these insights, we present a minimalist R1-Zero recipe\nthat achieves 43.3% accuracy on AIME 2024 with a 7B base model, establishing a\nnew state-of-the-art. Our code is available at\nhttps://github.com/sail-sg/understand-r1-zero.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepSeek-R1-Zero has shown that reinforcement learning (RL) at scale can\ndirectly enhance the reasoning capabilities of LLMs without supervised\nfine-tuning. In this work, we critically examine R1-Zero-like training by\nanalyzing its two core components: base models and RL. We investigate a wide\nrange of base models, including DeepSeek-V3-Base, to understand how pretraining\ncharacteristics influence RL performance. Our analysis reveals that\nDeepSeek-V3-Base already exhibit ''Aha moment'', while Qwen2.5 base models\ndemonstrate strong reasoning capabilities even without prompt templates,\nsuggesting potential pretraining biases. Additionally, we identify an\noptimization bias in Group Relative Policy Optimization (GRPO), which\nartificially increases response length (especially for incorrect outputs)\nduring training. To address this, we introduce Dr. GRPO, an unbiased\noptimization method that improves token efficiency while maintaining reasoning\nperformance. Leveraging these insights, we present a minimalist R1-Zero recipe\nthat achieves 43.3% accuracy on AIME 2024 with a 7B base model, establishing a\nnew state-of-the-art. Our code is available at\nhttps://github.com/sail-sg/understand-r1-zero."
                },
                "authors": [
                    {
                        "name": "Zichen Liu"
                    },
                    {
                        "name": "Changyu Chen"
                    },
                    {
                        "name": "Wenjun Li"
                    },
                    {
                        "name": "Penghui Qi"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Wee Sun Lee"
                    },
                    {
                        "name": "Min Lin"
                    }
                ],
                "author_detail": {
                    "name": "Min Lin"
                },
                "author": "Min Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20783v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20783v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20776v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20776v1",
                "updated": "2025-03-26T17:56:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    17,
                    56,
                    16,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T17:56:16Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    17,
                    56,
                    16,
                    2,
                    85,
                    0
                ],
                "title": "Feature4X: Bridging Any Monocular Video to 4D Agentic AI with Versatile\n  Gaussian Feature Fields",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feature4X: Bridging Any Monocular Video to 4D Agentic AI with Versatile\n  Gaussian Feature Fields"
                },
                "summary": "Recent advancements in 2D and multimodal models have achieved remarkable\nsuccess by leveraging large-scale training on extensive datasets. However,\nextending these achievements to enable free-form interactions and high-level\nsemantic operations with complex 3D/4D scenes remains challenging. This\ndifficulty stems from the limited availability of large-scale, annotated 3D/4D\nor multi-view datasets, which are crucial for generalizable vision and language\ntasks such as open-vocabulary and prompt-based segmentation, language-guided\nediting, and visual question answering (VQA). In this paper, we introduce\nFeature4X, a universal framework designed to extend any functionality from 2D\nvision foundation model into the 4D realm, using only monocular video input,\nwhich is widely available from user-generated content. The \"X\" in Feature4X\nrepresents its versatility, enabling any task through adaptable,\nmodel-conditioned 4D feature field distillation. At the core of our framework\nis a dynamic optimization strategy that unifies multiple model capabilities\ninto a single representation. Additionally, to the best of our knowledge,\nFeature4X is the first method to distill and lift the features of video\nfoundation models (e.g. SAM2, InternVideo2) into an explicit 4D feature field\nusing Gaussian Splatting. Our experiments showcase novel view segment anything,\ngeometric and appearance scene editing, and free-form VQA across all time\nsteps, empowered by LLMs in feedback loops. These advancements broaden the\nscope of agentic AI applications by providing a foundation for scalable,\ncontextually and spatiotemporally aware systems capable of immersive dynamic 4D\nscene interaction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in 2D and multimodal models have achieved remarkable\nsuccess by leveraging large-scale training on extensive datasets. However,\nextending these achievements to enable free-form interactions and high-level\nsemantic operations with complex 3D/4D scenes remains challenging. This\ndifficulty stems from the limited availability of large-scale, annotated 3D/4D\nor multi-view datasets, which are crucial for generalizable vision and language\ntasks such as open-vocabulary and prompt-based segmentation, language-guided\nediting, and visual question answering (VQA). In this paper, we introduce\nFeature4X, a universal framework designed to extend any functionality from 2D\nvision foundation model into the 4D realm, using only monocular video input,\nwhich is widely available from user-generated content. The \"X\" in Feature4X\nrepresents its versatility, enabling any task through adaptable,\nmodel-conditioned 4D feature field distillation. At the core of our framework\nis a dynamic optimization strategy that unifies multiple model capabilities\ninto a single representation. Additionally, to the best of our knowledge,\nFeature4X is the first method to distill and lift the features of video\nfoundation models (e.g. SAM2, InternVideo2) into an explicit 4D feature field\nusing Gaussian Splatting. Our experiments showcase novel view segment anything,\ngeometric and appearance scene editing, and free-form VQA across all time\nsteps, empowered by LLMs in feedback loops. These advancements broaden the\nscope of agentic AI applications by providing a foundation for scalable,\ncontextually and spatiotemporally aware systems capable of immersive dynamic 4D\nscene interaction."
                },
                "authors": [
                    {
                        "name": "Shijie Zhou"
                    },
                    {
                        "name": "Hui Ren"
                    },
                    {
                        "name": "Yijia Weng"
                    },
                    {
                        "name": "Shuwang Zhang"
                    },
                    {
                        "name": "Zhen Wang"
                    },
                    {
                        "name": "Dejia Xu"
                    },
                    {
                        "name": "Zhiwen Fan"
                    },
                    {
                        "name": "Suya You"
                    },
                    {
                        "name": "Zhangyang Wang"
                    },
                    {
                        "name": "Leonidas Guibas"
                    },
                    {
                        "name": "Achuta Kadambi"
                    }
                ],
                "author_detail": {
                    "name": "Achuta Kadambi"
                },
                "author": "Achuta Kadambi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20776v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20776v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20774v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20774v1",
                "updated": "2025-03-26T17:54:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    17,
                    54,
                    4,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T17:54:04Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    17,
                    54,
                    4,
                    2,
                    85,
                    0
                ],
                "title": "PUREPath-B: A Tessellated Bayesian Model for Recovering CMB B-modes over\n  Large Angular Scales of the Sky",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PUREPath-B: A Tessellated Bayesian Model for Recovering CMB B-modes over\n  Large Angular Scales of the Sky"
                },
                "summary": "We introduce a comprehensive, custom-developed neural network, the\nPUREPath-B, that yields a posterior predictive distribution of Cosmic Microwave\nBackground (CMB) B-mode signal conditioned on the foreground contaminated CMB\ndata and informed by the training dataset. Our network employs nested\nprobabilistic multi-modal U-Net framework, enhanced with probabilistic ResNets\nat skip connections and seamlessly integrates Bayesian statistics and\nvariational methods to minimize the foreground and noise contaminations. During\ntraining, the initial prior distribution over network parameters evolves into\napproximate posterior distributions through Bayesian inference, constrained by\nthe training data. From the approximate joint full posterior of the model\nparameters, our network infers a predictive CMB posterior during inference and\nyields summary statistics such as predictive mean, variance of the cleaned map.\nThe predictive standard deviation provides an interpretable measure of\nper-pixel uncertainty in the predicted mean CMB map. For loss function, we use\na linear combination of KL-Divergence loss and weighted MAE-which ensures that\nmaps with higher amplitudes do not dominate the loss disproportionately.\nFurthermore, the results from the cosmological parameter estimation using the\ncleaned B-mode power spectrum, along with its error estimates demonstrates our\nnetwork minimizes the foreground contaminations effectively, enabling accurate\nrecovery of tensor-to-scalar ratio and lensing amplitude.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a comprehensive, custom-developed neural network, the\nPUREPath-B, that yields a posterior predictive distribution of Cosmic Microwave\nBackground (CMB) B-mode signal conditioned on the foreground contaminated CMB\ndata and informed by the training dataset. Our network employs nested\nprobabilistic multi-modal U-Net framework, enhanced with probabilistic ResNets\nat skip connections and seamlessly integrates Bayesian statistics and\nvariational methods to minimize the foreground and noise contaminations. During\ntraining, the initial prior distribution over network parameters evolves into\napproximate posterior distributions through Bayesian inference, constrained by\nthe training data. From the approximate joint full posterior of the model\nparameters, our network infers a predictive CMB posterior during inference and\nyields summary statistics such as predictive mean, variance of the cleaned map.\nThe predictive standard deviation provides an interpretable measure of\nper-pixel uncertainty in the predicted mean CMB map. For loss function, we use\na linear combination of KL-Divergence loss and weighted MAE-which ensures that\nmaps with higher amplitudes do not dominate the loss disproportionately.\nFurthermore, the results from the cosmological parameter estimation using the\ncleaned B-mode power spectrum, along with its error estimates demonstrates our\nnetwork minimizes the foreground contaminations effectively, enabling accurate\nrecovery of tensor-to-scalar ratio and lensing amplitude."
                },
                "authors": [
                    {
                        "name": "Vipin Sudevan"
                    },
                    {
                        "name": "Pisin Chen"
                    }
                ],
                "author_detail": {
                    "name": "Pisin Chen"
                },
                "author": "Pisin Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20774v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20774v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20769v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20769v1",
                "updated": "2025-03-26T17:53:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    17,
                    53,
                    6,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T17:53:06Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    17,
                    53,
                    6,
                    2,
                    85,
                    0
                ],
                "title": "Inferring Treatment Effects in Large Panels by Uncovering Latent\n  Similarities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring Treatment Effects in Large Panels by Uncovering Latent\n  Similarities"
                },
                "summary": "The presence of unobserved confounders is one of the main challenges in\nidentifying treatment effects. In this paper, we propose a new approach to\ncausal inference using panel data with large large $N$ and $T$. Our approach\nimputes the untreated potential outcomes for treated units using the outcomes\nfor untreated individuals with similar values of the latent confounders. In\norder to find units with similar latent characteristics, we utilize long\npre-treatment histories of the outcomes. Our analysis is based on a\nnonparametric, nonlinear, and nonseparable factor model for untreated potential\noutcomes and treatments. The model satisfies minimal smoothness requirements.\nWe impute both missing counterfactual outcomes and propensity scores using\nkernel smoothing based on the constructed measure of latent similarity between\nunits, and demonstrate that our estimates can achieve the optimal nonparametric\nrate of convergence up to log terms. Using these estimates, we construct a\ndoubly robust estimator of the period-specifc average treatment effect on the\ntreated (ATT), and provide conditions, under which this estimator is\n$\\sqrt{N}$-consistent, and asymptotically normal and unbiased. Our simulation\nstudy demonstrates that our method provides accurate inference for a wide range\nof data generating processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The presence of unobserved confounders is one of the main challenges in\nidentifying treatment effects. In this paper, we propose a new approach to\ncausal inference using panel data with large large $N$ and $T$. Our approach\nimputes the untreated potential outcomes for treated units using the outcomes\nfor untreated individuals with similar values of the latent confounders. In\norder to find units with similar latent characteristics, we utilize long\npre-treatment histories of the outcomes. Our analysis is based on a\nnonparametric, nonlinear, and nonseparable factor model for untreated potential\noutcomes and treatments. The model satisfies minimal smoothness requirements.\nWe impute both missing counterfactual outcomes and propensity scores using\nkernel smoothing based on the constructed measure of latent similarity between\nunits, and demonstrate that our estimates can achieve the optimal nonparametric\nrate of convergence up to log terms. Using these estimates, we construct a\ndoubly robust estimator of the period-specifc average treatment effect on the\ntreated (ATT), and provide conditions, under which this estimator is\n$\\sqrt{N}$-consistent, and asymptotically normal and unbiased. Our simulation\nstudy demonstrates that our method provides accurate inference for a wide range\nof data generating processes."
                },
                "authors": [
                    {
                        "name": "Ben Deaner"
                    },
                    {
                        "name": "Chen-Wei Hsiang"
                    },
                    {
                        "name": "Andrei Zeleneev"
                    }
                ],
                "author_detail": {
                    "name": "Andrei Zeleneev"
                },
                "author": "Andrei Zeleneev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20769v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20769v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20767v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20767v1",
                "updated": "2025-03-26T17:52:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    17,
                    52,
                    19,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T17:52:19Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    17,
                    52,
                    19,
                    2,
                    85,
                    0
                ],
                "title": "Reliable algorithm selection for machine learning-guided design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliable algorithm selection for machine learning-guided design"
                },
                "summary": "Algorithms for machine learning-guided design, or design algorithms, use\nmachine learning-based predictions to propose novel objects with desired\nproperty values. Given a new design task -- for example, to design novel\nproteins with high binding affinity to a therapeutic target -- one must choose\na design algorithm and specify any hyperparameters and predictive and/or\ngenerative models involved. How can these decisions be made such that the\nresulting designs are successful? This paper proposes a method for design\nalgorithm selection, which aims to select design algorithms that will produce a\ndistribution of design labels satisfying a user-specified success criterion --\nfor example, that at least ten percent of designs' labels exceed a threshold.\nIt does so by combining designs' predicted property values with held-out\nlabeled data to reliably forecast characteristics of the label distributions\nproduced by different design algorithms, building upon techniques from\nprediction-powered inference. The method is guaranteed with high probability to\nreturn design algorithms that yield successful label distributions (or the null\nset if none exist), if the density ratios between the design and labeled data\ndistributions are known. We demonstrate the method's effectiveness in simulated\nprotein and RNA design tasks, in settings with either known or estimated\ndensity ratios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Algorithms for machine learning-guided design, or design algorithms, use\nmachine learning-based predictions to propose novel objects with desired\nproperty values. Given a new design task -- for example, to design novel\nproteins with high binding affinity to a therapeutic target -- one must choose\na design algorithm and specify any hyperparameters and predictive and/or\ngenerative models involved. How can these decisions be made such that the\nresulting designs are successful? This paper proposes a method for design\nalgorithm selection, which aims to select design algorithms that will produce a\ndistribution of design labels satisfying a user-specified success criterion --\nfor example, that at least ten percent of designs' labels exceed a threshold.\nIt does so by combining designs' predicted property values with held-out\nlabeled data to reliably forecast characteristics of the label distributions\nproduced by different design algorithms, building upon techniques from\nprediction-powered inference. The method is guaranteed with high probability to\nreturn design algorithms that yield successful label distributions (or the null\nset if none exist), if the density ratios between the design and labeled data\ndistributions are known. We demonstrate the method's effectiveness in simulated\nprotein and RNA design tasks, in settings with either known or estimated\ndensity ratios."
                },
                "authors": [
                    {
                        "name": "Clara Fannjiang"
                    },
                    {
                        "name": "Ji Won Park"
                    }
                ],
                "author_detail": {
                    "name": "Ji Won Park"
                },
                "author": "Ji Won Park",
                "arxiv_comment": "25 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20767v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20767v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16974v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16974v2",
                "updated": "2025-03-26T17:48:00Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    17,
                    48,
                    0,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-21T09:43:37Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    9,
                    43,
                    37,
                    4,
                    80,
                    0
                ],
                "title": "Assessing Consistency and Reproducibility in the Outputs of Large\n  Language Models: Evidence Across Diverse Finance and Accounting Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing Consistency and Reproducibility in the Outputs of Large\n  Language Models: Evidence Across Diverse Finance and Accounting Tasks"
                },
                "summary": "This study provides the first comprehensive assessment of consistency and\nreproducibility in Large Language Model (LLM) outputs in finance and accounting\nresearch. We evaluate how consistently LLMs produce outputs given identical\ninputs through extensive experimentation with 50 independent runs across five\ncommon tasks: classification, sentiment analysis, summarization, text\ngeneration, and prediction. Using three OpenAI models (GPT-3.5-turbo,\nGPT-4o-mini, and GPT-4o), we generate over 3.4 million outputs from diverse\nfinancial source texts and data, covering MD&As, FOMC statements, finance news\narticles, earnings call transcripts, and financial statements. Our findings\nreveal substantial but task-dependent consistency, with binary classification\nand sentiment analysis achieving near-perfect reproducibility, while complex\ntasks show greater variability. More advanced models do not consistently\ndemonstrate better consistency and reproducibility, with task-specific patterns\nemerging. LLMs significantly outperform expert human annotators in consistency\nand maintain high agreement even where human experts significantly disagree. We\nfurther find that simple aggregation strategies across 3-5 runs dramatically\nimprove consistency. We also find that aggregation may come with an additional\nbenefit of improved accuracy for sentiment analysis when using newer models.\nSimulation analysis reveals that despite measurable inconsistency in LLM\noutputs, downstream statistical inferences remain remarkably robust. These\nfindings address concerns about what we term \"G-hacking,\" the selective\nreporting of favorable outcomes from multiple Generative AI runs, by\ndemonstrating that such risks are relatively low for finance and accounting\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study provides the first comprehensive assessment of consistency and\nreproducibility in Large Language Model (LLM) outputs in finance and accounting\nresearch. We evaluate how consistently LLMs produce outputs given identical\ninputs through extensive experimentation with 50 independent runs across five\ncommon tasks: classification, sentiment analysis, summarization, text\ngeneration, and prediction. Using three OpenAI models (GPT-3.5-turbo,\nGPT-4o-mini, and GPT-4o), we generate over 3.4 million outputs from diverse\nfinancial source texts and data, covering MD&As, FOMC statements, finance news\narticles, earnings call transcripts, and financial statements. Our findings\nreveal substantial but task-dependent consistency, with binary classification\nand sentiment analysis achieving near-perfect reproducibility, while complex\ntasks show greater variability. More advanced models do not consistently\ndemonstrate better consistency and reproducibility, with task-specific patterns\nemerging. LLMs significantly outperform expert human annotators in consistency\nand maintain high agreement even where human experts significantly disagree. We\nfurther find that simple aggregation strategies across 3-5 runs dramatically\nimprove consistency. We also find that aggregation may come with an additional\nbenefit of improved accuracy for sentiment analysis when using newer models.\nSimulation analysis reveals that despite measurable inconsistency in LLM\noutputs, downstream statistical inferences remain remarkably robust. These\nfindings address concerns about what we term \"G-hacking,\" the selective\nreporting of favorable outcomes from multiple Generative AI runs, by\ndemonstrating that such risks are relatively low for finance and accounting\ntasks."
                },
                "authors": [
                    {
                        "name": "Julian Junyan Wang"
                    },
                    {
                        "name": "Victor Xiaoqi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Victor Xiaoqi Wang"
                },
                "author": "Victor Xiaoqi Wang",
                "arxiv_comment": "97 pages, 20 tables, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16974v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16974v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20757v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20757v1",
                "updated": "2025-03-26T17:46:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    17,
                    46,
                    8,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T17:46:08Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    17,
                    46,
                    8,
                    2,
                    85,
                    0
                ],
                "title": "MCTS-RAG: Enhancing Retrieval-Augmented Generation with Monte Carlo Tree\n  Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MCTS-RAG: Enhancing Retrieval-Augmented Generation with Monte Carlo Tree\n  Search"
                },
                "summary": "We introduce MCTS-RAG, a novel approach that enhances the reasoning\ncapabilities of small language models on knowledge-intensive tasks by\nleveraging retrieval-augmented generation (RAG) to provide relevant context and\nMonte Carlo Tree Search (MCTS) to refine reasoning paths. MCTS-RAG dynamically\nintegrates retrieval and reasoning through an iterative decision-making\nprocess. Unlike standard RAG methods, which typically retrieve information\nindependently from reasoning and thus integrate knowledge suboptimally, or\nconventional MCTS reasoning, which depends solely on internal model knowledge\nwithout external facts, MCTS-RAG combines structured reasoning with adaptive\nretrieval. This integrated approach enhances decision-making, reduces\nhallucinations, and ensures improved factual accuracy and response consistency.\nThe experimental results on multiple reasoning and knowledge-intensive datasets\ndatasets (i.e., ComplexWebQA, GPQA, and FoolMeTwice) show that our method\nenables small-scale LMs to achieve performance comparable to frontier LLMs like\nGPT-4o by effectively scaling inference-time compute, setting a new standard\nfor reasoning in small-scale models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce MCTS-RAG, a novel approach that enhances the reasoning\ncapabilities of small language models on knowledge-intensive tasks by\nleveraging retrieval-augmented generation (RAG) to provide relevant context and\nMonte Carlo Tree Search (MCTS) to refine reasoning paths. MCTS-RAG dynamically\nintegrates retrieval and reasoning through an iterative decision-making\nprocess. Unlike standard RAG methods, which typically retrieve information\nindependently from reasoning and thus integrate knowledge suboptimally, or\nconventional MCTS reasoning, which depends solely on internal model knowledge\nwithout external facts, MCTS-RAG combines structured reasoning with adaptive\nretrieval. This integrated approach enhances decision-making, reduces\nhallucinations, and ensures improved factual accuracy and response consistency.\nThe experimental results on multiple reasoning and knowledge-intensive datasets\ndatasets (i.e., ComplexWebQA, GPQA, and FoolMeTwice) show that our method\nenables small-scale LMs to achieve performance comparable to frontier LLMs like\nGPT-4o by effectively scaling inference-time compute, setting a new standard\nfor reasoning in small-scale models."
                },
                "authors": [
                    {
                        "name": "Yunhai Hu"
                    },
                    {
                        "name": "Yilun Zhao"
                    },
                    {
                        "name": "Chen Zhao"
                    },
                    {
                        "name": "Arman Cohan"
                    }
                ],
                "author_detail": {
                    "name": "Arman Cohan"
                },
                "author": "Arman Cohan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20757v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20757v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11049v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11049v4",
                "updated": "2025-03-26T17:42:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    17,
                    42,
                    17,
                    2,
                    85,
                    0
                ],
                "published": "2024-08-20T17:57:31Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    57,
                    31,
                    1,
                    233,
                    0
                ],
                "title": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding"
                },
                "summary": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency losslessly, but the conventional wisdom suggests\nthat its efficacy is limited to small batch sizes. In MagicDec, we show that\nsurprisingly SD can achieve speedup even for a high throughput inference regime\nfor moderate to long sequences. More interestingly, an intelligent drafting\nstrategy can achieve better speedup with increasing batch size based on our\nrigorous analysis. MagicDec first identifies the bottleneck shifts with\nincreasing batch size and sequence length, and uses these insights to deploy SD\nmore effectively for high throughput inference. We leverage draft model with\nsparse KV cache to address the KV bottleneck, which scales with both sequence\nlength and batch size. Additionally, we propose a theoretical model to select\nthe optimal drafting strategy for maximum speedup. Our work highlights the\nbroad applicability of speculative decoding in long-context serving, as it can\nenhance throughput and reduce latency without compromising accuracy. For\nmoderate to long sequences, we demonstrate up to 2.51x speedup for Llama3.1-8B\nwhen serving batch sizes ranging from 32 to 256 on various types of hardware\nand tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency losslessly, but the conventional wisdom suggests\nthat its efficacy is limited to small batch sizes. In MagicDec, we show that\nsurprisingly SD can achieve speedup even for a high throughput inference regime\nfor moderate to long sequences. More interestingly, an intelligent drafting\nstrategy can achieve better speedup with increasing batch size based on our\nrigorous analysis. MagicDec first identifies the bottleneck shifts with\nincreasing batch size and sequence length, and uses these insights to deploy SD\nmore effectively for high throughput inference. We leverage draft model with\nsparse KV cache to address the KV bottleneck, which scales with both sequence\nlength and batch size. Additionally, we propose a theoretical model to select\nthe optimal drafting strategy for maximum speedup. Our work highlights the\nbroad applicability of speculative decoding in long-context serving, as it can\nenhance throughput and reduce latency without compromising accuracy. For\nmoderate to long sequences, we demonstrate up to 2.51x speedup for Llama3.1-8B\nwhen serving batch sizes ranging from 32 to 256 on various types of hardware\nand tasks."
                },
                "authors": [
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Jian Chen"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Vashisth Tiwari"
                    },
                    {
                        "name": "Ruihang Lai"
                    },
                    {
                        "name": "Jinyuan Shi"
                    },
                    {
                        "name": "Ian En-Hsu Yen"
                    },
                    {
                        "name": "Avner May"
                    },
                    {
                        "name": "Tianqi Chen"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11049v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11049v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10879v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10879v2",
                "updated": "2025-03-26T17:39:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    17,
                    39,
                    57,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-13T20:50:21Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    20,
                    50,
                    21,
                    3,
                    72,
                    0
                ],
                "title": "Task-Specific Activation Functions for Neuroevolution using Grammatical\n  Evolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task-Specific Activation Functions for Neuroevolution using Grammatical\n  Evolution"
                },
                "summary": "Activation functions play a critical role in the performance and behaviour of\nneural networks, significantly impacting their ability to learn and generalise.\nTraditional activation functions, such as ReLU, sigmoid, and tanh, have been\nwidely used with considerable success. However, these functions may not always\nprovide optimal performance for all tasks and datasets. In this paper, we\nintroduce Neuvo GEAF - an innovative approach leveraging grammatical evolution\n(GE) to automatically evolve novel activation functions tailored to specific\nneural network architectures and datasets. Experiments conducted on well-known\nbinary classification datasets show statistically significant improvements in\nF1-score (between 2.4% and 9.4%) over ReLU using identical network\narchitectures. Notably, these performance gains were achieved without\nincreasing the network's parameter count, supporting the trend toward more\nefficient neural networks that can operate effectively on resource-constrained\nedge devices. This paper's findings suggest that evolved activation functions\ncan provide significant performance improvements for compact networks while\nmaintaining energy efficiency during both training and inference phases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activation functions play a critical role in the performance and behaviour of\nneural networks, significantly impacting their ability to learn and generalise.\nTraditional activation functions, such as ReLU, sigmoid, and tanh, have been\nwidely used with considerable success. However, these functions may not always\nprovide optimal performance for all tasks and datasets. In this paper, we\nintroduce Neuvo GEAF - an innovative approach leveraging grammatical evolution\n(GE) to automatically evolve novel activation functions tailored to specific\nneural network architectures and datasets. Experiments conducted on well-known\nbinary classification datasets show statistically significant improvements in\nF1-score (between 2.4% and 9.4%) over ReLU using identical network\narchitectures. Notably, these performance gains were achieved without\nincreasing the network's parameter count, supporting the trend toward more\nefficient neural networks that can operate effectively on resource-constrained\nedge devices. This paper's findings suggest that evolved activation functions\ncan provide significant performance improvements for compact networks while\nmaintaining energy efficiency during both training and inference phases."
                },
                "authors": [
                    {
                        "name": "Benjamin David Winter"
                    },
                    {
                        "name": "William John Teahan"
                    }
                ],
                "author_detail": {
                    "name": "William John Teahan"
                },
                "author": "William John Teahan",
                "arxiv_comment": "8 pages, 4 figures, IEEE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10879v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10879v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20749v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20749v2",
                "updated": "2025-03-27T02:42:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    2,
                    42,
                    3,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-26T17:33:27Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    17,
                    33,
                    27,
                    2,
                    85,
                    0
                ],
                "title": "Beyond Believability: Accurate Human Behavior Simulation with Fine-Tuned\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Believability: Accurate Human Behavior Simulation with Fine-Tuned\n  LLMs"
                },
                "summary": "Recent research shows that LLMs can simulate ``believable'' human behaviors\nto power LLM agents via prompt-only methods. In this work, we focus on\nevaluating and improving LLM's objective ``accuracy'' rather than the\nsubjective ``believability'' in the web action generation task, leveraging a\nlarge-scale, real-world dataset collected from online shopping human actions.\nWe present the first comprehensive quantitative evaluation of state-of-the-art\nLLMs (e.g., DeepSeek-R1, Llama, and Claude) on the task of web action\ngeneration. Our results show that fine-tuning LLMs on real-world behavioral\ndata substantially improves their ability to generate actions compared to\nprompt-only methods. Furthermore, incorporating synthesized reasoning traces\ninto model training leads to additional performance gains, demonstrating the\nvalue of explicit rationale in behavior modeling. This work establishes a new\nbenchmark for evaluating LLMs in behavior simulation and offers actionable\ninsights into how real-world action data and reasoning augmentation can enhance\nthe fidelity of LLM agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research shows that LLMs can simulate ``believable'' human behaviors\nto power LLM agents via prompt-only methods. In this work, we focus on\nevaluating and improving LLM's objective ``accuracy'' rather than the\nsubjective ``believability'' in the web action generation task, leveraging a\nlarge-scale, real-world dataset collected from online shopping human actions.\nWe present the first comprehensive quantitative evaluation of state-of-the-art\nLLMs (e.g., DeepSeek-R1, Llama, and Claude) on the task of web action\ngeneration. Our results show that fine-tuning LLMs on real-world behavioral\ndata substantially improves their ability to generate actions compared to\nprompt-only methods. Furthermore, incorporating synthesized reasoning traces\ninto model training leads to additional performance gains, demonstrating the\nvalue of explicit rationale in behavior modeling. This work establishes a new\nbenchmark for evaluating LLMs in behavior simulation and offers actionable\ninsights into how real-world action data and reasoning augmentation can enhance\nthe fidelity of LLM agents."
                },
                "authors": [
                    {
                        "name": "Yuxuan Lu"
                    },
                    {
                        "name": "Jing Huang"
                    },
                    {
                        "name": "Yan Han"
                    },
                    {
                        "name": "Bennet Bei"
                    },
                    {
                        "name": "Yaochen Xie"
                    },
                    {
                        "name": "Dakuo Wang"
                    },
                    {
                        "name": "Jessie Wang"
                    },
                    {
                        "name": "Qi He"
                    }
                ],
                "author_detail": {
                    "name": "Qi He"
                },
                "author": "Qi He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20749v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20749v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20730v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20730v1",
                "updated": "2025-03-26T17:11:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    17,
                    11,
                    47,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T17:11:47Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    17,
                    11,
                    47,
                    2,
                    85,
                    0
                ],
                "title": "Benchmarking and optimizing organism wide single-cell RNA alignment\n  methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking and optimizing organism wide single-cell RNA alignment\n  methods"
                },
                "summary": "Many methods have been proposed for removing batch effects and aligning\nsingle-cell RNA (scRNA) datasets. However, performance is typically evaluated\nbased on multiple parameters and few datasets, creating challenges in assessing\nwhich method is best for aligning data at scale. Here, we introduce the\nK-Neighbors Intersection (KNI) score, a single score that both penalizes batch\neffects and measures accuracy at cross-dataset cell-type label prediction\nalongside carefully curated small (scMARK) and large (scREF) benchmarks\ncomprising 11 and 46 human scRNA studies respectively, where we have\nstandardized author labels. Using the KNI score, we evaluate and optimize\napproaches for cross-dataset single-cell RNA integration. We introduce Batch\nAdversarial single-cell Variational Inference (BA-scVI), as a new variant of\nscVI that uses adversarial training to penalize batch-effects in the encoder\nand decoder, and show this approach outperforms other methods. In the resulting\naligned space, we find that the granularity of cell-type groupings is\nconserved, supporting the notion that whole-organism cell-type maps can be\ncreated by a single model without loss of information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many methods have been proposed for removing batch effects and aligning\nsingle-cell RNA (scRNA) datasets. However, performance is typically evaluated\nbased on multiple parameters and few datasets, creating challenges in assessing\nwhich method is best for aligning data at scale. Here, we introduce the\nK-Neighbors Intersection (KNI) score, a single score that both penalizes batch\neffects and measures accuracy at cross-dataset cell-type label prediction\nalongside carefully curated small (scMARK) and large (scREF) benchmarks\ncomprising 11 and 46 human scRNA studies respectively, where we have\nstandardized author labels. Using the KNI score, we evaluate and optimize\napproaches for cross-dataset single-cell RNA integration. We introduce Batch\nAdversarial single-cell Variational Inference (BA-scVI), as a new variant of\nscVI that uses adversarial training to penalize batch-effects in the encoder\nand decoder, and show this approach outperforms other methods. In the resulting\naligned space, we find that the granularity of cell-type groupings is\nconserved, supporting the notion that whole-organism cell-type maps can be\ncreated by a single model without loss of information."
                },
                "authors": [
                    {
                        "name": "Juan Javier Diaz-Mejia"
                    },
                    {
                        "name": "Elias Williams"
                    },
                    {
                        "name": "Octavian Focsa"
                    },
                    {
                        "name": "Dylan Mendonca"
                    },
                    {
                        "name": "Swechha Singh"
                    },
                    {
                        "name": "Brendan Innes"
                    },
                    {
                        "name": "Sam Cooper"
                    }
                ],
                "author_detail": {
                    "name": "Sam Cooper"
                },
                "author": "Sam Cooper",
                "arxiv_comment": "Accepted to ICLR 2025 LMRL workshop (International Conference on\n  Learning Representations, Learning Meaningful Representations of Life\n  Workshop)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20730v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20730v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20719v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20719v1",
                "updated": "2025-03-26T16:54:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    16,
                    54,
                    56,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T16:54:56Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    16,
                    54,
                    56,
                    2,
                    85,
                    0
                ],
                "title": "Learning Straight Flows by Learning Curved Interpolants",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Straight Flows by Learning Curved Interpolants"
                },
                "summary": "Flow matching models typically use linear interpolants to define the\nforward/noise addition process. This, together with the independent coupling\nbetween noise and target distributions, yields a vector field which is often\nnon-straight. Such curved fields lead to a slow inference/generation process.\nIn this work, we propose to learn flexible (potentially curved) interpolants in\norder to learn straight vector fields to enable faster generation. We formulate\nthis via a multi-level optimization problem and propose an efficient\napproximate procedure to solve it. Our framework provides an end-to-end and\nsimulation-free optimization procedure, which can be leveraged to learn\nstraight line generative trajectories.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flow matching models typically use linear interpolants to define the\nforward/noise addition process. This, together with the independent coupling\nbetween noise and target distributions, yields a vector field which is often\nnon-straight. Such curved fields lead to a slow inference/generation process.\nIn this work, we propose to learn flexible (potentially curved) interpolants in\norder to learn straight vector fields to enable faster generation. We formulate\nthis via a multi-level optimization problem and propose an efficient\napproximate procedure to solve it. Our framework provides an end-to-end and\nsimulation-free optimization procedure, which can be leveraged to learn\nstraight line generative trajectories."
                },
                "authors": [
                    {
                        "name": "Shiv Shankar"
                    },
                    {
                        "name": "Tomas Geffner"
                    }
                ],
                "author_detail": {
                    "name": "Tomas Geffner"
                },
                "author": "Tomas Geffner",
                "arxiv_comment": "Delta workshop at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20719v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20719v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20715v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20715v1",
                "updated": "2025-03-26T16:52:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    16,
                    52,
                    40,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T16:52:40Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    16,
                    52,
                    40,
                    2,
                    85,
                    0
                ],
                "title": "From Annotation to Adaptation: Metrics, Synthetic Data, and Aspect\n  Extraction for Aspect-Based Sentiment Analysis with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Annotation to Adaptation: Metrics, Synthetic Data, and Aspect\n  Extraction for Aspect-Based Sentiment Analysis with Large Language Models"
                },
                "summary": "This study examines the performance of Large Language Models (LLMs) in\nAspect-Based Sentiment Analysis (ABSA), with a focus on implicit aspect\nextraction in a novel domain. Using a synthetic sports feedback dataset, we\nevaluate open-weight LLMs' ability to extract aspect-polarity pairs and propose\na metric to facilitate the evaluation of aspect extraction with generative\nmodels. Our findings highlight both the potential and limitations of LLMs in\nthe ABSA task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study examines the performance of Large Language Models (LLMs) in\nAspect-Based Sentiment Analysis (ABSA), with a focus on implicit aspect\nextraction in a novel domain. Using a synthetic sports feedback dataset, we\nevaluate open-weight LLMs' ability to extract aspect-polarity pairs and propose\na metric to facilitate the evaluation of aspect extraction with generative\nmodels. Our findings highlight both the potential and limitations of LLMs in\nthe ABSA task."
                },
                "authors": [
                    {
                        "name": "Nikita Neveditsin"
                    },
                    {
                        "name": "Pawan Lingras"
                    },
                    {
                        "name": "Vijay Mago"
                    }
                ],
                "author_detail": {
                    "name": "Vijay Mago"
                },
                "author": "Vijay Mago",
                "arxiv_comment": "Accepted to NAACL SRW 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20715v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20715v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20711v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20711v1",
                "updated": "2025-03-26T16:47:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    16,
                    47,
                    14,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T16:47:14Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    16,
                    47,
                    14,
                    2,
                    85,
                    0
                ],
                "title": "Demand Estimation with Text and Image Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demand Estimation with Text and Image Data"
                },
                "summary": "We propose a demand estimation method that leverages unstructured text and\nimage data to infer substitution patterns. Using pre-trained deep learning\nmodels, we extract embeddings from product images and textual descriptions and\nincorporate them into a random coefficients logit model. This approach enables\nresearchers to estimate demand even when they lack data on product attributes\nor when consumers value hard-to-quantify attributes, such as visual design or\nfunctional benefits. Using data from a choice experiment, we show that our\napproach outperforms standard attribute-based models in counterfactual\npredictions of consumers' second choices. We also apply it across 40 product\ncategories on Amazon.com and consistently find that text and image data help\nidentify close substitutes within each category.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a demand estimation method that leverages unstructured text and\nimage data to infer substitution patterns. Using pre-trained deep learning\nmodels, we extract embeddings from product images and textual descriptions and\nincorporate them into a random coefficients logit model. This approach enables\nresearchers to estimate demand even when they lack data on product attributes\nor when consumers value hard-to-quantify attributes, such as visual design or\nfunctional benefits. Using data from a choice experiment, we show that our\napproach outperforms standard attribute-based models in counterfactual\npredictions of consumers' second choices. We also apply it across 40 product\ncategories on Amazon.com and consistently find that text and image data help\nidentify close substitutes within each category."
                },
                "authors": [
                    {
                        "name": "Giovanni Compiani"
                    },
                    {
                        "name": "Ilya Morozov"
                    },
                    {
                        "name": "Stephan Seiler"
                    }
                ],
                "author_detail": {
                    "name": "Stephan Seiler"
                },
                "author": "Stephan Seiler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20711v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20711v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13696v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13696v2",
                "updated": "2025-03-26T16:33:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    16,
                    33,
                    16,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-17T20:09:03Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    20,
                    9,
                    3,
                    0,
                    76,
                    0
                ],
                "title": "Treatment Effect Heterogeneity in Regression Discontinuity Designs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Treatment Effect Heterogeneity in Regression Discontinuity Designs"
                },
                "summary": "Empirical studies using Regression Discontinuity (RD) designs often explore\nheterogeneous treatment effects based on pretreatment covariates. However, the\nlack of formal statistical methods has led to the widespread use of ad hoc\napproaches in applications. Motivated by common empirical practice, we develop\na unified, theoretically grounded framework for RD heterogeneity analysis. We\nshow that a fully interacted local linear (in functional parameters) model\neffectively captures heterogeneity while still being tractable and\ninterpretable in applications. The model structure holds without loss of\ngenerality for discrete covariates, while for continuous covariates our\nproposed (local functional linear-in-parameters) model can be potentially\nrestrictive, but it nonetheless naturally matches standard empirical practice\nand offers a causal interpretation for RD applications. We establish principled\nbandwidth selection and robust bias-corrected inference methods to analyze\nheterogeneous treatment effects and test group differences. We provide\ncompanion software to facilitate implementation of our results. An empirical\napplication illustrates the practical relevance of our methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empirical studies using Regression Discontinuity (RD) designs often explore\nheterogeneous treatment effects based on pretreatment covariates. However, the\nlack of formal statistical methods has led to the widespread use of ad hoc\napproaches in applications. Motivated by common empirical practice, we develop\na unified, theoretically grounded framework for RD heterogeneity analysis. We\nshow that a fully interacted local linear (in functional parameters) model\neffectively captures heterogeneity while still being tractable and\ninterpretable in applications. The model structure holds without loss of\ngenerality for discrete covariates, while for continuous covariates our\nproposed (local functional linear-in-parameters) model can be potentially\nrestrictive, but it nonetheless naturally matches standard empirical practice\nand offers a causal interpretation for RD applications. We establish principled\nbandwidth selection and robust bias-corrected inference methods to analyze\nheterogeneous treatment effects and test group differences. We provide\ncompanion software to facilitate implementation of our results. An empirical\napplication illustrates the practical relevance of our methods."
                },
                "authors": [
                    {
                        "name": "Sebastian Calonico"
                    },
                    {
                        "name": "Matias D. Cattaneo"
                    },
                    {
                        "name": "Max H. Farrell"
                    },
                    {
                        "name": "Filippo Palomba"
                    },
                    {
                        "name": "Rocio Titiunik"
                    }
                ],
                "author_detail": {
                    "name": "Rocio Titiunik"
                },
                "author": "Rocio Titiunik",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13696v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13696v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14738v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14738v2",
                "updated": "2025-03-26T16:28:13Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    16,
                    28,
                    13,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-18T21:14:12Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    21,
                    14,
                    12,
                    1,
                    77,
                    0
                ],
                "title": "DESI DR2 Results II: Measurements of Baryon Acoustic Oscillations and\n  Cosmological Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DESI DR2 Results II: Measurements of Baryon Acoustic Oscillations and\n  Cosmological Constraints"
                },
                "summary": "We present baryon acoustic oscillation (BAO) measurements from more than 14\nmillion galaxies and quasars drawn from the Dark Energy Spectroscopic\nInstrument (DESI) Data Release 2 (DR2), based on three years of operation. For\ncosmology inference, these galaxy measurements are combined with DESI\nLyman-$\\alpha$ forest BAO results presented in a companion paper. The DR2 BAO\nresults are consistent with DESI DR1 and SDSS, and their distance-redshift\nrelationship matches those from recent compilations of supernovae (SNe) over\nthe same redshift range. The results are well described by a flat $\\Lambda$CDM\nmodel, but the parameters preferred by BAO are in mild, $2.3\\sigma$ tension\nwith those determined from the cosmic microwave background (CMB), although the\nDESI results are consistent with the acoustic angular scale $\\theta_*$ that is\nwell-measured by Planck. This tension is alleviated by dark energy with a\ntime-evolving equation of state parametrized by $w_0$ and $w_a$, which provides\na better fit to the data, with a favored solution in the quadrant with $w_0>-1$\nand $w_a<0$. This solution is preferred over $\\Lambda$CDM at $3.1\\sigma$ for\nthe combination of DESI BAO and CMB data. When also including SNe, the\npreference for a dynamical dark energy model over $\\Lambda$CDM ranges from\n$2.8-4.2\\sigma$ depending on which SNe sample is used. We present evidence from\nother data combinations which also favor the same behavior at high\nsignificance. From the combination of DESI and CMB we derive 95% upper limits\non the sum of neutrino masses, finding $\\sum m_\\nu<0.064$ eV assuming\n$\\Lambda$CDM and $\\sum m_\\nu<0.16$ eV in the $w_0w_a$ model. Unless there is an\nunknown systematic error associated with one or more datasets, it is clear that\n$\\Lambda$CDM is being challenged by the combination of DESI BAO with other\nmeasurements and that dynamical dark energy offers a possible solution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present baryon acoustic oscillation (BAO) measurements from more than 14\nmillion galaxies and quasars drawn from the Dark Energy Spectroscopic\nInstrument (DESI) Data Release 2 (DR2), based on three years of operation. For\ncosmology inference, these galaxy measurements are combined with DESI\nLyman-$\\alpha$ forest BAO results presented in a companion paper. The DR2 BAO\nresults are consistent with DESI DR1 and SDSS, and their distance-redshift\nrelationship matches those from recent compilations of supernovae (SNe) over\nthe same redshift range. The results are well described by a flat $\\Lambda$CDM\nmodel, but the parameters preferred by BAO are in mild, $2.3\\sigma$ tension\nwith those determined from the cosmic microwave background (CMB), although the\nDESI results are consistent with the acoustic angular scale $\\theta_*$ that is\nwell-measured by Planck. This tension is alleviated by dark energy with a\ntime-evolving equation of state parametrized by $w_0$ and $w_a$, which provides\na better fit to the data, with a favored solution in the quadrant with $w_0>-1$\nand $w_a<0$. This solution is preferred over $\\Lambda$CDM at $3.1\\sigma$ for\nthe combination of DESI BAO and CMB data. When also including SNe, the\npreference for a dynamical dark energy model over $\\Lambda$CDM ranges from\n$2.8-4.2\\sigma$ depending on which SNe sample is used. We present evidence from\nother data combinations which also favor the same behavior at high\nsignificance. From the combination of DESI and CMB we derive 95% upper limits\non the sum of neutrino masses, finding $\\sum m_\\nu<0.064$ eV assuming\n$\\Lambda$CDM and $\\sum m_\\nu<0.16$ eV in the $w_0w_a$ model. Unless there is an\nunknown systematic error associated with one or more datasets, it is clear that\n$\\Lambda$CDM is being challenged by the combination of DESI BAO with other\nmeasurements and that dynamical dark energy offers a possible solution."
                },
                "authors": [
                    {
                        "name": "DESI Collaboration"
                    },
                    {
                        "name": "M. Abdul Karim"
                    },
                    {
                        "name": "J. Aguilar"
                    },
                    {
                        "name": "S. Ahlen"
                    },
                    {
                        "name": "S. Alam"
                    },
                    {
                        "name": "L. Allen"
                    },
                    {
                        "name": "C. Allende Prieto"
                    },
                    {
                        "name": "O. Alves"
                    },
                    {
                        "name": "A. Anand"
                    },
                    {
                        "name": "U. Andrade"
                    },
                    {
                        "name": "E. Armengaud"
                    },
                    {
                        "name": "A. Aviles"
                    },
                    {
                        "name": "S. Bailey"
                    },
                    {
                        "name": "C. Baltay"
                    },
                    {
                        "name": "P. Bansal"
                    },
                    {
                        "name": "A. Bault"
                    },
                    {
                        "name": "J. Behera"
                    },
                    {
                        "name": "S. BenZvi"
                    },
                    {
                        "name": "D. Bianchi"
                    },
                    {
                        "name": "C. Blake"
                    },
                    {
                        "name": "S. Brieden"
                    },
                    {
                        "name": "A. Brodzeller"
                    },
                    {
                        "name": "D. Brooks"
                    },
                    {
                        "name": "E. Buckley-Geer"
                    },
                    {
                        "name": "E. Burtin"
                    },
                    {
                        "name": "R. Calderon"
                    },
                    {
                        "name": "R. Canning"
                    },
                    {
                        "name": "A. Carnero Rosell"
                    },
                    {
                        "name": "P. Carrilho"
                    },
                    {
                        "name": "L. Casas"
                    },
                    {
                        "name": "F. J. Castander"
                    },
                    {
                        "name": "R. Cereskaite"
                    },
                    {
                        "name": "M. Charles"
                    },
                    {
                        "name": "E. Chaussidon"
                    },
                    {
                        "name": "J. Chaves-Montero"
                    },
                    {
                        "name": "D. Chebat"
                    },
                    {
                        "name": "X. Chen"
                    },
                    {
                        "name": "T. Claybaugh"
                    },
                    {
                        "name": "S. Cole"
                    },
                    {
                        "name": "A. P. Cooper"
                    },
                    {
                        "name": "A. Cuceu"
                    },
                    {
                        "name": "K. S. Dawson"
                    },
                    {
                        "name": "A. de la Macorra"
                    },
                    {
                        "name": "A. de Mattia"
                    },
                    {
                        "name": "N. Deiosso"
                    },
                    {
                        "name": "J. Della Costa"
                    },
                    {
                        "name": "R. Demina"
                    },
                    {
                        "name": "A. Dey"
                    },
                    {
                        "name": "B. Dey"
                    },
                    {
                        "name": "Z. Ding"
                    },
                    {
                        "name": "P. Doel"
                    },
                    {
                        "name": "J. Edelstein"
                    },
                    {
                        "name": "D. J. Eisenstein"
                    },
                    {
                        "name": "W. Elbers"
                    },
                    {
                        "name": "P. Fagrelius"
                    },
                    {
                        "name": "K. Fanning"
                    },
                    {
                        "name": "E. Fernández-García"
                    },
                    {
                        "name": "S. Ferraro"
                    },
                    {
                        "name": "A. Font-Ribera"
                    },
                    {
                        "name": "J. E. Forero-Romero"
                    },
                    {
                        "name": "C. S. Frenk"
                    },
                    {
                        "name": "C. Garcia-Quintero"
                    },
                    {
                        "name": "L. H. Garrison"
                    },
                    {
                        "name": "E. Gaztañaga"
                    },
                    {
                        "name": "H. Gil-Marín"
                    },
                    {
                        "name": "S. Gontcho A Gontcho"
                    },
                    {
                        "name": "D. Gonzalez"
                    },
                    {
                        "name": "A. X. Gonzalez-Morales"
                    },
                    {
                        "name": "C. Gordon"
                    },
                    {
                        "name": "D. Green"
                    },
                    {
                        "name": "G. Gutierrez"
                    },
                    {
                        "name": "J. Guy"
                    },
                    {
                        "name": "B. Hadzhiyska"
                    },
                    {
                        "name": "C. Hahn"
                    },
                    {
                        "name": "S. He"
                    },
                    {
                        "name": "M. Herbold"
                    },
                    {
                        "name": "H. K. Herrera-Alcantar"
                    },
                    {
                        "name": "M. Ho"
                    },
                    {
                        "name": "K. Honscheid"
                    },
                    {
                        "name": "C. Howlett"
                    },
                    {
                        "name": "D. Huterer"
                    },
                    {
                        "name": "M. Ishak"
                    },
                    {
                        "name": "S. Juneau"
                    },
                    {
                        "name": "N. V. Kamble"
                    },
                    {
                        "name": "N. G. Karaçaylı"
                    },
                    {
                        "name": "R. Kehoe"
                    },
                    {
                        "name": "S. Kent"
                    },
                    {
                        "name": "A. G. Kim"
                    },
                    {
                        "name": "D. Kirkby"
                    },
                    {
                        "name": "T. Kisner"
                    },
                    {
                        "name": "S. E. Koposov"
                    },
                    {
                        "name": "A. Kremin"
                    },
                    {
                        "name": "A. Krolewski"
                    },
                    {
                        "name": "O. Lahav"
                    },
                    {
                        "name": "C. Lamman"
                    },
                    {
                        "name": "M. Landriau"
                    },
                    {
                        "name": "D. Lang"
                    },
                    {
                        "name": "J. Lasker"
                    },
                    {
                        "name": "J. M. Le Goff"
                    },
                    {
                        "name": "L. Le Guillou"
                    },
                    {
                        "name": "A. Leauthaud"
                    },
                    {
                        "name": "M. E. Levi"
                    },
                    {
                        "name": "Q. Li"
                    },
                    {
                        "name": "T. S. Li"
                    },
                    {
                        "name": "K. Lodha"
                    },
                    {
                        "name": "M. Lokken"
                    },
                    {
                        "name": "F. Lozano-Rodríguez"
                    },
                    {
                        "name": "C. Magneville"
                    },
                    {
                        "name": "M. Manera"
                    },
                    {
                        "name": "P. Martini"
                    },
                    {
                        "name": "W. L. Matthewson"
                    },
                    {
                        "name": "A. Meisner"
                    },
                    {
                        "name": "J. Mena-Fernández"
                    },
                    {
                        "name": "A. Menegas"
                    },
                    {
                        "name": "T. Mergulhão"
                    },
                    {
                        "name": "R. Miquel"
                    },
                    {
                        "name": "J. Moustakas"
                    },
                    {
                        "name": "A. Muñoz-Gutiérrez"
                    },
                    {
                        "name": "D. Muñoz-Santos"
                    },
                    {
                        "name": "A. D. Myers"
                    },
                    {
                        "name": "S. Nadathur"
                    },
                    {
                        "name": "K. Naidoo"
                    },
                    {
                        "name": "L. Napolitano"
                    },
                    {
                        "name": "J. A. Newman"
                    },
                    {
                        "name": "G. Niz"
                    },
                    {
                        "name": "H. E. Noriega"
                    },
                    {
                        "name": "E. Paillas"
                    },
                    {
                        "name": "N. Palanque-Delabrouille"
                    },
                    {
                        "name": "J. Pan"
                    },
                    {
                        "name": "J. Peacock"
                    },
                    {
                        "name": "Marcos Pellejero Ibanez"
                    },
                    {
                        "name": "W. J. Percival"
                    },
                    {
                        "name": "A. Pérez-Fernández"
                    },
                    {
                        "name": "I. Pérez-Ràfols"
                    },
                    {
                        "name": "M. M. Pieri"
                    },
                    {
                        "name": "C. Poppett"
                    },
                    {
                        "name": "F. Prada"
                    },
                    {
                        "name": "D. Rabinowitz"
                    },
                    {
                        "name": "A. Raichoor"
                    },
                    {
                        "name": "C. Ramírez-Pérez"
                    },
                    {
                        "name": "M. Rashkovetskyi"
                    },
                    {
                        "name": "C. Ravoux"
                    },
                    {
                        "name": "J. Rich"
                    },
                    {
                        "name": "A. Rocher"
                    },
                    {
                        "name": "C. Rockosi"
                    },
                    {
                        "name": "J. Rohlf"
                    },
                    {
                        "name": "J. O. Román-Herrera"
                    },
                    {
                        "name": "A. J. Ross"
                    },
                    {
                        "name": "G. Rossi"
                    },
                    {
                        "name": "R. Ruggeri"
                    },
                    {
                        "name": "V. Ruhlmann-Kleider"
                    },
                    {
                        "name": "L. Samushia"
                    },
                    {
                        "name": "E. Sanchez"
                    },
                    {
                        "name": "N. Sanders"
                    },
                    {
                        "name": "D. Schlegel"
                    },
                    {
                        "name": "M. Schubnell"
                    },
                    {
                        "name": "H. Seo"
                    },
                    {
                        "name": "A. Shafieloo"
                    },
                    {
                        "name": "R. Sharples"
                    },
                    {
                        "name": "J. Silber"
                    },
                    {
                        "name": "F. Sinigaglia"
                    },
                    {
                        "name": "D. Sprayberry"
                    },
                    {
                        "name": "T. Tan"
                    },
                    {
                        "name": "G. Tarlé"
                    },
                    {
                        "name": "P. Taylor"
                    },
                    {
                        "name": "W. Turner"
                    },
                    {
                        "name": "L. A. Ureña-López"
                    },
                    {
                        "name": "R. Vaisakh"
                    },
                    {
                        "name": "F. Valdes"
                    },
                    {
                        "name": "G. Valogiannis"
                    },
                    {
                        "name": "M. Vargas-Magaña"
                    },
                    {
                        "name": "L. Verde"
                    },
                    {
                        "name": "M. Walther"
                    },
                    {
                        "name": "B. A. Weaver"
                    },
                    {
                        "name": "D. H. Weinberg"
                    },
                    {
                        "name": "M. White"
                    },
                    {
                        "name": "M. Wolfson"
                    },
                    {
                        "name": "C. Yèche"
                    },
                    {
                        "name": "J. Yu"
                    },
                    {
                        "name": "E. A. Zaborowski"
                    },
                    {
                        "name": "P. Zarrouk"
                    },
                    {
                        "name": "Z. Zhai"
                    },
                    {
                        "name": "H. Zhang"
                    },
                    {
                        "name": "C. Zhao"
                    },
                    {
                        "name": "G. B. Zhao"
                    },
                    {
                        "name": "R. Zhou"
                    },
                    {
                        "name": "H. Zou"
                    }
                ],
                "author_detail": {
                    "name": "H. Zou"
                },
                "author": "H. Zou",
                "arxiv_comment": "40 pages, 18 figures. This DESI Collaboration Publication is part of\n  the Data Release 2 publication series (see\n  https://data.desi.lbl.gov/doc/papers )",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14738v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14738v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20682v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20682v1",
                "updated": "2025-03-26T16:18:25Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    16,
                    18,
                    25,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T16:18:25Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    16,
                    18,
                    25,
                    2,
                    85,
                    0
                ],
                "title": "GLRD: Global-Local Collaborative Reason and Debate with PSL for 3D\n  Open-Vocabulary Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GLRD: Global-Local Collaborative Reason and Debate with PSL for 3D\n  Open-Vocabulary Detection"
                },
                "summary": "The task of LiDAR-based 3D Open-Vocabulary Detection (3D OVD) requires the\ndetector to learn to detect novel objects from point clouds without\noff-the-shelf training labels. Previous methods focus on the learning of\nobject-level representations and ignore the scene-level information, thus it is\nhard to distinguish objects with similar classes. In this work, we propose a\nGlobal-Local Collaborative Reason and Debate with PSL (GLRD) framework for the\n3D OVD task, considering both local object-level information and global\nscene-level information. Specifically, LLM is utilized to perform common sense\nreasoning based on object-level and scene-level information, where the\ndetection result is refined accordingly. To further boost the LLM's ability of\nprecise decisions, we also design a probabilistic soft logic solver (OV-PSL) to\nsearch for the optimal solution, and a debate scheme to confirm the class of\nconfusable objects. In addition, to alleviate the uneven distribution of\nclasses, a static balance scheme (SBC) and a dynamic balance scheme (DBC) are\ndesigned. In addition, to reduce the influence of noise in data and training,\nwe further propose Reflected Pseudo Labels Generation (RPLG) and\nBackground-Aware Object Localization (BAOL). Extensive experiments conducted on\nScanNet and SUN RGB-D demonstrate the superiority of GLRD, where absolute\nimprovements in mean average precision are $+2.82\\%$ on SUN RGB-D and $+3.72\\%$\non ScanNet in the partial open-vocabulary setting. In the full open-vocabulary\nsetting, the absolute improvements in mean average precision are $+4.03\\%$ on\nScanNet and $+14.11\\%$ on SUN RGB-D.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The task of LiDAR-based 3D Open-Vocabulary Detection (3D OVD) requires the\ndetector to learn to detect novel objects from point clouds without\noff-the-shelf training labels. Previous methods focus on the learning of\nobject-level representations and ignore the scene-level information, thus it is\nhard to distinguish objects with similar classes. In this work, we propose a\nGlobal-Local Collaborative Reason and Debate with PSL (GLRD) framework for the\n3D OVD task, considering both local object-level information and global\nscene-level information. Specifically, LLM is utilized to perform common sense\nreasoning based on object-level and scene-level information, where the\ndetection result is refined accordingly. To further boost the LLM's ability of\nprecise decisions, we also design a probabilistic soft logic solver (OV-PSL) to\nsearch for the optimal solution, and a debate scheme to confirm the class of\nconfusable objects. In addition, to alleviate the uneven distribution of\nclasses, a static balance scheme (SBC) and a dynamic balance scheme (DBC) are\ndesigned. In addition, to reduce the influence of noise in data and training,\nwe further propose Reflected Pseudo Labels Generation (RPLG) and\nBackground-Aware Object Localization (BAOL). Extensive experiments conducted on\nScanNet and SUN RGB-D demonstrate the superiority of GLRD, where absolute\nimprovements in mean average precision are $+2.82\\%$ on SUN RGB-D and $+3.72\\%$\non ScanNet in the partial open-vocabulary setting. In the full open-vocabulary\nsetting, the absolute improvements in mean average precision are $+4.03\\%$ on\nScanNet and $+14.11\\%$ on SUN RGB-D."
                },
                "authors": [
                    {
                        "name": "Xingyu Peng"
                    },
                    {
                        "name": "Si Liu"
                    },
                    {
                        "name": "Chen Gao"
                    },
                    {
                        "name": "Yan Bai"
                    },
                    {
                        "name": "Beipeng Mu"
                    },
                    {
                        "name": "Xiaofei Wang"
                    },
                    {
                        "name": "Huaxia Xia"
                    }
                ],
                "author_detail": {
                    "name": "Huaxia Xia"
                },
                "author": "Huaxia Xia",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20682v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20682v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20680v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20680v1",
                "updated": "2025-03-26T16:15:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    16,
                    15,
                    42,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T16:15:42Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    16,
                    15,
                    42,
                    2,
                    85,
                    0
                ],
                "title": "Vision as LoRA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision as LoRA"
                },
                "summary": "We introduce Vision as LoRA (VoRA), a novel paradigm for transforming an LLM\ninto an MLLM. Unlike prevalent MLLM architectures that rely on external vision\nmodules for vision encoding, VoRA internalizes visual capabilities by\nintegrating vision-specific LoRA layers directly into the LLM. This design\nallows the added parameters to be seamlessly merged into the LLM during\ninference, eliminating structural complexity and minimizing computational\noverhead. Moreover, inheriting the LLM's ability of handling flexible context,\nVoRA can process inputs at arbitrary resolutions.\n  To further strengthen VoRA's visual capabilities, we introduce a block-wise\ndistillation method that transfers visual priors from a pre-trained ViT into\nthe LoRA layers, effectively accelerating training by injecting visual\nknowledge. Additionally, we apply bi-directional attention masks to better\ncapture the context information of an image. We successfully demonstrate that\nwith additional pre-training data, VoRA can perform comparably with\nconventional encode-based MLLMs. All training data, codes, and model weights\nwill be released at https://github.com/Hon-Wong/VoRA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Vision as LoRA (VoRA), a novel paradigm for transforming an LLM\ninto an MLLM. Unlike prevalent MLLM architectures that rely on external vision\nmodules for vision encoding, VoRA internalizes visual capabilities by\nintegrating vision-specific LoRA layers directly into the LLM. This design\nallows the added parameters to be seamlessly merged into the LLM during\ninference, eliminating structural complexity and minimizing computational\noverhead. Moreover, inheriting the LLM's ability of handling flexible context,\nVoRA can process inputs at arbitrary resolutions.\n  To further strengthen VoRA's visual capabilities, we introduce a block-wise\ndistillation method that transfers visual priors from a pre-trained ViT into\nthe LoRA layers, effectively accelerating training by injecting visual\nknowledge. Additionally, we apply bi-directional attention masks to better\ncapture the context information of an image. We successfully demonstrate that\nwith additional pre-training data, VoRA can perform comparably with\nconventional encode-based MLLMs. All training data, codes, and model weights\nwill be released at https://github.com/Hon-Wong/VoRA."
                },
                "authors": [
                    {
                        "name": "Han Wang"
                    },
                    {
                        "name": "Yongjie Ye"
                    },
                    {
                        "name": "Bingru Li"
                    },
                    {
                        "name": "Yuxiang Nie"
                    },
                    {
                        "name": "Jinghui Lu"
                    },
                    {
                        "name": "Jingqun Tang"
                    },
                    {
                        "name": "Yanjie Wang"
                    },
                    {
                        "name": "Can Huang"
                    }
                ],
                "author_detail": {
                    "name": "Can Huang"
                },
                "author": "Can Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20680v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20680v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20676v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20676v1",
                "updated": "2025-03-26T16:09:54Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    16,
                    9,
                    54,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T16:09:54Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    16,
                    9,
                    54,
                    2,
                    85,
                    0
                ],
                "title": "Inductive Link Prediction on N-ary Relational Facts via Semantic\n  Hypergraph Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inductive Link Prediction on N-ary Relational Facts via Semantic\n  Hypergraph Reasoning"
                },
                "summary": "N-ary relational facts represent semantic correlations among more than two\nentities. While recent studies have developed link prediction (LP) methods to\ninfer missing relations for knowledge graphs (KGs) containing n-ary relational\nfacts, they are generally limited to transductive settings. Fully inductive\nsettings, where predictions are made on previously unseen entities, remain a\nsignificant challenge. As existing methods are mainly entity embedding-based,\nthey struggle to capture entity-independent logical rules. To fill in this gap,\nwe propose an n-ary subgraph reasoning framework for fully inductive link\nprediction (ILP) on n-ary relational facts. This framework reasons over local\nsubgraphs and has a strong inductive inference ability to capture n-ary\npatterns. Specifically, we introduce a novel graph structure, the n-ary\nsemantic hypergraph, to facilitate subgraph extraction. Moreover, we develop a\nsubgraph aggregating network, NS-HART, to effectively mine complex semantic\ncorrelations within subgraphs. Theoretically, we provide a thorough analysis\nfrom the score function optimization perspective to shed light on NS-HART's\neffectiveness for n-ary ILP tasks. Empirically, we conduct extensive\nexperiments on a series of inductive benchmarks, including transfer reasoning\n(with and without entity features) and pairwise subgraph reasoning. The results\nhighlight the superiority of the n-ary subgraph reasoning framework and the\nexceptional inductive ability of NS-HART. The source code of this paper has\nbeen made publicly available at\nhttps://github.com/yin-gz/Nary-Inductive-SubGraph.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "N-ary relational facts represent semantic correlations among more than two\nentities. While recent studies have developed link prediction (LP) methods to\ninfer missing relations for knowledge graphs (KGs) containing n-ary relational\nfacts, they are generally limited to transductive settings. Fully inductive\nsettings, where predictions are made on previously unseen entities, remain a\nsignificant challenge. As existing methods are mainly entity embedding-based,\nthey struggle to capture entity-independent logical rules. To fill in this gap,\nwe propose an n-ary subgraph reasoning framework for fully inductive link\nprediction (ILP) on n-ary relational facts. This framework reasons over local\nsubgraphs and has a strong inductive inference ability to capture n-ary\npatterns. Specifically, we introduce a novel graph structure, the n-ary\nsemantic hypergraph, to facilitate subgraph extraction. Moreover, we develop a\nsubgraph aggregating network, NS-HART, to effectively mine complex semantic\ncorrelations within subgraphs. Theoretically, we provide a thorough analysis\nfrom the score function optimization perspective to shed light on NS-HART's\neffectiveness for n-ary ILP tasks. Empirically, we conduct extensive\nexperiments on a series of inductive benchmarks, including transfer reasoning\n(with and without entity features) and pairwise subgraph reasoning. The results\nhighlight the superiority of the n-ary subgraph reasoning framework and the\nexceptional inductive ability of NS-HART. The source code of this paper has\nbeen made publicly available at\nhttps://github.com/yin-gz/Nary-Inductive-SubGraph."
                },
                "authors": [
                    {
                        "name": "Gongzhu Yin"
                    },
                    {
                        "name": "Hongli Zhang"
                    },
                    {
                        "name": "Yuchen Yang"
                    },
                    {
                        "name": "Yi Luo"
                    }
                ],
                "author_detail": {
                    "name": "Yi Luo"
                },
                "author": "Yi Luo",
                "arxiv_doi": "10.1145/3690624.3709195",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3690624.3709195",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.20676v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20676v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "To be published in Proceedings of the 31st ACM SIGKDD Conference on\n  Knowledge Discovery and Data Mining V.1 (KDD'25)",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20672v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20672v1",
                "updated": "2025-03-26T16:04:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    16,
                    4,
                    57,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T16:04:57Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    16,
                    4,
                    57,
                    2,
                    85,
                    0
                ],
                "title": "BizGen: Advancing Article-level Visual Text Rendering for Infographics\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BizGen: Advancing Article-level Visual Text Rendering for Infographics\n  Generation"
                },
                "summary": "Recently, state-of-the-art text-to-image generation models, such as Flux and\nIdeogram 2.0, have made significant progress in sentence-level visual text\nrendering. In this paper, we focus on the more challenging scenarios of\narticle-level visual text rendering and address a novel task of generating\nhigh-quality business content, including infographics and slides, based on user\nprovided article-level descriptive prompts and ultra-dense layouts. The\nfundamental challenges are twofold: significantly longer context lengths and\nthe scarcity of high-quality business content data.\n  In contrast to most previous works that focus on a limited number of\nsub-regions and sentence-level prompts, ensuring precise adherence to\nultra-dense layouts with tens or even hundreds of sub-regions in business\ncontent is far more challenging. We make two key technical contributions: (i)\nthe construction of scalable, high-quality business content dataset, i.e.,\nInfographics-650K, equipped with ultra-dense layouts and prompts by\nimplementing a layer-wise retrieval-augmented infographic generation scheme;\nand (ii) a layout-guided cross attention scheme, which injects tens of\nregion-wise prompts into a set of cropped region latent space according to the\nultra-dense layouts, and refine each sub-regions flexibly during inference\nusing a layout conditional CFG.\n  We demonstrate the strong results of our system compared to previous SOTA\nsystems such as Flux and SD3 on our BizEval prompt set. Additionally, we\nconduct thorough ablation experiments to verify the effectiveness of each\ncomponent. We hope our constructed Infographics-650K and BizEval can encourage\nthe broader community to advance the progress of business content generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, state-of-the-art text-to-image generation models, such as Flux and\nIdeogram 2.0, have made significant progress in sentence-level visual text\nrendering. In this paper, we focus on the more challenging scenarios of\narticle-level visual text rendering and address a novel task of generating\nhigh-quality business content, including infographics and slides, based on user\nprovided article-level descriptive prompts and ultra-dense layouts. The\nfundamental challenges are twofold: significantly longer context lengths and\nthe scarcity of high-quality business content data.\n  In contrast to most previous works that focus on a limited number of\nsub-regions and sentence-level prompts, ensuring precise adherence to\nultra-dense layouts with tens or even hundreds of sub-regions in business\ncontent is far more challenging. We make two key technical contributions: (i)\nthe construction of scalable, high-quality business content dataset, i.e.,\nInfographics-650K, equipped with ultra-dense layouts and prompts by\nimplementing a layer-wise retrieval-augmented infographic generation scheme;\nand (ii) a layout-guided cross attention scheme, which injects tens of\nregion-wise prompts into a set of cropped region latent space according to the\nultra-dense layouts, and refine each sub-regions flexibly during inference\nusing a layout conditional CFG.\n  We demonstrate the strong results of our system compared to previous SOTA\nsystems such as Flux and SD3 on our BizEval prompt set. Additionally, we\nconduct thorough ablation experiments to verify the effectiveness of each\ncomponent. We hope our constructed Infographics-650K and BizEval can encourage\nthe broader community to advance the progress of business content generation."
                },
                "authors": [
                    {
                        "name": "Yuyang Peng"
                    },
                    {
                        "name": "Shishi Xiao"
                    },
                    {
                        "name": "Keming Wu"
                    },
                    {
                        "name": "Qisheng Liao"
                    },
                    {
                        "name": "Bohan Chen"
                    },
                    {
                        "name": "Kevin Lin"
                    },
                    {
                        "name": "Danqing Huang"
                    },
                    {
                        "name": "Ji Li"
                    },
                    {
                        "name": "Yuhui Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Yuhui Yuan"
                },
                "author": "Yuhui Yuan",
                "arxiv_comment": "Accepted by CVPR 2025. Project Page: https://bizgen-msra.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20672v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20672v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08818v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08818v2",
                "updated": "2025-03-26T15:58:26Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    15,
                    58,
                    26,
                    2,
                    85,
                    0
                ],
                "published": "2025-02-12T22:11:07Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    22,
                    11,
                    7,
                    2,
                    43,
                    0
                ],
                "title": "Lexical Manifold Reconfiguration in Large Language Models: A Novel\n  Architectural Approach for Contextual Modulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lexical Manifold Reconfiguration in Large Language Models: A Novel\n  Architectural Approach for Contextual Modulation"
                },
                "summary": "Contextual adaptation in token embeddings plays a central role in determining\nhow well language models maintain coherence and retain semantic relationships\nover extended text sequences. Static embeddings often impose constraints on\nlexical flexibility, leading to suboptimal performance when faced with complex\nsentence structures or domain-specific terminology shifts. To address this\nlimitation, a structured approach was developed for dynamically reconfiguring\ntoken embeddings through continuous geometric transformations, ensuring that\nrepresentations evolved in response to evolving discourse structures. A\nmanifold-based transformation mechanism was integrated to regulate lexical\npositioning, allowing embeddings to undergo controlled shifts while preserving\nlinguistic relationships across varying textual contexts. Empirical evaluations\ndemonstrated that embedding reconfiguration contributed to reductions in\nperplexity, improved lexical coherence, and enhanced sentence-level continuity,\nparticularly in structured and domain-adaptive text generation tasks.\nComparative analyses of embedding drift indicated that dynamically restructured\nrepresentations maintained stronger contextual consistency, reducing\nmisalignment in token dependencies while preserving fluency in language\nmodeling outputs. Computational overhead assessments confirmed that while\ntraining complexity increased due to the iterative refinement of embeddings,\ninference remained efficient, ensuring practical feasibility for real-time\ngeneration. Evaluations across multiple datasets further demonstrated that\ndynamically modulated embeddings exhibited broader lexical diversity, reducing\nrepetitive token patterns and enabling a more adaptable representation learning\nprocess.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contextual adaptation in token embeddings plays a central role in determining\nhow well language models maintain coherence and retain semantic relationships\nover extended text sequences. Static embeddings often impose constraints on\nlexical flexibility, leading to suboptimal performance when faced with complex\nsentence structures or domain-specific terminology shifts. To address this\nlimitation, a structured approach was developed for dynamically reconfiguring\ntoken embeddings through continuous geometric transformations, ensuring that\nrepresentations evolved in response to evolving discourse structures. A\nmanifold-based transformation mechanism was integrated to regulate lexical\npositioning, allowing embeddings to undergo controlled shifts while preserving\nlinguistic relationships across varying textual contexts. Empirical evaluations\ndemonstrated that embedding reconfiguration contributed to reductions in\nperplexity, improved lexical coherence, and enhanced sentence-level continuity,\nparticularly in structured and domain-adaptive text generation tasks.\nComparative analyses of embedding drift indicated that dynamically restructured\nrepresentations maintained stronger contextual consistency, reducing\nmisalignment in token dependencies while preserving fluency in language\nmodeling outputs. Computational overhead assessments confirmed that while\ntraining complexity increased due to the iterative refinement of embeddings,\ninference remained efficient, ensuring practical feasibility for real-time\ngeneration. Evaluations across multiple datasets further demonstrated that\ndynamically modulated embeddings exhibited broader lexical diversity, reducing\nrepetitive token patterns and enabling a more adaptable representation learning\nprocess."
                },
                "authors": [
                    {
                        "name": "Koinis Vassilis"
                    },
                    {
                        "name": "Godfrey Milbourne"
                    },
                    {
                        "name": "Harriet Featherstone"
                    },
                    {
                        "name": "Xanthe Peverell"
                    },
                    {
                        "name": "Yorick Bletchley"
                    },
                    {
                        "name": "Zachary Montford"
                    }
                ],
                "author_detail": {
                    "name": "Zachary Montford"
                },
                "author": "Zachary Montford",
                "arxiv_comment": "arXiv admin note: This paper has been withdrawn by arXiv due to\n  disputed and unverifiable authorship",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08818v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08818v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20666v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20666v1",
                "updated": "2025-03-26T15:58:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    15,
                    58,
                    16,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T15:58:16Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    15,
                    58,
                    16,
                    2,
                    85,
                    0
                ],
                "title": "TAMA: A Human-AI Collaborative Thematic Analysis Framework Using\n  Multi-Agent LLMs for Clinical Interviews",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TAMA: A Human-AI Collaborative Thematic Analysis Framework Using\n  Multi-Agent LLMs for Clinical Interviews"
                },
                "summary": "Thematic analysis (TA) is a widely used qualitative approach for uncovering\nlatent meanings in unstructured text data. TA provides valuable insights in\nhealthcare but is resource-intensive. Large Language Models (LLMs) have been\nintroduced to perform TA, yet their applications in healthcare remain\nunexplored. Here, we propose TAMA: A Human-AI Collaborative Thematic Analysis\nframework using Multi-Agent LLMs for clinical interviews. We leverage the\nscalability and coherence of multi-agent systems through structured\nconversations between agents and coordinate the expertise of cardiac experts in\nTA. Using interview transcripts from parents of children with Anomalous Aortic\nOrigin of a Coronary Artery (AAOCA), a rare congenital heart disease, we\ndemonstrate that TAMA outperforms existing LLM-assisted TA approaches,\nachieving higher thematic hit rate, coverage, and distinctiveness. TAMA\ndemonstrates strong potential for automated TA in clinical settings by\nleveraging multi-agent LLM systems with human-in-the-loop integration by\nenhancing quality while significantly reducing manual workload.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thematic analysis (TA) is a widely used qualitative approach for uncovering\nlatent meanings in unstructured text data. TA provides valuable insights in\nhealthcare but is resource-intensive. Large Language Models (LLMs) have been\nintroduced to perform TA, yet their applications in healthcare remain\nunexplored. Here, we propose TAMA: A Human-AI Collaborative Thematic Analysis\nframework using Multi-Agent LLMs for clinical interviews. We leverage the\nscalability and coherence of multi-agent systems through structured\nconversations between agents and coordinate the expertise of cardiac experts in\nTA. Using interview transcripts from parents of children with Anomalous Aortic\nOrigin of a Coronary Artery (AAOCA), a rare congenital heart disease, we\ndemonstrate that TAMA outperforms existing LLM-assisted TA approaches,\nachieving higher thematic hit rate, coverage, and distinctiveness. TAMA\ndemonstrates strong potential for automated TA in clinical settings by\nleveraging multi-agent LLM systems with human-in-the-loop integration by\nenhancing quality while significantly reducing manual workload."
                },
                "authors": [
                    {
                        "name": "Huimin Xu"
                    },
                    {
                        "name": "Seungjun Yi"
                    },
                    {
                        "name": "Terence Lim"
                    },
                    {
                        "name": "Jiawei Xu"
                    },
                    {
                        "name": "Andrew Well"
                    },
                    {
                        "name": "Carlos Mery"
                    },
                    {
                        "name": "Aidong Zhang"
                    },
                    {
                        "name": "Yuji Zhang"
                    },
                    {
                        "name": "Heng Ji"
                    },
                    {
                        "name": "Keshav Pingali"
                    },
                    {
                        "name": "Yan Leng"
                    },
                    {
                        "name": "Ying Ding"
                    }
                ],
                "author_detail": {
                    "name": "Ying Ding"
                },
                "author": "Ying Ding",
                "arxiv_comment": "Submitted to the American Medical Informatics Association (AMIA) 2025\n  Annual Symposium, 10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20666v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20666v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20662v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20662v1",
                "updated": "2025-03-26T15:56:48Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    15,
                    56,
                    48,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T15:56:48Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    15,
                    56,
                    48,
                    2,
                    85,
                    0
                ],
                "title": "AutoRad-Lung: A Radiomic-Guided Prompting Autoregressive Vision-Language\n  Model for Lung Nodule Malignancy Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoRad-Lung: A Radiomic-Guided Prompting Autoregressive Vision-Language\n  Model for Lung Nodule Malignancy Prediction"
                },
                "summary": "Lung cancer remains one of the leading causes of cancer-related mortality\nworldwide. A crucial challenge for early diagnosis is differentiating uncertain\ncases with similar visual characteristics and closely annotation scores. In\nclinical practice, radiologists rely on quantitative, hand-crafted Radiomic\nfeatures extracted from Computed Tomography (CT) images, while recent research\nhas primarily focused on deep learning solutions. More recently,\nVision-Language Models (VLMs), particularly Contrastive Language-Image\nPre-Training (CLIP)-based models, have gained attention for their ability to\nintegrate textual knowledge into lung cancer diagnosis. While CLIP-Lung models\nhave shown promising results, we identified the following potential\nlimitations: (a) dependence on radiologists' annotated attributes, which are\ninherently subjective and error-prone, (b) use of textual information only\nduring training, limiting direct applicability at inference, and (c)\nConvolutional-based vision encoder with randomly initialized weights, which\ndisregards prior knowledge. To address these limitations, we introduce\nAutoRad-Lung, which couples an autoregressively pre-trained VLM, with prompts\ngenerated from hand-crafted Radiomics. AutoRad-Lung uses the vision encoder of\nthe Large-Scale Autoregressive Image Model (AIMv2), pre-trained using a\nmulti-modal autoregressive objective. Given that lung tumors are typically\nsmall, irregularly shaped, and visually similar to healthy tissue, AutoRad-Lung\noffers significant advantages over its CLIP-based counterparts by capturing\npixel-level differences. Additionally, we introduce conditional context\noptimization, which dynamically generates context-specific prompts based on\ninput Radiomics, improving cross-modal alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lung cancer remains one of the leading causes of cancer-related mortality\nworldwide. A crucial challenge for early diagnosis is differentiating uncertain\ncases with similar visual characteristics and closely annotation scores. In\nclinical practice, radiologists rely on quantitative, hand-crafted Radiomic\nfeatures extracted from Computed Tomography (CT) images, while recent research\nhas primarily focused on deep learning solutions. More recently,\nVision-Language Models (VLMs), particularly Contrastive Language-Image\nPre-Training (CLIP)-based models, have gained attention for their ability to\nintegrate textual knowledge into lung cancer diagnosis. While CLIP-Lung models\nhave shown promising results, we identified the following potential\nlimitations: (a) dependence on radiologists' annotated attributes, which are\ninherently subjective and error-prone, (b) use of textual information only\nduring training, limiting direct applicability at inference, and (c)\nConvolutional-based vision encoder with randomly initialized weights, which\ndisregards prior knowledge. To address these limitations, we introduce\nAutoRad-Lung, which couples an autoregressively pre-trained VLM, with prompts\ngenerated from hand-crafted Radiomics. AutoRad-Lung uses the vision encoder of\nthe Large-Scale Autoregressive Image Model (AIMv2), pre-trained using a\nmulti-modal autoregressive objective. Given that lung tumors are typically\nsmall, irregularly shaped, and visually similar to healthy tissue, AutoRad-Lung\noffers significant advantages over its CLIP-based counterparts by capturing\npixel-level differences. Additionally, we introduce conditional context\noptimization, which dynamically generates context-specific prompts based on\ninput Radiomics, improving cross-modal alignment."
                },
                "authors": [
                    {
                        "name": "Sadaf Khademi"
                    },
                    {
                        "name": "Mehran Shabanpour"
                    },
                    {
                        "name": "Reza Taleei"
                    },
                    {
                        "name": "Anastasia Oikonomou"
                    },
                    {
                        "name": "Arash Mohammadi"
                    }
                ],
                "author_detail": {
                    "name": "Arash Mohammadi"
                },
                "author": "Arash Mohammadi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20662v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20662v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08026v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08026v3",
                "updated": "2025-03-26T15:54:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    15,
                    54,
                    4,
                    2,
                    85,
                    0
                ],
                "published": "2025-02-12T00:00:37Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    0,
                    0,
                    37,
                    2,
                    43,
                    0
                ],
                "title": "Contextual Subspace Manifold Projection for Structural Refinement of\n  Large Language Model Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contextual Subspace Manifold Projection for Structural Refinement of\n  Large Language Model Representations"
                },
                "summary": "Internal representations within deep neural architectures encode\nhigh-dimensional abstractions of linguistic structures, yet they often exhibit\ninefficiencies in feature distribution, limiting expressiveness and\nadaptability. Contextual Subspace Manifold Projection introduces a structured\nrefinement technique that selectively reconfigures token embeddings through\ncontrolled subspace constraints, ensuring more stable and geometrically\nwell-defined feature distributions. Empirical evaluations demonstrated that the\nstructured intervention reduced anisotropy, leading to improved representation\ncompactness while preserving semantic fidelity across transformer layers.\nClustering analyses indicated that token embeddings exhibited greater feature\nseparability, reinforcing the hypothesis that structured projection techniques\nenhance internal representation organization without sacrificing linguistic\ncoherence. Gradient magnitude distributions suggested that the method\nintroduced a smoother optimization trajectory, potentially contributing to more\nstable parameter updates throughout training. Computational overhead associated\nwith the projection operations remained minimal, ensuring that the refinements\ndid not introduce significant trade-offs in model efficiency or inference\nspeed. Comparisons with standard embedding refinement techniques highlighted\nthat structured manifold constraints provided a direct mechanism for improving\nrepresentation quality without requiring additional gradient-based\noptimization. Perplexity evaluations confirmed that the adjustments did not\nnegatively impact sequence coherence, further validating the effectiveness of\nthe proposed approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Internal representations within deep neural architectures encode\nhigh-dimensional abstractions of linguistic structures, yet they often exhibit\ninefficiencies in feature distribution, limiting expressiveness and\nadaptability. Contextual Subspace Manifold Projection introduces a structured\nrefinement technique that selectively reconfigures token embeddings through\ncontrolled subspace constraints, ensuring more stable and geometrically\nwell-defined feature distributions. Empirical evaluations demonstrated that the\nstructured intervention reduced anisotropy, leading to improved representation\ncompactness while preserving semantic fidelity across transformer layers.\nClustering analyses indicated that token embeddings exhibited greater feature\nseparability, reinforcing the hypothesis that structured projection techniques\nenhance internal representation organization without sacrificing linguistic\ncoherence. Gradient magnitude distributions suggested that the method\nintroduced a smoother optimization trajectory, potentially contributing to more\nstable parameter updates throughout training. Computational overhead associated\nwith the projection operations remained minimal, ensuring that the refinements\ndid not introduce significant trade-offs in model efficiency or inference\nspeed. Comparisons with standard embedding refinement techniques highlighted\nthat structured manifold constraints provided a direct mechanism for improving\nrepresentation quality without requiring additional gradient-based\noptimization. Perplexity evaluations confirmed that the adjustments did not\nnegatively impact sequence coherence, further validating the effectiveness of\nthe proposed approach."
                },
                "authors": [
                    {
                        "name": "Alistair Wren"
                    },
                    {
                        "name": "Beatrice Loxley"
                    },
                    {
                        "name": "Hamish Cadwallader"
                    },
                    {
                        "name": "Simon Beckwith"
                    },
                    {
                        "name": "Fabian Pargeter"
                    },
                    {
                        "name": "James Blades"
                    }
                ],
                "author_detail": {
                    "name": "James Blades"
                },
                "author": "James Blades",
                "arxiv_comment": "arXiv admin note: This paper has been withdrawn by arXiv due to\n  disputed and unverifiable authorship",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08026v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08026v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06302v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06302v2",
                "updated": "2025-03-26T15:53:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    15,
                    53,
                    34,
                    2,
                    85,
                    0
                ],
                "published": "2025-02-10T09:46:33Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    9,
                    46,
                    33,
                    0,
                    41,
                    0
                ],
                "title": "Latent Convergence Modulation in Large Language Models: A Novel Approach\n  to Iterative Contextual Realignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latent Convergence Modulation in Large Language Models: A Novel Approach\n  to Iterative Contextual Realignment"
                },
                "summary": "Token prediction stability remains a challenge in autoregressive generative\nmodels, where minor variations in early inference steps often lead to\nsignificant semantic drift over extended sequences. A structured modulation\nmechanism was introduced to regulate hidden state transitions, ensuring that\nlatent representation trajectories remain aligned with prior contextual\ndependencies while preserving generative flexibility. The modulation framework\nwas designed to function within transformer-based architectures, dynamically\nconstraining representation evolution without imposing external memory\ndependencies or extensive architectural modifications. Empirical evaluations\ndemonstrated that structured latent adjustments contributed to reductions in\nperplexity fluctuations, entropy variance, and lexical instability, improving\ncoherence in long-form text generation. Gradient propagation stability was\nfurther analyzed, revealing that the modulation process led to smoother\noptimization pathways, mitigating erratic fluctuations in weight updates across\nsuccessive inference steps. The computational efficiency of the modulation\nprocess was assessed, showing that its integration within transformer-based\narchitectures introduced only marginal overhead while maintaining compatibility\nwith existing optimization frameworks. The structured modulation constraints\nalso influenced syntactic variation, preventing excessive repetition while\nmaintaining balanced sentence length distributions. Comparative evaluations\nagainst baseline models reinforced the role of controlled latent state\nevolution in improving pronoun resolution, logical consistency, and contextual\nalignment across autoregressive text generation tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token prediction stability remains a challenge in autoregressive generative\nmodels, where minor variations in early inference steps often lead to\nsignificant semantic drift over extended sequences. A structured modulation\nmechanism was introduced to regulate hidden state transitions, ensuring that\nlatent representation trajectories remain aligned with prior contextual\ndependencies while preserving generative flexibility. The modulation framework\nwas designed to function within transformer-based architectures, dynamically\nconstraining representation evolution without imposing external memory\ndependencies or extensive architectural modifications. Empirical evaluations\ndemonstrated that structured latent adjustments contributed to reductions in\nperplexity fluctuations, entropy variance, and lexical instability, improving\ncoherence in long-form text generation. Gradient propagation stability was\nfurther analyzed, revealing that the modulation process led to smoother\noptimization pathways, mitigating erratic fluctuations in weight updates across\nsuccessive inference steps. The computational efficiency of the modulation\nprocess was assessed, showing that its integration within transformer-based\narchitectures introduced only marginal overhead while maintaining compatibility\nwith existing optimization frameworks. The structured modulation constraints\nalso influenced syntactic variation, preventing excessive repetition while\nmaintaining balanced sentence length distributions. Comparative evaluations\nagainst baseline models reinforced the role of controlled latent state\nevolution in improving pronoun resolution, logical consistency, and contextual\nalignment across autoregressive text generation tasks."
                },
                "authors": [
                    {
                        "name": "Patricia Porretta"
                    },
                    {
                        "name": "Sylvester Pakenham"
                    },
                    {
                        "name": "Huxley Ainsworth"
                    },
                    {
                        "name": "Gregory Chatten"
                    },
                    {
                        "name": "Godfrey Allerton"
                    },
                    {
                        "name": "Simon Hollingsworth"
                    },
                    {
                        "name": "Vance Periwinkle"
                    }
                ],
                "author_detail": {
                    "name": "Vance Periwinkle"
                },
                "author": "Vance Periwinkle",
                "arxiv_comment": "arXiv admin note: This paper has been withdrawn by arXiv due to\n  disputed and unverifiable authorship",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06302v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06302v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18205v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18205v3",
                "updated": "2025-03-26T15:53:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    15,
                    53,
                    32,
                    2,
                    85,
                    0
                ],
                "published": "2025-01-30T08:51:48Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    8,
                    51,
                    48,
                    3,
                    30,
                    0
                ],
                "title": "Contextually Structured Token Dependency Encoding for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contextually Structured Token Dependency Encoding for Large Language\n  Models"
                },
                "summary": "Token representation strategies within large-scale neural architectures often\nrely on contextually refined embeddings, yet conventional approaches seldom\nencode structured relationships explicitly within token interactions.\nSelf-attention mechanisms effectively capture dynamic contextual dependencies,\nbut their reliance on learned weight distributions limits the preservation of\nlong-range hierarchical structures in generated sequences. Dependency-aware\ntoken encoding introduces a structured approach to embedding initialization,\nensuring that relational constraints are embedded within token representations\nrather than inferred solely through attention dynamics. The proposed encoding\nmechanism refines token interactions through dependency-weighted attention\ncomputations, ensuring that syntactic and semantic dependencies are retained\nacross multiple processing layers. Empirical evaluations indicate reductions in\nperplexity across diverse linguistic benchmarks, suggesting improvements in\ncontextual coherence and predictive consistency in autoregressive text\ngeneration. Computational efficiency assessments reveal a moderate increase in\nmemory consumption and training time, attributed to additional matrix\ncomputations within the encoding module, yet scalability remains feasible\nwithin conventional transformer architectures. Structured encoding enhances\nlexical variation and dependency retention, reinforcing linguistic coherence\nwithout requiring external syntactic annotations or auxiliary training\nobjectives. Statistical comparisons highlight improvements in dependency\nalignment, particularly in longer sequences where conventional self-attention\nmodels exhibit degradation in hierarchical consistency. Sentence length\ndistributions indicate a reduction in abrupt phrase transitions, further\nsupporting the hypothesis that explicit dependency encoding facilitates more\nstructured phrase generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token representation strategies within large-scale neural architectures often\nrely on contextually refined embeddings, yet conventional approaches seldom\nencode structured relationships explicitly within token interactions.\nSelf-attention mechanisms effectively capture dynamic contextual dependencies,\nbut their reliance on learned weight distributions limits the preservation of\nlong-range hierarchical structures in generated sequences. Dependency-aware\ntoken encoding introduces a structured approach to embedding initialization,\nensuring that relational constraints are embedded within token representations\nrather than inferred solely through attention dynamics. The proposed encoding\nmechanism refines token interactions through dependency-weighted attention\ncomputations, ensuring that syntactic and semantic dependencies are retained\nacross multiple processing layers. Empirical evaluations indicate reductions in\nperplexity across diverse linguistic benchmarks, suggesting improvements in\ncontextual coherence and predictive consistency in autoregressive text\ngeneration. Computational efficiency assessments reveal a moderate increase in\nmemory consumption and training time, attributed to additional matrix\ncomputations within the encoding module, yet scalability remains feasible\nwithin conventional transformer architectures. Structured encoding enhances\nlexical variation and dependency retention, reinforcing linguistic coherence\nwithout requiring external syntactic annotations or auxiliary training\nobjectives. Statistical comparisons highlight improvements in dependency\nalignment, particularly in longer sequences where conventional self-attention\nmodels exhibit degradation in hierarchical consistency. Sentence length\ndistributions indicate a reduction in abrupt phrase transitions, further\nsupporting the hypothesis that explicit dependency encoding facilitates more\nstructured phrase generation."
                },
                "authors": [
                    {
                        "name": "James Blades"
                    },
                    {
                        "name": "Frederick Somerfield"
                    },
                    {
                        "name": "William Langley"
                    },
                    {
                        "name": "Susan Everingham"
                    },
                    {
                        "name": "Maurice Witherington"
                    }
                ],
                "author_detail": {
                    "name": "Maurice Witherington"
                },
                "author": "Maurice Witherington",
                "arxiv_comment": "arXiv admin note: This paper has been withdrawn by arXiv due to\n  disputed and unverifiable authorship",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18205v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18205v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11706v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11706v3",
                "updated": "2025-03-26T15:44:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    15,
                    44,
                    1,
                    2,
                    85,
                    0
                ],
                "published": "2024-11-18T16:33:52Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    16,
                    33,
                    52,
                    0,
                    323,
                    0
                ],
                "title": "MC-LLaVA: Multi-Concept Personalized Vision-Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MC-LLaVA: Multi-Concept Personalized Vision-Language Model"
                },
                "summary": "Current vision-language models (VLMs) show exceptional abilities across\ndiverse tasks, such as visual question answering. To enhance user experience,\nrecent studies investigate VLM personalization to understand user-provided\nconcepts. However, they mainly focus on single-concept personalization,\nneglecting the existence and interplay of multiple concepts, which limits\nreal-world applicability. This paper proposes the first multi-concept\npersonalization paradigm, MC-LLaVA. Specifically, MC-LLaVA employs a\nmulti-concept instruction tuning strategy, effectively integrating multiple\nconcepts in a single training step. To reduce the costs related to joint\ntraining, we propose a personalized textual prompt that uses visual token\ninformation to initialize concept tokens. Additionally, we introduce a\npersonalized visual prompt during inference, aggregating location confidence\nmaps for enhanced recognition and grounding capabilities. To advance\nmulti-concept personalization research, we further contribute a high-quality\ninstruction tuning dataset. We carefully collect images with multiple\ncharacters and objects from movies and manually generate question-answer\nsamples for multi-concept scenarios, featuring superior diversity.\nComprehensive qualitative and quantitative experiments demonstrate that\nMC-LLaVA can achieve impressive multi-concept personalized responses, paving\nthe way for VLMs to become better user-specific assistants. The code and\ndataset will be publicly available at https://github.com/arctanxarc/MC-LLaVA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current vision-language models (VLMs) show exceptional abilities across\ndiverse tasks, such as visual question answering. To enhance user experience,\nrecent studies investigate VLM personalization to understand user-provided\nconcepts. However, they mainly focus on single-concept personalization,\nneglecting the existence and interplay of multiple concepts, which limits\nreal-world applicability. This paper proposes the first multi-concept\npersonalization paradigm, MC-LLaVA. Specifically, MC-LLaVA employs a\nmulti-concept instruction tuning strategy, effectively integrating multiple\nconcepts in a single training step. To reduce the costs related to joint\ntraining, we propose a personalized textual prompt that uses visual token\ninformation to initialize concept tokens. Additionally, we introduce a\npersonalized visual prompt during inference, aggregating location confidence\nmaps for enhanced recognition and grounding capabilities. To advance\nmulti-concept personalization research, we further contribute a high-quality\ninstruction tuning dataset. We carefully collect images with multiple\ncharacters and objects from movies and manually generate question-answer\nsamples for multi-concept scenarios, featuring superior diversity.\nComprehensive qualitative and quantitative experiments demonstrate that\nMC-LLaVA can achieve impressive multi-concept personalized responses, paving\nthe way for VLMs to become better user-specific assistants. The code and\ndataset will be publicly available at https://github.com/arctanxarc/MC-LLaVA."
                },
                "authors": [
                    {
                        "name": "Ruichuan An"
                    },
                    {
                        "name": "Sihan Yang"
                    },
                    {
                        "name": "Ming Lu"
                    },
                    {
                        "name": "Renrui Zhang"
                    },
                    {
                        "name": "Kai Zeng"
                    },
                    {
                        "name": "Yulin Luo"
                    },
                    {
                        "name": "Jiajun Cao"
                    },
                    {
                        "name": "Hao Liang"
                    },
                    {
                        "name": "Ying Chen"
                    },
                    {
                        "name": "Qi She"
                    },
                    {
                        "name": "Shanghang Zhang"
                    },
                    {
                        "name": "Wentao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wentao Zhang"
                },
                "author": "Wentao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11706v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11706v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20648v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20648v1",
                "updated": "2025-03-26T15:40:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    15,
                    40,
                    40,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T15:40:40Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    15,
                    40,
                    40,
                    2,
                    85,
                    0
                ],
                "title": "TN-Eval: Rubric and Evaluation Protocols for Measuring the Quality of\n  Behavioral Therapy Notes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TN-Eval: Rubric and Evaluation Protocols for Measuring the Quality of\n  Behavioral Therapy Notes"
                },
                "summary": "Behavioral therapy notes are important for both legal compliance and patient\ncare. Unlike progress notes in physical health, quality standards for\nbehavioral therapy notes remain underdeveloped. To address this gap, we\ncollaborated with licensed therapists to design a comprehensive rubric for\nevaluating therapy notes across key dimensions: completeness, conciseness, and\nfaithfulness. Further, we extend a public dataset of behavioral health\nconversations with therapist-written notes and LLM-generated notes, and apply\nour evaluation framework to measure their quality. We find that: (1) A\nrubric-based manual evaluation protocol offers more reliable and interpretable\nresults than traditional Likert-scale annotations. (2) LLMs can mimic human\nevaluators in assessing completeness and conciseness but struggle with\nfaithfulness. (3) Therapist-written notes often lack completeness and\nconciseness, while LLM-generated notes contain hallucination. Surprisingly, in\na blind test, therapists prefer and judge LLM-generated notes to be superior to\ntherapist-written notes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Behavioral therapy notes are important for both legal compliance and patient\ncare. Unlike progress notes in physical health, quality standards for\nbehavioral therapy notes remain underdeveloped. To address this gap, we\ncollaborated with licensed therapists to design a comprehensive rubric for\nevaluating therapy notes across key dimensions: completeness, conciseness, and\nfaithfulness. Further, we extend a public dataset of behavioral health\nconversations with therapist-written notes and LLM-generated notes, and apply\nour evaluation framework to measure their quality. We find that: (1) A\nrubric-based manual evaluation protocol offers more reliable and interpretable\nresults than traditional Likert-scale annotations. (2) LLMs can mimic human\nevaluators in assessing completeness and conciseness but struggle with\nfaithfulness. (3) Therapist-written notes often lack completeness and\nconciseness, while LLM-generated notes contain hallucination. Surprisingly, in\na blind test, therapists prefer and judge LLM-generated notes to be superior to\ntherapist-written notes."
                },
                "authors": [
                    {
                        "name": "Raj Sanjay Shah"
                    },
                    {
                        "name": "Lei Xu"
                    },
                    {
                        "name": "Qianchu Liu"
                    },
                    {
                        "name": "Jon Burnsky"
                    },
                    {
                        "name": "Drew Bertagnolli"
                    },
                    {
                        "name": "Chaitanya Shivade"
                    }
                ],
                "author_detail": {
                    "name": "Chaitanya Shivade"
                },
                "author": "Chaitanya Shivade",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20648v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20648v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20644v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20644v1",
                "updated": "2025-03-26T15:37:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    15,
                    37,
                    17,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T15:37:17Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    15,
                    37,
                    17,
                    2,
                    85,
                    0
                ],
                "title": "MMGen: Unified Multi-modal Image Generation and Understanding in One Go",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MMGen: Unified Multi-modal Image Generation and Understanding in One Go"
                },
                "summary": "A unified diffusion framework for multi-modal generation and understanding\nhas the transformative potential to achieve seamless and controllable image\ndiffusion and other cross-modal tasks. In this paper, we introduce MMGen, a\nunified framework that integrates multiple generative tasks into a single\ndiffusion model. This includes: (1) multi-modal category-conditioned\ngeneration, where multi-modal outputs are generated simultaneously through a\nsingle inference process, given category information; (2) multi-modal visual\nunderstanding, which accurately predicts depth, surface normals, and\nsegmentation maps from RGB images; and (3) multi-modal conditioned generation,\nwhich produces corresponding RGB images based on specific modality conditions\nand other aligned modalities. Our approach develops a novel diffusion\ntransformer that flexibly supports multi-modal output, along with a simple\nmodality-decoupling strategy to unify various tasks. Extensive experiments and\napplications demonstrate the effectiveness and superiority of MMGen across\ndiverse tasks and conditions, highlighting its potential for applications that\nrequire simultaneous generation and understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A unified diffusion framework for multi-modal generation and understanding\nhas the transformative potential to achieve seamless and controllable image\ndiffusion and other cross-modal tasks. In this paper, we introduce MMGen, a\nunified framework that integrates multiple generative tasks into a single\ndiffusion model. This includes: (1) multi-modal category-conditioned\ngeneration, where multi-modal outputs are generated simultaneously through a\nsingle inference process, given category information; (2) multi-modal visual\nunderstanding, which accurately predicts depth, surface normals, and\nsegmentation maps from RGB images; and (3) multi-modal conditioned generation,\nwhich produces corresponding RGB images based on specific modality conditions\nand other aligned modalities. Our approach develops a novel diffusion\ntransformer that flexibly supports multi-modal output, along with a simple\nmodality-decoupling strategy to unify various tasks. Extensive experiments and\napplications demonstrate the effectiveness and superiority of MMGen across\ndiverse tasks and conditions, highlighting its potential for applications that\nrequire simultaneous generation and understanding."
                },
                "authors": [
                    {
                        "name": "Jiepeng Wang"
                    },
                    {
                        "name": "Zhaoqing Wang"
                    },
                    {
                        "name": "Hao Pan"
                    },
                    {
                        "name": "Yuan Liu"
                    },
                    {
                        "name": "Dongdong Yu"
                    },
                    {
                        "name": "Changhu Wang"
                    },
                    {
                        "name": "Wenping Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wenping Wang"
                },
                "author": "Wenping Wang",
                "arxiv_comment": "Our project page: https://jiepengwang.github.io/MMGen/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20644v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20644v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20641v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20641v1",
                "updated": "2025-03-26T15:34:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    15,
                    34,
                    37,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T15:34:37Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    15,
                    34,
                    37,
                    2,
                    85,
                    0
                ],
                "title": "Unlocking Efficient Long-to-Short LLM Reasoning with Model Merging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking Efficient Long-to-Short LLM Reasoning with Model Merging"
                },
                "summary": "The transition from System 1 to System 2 reasoning in large language models\n(LLMs) has marked significant advancements in handling complex tasks through\ndeliberate, iterative thinking. However, this progress often comes at the cost\nof efficiency, as models tend to overthink, generating redundant reasoning\nsteps without proportional improvements in output quality. Long-to-Short (L2S)\nreasoning has emerged as a promising solution to this challenge, aiming to\nbalance reasoning depth with practical efficiency. While existing approaches,\nsuch as supervised fine-tuning (SFT), reinforcement learning (RL), and prompt\nengineering, have shown potential, they are either computationally expensive or\nunstable. Model merging, on the other hand, offers a cost-effective and robust\nalternative by integrating the quick-thinking capabilities of System 1 models\nwith the methodical reasoning of System 2 models. In this work, we present a\ncomprehensive empirical study on model merging for L2S reasoning, exploring\ndiverse methodologies, including task-vector-based, SVD-based, and\nactivation-informed merging. Our experiments reveal that model merging can\nreduce average response length by up to 55% while preserving or even improving\nbaseline performance. We also identify a strong correlation between model scale\nand merging efficacy with extensive evaluations on 1.5B/7B/14B/32B models.\nFurthermore, we investigate the merged model's ability to self-critique and\nself-correct, as well as its adaptive response length based on task complexity.\nOur findings highlight model merging as a highly efficient and effective\nparadigm for L2S reasoning, offering a practical solution to the overthinking\nproblem while maintaining the robustness of System 2 reasoning. This work can\nbe found on Github https://github.com/hahahawu/Long-to-Short-via-Model-Merging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The transition from System 1 to System 2 reasoning in large language models\n(LLMs) has marked significant advancements in handling complex tasks through\ndeliberate, iterative thinking. However, this progress often comes at the cost\nof efficiency, as models tend to overthink, generating redundant reasoning\nsteps without proportional improvements in output quality. Long-to-Short (L2S)\nreasoning has emerged as a promising solution to this challenge, aiming to\nbalance reasoning depth with practical efficiency. While existing approaches,\nsuch as supervised fine-tuning (SFT), reinforcement learning (RL), and prompt\nengineering, have shown potential, they are either computationally expensive or\nunstable. Model merging, on the other hand, offers a cost-effective and robust\nalternative by integrating the quick-thinking capabilities of System 1 models\nwith the methodical reasoning of System 2 models. In this work, we present a\ncomprehensive empirical study on model merging for L2S reasoning, exploring\ndiverse methodologies, including task-vector-based, SVD-based, and\nactivation-informed merging. Our experiments reveal that model merging can\nreduce average response length by up to 55% while preserving or even improving\nbaseline performance. We also identify a strong correlation between model scale\nand merging efficacy with extensive evaluations on 1.5B/7B/14B/32B models.\nFurthermore, we investigate the merged model's ability to self-critique and\nself-correct, as well as its adaptive response length based on task complexity.\nOur findings highlight model merging as a highly efficient and effective\nparadigm for L2S reasoning, offering a practical solution to the overthinking\nproblem while maintaining the robustness of System 2 reasoning. This work can\nbe found on Github https://github.com/hahahawu/Long-to-Short-via-Model-Merging."
                },
                "authors": [
                    {
                        "name": "Han Wu"
                    },
                    {
                        "name": "Yuxuan Yao"
                    },
                    {
                        "name": "Shuqi Liu"
                    },
                    {
                        "name": "Zehua Liu"
                    },
                    {
                        "name": "Xiaojin Fu"
                    },
                    {
                        "name": "Xiongwei Han"
                    },
                    {
                        "name": "Xing Li"
                    },
                    {
                        "name": "Hui-Ling Zhen"
                    },
                    {
                        "name": "Tao Zhong"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Mingxuan Yuan"
                },
                "author": "Mingxuan Yuan",
                "arxiv_comment": "Work in progress; technical report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20641v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20641v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20629v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20629v1",
                "updated": "2025-03-26T15:24:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    15,
                    24,
                    1,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T15:24:01Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    15,
                    24,
                    1,
                    2,
                    85,
                    0
                ],
                "title": "Tracking the topology of neural manifolds across populations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tracking the topology of neural manifolds across populations"
                },
                "summary": "Neural manifolds summarize the intrinsic structure of the information encoded\nby a population of neurons. Advances in experimental techniques have made\nsimultaneous recordings from multiple brain regions increasingly commonplace,\nraising the possibility of studying how these manifolds relate across\npopulations. However, when the manifolds are nonlinear and possibly code for\nmultiple unknown variables, it is challenging to extract robust and falsifiable\ninformation about their relationships. We introduce a framework, called the\nmethod of analogous cycles, for matching topological features of neural\nmanifolds using only observed dissimilarity matrices within and between neural\npopulations. We demonstrate via analysis of simulations and \\emph{in vivo}\nexperimental data that this method can be used to correctly identify multiple\nshared circular coordinate systems across both stimuli and inferred neural\nmanifolds. Conversely, the method rejects matching features that are not\nintrinsic to one of the systems. Further, as this method is deterministic and\ndoes not rely on dimensionality reduction or optimization methods, it is\namenable to direct mathematical investigation and interpretation in terms of\nthe underlying neural activity. We thus propose the method of analogous cycles\nas a suitable foundation for a theory of cross-population analysis via neural\nmanifolds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural manifolds summarize the intrinsic structure of the information encoded\nby a population of neurons. Advances in experimental techniques have made\nsimultaneous recordings from multiple brain regions increasingly commonplace,\nraising the possibility of studying how these manifolds relate across\npopulations. However, when the manifolds are nonlinear and possibly code for\nmultiple unknown variables, it is challenging to extract robust and falsifiable\ninformation about their relationships. We introduce a framework, called the\nmethod of analogous cycles, for matching topological features of neural\nmanifolds using only observed dissimilarity matrices within and between neural\npopulations. We demonstrate via analysis of simulations and \\emph{in vivo}\nexperimental data that this method can be used to correctly identify multiple\nshared circular coordinate systems across both stimuli and inferred neural\nmanifolds. Conversely, the method rejects matching features that are not\nintrinsic to one of the systems. Further, as this method is deterministic and\ndoes not rely on dimensionality reduction or optimization methods, it is\namenable to direct mathematical investigation and interpretation in terms of\nthe underlying neural activity. We thus propose the method of analogous cycles\nas a suitable foundation for a theory of cross-population analysis via neural\nmanifolds."
                },
                "authors": [
                    {
                        "name": "Iris H. R. Yoon"
                    },
                    {
                        "name": "Gregory Henselman-Petrusek"
                    },
                    {
                        "name": "Yiyi Yu"
                    },
                    {
                        "name": "Robert Ghrist"
                    },
                    {
                        "name": "Spencer LaVere Smith"
                    },
                    {
                        "name": "Chad Giusti"
                    }
                ],
                "author_detail": {
                    "name": "Chad Giusti"
                },
                "author": "Chad Giusti",
                "arxiv_doi": "10.1073/pnas.2407997121",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1073/pnas.2407997121",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.20629v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20629v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Proceedings of the National Academy of Sciences, 2024, 121(46),\n  e2407997121",
                "arxiv_primary_category": {
                    "term": "q-bio.NC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.AT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13396v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13396v2",
                "updated": "2025-03-26T15:23:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    15,
                    23,
                    45,
                    2,
                    85,
                    0
                ],
                "published": "2024-08-23T22:32:20Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    22,
                    32,
                    20,
                    4,
                    236,
                    0
                ],
                "title": "Unveiling galaxy chemical enrichment mechanisms out to z~8 from direct\n  determination of O & Ar abundances from JWST/NIRSPEC spectroscopy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling galaxy chemical enrichment mechanisms out to z~8 from direct\n  determination of O & Ar abundances from JWST/NIRSPEC spectroscopy"
                },
                "summary": "Galaxy chemical enrichment mechanisms have primarily been constrained by\n[$\\alpha$/Fe] and [Fe/H] measurements of individual stars and integrated light\nfrom stellar populations. However such measurements are limited at higher\nredshifts (z>1). Recently, we proposed an analogous diagram of the\noxygen-to-argon abundance ratio, log(O/Ar), vs Ar abundance, 12+log(Ar/H), as a\nnew diagnostic window for emission nebulae. In this Letter, using robust line\nflux measurements including temperature sensitive auroral lines, we present\ndirect determination of O and Ar abundances in nine SFGs from JWST/NIRSPEC\nspectra at z$\\sim$1.3-7.7, and two more with Keck/MOSFIRE spectra at\nz$\\sim$2.2. Utilising their positions on the log(O/Ar) vs 12+log(Ar/H) plane,\nwe present the first inference of galaxy chemical enrichment mechanisms from an\nensemble of galaxies. The SFGs at z$\\sim$1.3-3.4 are consistent with the solar\nneighbourhood galactic chemical enrichment models of the Milky Way Galaxy that\nare driven by core-collapse and Type Ia supernovae. Such enrichment mechanisms\nthus occur at least out to z$\\sim$3.4. However, the highest-redshift SFGs\n(z$\\sim$3.6-7.7) have very low log(O/Ar) values, revealing a different\nenrichment process at z>3.6. Such low log(O/Ar) values may be caused by a rapid\nbut intermittent star-formation and/or additional sources. The new diagnostic\nwindow for SFGs enables us to reveal the unique fingerprints of galaxy chemical\nenrichment out to cosmic dawn.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Galaxy chemical enrichment mechanisms have primarily been constrained by\n[$\\alpha$/Fe] and [Fe/H] measurements of individual stars and integrated light\nfrom stellar populations. However such measurements are limited at higher\nredshifts (z>1). Recently, we proposed an analogous diagram of the\noxygen-to-argon abundance ratio, log(O/Ar), vs Ar abundance, 12+log(Ar/H), as a\nnew diagnostic window for emission nebulae. In this Letter, using robust line\nflux measurements including temperature sensitive auroral lines, we present\ndirect determination of O and Ar abundances in nine SFGs from JWST/NIRSPEC\nspectra at z$\\sim$1.3-7.7, and two more with Keck/MOSFIRE spectra at\nz$\\sim$2.2. Utilising their positions on the log(O/Ar) vs 12+log(Ar/H) plane,\nwe present the first inference of galaxy chemical enrichment mechanisms from an\nensemble of galaxies. The SFGs at z$\\sim$1.3-3.4 are consistent with the solar\nneighbourhood galactic chemical enrichment models of the Milky Way Galaxy that\nare driven by core-collapse and Type Ia supernovae. Such enrichment mechanisms\nthus occur at least out to z$\\sim$3.4. However, the highest-redshift SFGs\n(z$\\sim$3.6-7.7) have very low log(O/Ar) values, revealing a different\nenrichment process at z>3.6. Such low log(O/Ar) values may be caused by a rapid\nbut intermittent star-formation and/or additional sources. The new diagnostic\nwindow for SFGs enables us to reveal the unique fingerprints of galaxy chemical\nenrichment out to cosmic dawn."
                },
                "authors": [
                    {
                        "name": "Souradeep Bhattacharya"
                    },
                    {
                        "name": "Magda Arnaboldi"
                    },
                    {
                        "name": "Ortwin Gerhard"
                    },
                    {
                        "name": "Chiaki Kobayashi"
                    },
                    {
                        "name": "Kanak Saha"
                    }
                ],
                "author_detail": {
                    "name": "Kanak Saha"
                },
                "author": "Kanak Saha",
                "arxiv_comment": "13 Pages, 4 Figures and 2 Tables; Accepted for publication in ApJ\n  Letters",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13396v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13396v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05223v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05223v2",
                "updated": "2025-03-26T15:18:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    15,
                    18,
                    53,
                    2,
                    85,
                    0
                ],
                "published": "2024-12-06T17:54:54Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    54,
                    54,
                    4,
                    341,
                    0
                ],
                "title": "100% Elimination of Hallucinations on RAGTruth for GPT-4 and GPT-3.5\n  Turbo",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "100% Elimination of Hallucinations on RAGTruth for GPT-4 and GPT-3.5\n  Turbo"
                },
                "summary": "The issue of hallucinations in large language models (LLMs) remains a\ncritical barrier to the adoption of AI in enterprise and other high-stakes\napplications. Despite advancements in retrieval-augmented generation (RAG)\nsystems, current state-of-the-art methods fail to achieve more than 80%\naccuracy in generating faithful and factually correct outputs, even when\nprovided with relevant and accurate context. In this work, we introduce Acurai,\na novel systematic approach that achieves 100% hallucination-free responses in\nLLMs by reformatting queries and context data prior to input. Leveraging a deep\nunderstanding of LLM internal representations, the importance of noun-phrase\ndominance, and the role of discrete functional units (DFUs), Acurai ensures\nalignment between input context and generated output. We validate this method\nusing the RAGTruth corpus, demonstrating its ability to eliminate 100%\nhallucinations for both GPT-4 and GPT-3.5 Turbo. Acurai sets a new standard for\nachieving consistent, accurate, and faithful AI responses, marking a\nsignificant step forward in the development of trustworthy AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The issue of hallucinations in large language models (LLMs) remains a\ncritical barrier to the adoption of AI in enterprise and other high-stakes\napplications. Despite advancements in retrieval-augmented generation (RAG)\nsystems, current state-of-the-art methods fail to achieve more than 80%\naccuracy in generating faithful and factually correct outputs, even when\nprovided with relevant and accurate context. In this work, we introduce Acurai,\na novel systematic approach that achieves 100% hallucination-free responses in\nLLMs by reformatting queries and context data prior to input. Leveraging a deep\nunderstanding of LLM internal representations, the importance of noun-phrase\ndominance, and the role of discrete functional units (DFUs), Acurai ensures\nalignment between input context and generated output. We validate this method\nusing the RAGTruth corpus, demonstrating its ability to eliminate 100%\nhallucinations for both GPT-4 and GPT-3.5 Turbo. Acurai sets a new standard for\nachieving consistent, accurate, and faithful AI responses, marking a\nsignificant step forward in the development of trustworthy AI systems."
                },
                "authors": [
                    {
                        "name": "Michael C. Wood"
                    },
                    {
                        "name": "Adam A. Forbes"
                    }
                ],
                "author_detail": {
                    "name": "Adam A. Forbes"
                },
                "author": "Adam A. Forbes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05223v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05223v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20623v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20623v1",
                "updated": "2025-03-26T15:10:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    15,
                    10,
                    47,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T15:10:47Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    15,
                    10,
                    47,
                    2,
                    85,
                    0
                ],
                "title": "Collaborative Storytelling and LLM: A Linguistic Analysis of\n  Automatically-Generated Role-Playing Game Sessions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collaborative Storytelling and LLM: A Linguistic Analysis of\n  Automatically-Generated Role-Playing Game Sessions"
                },
                "summary": "Role-playing games (RPG) are games in which players interact with one another\nto create narratives. The role of players in the RPG is largely based on the\ninteraction between players and their characters. This emerging form of shared\nnarrative, primarily oral, is receiving increasing attention. In particular,\nmany authors investigated the use of an LLM as an actor in the game. In this\npaper, we aim to discover to what extent the language of Large Language Models\n(LLMs) exhibit oral or written features when asked to generate an RPG session\nwithout human interference. We will conduct a linguistic analysis of the\nlexical and syntactic features of the generated texts and compare the results\nwith analyses of conversations, transcripts of human RPG sessions, and books.\nWe found that LLMs exhibit a pattern that is distinct from all other text\ncategories, including oral conversations, human RPG sessions and books. Our\nanalysis has shown how training influences the way LLMs express themselves and\nprovides important indications of the narrative capabilities of these tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Role-playing games (RPG) are games in which players interact with one another\nto create narratives. The role of players in the RPG is largely based on the\ninteraction between players and their characters. This emerging form of shared\nnarrative, primarily oral, is receiving increasing attention. In particular,\nmany authors investigated the use of an LLM as an actor in the game. In this\npaper, we aim to discover to what extent the language of Large Language Models\n(LLMs) exhibit oral or written features when asked to generate an RPG session\nwithout human interference. We will conduct a linguistic analysis of the\nlexical and syntactic features of the generated texts and compare the results\nwith analyses of conversations, transcripts of human RPG sessions, and books.\nWe found that LLMs exhibit a pattern that is distinct from all other text\ncategories, including oral conversations, human RPG sessions and books. Our\nanalysis has shown how training influences the way LLMs express themselves and\nprovides important indications of the narrative capabilities of these tools."
                },
                "authors": [
                    {
                        "name": "Alessandro Maisto"
                    }
                ],
                "author_detail": {
                    "name": "Alessandro Maisto"
                },
                "author": "Alessandro Maisto",
                "arxiv_comment": "17 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20623v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20623v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16302v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16302v2",
                "updated": "2025-03-26T15:08:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    15,
                    8,
                    12,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-20T16:23:44Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    16,
                    23,
                    44,
                    3,
                    79,
                    0
                ],
                "title": "Unleashing Vecset Diffusion Model for Fast Shape Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing Vecset Diffusion Model for Fast Shape Generation"
                },
                "summary": "3D shape generation has greatly flourished through the development of\nso-called \"native\" 3D diffusion, particularly through the Vecset Diffusion\nModel (VDM). While recent advancements have shown promising results in\ngenerating high-resolution 3D shapes, VDM still struggles with high-speed\ngeneration. Challenges exist because of difficulties not only in accelerating\ndiffusion sampling but also VAE decoding in VDM, areas under-explored in\nprevious works. To address these challenges, we present FlashVDM, a systematic\nframework for accelerating both VAE and DiT in VDM. For DiT, FlashVDM enables\nflexible diffusion sampling with as few as 5 inference steps and comparable\nquality, which is made possible by stabilizing consistency distillation with\nour newly introduced Progressive Flow Distillation. For VAE, we introduce a\nlightning vecset decoder equipped with Adaptive KV Selection, Hierarchical\nVolume Decoding, and Efficient Network Design. By exploiting the locality of\nthe vecset and the sparsity of shape surface in the volume, our decoder\ndrastically lowers FLOPs, minimizing the overall decoding overhead. We apply\nFlashVDM to Hunyuan3D-2 to obtain Hunyuan3D-2 Turbo. Through systematic\nevaluation, we show that our model significantly outperforms existing fast 3D\ngeneration methods, achieving comparable performance to the state-of-the-art\nwhile reducing inference time by over 45x for reconstruction and 32x for\ngeneration. Code and models are available at\nhttps://github.com/Tencent/FlashVDM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D shape generation has greatly flourished through the development of\nso-called \"native\" 3D diffusion, particularly through the Vecset Diffusion\nModel (VDM). While recent advancements have shown promising results in\ngenerating high-resolution 3D shapes, VDM still struggles with high-speed\ngeneration. Challenges exist because of difficulties not only in accelerating\ndiffusion sampling but also VAE decoding in VDM, areas under-explored in\nprevious works. To address these challenges, we present FlashVDM, a systematic\nframework for accelerating both VAE and DiT in VDM. For DiT, FlashVDM enables\nflexible diffusion sampling with as few as 5 inference steps and comparable\nquality, which is made possible by stabilizing consistency distillation with\nour newly introduced Progressive Flow Distillation. For VAE, we introduce a\nlightning vecset decoder equipped with Adaptive KV Selection, Hierarchical\nVolume Decoding, and Efficient Network Design. By exploiting the locality of\nthe vecset and the sparsity of shape surface in the volume, our decoder\ndrastically lowers FLOPs, minimizing the overall decoding overhead. We apply\nFlashVDM to Hunyuan3D-2 to obtain Hunyuan3D-2 Turbo. Through systematic\nevaluation, we show that our model significantly outperforms existing fast 3D\ngeneration methods, achieving comparable performance to the state-of-the-art\nwhile reducing inference time by over 45x for reconstruction and 32x for\ngeneration. Code and models are available at\nhttps://github.com/Tencent/FlashVDM."
                },
                "authors": [
                    {
                        "name": "Zeqiang Lai"
                    },
                    {
                        "name": "Yunfei Zhao"
                    },
                    {
                        "name": "Zibo Zhao"
                    },
                    {
                        "name": "Haolin Liu"
                    },
                    {
                        "name": "Fuyun Wang"
                    },
                    {
                        "name": "Huiwen Shi"
                    },
                    {
                        "name": "Xianghui Yang"
                    },
                    {
                        "name": "Qingxiang Lin"
                    },
                    {
                        "name": "Jingwei Huang"
                    },
                    {
                        "name": "Yuhong Liu"
                    },
                    {
                        "name": "Jie Jiang"
                    },
                    {
                        "name": "Chunchao Guo"
                    },
                    {
                        "name": "Xiangyu Yue"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Yue"
                },
                "author": "Xiangyu Yue",
                "arxiv_comment": "Technical report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16302v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16302v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20589v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20589v1",
                "updated": "2025-03-26T14:41:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    14,
                    41,
                    38,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T14:41:38Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    14,
                    41,
                    38,
                    2,
                    85,
                    0
                ],
                "title": "What to Retrieve for Effective Retrieval-Augmented Code Generation? An\n  Empirical Study and Beyond",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What to Retrieve for Effective Retrieval-Augmented Code Generation? An\n  Empirical Study and Beyond"
                },
                "summary": "Repository-level code generation remains challenging due to complex code\ndependencies and the limitations of large language models (LLMs) in processing\nlong contexts. While retrieval-augmented generation (RAG) frameworks are widely\nadopted, the effectiveness of different retrieved information\nsources-contextual code, APIs, and similar snippets-has not been rigorously\nanalyzed. Through an empirical study on two benchmarks, we demonstrate that\nin-context code and potential API information significantly enhance LLM\nperformance, whereas retrieved similar code often introduces noise, degrading\nresults by up to 15%. Based on the preliminary results, we propose\nAllianceCoder, a novel context-integrated method that employs chain-of-thought\nprompting to decompose user queries into implementation steps and retrieves\nAPIs via semantic description matching. Through extensive experiments on\nCoderEval and RepoExec, AllianceCoder achieves state-of-the-art performance,\nimproving Pass@1 by up to 20% over existing approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Repository-level code generation remains challenging due to complex code\ndependencies and the limitations of large language models (LLMs) in processing\nlong contexts. While retrieval-augmented generation (RAG) frameworks are widely\nadopted, the effectiveness of different retrieved information\nsources-contextual code, APIs, and similar snippets-has not been rigorously\nanalyzed. Through an empirical study on two benchmarks, we demonstrate that\nin-context code and potential API information significantly enhance LLM\nperformance, whereas retrieved similar code often introduces noise, degrading\nresults by up to 15%. Based on the preliminary results, we propose\nAllianceCoder, a novel context-integrated method that employs chain-of-thought\nprompting to decompose user queries into implementation steps and retrieves\nAPIs via semantic description matching. Through extensive experiments on\nCoderEval and RepoExec, AllianceCoder achieves state-of-the-art performance,\nimproving Pass@1 by up to 20% over existing approaches."
                },
                "authors": [
                    {
                        "name": "Wenchao Gu"
                    },
                    {
                        "name": "Juntao Chen"
                    },
                    {
                        "name": "Yanlin Wang"
                    },
                    {
                        "name": "Tianyue Jiang"
                    },
                    {
                        "name": "Xingzhe Li"
                    },
                    {
                        "name": "Mingwei Liu"
                    },
                    {
                        "name": "Xilin Liu"
                    },
                    {
                        "name": "Yuchi Ma"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20589v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20589v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20588v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20588v1",
                "updated": "2025-03-26T14:41:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    14,
                    41,
                    4,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T14:41:04Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    14,
                    41,
                    4,
                    2,
                    85,
                    0
                ],
                "title": "Synthetic Data Augmentation for Cross-domain Implicit Discourse Relation\n  Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthetic Data Augmentation for Cross-domain Implicit Discourse Relation\n  Recognition"
                },
                "summary": "Implicit discourse relation recognition (IDRR) -- the task of identifying the\nimplicit coherence relation between two text spans -- requires deep semantic\nunderstanding. Recent studies have shown that zero- or few-shot approaches\nsignificantly lag behind supervised models, but LLMs may be useful for\nsynthetic data augmentation, where LLMs generate a second argument following a\nspecified coherence relation. We applied this approach in a cross-domain\nsetting, generating discourse continuations using unlabelled target-domain data\nto adapt a base model which was trained on source-domain labelled data.\nEvaluations conducted on a large-scale test set revealed that different\nvariations of the approach did not result in any significant improvements. We\nconclude that LLMs often fail to generate useful samples for IDRR, and\nemphasize the importance of considering both statistical significance and\ncomparability when evaluating IDRR models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implicit discourse relation recognition (IDRR) -- the task of identifying the\nimplicit coherence relation between two text spans -- requires deep semantic\nunderstanding. Recent studies have shown that zero- or few-shot approaches\nsignificantly lag behind supervised models, but LLMs may be useful for\nsynthetic data augmentation, where LLMs generate a second argument following a\nspecified coherence relation. We applied this approach in a cross-domain\nsetting, generating discourse continuations using unlabelled target-domain data\nto adapt a base model which was trained on source-domain labelled data.\nEvaluations conducted on a large-scale test set revealed that different\nvariations of the approach did not result in any significant improvements. We\nconclude that LLMs often fail to generate useful samples for IDRR, and\nemphasize the importance of considering both statistical significance and\ncomparability when evaluating IDRR models."
                },
                "authors": [
                    {
                        "name": "Frances Yung"
                    },
                    {
                        "name": "Varsha Suresh"
                    },
                    {
                        "name": "Zaynab Reza"
                    },
                    {
                        "name": "Mansoor Ahmad"
                    },
                    {
                        "name": "Vera Demberg"
                    }
                ],
                "author_detail": {
                    "name": "Vera Demberg"
                },
                "author": "Vera Demberg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20588v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20588v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20580v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20580v1",
                "updated": "2025-03-26T14:26:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    14,
                    26,
                    23,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T14:26:23Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    14,
                    26,
                    23,
                    2,
                    85,
                    0
                ],
                "title": "Experiments and modeling of mechanically-soft, hard magnetorheological\n  foams with potential applications in haptic sensing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Experiments and modeling of mechanically-soft, hard magnetorheological\n  foams with potential applications in haptic sensing"
                },
                "summary": "This study proposes a novel mechanically-soft and magnetically-hard\nmagnetorheological foam that, upon deformation, leads to robust and measurable\nmagnetic flux changes in its surroundings. This allows to infer qualitatively\nand even quantitatively the imposed deformation and, eventually from that, an\nestimation of the stiffness and average stress on the sample even in complex\nloading scenarios involving combinations of uniform or nonuniform\ncompression/tension with superposed shearing at in different directions. The\nwork provides a complete experimental, theoretical and numerical framework on\nfinite strain, compressible magneto-elasticity, thereby allowing to measure and\npredict coupled magneto-mechanical properties of such materials and then use it\nto estimate and design potential haptic sensing devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study proposes a novel mechanically-soft and magnetically-hard\nmagnetorheological foam that, upon deformation, leads to robust and measurable\nmagnetic flux changes in its surroundings. This allows to infer qualitatively\nand even quantitatively the imposed deformation and, eventually from that, an\nestimation of the stiffness and average stress on the sample even in complex\nloading scenarios involving combinations of uniform or nonuniform\ncompression/tension with superposed shearing at in different directions. The\nwork provides a complete experimental, theoretical and numerical framework on\nfinite strain, compressible magneto-elasticity, thereby allowing to measure and\npredict coupled magneto-mechanical properties of such materials and then use it\nto estimate and design potential haptic sensing devices."
                },
                "authors": [
                    {
                        "name": "Zehui Lin"
                    },
                    {
                        "name": "Zahra Hooshmand-Ahoor"
                    },
                    {
                        "name": "Laurence Bodelot"
                    },
                    {
                        "name": "Kostas Danas"
                    }
                ],
                "author_detail": {
                    "name": "Kostas Danas"
                },
                "author": "Kostas Danas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20580v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20580v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.soft",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20579v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20579v1",
                "updated": "2025-03-26T14:25:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    14,
                    25,
                    27,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T14:25:27Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    14,
                    25,
                    27,
                    2,
                    85,
                    0
                ],
                "title": "Is Reuse All You Need? A Systematic Comparison of Regular Expression\n  Composition Strategies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is Reuse All You Need? A Systematic Comparison of Regular Expression\n  Composition Strategies"
                },
                "summary": "Composing regular expressions (regexes) is a common but challenging\nengineering activity. Software engineers struggle with regex complexity,\nleading to defects, performance issues, and security vulnerabilities.\nResearchers have proposed tools to synthesize regexes automatically, and recent\ngenerative AI techniques are also promising. Meanwhile, developers commonly\nreuse existing regexes from Internet sources and codebases. In this study, we\nask a simple question: are regex composition tasks unique enough to merit\ndedicated machinery, or is reuse all we need?\n  We answer this question through a systematic evaluation of state-of-the-art\nregex reuse and synthesis strategies. We begin by collecting a novel dataset of\nregex composition tasks mined from GitHub and RegExLib (55,137 unique tasks\nwith solution regexes). To address the absence of an automated regex reuse\nformulation, we introduce reuse-by-example, a Programming by Example (PbE)\napproach that leverages a curated database of production-ready regexes.\nAlthough all approaches can solve these composition tasks accurately,\nreuse-by-example and LLMs both do far better over the range of metrics we\napplied. Our evaluation then uses multiple dimensions, including a novel\nmetric, to compare reuse-by-example against two synthesis approaches: formal\nregex synthesizers and generative AI (LLMs). Although all approaches can solve\nthese composition tasks accurately, reuse and LLMs both do far better over the\nrange of metrics we applied. Ceteris paribus, prefer the cheaper solution --\nfor regex composition, perhaps reuse is all you need. Our findings provide\nactionable insights for developers selecting regex composition strategies and\ninform the design of future tools to improve regex reliability in software\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Composing regular expressions (regexes) is a common but challenging\nengineering activity. Software engineers struggle with regex complexity,\nleading to defects, performance issues, and security vulnerabilities.\nResearchers have proposed tools to synthesize regexes automatically, and recent\ngenerative AI techniques are also promising. Meanwhile, developers commonly\nreuse existing regexes from Internet sources and codebases. In this study, we\nask a simple question: are regex composition tasks unique enough to merit\ndedicated machinery, or is reuse all we need?\n  We answer this question through a systematic evaluation of state-of-the-art\nregex reuse and synthesis strategies. We begin by collecting a novel dataset of\nregex composition tasks mined from GitHub and RegExLib (55,137 unique tasks\nwith solution regexes). To address the absence of an automated regex reuse\nformulation, we introduce reuse-by-example, a Programming by Example (PbE)\napproach that leverages a curated database of production-ready regexes.\nAlthough all approaches can solve these composition tasks accurately,\nreuse-by-example and LLMs both do far better over the range of metrics we\napplied. Our evaluation then uses multiple dimensions, including a novel\nmetric, to compare reuse-by-example against two synthesis approaches: formal\nregex synthesizers and generative AI (LLMs). Although all approaches can solve\nthese composition tasks accurately, reuse and LLMs both do far better over the\nrange of metrics we applied. Ceteris paribus, prefer the cheaper solution --\nfor regex composition, perhaps reuse is all you need. Our findings provide\nactionable insights for developers selecting regex composition strategies and\ninform the design of future tools to improve regex reliability in software\nsystems."
                },
                "authors": [
                    {
                        "name": "Berk Çakar"
                    },
                    {
                        "name": "Charles M. Sale"
                    },
                    {
                        "name": "Sophie Chen"
                    },
                    {
                        "name": "Ethan H. Burmane"
                    },
                    {
                        "name": "Dongyoon Lee"
                    },
                    {
                        "name": "James C. Davis"
                    }
                ],
                "author_detail": {
                    "name": "James C. Davis"
                },
                "author": "James C. Davis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20579v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20579v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20578v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20578v1",
                "updated": "2025-03-26T14:25:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    14,
                    25,
                    1,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T14:25:01Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    14,
                    25,
                    1,
                    2,
                    85,
                    0
                ],
                "title": "LLPut: Investigating Large Language Models for Bug Report-Based Input\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLPut: Investigating Large Language Models for Bug Report-Based Input\n  Generation"
                },
                "summary": "Failure-inducing inputs play a crucial role in diagnosing and analyzing\nsoftware bugs. Bug reports typically contain these inputs, which developers\nextract to facilitate debugging. Since bug reports are written in natural\nlanguage, prior research has leveraged various Natural Language Processing\n(NLP) techniques for automated input extraction. With the advent of Large\nLanguage Models (LLMs), an important research question arises: how effectively\ncan generative LLMs extract failure-inducing inputs from bug reports? In this\npaper, we propose LLPut, a technique to empirically evaluate the performance of\nthree open-source generative LLMs -- LLaMA, Qwen, and Qwen-Coder -- in\nextracting relevant inputs from bug reports. We conduct an experimental\nevaluation on a dataset of 206 bug reports to assess the accuracy and\neffectiveness of these models. Our findings provide insights into the\ncapabilities and limitations of generative LLMs in automated bug diagnosis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Failure-inducing inputs play a crucial role in diagnosing and analyzing\nsoftware bugs. Bug reports typically contain these inputs, which developers\nextract to facilitate debugging. Since bug reports are written in natural\nlanguage, prior research has leveraged various Natural Language Processing\n(NLP) techniques for automated input extraction. With the advent of Large\nLanguage Models (LLMs), an important research question arises: how effectively\ncan generative LLMs extract failure-inducing inputs from bug reports? In this\npaper, we propose LLPut, a technique to empirically evaluate the performance of\nthree open-source generative LLMs -- LLaMA, Qwen, and Qwen-Coder -- in\nextracting relevant inputs from bug reports. We conduct an experimental\nevaluation on a dataset of 206 bug reports to assess the accuracy and\neffectiveness of these models. Our findings provide insights into the\ncapabilities and limitations of generative LLMs in automated bug diagnosis."
                },
                "authors": [
                    {
                        "name": "Alif Al Hasan"
                    },
                    {
                        "name": "Subarna Saha"
                    },
                    {
                        "name": "Mia Mohammad Imran"
                    },
                    {
                        "name": "Tarannum Shaila Zaman"
                    }
                ],
                "author_detail": {
                    "name": "Tarannum Shaila Zaman"
                },
                "author": "Tarannum Shaila Zaman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20578v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20578v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20576v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20576v1",
                "updated": "2025-03-26T14:23:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    14,
                    23,
                    59,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T14:23:59Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    14,
                    23,
                    59,
                    2,
                    85,
                    0
                ],
                "title": "Optimizing Case-Based Reasoning System for Functional Test Script\n  Generation with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Case-Based Reasoning System for Functional Test Script\n  Generation with Large Language Models"
                },
                "summary": "In this work, we explore the potential of large language models (LLMs) for\ngenerating functional test scripts, which necessitates understanding the\ndynamically evolving code structure of the target software. To achieve this, we\npropose a case-based reasoning (CBR) system utilizing a 4R cycle (i.e.,\nretrieve, reuse, revise, and retain), which maintains and leverages a case bank\nof test intent descriptions and corresponding test scripts to facilitate LLMs\nfor test script generation. To improve user experience further, we introduce\nRe4, an optimization method for the CBR system, comprising reranking-based\nretrieval finetuning and reinforced reuse finetuning. Specifically, we first\nidentify positive examples with high semantic and script similarity, providing\nreliable pseudo-labels for finetuning the retriever model without costly\nlabeling. Then, we apply supervised finetuning, followed by a reinforcement\nlearning finetuning stage, to align LLMs with our production scenarios,\nensuring the faithful reuse of retrieved cases. Extensive experimental results\non two product development units from Huawei Datacom demonstrate the\nsuperiority of the proposed CBR+Re4. Notably, we also show that the proposed\nRe4 method can help alleviate the repetitive generation issues with LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we explore the potential of large language models (LLMs) for\ngenerating functional test scripts, which necessitates understanding the\ndynamically evolving code structure of the target software. To achieve this, we\npropose a case-based reasoning (CBR) system utilizing a 4R cycle (i.e.,\nretrieve, reuse, revise, and retain), which maintains and leverages a case bank\nof test intent descriptions and corresponding test scripts to facilitate LLMs\nfor test script generation. To improve user experience further, we introduce\nRe4, an optimization method for the CBR system, comprising reranking-based\nretrieval finetuning and reinforced reuse finetuning. Specifically, we first\nidentify positive examples with high semantic and script similarity, providing\nreliable pseudo-labels for finetuning the retriever model without costly\nlabeling. Then, we apply supervised finetuning, followed by a reinforcement\nlearning finetuning stage, to align LLMs with our production scenarios,\nensuring the faithful reuse of retrieved cases. Extensive experimental results\non two product development units from Huawei Datacom demonstrate the\nsuperiority of the proposed CBR+Re4. Notably, we also show that the proposed\nRe4 method can help alleviate the repetitive generation issues with LLMs."
                },
                "authors": [
                    {
                        "name": "Siyuan Guo"
                    },
                    {
                        "name": "Huiwu Liu"
                    },
                    {
                        "name": "Xiaolong Chen"
                    },
                    {
                        "name": "Yuming Xie"
                    },
                    {
                        "name": "Liang Zhang"
                    },
                    {
                        "name": "Tao Han"
                    },
                    {
                        "name": "Hechang Chen"
                    },
                    {
                        "name": "Yi Chang"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20576v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20576v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12747v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12747v2",
                "updated": "2025-03-26T14:15:54Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    14,
                    15,
                    54,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-17T02:31:56Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    2,
                    31,
                    56,
                    0,
                    76,
                    0
                ],
                "title": "Statistical Inference for Weighted Sample Average Approximation in\n  Contextual Stochastic Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistical Inference for Weighted Sample Average Approximation in\n  Contextual Stochastic Optimization"
                },
                "summary": "Contextual stochastic optimization provides a framework for decision-making\nunder uncertainty incorporating observable contextual information through\ncovariates. We analyze statistical inference for weighted sample average\napproximation (wSAA), a widely-used method for solving contextual stochastic\noptimization problems. We first establish central limit theorems for wSAA\nestimates of optimal values when problems can be solved exactly, characterizing\nhow estimation uncertainty scales with covariate sample size. We then\ninvestigate practical scenarios with computational budget constraints,\nrevealing a fundamental tradeoff between statistical accuracy and computational\ncost as sample sizes increase. Through central limit theorems for\nbudget-constrained wSAA estimates, we precisely characterize this\nstatistical-computational tradeoff. We also develop \"over-optimizing\"\nstrategies for solving wSAA problems that ensure valid statistical inference.\nExtensive numerical experiments on both synthetic and real-world datasets\nvalidate our theoretical findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contextual stochastic optimization provides a framework for decision-making\nunder uncertainty incorporating observable contextual information through\ncovariates. We analyze statistical inference for weighted sample average\napproximation (wSAA), a widely-used method for solving contextual stochastic\noptimization problems. We first establish central limit theorems for wSAA\nestimates of optimal values when problems can be solved exactly, characterizing\nhow estimation uncertainty scales with covariate sample size. We then\ninvestigate practical scenarios with computational budget constraints,\nrevealing a fundamental tradeoff between statistical accuracy and computational\ncost as sample sizes increase. Through central limit theorems for\nbudget-constrained wSAA estimates, we precisely characterize this\nstatistical-computational tradeoff. We also develop \"over-optimizing\"\nstrategies for solving wSAA problems that ensure valid statistical inference.\nExtensive numerical experiments on both synthetic and real-world datasets\nvalidate our theoretical findings."
                },
                "authors": [
                    {
                        "name": "Yanyuan Wang"
                    },
                    {
                        "name": "Xiaowei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowei Zhang"
                },
                "author": "Xiaowei Zhang",
                "arxiv_comment": "Main body: 34 pages, 8 figures; supplemental material: 38 pages, 11\n  figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12747v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12747v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20568v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20568v1",
                "updated": "2025-03-26T14:07:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    14,
                    7,
                    40,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T14:07:40Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    14,
                    7,
                    40,
                    2,
                    85,
                    0
                ],
                "title": "Low-resource Information Extraction with the European Clinical Case\n  Corpus",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-resource Information Extraction with the European Clinical Case\n  Corpus"
                },
                "summary": "We present E3C-3.0, a multilingual dataset in the medical domain, comprising\nclinical cases annotated with diseases and test-result relations. The dataset\nincludes both native texts in five languages (English, French, Italian, Spanish\nand Basque) and texts translated and projected from the English source into\nfive target languages (Greek, Italian, Polish, Slovak, and Slovenian). A\nsemi-automatic approach has been implemented, including automatic annotation\nprojection based on Large Language Models (LLMs) and human revision. We present\nseveral experiments showing that current state-of-the-art LLMs can benefit from\nbeing fine-tuned on the E3C-3.0 dataset. We also show that transfer learning in\ndifferent languages is very effective, mitigating the scarcity of data.\nFinally, we compare performance both on native data and on projected data. We\nrelease the data at\nhttps://huggingface.co/collections/NLP-FBK/e3c-projected-676a7d6221608d60e4e9fd89 .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present E3C-3.0, a multilingual dataset in the medical domain, comprising\nclinical cases annotated with diseases and test-result relations. The dataset\nincludes both native texts in five languages (English, French, Italian, Spanish\nand Basque) and texts translated and projected from the English source into\nfive target languages (Greek, Italian, Polish, Slovak, and Slovenian). A\nsemi-automatic approach has been implemented, including automatic annotation\nprojection based on Large Language Models (LLMs) and human revision. We present\nseveral experiments showing that current state-of-the-art LLMs can benefit from\nbeing fine-tuned on the E3C-3.0 dataset. We also show that transfer learning in\ndifferent languages is very effective, mitigating the scarcity of data.\nFinally, we compare performance both on native data and on projected data. We\nrelease the data at\nhttps://huggingface.co/collections/NLP-FBK/e3c-projected-676a7d6221608d60e4e9fd89 ."
                },
                "authors": [
                    {
                        "name": "Soumitra Ghosh"
                    },
                    {
                        "name": "Begona Altuna"
                    },
                    {
                        "name": "Saeed Farzi"
                    },
                    {
                        "name": "Pietro Ferrazzi"
                    },
                    {
                        "name": "Alberto Lavelli"
                    },
                    {
                        "name": "Giulia Mezzanotte"
                    },
                    {
                        "name": "Manuela Speranza"
                    },
                    {
                        "name": "Bernardo Magnini"
                    }
                ],
                "author_detail": {
                    "name": "Bernardo Magnini"
                },
                "author": "Bernardo Magnini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20568v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20568v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13996v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13996v3",
                "updated": "2025-03-26T13:59:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    13,
                    59,
                    53,
                    2,
                    85,
                    0
                ],
                "published": "2024-07-19T03:01:32Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    3,
                    1,
                    32,
                    4,
                    201,
                    0
                ],
                "title": "SGDRC: Software-Defined Dynamic Resource Control for Concurrent DNN\n  Inference on NVIDIA GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SGDRC: Software-Defined Dynamic Resource Control for Concurrent DNN\n  Inference on NVIDIA GPUs"
                },
                "summary": "Cloud service providers heavily colocate high-priority, latency-sensitive\n(LS), and low-priority, best-effort (BE) DNN inference services on the same GPU\nto improve resource utilization in data centers. Among the critical shared GPU\nresources, there has been very limited analysis on the dynamic allocation of\ncompute units and VRAM bandwidth, mainly for two reasons: (1) The native GPU\nresource management solutions are either hardware-specific, or unable to\ndynamically allocate resources to different tenants, or both; (2) NVIDIA\ndoesn't expose interfaces for VRAM bandwidth allocation, and the software stack\nand VRAM channel architectures are black-box, both of which limit the\nsoftware-level resource management. These drive prior work to design either\nconservative sharing policies detrimental to throughput, or static resource\npartitioning only applicable to a few GPU models.\n  To bridge this gap, this paper proposes SGDRC, a fully software-defined\ndynamic VRAM bandwidth and compute unit management solution for concurrent DNN\ninference services. SGDRC aims at guaranteeing service quality, maximizing the\noverall throughput, and providing general applicability to NVIDIA GPUs. SGDRC\nfirst reveals a general VRAM channel hash mapping architecture of NVIDIA GPUs\nthrough comprehensive reverse engineering and eliminates VRAM channel conflicts\nusing software-level cache coloring. SGDRC applies bimodal tensors and tidal SM\nmasking to dynamically allocate VRAM bandwidth and compute units, and guides\nthe allocation of resources based on offline profiling. We evaluate 11\nmainstream DNNs with real-world workloads on two NVIDIA GPUs. The results show\nthat compared with the state-of-the-art GPU sharing solutions, SGDRC achieves\nthe highest SLO attainment rates (99.0% on average), and improves overall\nthroughput by up to 1.47x and BE job throughput by up to 2.36x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud service providers heavily colocate high-priority, latency-sensitive\n(LS), and low-priority, best-effort (BE) DNN inference services on the same GPU\nto improve resource utilization in data centers. Among the critical shared GPU\nresources, there has been very limited analysis on the dynamic allocation of\ncompute units and VRAM bandwidth, mainly for two reasons: (1) The native GPU\nresource management solutions are either hardware-specific, or unable to\ndynamically allocate resources to different tenants, or both; (2) NVIDIA\ndoesn't expose interfaces for VRAM bandwidth allocation, and the software stack\nand VRAM channel architectures are black-box, both of which limit the\nsoftware-level resource management. These drive prior work to design either\nconservative sharing policies detrimental to throughput, or static resource\npartitioning only applicable to a few GPU models.\n  To bridge this gap, this paper proposes SGDRC, a fully software-defined\ndynamic VRAM bandwidth and compute unit management solution for concurrent DNN\ninference services. SGDRC aims at guaranteeing service quality, maximizing the\noverall throughput, and providing general applicability to NVIDIA GPUs. SGDRC\nfirst reveals a general VRAM channel hash mapping architecture of NVIDIA GPUs\nthrough comprehensive reverse engineering and eliminates VRAM channel conflicts\nusing software-level cache coloring. SGDRC applies bimodal tensors and tidal SM\nmasking to dynamically allocate VRAM bandwidth and compute units, and guides\nthe allocation of resources based on offline profiling. We evaluate 11\nmainstream DNNs with real-world workloads on two NVIDIA GPUs. The results show\nthat compared with the state-of-the-art GPU sharing solutions, SGDRC achieves\nthe highest SLO attainment rates (99.0% on average), and improves overall\nthroughput by up to 1.47x and BE job throughput by up to 2.36x."
                },
                "authors": [
                    {
                        "name": "Yongkang Zhang"
                    },
                    {
                        "name": "Haoxuan Yu"
                    },
                    {
                        "name": "Chenxia Han"
                    },
                    {
                        "name": "Cheng Wang"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Yunzhe Li"
                    },
                    {
                        "name": "Zhifeng Jiang"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Xiaowen Chu"
                    },
                    {
                        "name": "Huaicheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Huaicheng Li"
                },
                "author": "Huaicheng Li",
                "arxiv_doi": "10.1145/3710848.3710863",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3710848.3710863",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.13996v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13996v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "15 pages, 19 figures",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.9; I.2.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20561v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20561v1",
                "updated": "2025-03-26T13:58:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    13,
                    58,
                    2,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T13:58:02Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    13,
                    58,
                    2,
                    2,
                    85,
                    0
                ],
                "title": "A Theoretical Framework for Prompt Engineering: Approximating Smooth\n  Functions with Transformer Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Theoretical Framework for Prompt Engineering: Approximating Smooth\n  Functions with Transformer Prompts"
                },
                "summary": "Prompt engineering has emerged as a powerful technique for guiding large\nlanguage models (LLMs) toward desired responses, significantly enhancing their\nperformance across diverse tasks. Beyond their role as static predictors, LLMs\nincreasingly function as intelligent agents, capable of reasoning,\ndecision-making, and adapting dynamically to complex environments. However, the\ntheoretical underpinnings of prompt engineering remain largely unexplored. In\nthis paper, we introduce a formal framework demonstrating that transformer\nmodels, when provided with carefully designed prompts, can act as a\nconfigurable computational system by emulating a ``virtual'' neural network\nduring inference. Specifically, input prompts effectively translate into the\ncorresponding network configuration, enabling LLMs to adjust their internal\ncomputations dynamically. Building on this construction, we establish an\napproximation theory for $\\beta$-times differentiable functions, proving that\ntransformers can approximate such functions with arbitrary precision when\nguided by appropriately structured prompts. Moreover, our framework provides\ntheoretical justification for several empirically successful prompt engineering\ntechniques, including the use of longer, structured prompts, filtering\nirrelevant information, enhancing prompt token diversity, and leveraging\nmulti-agent interactions. By framing LLMs as adaptable agents rather than\nstatic models, our findings underscore their potential for autonomous reasoning\nand problem-solving, paving the way for more robust and theoretically grounded\nadvancements in prompt engineering and AI agent design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt engineering has emerged as a powerful technique for guiding large\nlanguage models (LLMs) toward desired responses, significantly enhancing their\nperformance across diverse tasks. Beyond their role as static predictors, LLMs\nincreasingly function as intelligent agents, capable of reasoning,\ndecision-making, and adapting dynamically to complex environments. However, the\ntheoretical underpinnings of prompt engineering remain largely unexplored. In\nthis paper, we introduce a formal framework demonstrating that transformer\nmodels, when provided with carefully designed prompts, can act as a\nconfigurable computational system by emulating a ``virtual'' neural network\nduring inference. Specifically, input prompts effectively translate into the\ncorresponding network configuration, enabling LLMs to adjust their internal\ncomputations dynamically. Building on this construction, we establish an\napproximation theory for $\\beta$-times differentiable functions, proving that\ntransformers can approximate such functions with arbitrary precision when\nguided by appropriately structured prompts. Moreover, our framework provides\ntheoretical justification for several empirically successful prompt engineering\ntechniques, including the use of longer, structured prompts, filtering\nirrelevant information, enhancing prompt token diversity, and leveraging\nmulti-agent interactions. By framing LLMs as adaptable agents rather than\nstatic models, our findings underscore their potential for autonomous reasoning\nand problem-solving, paving the way for more robust and theoretically grounded\nadvancements in prompt engineering and AI agent design."
                },
                "authors": [
                    {
                        "name": "Ryumei Nakada"
                    },
                    {
                        "name": "Wenlong Ji"
                    },
                    {
                        "name": "Tianxi Cai"
                    },
                    {
                        "name": "James Zou"
                    },
                    {
                        "name": "Linjun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linjun Zhang"
                },
                "author": "Linjun Zhang",
                "arxiv_comment": "55 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20561v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20561v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20552v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20552v1",
                "updated": "2025-03-26T13:48:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    13,
                    48,
                    35,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T13:48:35Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    13,
                    48,
                    35,
                    2,
                    85,
                    0
                ],
                "title": "Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and\n  Throughput via Attention Disaggregation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and\n  Throughput via Attention Disaggregation"
                },
                "summary": "In large language model (LLM) serving systems, executing each request\nconsists of two phases: the compute-intensive prefill phase and the\nmemory-intensive decoding phase. To prevent performance interference between\nthe two phases, current LLM serving systems typically adopt prefill-decoding\ndisaggregation, where the two phases are split across separate machines.\nHowever, we observe this approach leads to significant resource\nunderutilization. Specifically, prefill instances that are compute-intensive\nsuffer from low memory utilization, while decoding instances that are\nmemory-intensive experience low compute utilization. To address this problem,\nthis paper proposes Adrenaline, an attention disaggregation and offloading\nmechanism designed to enhance resource utilization and performance in LLM\nserving systems. Adrenaline's key innovation lies in disaggregating part of the\nattention computation in the decoding phase and offloading them to prefill\ninstances. The memory-bound nature of decoding-phase attention computation\ninherently enables an effective offloading strategy, yielding two complementary\nadvantages: 1) improved memory capacity and bandwidth utilization in prefill\ninstances, and 2) increased decoding batch sizes that enhance compute\nutilization in decoding instances, collectively boosting overall system\nperformance. Adrenaline achieves these gains through three key techniques:\nlow-latency decoding synchronization, resource-efficient prefill colocation,\nand load-aware offloading scheduling. Experimental results show that Adrenaline\nachieves 2.28x higher memory capacity and 2.07x better memory bandwidth\nutilization in prefill instances, up to 1.67x improvements in compute\nutilization for decoding instances, and 1.68x higher overall inference\nthroughput compared to state-of-the-art systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In large language model (LLM) serving systems, executing each request\nconsists of two phases: the compute-intensive prefill phase and the\nmemory-intensive decoding phase. To prevent performance interference between\nthe two phases, current LLM serving systems typically adopt prefill-decoding\ndisaggregation, where the two phases are split across separate machines.\nHowever, we observe this approach leads to significant resource\nunderutilization. Specifically, prefill instances that are compute-intensive\nsuffer from low memory utilization, while decoding instances that are\nmemory-intensive experience low compute utilization. To address this problem,\nthis paper proposes Adrenaline, an attention disaggregation and offloading\nmechanism designed to enhance resource utilization and performance in LLM\nserving systems. Adrenaline's key innovation lies in disaggregating part of the\nattention computation in the decoding phase and offloading them to prefill\ninstances. The memory-bound nature of decoding-phase attention computation\ninherently enables an effective offloading strategy, yielding two complementary\nadvantages: 1) improved memory capacity and bandwidth utilization in prefill\ninstances, and 2) increased decoding batch sizes that enhance compute\nutilization in decoding instances, collectively boosting overall system\nperformance. Adrenaline achieves these gains through three key techniques:\nlow-latency decoding synchronization, resource-efficient prefill colocation,\nand load-aware offloading scheduling. Experimental results show that Adrenaline\nachieves 2.28x higher memory capacity and 2.07x better memory bandwidth\nutilization in prefill instances, up to 1.67x improvements in compute\nutilization for decoding instances, and 1.68x higher overall inference\nthroughput compared to state-of-the-art systems."
                },
                "authors": [
                    {
                        "name": "Yunkai Liang"
                    },
                    {
                        "name": "Zhangyu Chen"
                    },
                    {
                        "name": "Pengfei Zuo"
                    },
                    {
                        "name": "Zhi Zhou"
                    },
                    {
                        "name": "Xu Chen"
                    },
                    {
                        "name": "Zhou Yu"
                    }
                ],
                "author_detail": {
                    "name": "Zhou Yu"
                },
                "author": "Zhou Yu",
                "arxiv_comment": "14 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20552v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20552v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16422v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16422v2",
                "updated": "2025-03-26T13:39:24Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    13,
                    39,
                    24,
                    2,
                    85,
                    0
                ],
                "published": "2025-01-27T19:00:01Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    19,
                    0,
                    1,
                    0,
                    27,
                    0
                ],
                "title": "Gravitational wave inference of star cluster properties from\n  intermediate-mass black hole mergers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gravitational wave inference of star cluster properties from\n  intermediate-mass black hole mergers"
                },
                "summary": "Next-generation ground-based gravitational wave observatories will observe\nmergers of intermediate-mass black holes (IMBHs) out to high redshift. Such\nIMBHs can form through runaway tidal encounters in the cores of dense stellar\nclusters. In this paper, we ask if the gravitational wave observation of a\nsingle merger event between two IMBHs, occurring in the aftermath of the\ncoalescence of the clusters in which they formed, can be used to infer the\nproperties of their host clusters, such as mass, redshift, and half-mass\nradius. We implement an astrophysically motivated analytic model for cluster\nevolution and IMBH growth, and we perform IMBH binary parameter estimation\nusing a network of three next-generation detectors. We find that inferring the\nstructural properties of clusters in this way is challenging due to model\ndegeneracy. However, the posteriors on the cluster formation redshifts have\nrelatively narrow peaks, and it may still be possible to infer the cluster\nformation history by measuring a whole population of IMBH binary merger events.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next-generation ground-based gravitational wave observatories will observe\nmergers of intermediate-mass black holes (IMBHs) out to high redshift. Such\nIMBHs can form through runaway tidal encounters in the cores of dense stellar\nclusters. In this paper, we ask if the gravitational wave observation of a\nsingle merger event between two IMBHs, occurring in the aftermath of the\ncoalescence of the clusters in which they formed, can be used to infer the\nproperties of their host clusters, such as mass, redshift, and half-mass\nradius. We implement an astrophysically motivated analytic model for cluster\nevolution and IMBH growth, and we perform IMBH binary parameter estimation\nusing a network of three next-generation detectors. We find that inferring the\nstructural properties of clusters in this way is challenging due to model\ndegeneracy. However, the posteriors on the cluster formation redshifts have\nrelatively narrow peaks, and it may still be possible to infer the cluster\nformation history by measuring a whole population of IMBH binary merger events."
                },
                "authors": [
                    {
                        "name": "Konstantinos Kritos"
                    },
                    {
                        "name": "Luca Reali"
                    },
                    {
                        "name": "Ken K. Y. Ng"
                    },
                    {
                        "name": "Fabio Antonini"
                    },
                    {
                        "name": "Emanuele Berti"
                    }
                ],
                "author_detail": {
                    "name": "Emanuele Berti"
                },
                "author": "Emanuele Berti",
                "arxiv_doi": "10.1103/PhysRevD.111.063056",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevD.111.063056",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.16422v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16422v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "29 pages, 16 figures. Matches the published version",
                "arxiv_journal_ref": "Phys.Rev.D 111 (2025) 6, 063056",
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18227v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18227v3",
                "updated": "2025-03-26T13:38:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    13,
                    38,
                    40,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-23T22:06:07Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    22,
                    6,
                    7,
                    6,
                    82,
                    0
                ],
                "title": "PG-SAM: Prior-Guided SAM with Medical for Multi-organ Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PG-SAM: Prior-Guided SAM with Medical for Multi-organ Segmentation"
                },
                "summary": "Segment Anything Model (SAM) demonstrates powerful zero-shot capabilities;\nhowever, its accuracy and robustness significantly decrease when applied to\nmedical image segmentation. Existing methods address this issue through\nmodality fusion, integrating textual and image information to provide more\ndetailed priors. In this study, we argue that the granularity of text and the\ndomain gap affect the accuracy of the priors. Furthermore, the discrepancy\nbetween high-level abstract semantics and pixel-level boundary details in\nimages can introduce noise into the fusion process. To address this, we propose\nPrior-Guided SAM (PG-SAM), which employs a fine-grained modality prior aligner\nto leverage specialized medical knowledge for better modality alignment. The\ncore of our method lies in efficiently addressing the domain gap with\nfine-grained text from a medical LLM. Meanwhile, it also enhances the priors'\nquality after modality alignment, ensuring more accurate segmentation. In\naddition, our decoder enhances the model's expressive capabilities through\nmulti-level feature fusion and iterative mask optimizer operations, supporting\nunprompted learning. We also propose a unified pipeline that effectively\nsupplies high-quality semantic information to SAM. Extensive experiments on the\nSynapse dataset demonstrate that the proposed PG-SAM achieves state-of-the-art\nperformance. Our code is released at https://github.com/logan-0623/PG-SAM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Segment Anything Model (SAM) demonstrates powerful zero-shot capabilities;\nhowever, its accuracy and robustness significantly decrease when applied to\nmedical image segmentation. Existing methods address this issue through\nmodality fusion, integrating textual and image information to provide more\ndetailed priors. In this study, we argue that the granularity of text and the\ndomain gap affect the accuracy of the priors. Furthermore, the discrepancy\nbetween high-level abstract semantics and pixel-level boundary details in\nimages can introduce noise into the fusion process. To address this, we propose\nPrior-Guided SAM (PG-SAM), which employs a fine-grained modality prior aligner\nto leverage specialized medical knowledge for better modality alignment. The\ncore of our method lies in efficiently addressing the domain gap with\nfine-grained text from a medical LLM. Meanwhile, it also enhances the priors'\nquality after modality alignment, ensuring more accurate segmentation. In\naddition, our decoder enhances the model's expressive capabilities through\nmulti-level feature fusion and iterative mask optimizer operations, supporting\nunprompted learning. We also propose a unified pipeline that effectively\nsupplies high-quality semantic information to SAM. Extensive experiments on the\nSynapse dataset demonstrate that the proposed PG-SAM achieves state-of-the-art\nperformance. Our code is released at https://github.com/logan-0623/PG-SAM."
                },
                "authors": [
                    {
                        "name": "Yiheng Zhong"
                    },
                    {
                        "name": "Zihong Luo"
                    },
                    {
                        "name": "Chengzhi Liu"
                    },
                    {
                        "name": "Feilong Tang"
                    },
                    {
                        "name": "Zelin Peng"
                    },
                    {
                        "name": "Ming Hu"
                    },
                    {
                        "name": "Yingzhen Hu"
                    },
                    {
                        "name": "Jionglong Su"
                    },
                    {
                        "name": "Zongyuan Ge"
                    },
                    {
                        "name": "Imran Razzak"
                    }
                ],
                "author_detail": {
                    "name": "Imran Razzak"
                },
                "author": "Imran Razzak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18227v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18227v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20540v1",
                "updated": "2025-03-26T13:38:10Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    13,
                    38,
                    10,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T13:38:10Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    13,
                    38,
                    10,
                    2,
                    85,
                    0
                ],
                "title": "Beyond Intermediate States: Explaining Visual Redundancy through\n  Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Intermediate States: Explaining Visual Redundancy through\n  Language"
                },
                "summary": "Multi-modal Large Langue Models (MLLMs) often process thousands of visual\ntokens, which consume a significant portion of the context window and impose a\nsubstantial computational burden. Prior work has empirically explored visual\ntoken pruning methods based on MLLMs' intermediate states (e.g., attention\nscores). However, they have limitations in precisely defining visual redundancy\ndue to their inability to capture the influence of visual tokens on MLLMs'\nvisual understanding (i.e., the predicted probabilities for textual token\ncandidates). To address this issue, we manipulate the visual input and\ninvestigate variations in the textual output from both token-centric and\ncontext-centric perspectives, achieving intuitive and comprehensive analysis.\nExperimental results reveal that visual tokens with low ViT-[cls] association\nand low text-to-image attention scores can contain recognizable information and\nsignificantly contribute to images' overall information. To develop a more\nreliable method for identifying and pruning redundant visual tokens, we\nintegrate these two perspectives and introduce a context-independent condition\nto identify redundant prototypes from training images, which probes the\nredundancy of each visual token during inference. Extensive experiments on\nsingle-image, multi-image and video comprehension tasks demonstrate the\neffectiveness of our method, notably achieving 90% to 110% of the performance\nwhile pruning 80% to 90% of visual tokens.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-modal Large Langue Models (MLLMs) often process thousands of visual\ntokens, which consume a significant portion of the context window and impose a\nsubstantial computational burden. Prior work has empirically explored visual\ntoken pruning methods based on MLLMs' intermediate states (e.g., attention\nscores). However, they have limitations in precisely defining visual redundancy\ndue to their inability to capture the influence of visual tokens on MLLMs'\nvisual understanding (i.e., the predicted probabilities for textual token\ncandidates). To address this issue, we manipulate the visual input and\ninvestigate variations in the textual output from both token-centric and\ncontext-centric perspectives, achieving intuitive and comprehensive analysis.\nExperimental results reveal that visual tokens with low ViT-[cls] association\nand low text-to-image attention scores can contain recognizable information and\nsignificantly contribute to images' overall information. To develop a more\nreliable method for identifying and pruning redundant visual tokens, we\nintegrate these two perspectives and introduce a context-independent condition\nto identify redundant prototypes from training images, which probes the\nredundancy of each visual token during inference. Extensive experiments on\nsingle-image, multi-image and video comprehension tasks demonstrate the\neffectiveness of our method, notably achieving 90% to 110% of the performance\nwhile pruning 80% to 90% of visual tokens."
                },
                "authors": [
                    {
                        "name": "Dingchen Yang"
                    },
                    {
                        "name": "Bowen Cao"
                    },
                    {
                        "name": "Anran Zhang"
                    },
                    {
                        "name": "Weibo Gu"
                    },
                    {
                        "name": "Winston Hu"
                    },
                    {
                        "name": "Guang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guang Chen"
                },
                "author": "Guang Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20537v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20537v1",
                "updated": "2025-03-26T13:35:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    13,
                    35,
                    43,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T13:35:43Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    13,
                    35,
                    43,
                    2,
                    85,
                    0
                ],
                "title": "TD-BFR: Truncated Diffusion Model for Efficient Blind Face Restoration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TD-BFR: Truncated Diffusion Model for Efficient Blind Face Restoration"
                },
                "summary": "Diffusion-based methodologies have shown significant potential in blind face\nrestoration (BFR), leveraging their robust generative capabilities. However,\nthey are often criticized for two significant problems: 1) slow training and\ninference speed, and 2) inadequate recovery of fine-grained facial details. To\naddress these problems, we propose a novel Truncated Diffusion model for\nefficient Blind Face Restoration (TD-BFR), a three-stage paradigm tailored for\nthe progressive resolution of degraded images. Specifically, TD-BFR utilizes an\ninnovative truncated sampling method, starting from low-quality (LQ) images at\nlow resolution to enhance sampling speed, and then introduces an adaptive\ndegradation removal module to handle unknown degradations and connect the\ngeneration processes across different resolutions. Additionally, we further\nadapt the priors of pre-trained diffusion models to recover rich facial\ndetails. Our method efficiently restores high-quality images in a\ncoarse-to-fine manner and experimental results demonstrate that TD-BFR is, on\naverage, \\textbf{4.75$\\times$} faster than current state-of-the-art\ndiffusion-based BFR methods while maintaining competitive quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based methodologies have shown significant potential in blind face\nrestoration (BFR), leveraging their robust generative capabilities. However,\nthey are often criticized for two significant problems: 1) slow training and\ninference speed, and 2) inadequate recovery of fine-grained facial details. To\naddress these problems, we propose a novel Truncated Diffusion model for\nefficient Blind Face Restoration (TD-BFR), a three-stage paradigm tailored for\nthe progressive resolution of degraded images. Specifically, TD-BFR utilizes an\ninnovative truncated sampling method, starting from low-quality (LQ) images at\nlow resolution to enhance sampling speed, and then introduces an adaptive\ndegradation removal module to handle unknown degradations and connect the\ngeneration processes across different resolutions. Additionally, we further\nadapt the priors of pre-trained diffusion models to recover rich facial\ndetails. Our method efficiently restores high-quality images in a\ncoarse-to-fine manner and experimental results demonstrate that TD-BFR is, on\naverage, \\textbf{4.75$\\times$} faster than current state-of-the-art\ndiffusion-based BFR methods while maintaining competitive quality."
                },
                "authors": [
                    {
                        "name": "Ziying Zhang"
                    },
                    {
                        "name": "Xiang Gao"
                    },
                    {
                        "name": "Zhixin Wang"
                    },
                    {
                        "name": "Qiang hu"
                    },
                    {
                        "name": "Xiaoyun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoyun Zhang"
                },
                "author": "Xiaoyun Zhang",
                "arxiv_comment": "Accepted by ICME 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20537v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20537v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20536v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20536v1",
                "updated": "2025-03-26T13:35:10Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    13,
                    35,
                    10,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T13:35:10Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    13,
                    35,
                    10,
                    2,
                    85,
                    0
                ],
                "title": "Knowledge-Based Multi-Agent Framework for Automated Software\n  Architecture Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge-Based Multi-Agent Framework for Automated Software\n  Architecture Design"
                },
                "summary": "Architecture design is a critical step in software development. However,\ncreating a high-quality architecture is often costly due to the significant\nneed for human expertise and manual effort. Recently, agents built upon Large\nLanguage Models (LLMs) have achieved remarkable success in various software\nengineering tasks. Despite this progress, the use of agents to automate the\narchitecture design process remains largely unexplored. To address this gap, we\nenvision a Knowledge-based Multi-Agent Architecture Design (MAAD) framework.\nMAAD uses agents to simulate human roles in the traditional software\narchitecture design process, thereby automating the design process. To empower\nthese agents, MAAD incorporates knowledge extracted from three key sources: 1)\nexisting system designs, 2) authoritative literature, and 3) architecture\nexperts. By envisioning the MAAD framework, we aim to advance the full\nautomation of application-level system development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Architecture design is a critical step in software development. However,\ncreating a high-quality architecture is often costly due to the significant\nneed for human expertise and manual effort. Recently, agents built upon Large\nLanguage Models (LLMs) have achieved remarkable success in various software\nengineering tasks. Despite this progress, the use of agents to automate the\narchitecture design process remains largely unexplored. To address this gap, we\nenvision a Knowledge-based Multi-Agent Architecture Design (MAAD) framework.\nMAAD uses agents to simulate human roles in the traditional software\narchitecture design process, thereby automating the design process. To empower\nthese agents, MAAD incorporates knowledge extracted from three key sources: 1)\nexisting system designs, 2) authoritative literature, and 3) architecture\nexperts. By envisioning the MAAD framework, we aim to advance the full\nautomation of application-level system development."
                },
                "authors": [
                    {
                        "name": "Yiran Zhang"
                    },
                    {
                        "name": "Ruiyin Li"
                    },
                    {
                        "name": "Peng Liang"
                    },
                    {
                        "name": "Weisong Sun"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20536v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20536v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02550v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02550v3",
                "updated": "2025-03-26T13:27:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    13,
                    27,
                    14,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-04T12:21:28Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    12,
                    21,
                    28,
                    1,
                    63,
                    0
                ],
                "title": "SpecInF: Exploiting Idle GPU Resources in Distributed DL Training via\n  Speculative Inference Filling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecInF: Exploiting Idle GPU Resources in Distributed DL Training via\n  Speculative Inference Filling"
                },
                "summary": "Deep Learning (DL), especially with Large Language Models (LLMs), brings\nbenefits to various areas. However, DL training systems usually yield prominent\nidling GPU resources due to many factors, such as resource allocation and\ncollective communication. To improve GPU utilization, we present SpecInF, which\nadopts a Speculative Inference Filling method to exploit idle GPU resources. It\ncollocates each primary training instance with additional inference instances\non the same GPU, detects the training bubbles and adaptively fills with online\nor offline inference workloads. Our results show that SpecInF can effectively\nenhance GPU utilization under mainstream parallel training modes, delivering\nadditional up to 14$\\times$ offline inference throughputs than TGS and 67\\%\nreduction in online inference p95 latency than MPS, while guaranteeing\ncollocated training throughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning (DL), especially with Large Language Models (LLMs), brings\nbenefits to various areas. However, DL training systems usually yield prominent\nidling GPU resources due to many factors, such as resource allocation and\ncollective communication. To improve GPU utilization, we present SpecInF, which\nadopts a Speculative Inference Filling method to exploit idle GPU resources. It\ncollocates each primary training instance with additional inference instances\non the same GPU, detects the training bubbles and adaptively fills with online\nor offline inference workloads. Our results show that SpecInF can effectively\nenhance GPU utilization under mainstream parallel training modes, delivering\nadditional up to 14$\\times$ offline inference throughputs than TGS and 67\\%\nreduction in online inference p95 latency than MPS, while guaranteeing\ncollocated training throughput."
                },
                "authors": [
                    {
                        "name": "Cunchi Lv"
                    },
                    {
                        "name": "Xiao Shi"
                    },
                    {
                        "name": "Dong Liang"
                    },
                    {
                        "name": "Wenting Tan"
                    },
                    {
                        "name": "Xiaofang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Xiaofang Zhao"
                },
                "author": "Xiaofang Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02550v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02550v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10927v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10927v2",
                "updated": "2025-03-26T13:24:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    13,
                    24,
                    43,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-13T22:28:38Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    22,
                    28,
                    38,
                    3,
                    72,
                    0
                ],
                "title": "OASST-ETC Dataset: Alignment Signals from Eye-tracking Analysis of LLM\n  Responses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OASST-ETC Dataset: Alignment Signals from Eye-tracking Analysis of LLM\n  Responses"
                },
                "summary": "While Large Language Models (LLMs) have significantly advanced natural\nlanguage processing, aligning them with human preferences remains an open\nchallenge. Although current alignment methods rely primarily on explicit\nfeedback, eye-tracking (ET) data offers insights into real-time cognitive\nprocessing during reading. In this paper, we present OASST-ETC, a novel\neye-tracking corpus capturing reading patterns from 24 participants, while\nevaluating LLM-generated responses from the OASST1 dataset. Our analysis\nreveals distinct reading patterns between preferred and non-preferred\nresponses, which we compare with synthetic eye-tracking data. Furthermore, we\nexamine the correlation between human reading measures and attention patterns\nfrom various transformer-based models, discovering stronger correlations in\npreferred responses. This work introduces a unique resource for studying human\ncognitive processing in LLM evaluation and suggests promising directions for\nincorporating eye-tracking data into alignment methods. The dataset and\nanalysis code are publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large Language Models (LLMs) have significantly advanced natural\nlanguage processing, aligning them with human preferences remains an open\nchallenge. Although current alignment methods rely primarily on explicit\nfeedback, eye-tracking (ET) data offers insights into real-time cognitive\nprocessing during reading. In this paper, we present OASST-ETC, a novel\neye-tracking corpus capturing reading patterns from 24 participants, while\nevaluating LLM-generated responses from the OASST1 dataset. Our analysis\nreveals distinct reading patterns between preferred and non-preferred\nresponses, which we compare with synthetic eye-tracking data. Furthermore, we\nexamine the correlation between human reading measures and attention patterns\nfrom various transformer-based models, discovering stronger correlations in\npreferred responses. This work introduces a unique resource for studying human\ncognitive processing in LLM evaluation and suggests promising directions for\nincorporating eye-tracking data into alignment methods. The dataset and\nanalysis code are publicly available."
                },
                "authors": [
                    {
                        "name": "Angela Lopez-Cardona"
                    },
                    {
                        "name": "Sebastian Idesis"
                    },
                    {
                        "name": "Miguel Barreda-Ángeles"
                    },
                    {
                        "name": "Sergi Abadal"
                    },
                    {
                        "name": "Ioannis Arapakis"
                    }
                ],
                "author_detail": {
                    "name": "Ioannis Arapakis"
                },
                "author": "Ioannis Arapakis",
                "arxiv_doi": "10.1145/3725840",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3725840",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.10927v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10927v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This paper has been accepted to ACM ETRA 2025 and published on\n  PACMHCI",
                "arxiv_journal_ref": "Proceedings of the ACM on Human-Computer Interaction. 2025",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05167v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05167v2",
                "updated": "2025-03-26T13:23:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    13,
                    23,
                    30,
                    2,
                    85,
                    0
                ],
                "published": "2025-02-07T18:49:46Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    18,
                    49,
                    46,
                    4,
                    38,
                    0
                ],
                "title": "NoLiMa: Long-Context Evaluation Beyond Literal Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NoLiMa: Long-Context Evaluation Beyond Literal Matching"
                },
                "summary": "Recent large language models (LLMs) support long contexts ranging from 128K\nto 1M tokens. A popular method for evaluating these capabilities is the\nneedle-in-a-haystack (NIAH) test, which involves retrieving a \"needle\"\n(relevant information) from a \"haystack\" (long irrelevant context). Extensions\nof this approach include increasing distractors, fact chaining, and in-context\nreasoning. However, in these benchmarks, models can exploit existing literal\nmatches between the needle and haystack to simplify the task. To address this,\nwe introduce NoLiMa, a benchmark extending NIAH with a carefully designed\nneedle set, where questions and needles have minimal lexical overlap, requiring\nmodels to infer latent associations to locate the needle within the haystack.\nWe evaluate 12 popular LLMs that claim to support contexts of at least 128K\ntokens. While they perform well in short contexts (<1K), performance degrades\nsignificantly as context length increases. At 32K, for instance, 10 models drop\nbelow 50% of their strong short-length baselines. Even GPT-4o, one of the\ntop-performing exceptions, experiences a reduction from an almost-perfect\nbaseline of 99.3% to 69.7%. Our analysis suggests these declines stem from the\nincreased difficulty the attention mechanism faces in longer contexts when\nliteral matches are absent, making it harder to retrieve relevant information.\nWe publicly release the dataset and evaluation code at\nhttps://github.com/adobe-research/NoLiMa.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language models (LLMs) support long contexts ranging from 128K\nto 1M tokens. A popular method for evaluating these capabilities is the\nneedle-in-a-haystack (NIAH) test, which involves retrieving a \"needle\"\n(relevant information) from a \"haystack\" (long irrelevant context). Extensions\nof this approach include increasing distractors, fact chaining, and in-context\nreasoning. However, in these benchmarks, models can exploit existing literal\nmatches between the needle and haystack to simplify the task. To address this,\nwe introduce NoLiMa, a benchmark extending NIAH with a carefully designed\nneedle set, where questions and needles have minimal lexical overlap, requiring\nmodels to infer latent associations to locate the needle within the haystack.\nWe evaluate 12 popular LLMs that claim to support contexts of at least 128K\ntokens. While they perform well in short contexts (<1K), performance degrades\nsignificantly as context length increases. At 32K, for instance, 10 models drop\nbelow 50% of their strong short-length baselines. Even GPT-4o, one of the\ntop-performing exceptions, experiences a reduction from an almost-perfect\nbaseline of 99.3% to 69.7%. Our analysis suggests these declines stem from the\nincreased difficulty the attention mechanism faces in longer contexts when\nliteral matches are absent, making it harder to retrieve relevant information.\nWe publicly release the dataset and evaluation code at\nhttps://github.com/adobe-research/NoLiMa."
                },
                "authors": [
                    {
                        "name": "Ali Modarressi"
                    },
                    {
                        "name": "Hanieh Deilamsalehy"
                    },
                    {
                        "name": "Franck Dernoncourt"
                    },
                    {
                        "name": "Trung Bui"
                    },
                    {
                        "name": "Ryan A. Rossi"
                    },
                    {
                        "name": "Seunghyun Yoon"
                    },
                    {
                        "name": "Hinrich Schütze"
                    }
                ],
                "author_detail": {
                    "name": "Hinrich Schütze"
                },
                "author": "Hinrich Schütze",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05167v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05167v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.08351v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.08351v2",
                "updated": "2025-03-26T13:19:10Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    13,
                    19,
                    10,
                    2,
                    85,
                    0
                ],
                "published": "2024-01-16T13:30:37Z",
                "published_parsed": [
                    2024,
                    1,
                    16,
                    13,
                    30,
                    37,
                    1,
                    16,
                    0
                ],
                "title": "Personalized Federated Learning of Probabilistic Models: A PAC-Bayesian\n  Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized Federated Learning of Probabilistic Models: A PAC-Bayesian\n  Approach"
                },
                "summary": "Federated Learning (FL) aims to infer a shared model from private and\ndecentralized data stored by multiple clients. Personalized FL (PFL) enhances\nthe model's fit for each client by adapting the global model to the clients. A\nsignificant level of personalization is required for highly heterogeneous\nclients but can be challenging to achieve, especially when clients' datasets\nare small. To address this issue, we introduce the PAC-PFL framework for PFL of\nprobabilistic models. PAC-PFL infers a shared hyper-posterior and treats each\nclient's posterior inference as the personalization step. Unlike previous PFL\nalgorithms, PAC-PFL does not regularize all personalized models towards a\nsingle shared model, thereby greatly enhancing its personalization flexibility.\nBy establishing and minimizing a PAC-Bayesian generalization bound on the\naverage true loss of clients, PAC-PFL effectively mitigates overfitting even in\ndata-poor scenarios. Additionally, PAC-PFL provides generalization bounds for\nnew clients joining later. PAC-PFL achieves accurate and well-calibrated\npredictions, as supported by our experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) aims to infer a shared model from private and\ndecentralized data stored by multiple clients. Personalized FL (PFL) enhances\nthe model's fit for each client by adapting the global model to the clients. A\nsignificant level of personalization is required for highly heterogeneous\nclients but can be challenging to achieve, especially when clients' datasets\nare small. To address this issue, we introduce the PAC-PFL framework for PFL of\nprobabilistic models. PAC-PFL infers a shared hyper-posterior and treats each\nclient's posterior inference as the personalization step. Unlike previous PFL\nalgorithms, PAC-PFL does not regularize all personalized models towards a\nsingle shared model, thereby greatly enhancing its personalization flexibility.\nBy establishing and minimizing a PAC-Bayesian generalization bound on the\naverage true loss of clients, PAC-PFL effectively mitigates overfitting even in\ndata-poor scenarios. Additionally, PAC-PFL provides generalization bounds for\nnew clients joining later. PAC-PFL achieves accurate and well-calibrated\npredictions, as supported by our experiments."
                },
                "authors": [
                    {
                        "name": "Mahrokh Ghoddousi Boroujeni"
                    },
                    {
                        "name": "Andreas Krause"
                    },
                    {
                        "name": "Giancarlo Ferrari Trecate"
                    }
                ],
                "author_detail": {
                    "name": "Giancarlo Ferrari Trecate"
                },
                "author": "Giancarlo Ferrari Trecate",
                "arxiv_journal_ref": "Boroujeni, M. G., Krause, A., & Ferrari-Trecate, G. (2025).\n  Personalized Federated Learning of Probabilistic Models: A PAC-Bayesian\n  Approach. Transactions on Machine Learning Research",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.08351v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.08351v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.03606v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.03606v2",
                "updated": "2025-03-26T13:15:09Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    13,
                    15,
                    9,
                    2,
                    85,
                    0
                ],
                "published": "2024-05-06T16:17:48Z",
                "published_parsed": [
                    2024,
                    5,
                    6,
                    16,
                    17,
                    48,
                    0,
                    127,
                    0
                ],
                "title": "Strang Splitting for Parametric Inference in Second-order Stochastic\n  Differential Equations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Strang Splitting for Parametric Inference in Second-order Stochastic\n  Differential Equations"
                },
                "summary": "We address parameter estimation in second-order stochastic differential\nequations (SDEs), which are prevalent in physics, biology, and ecology. The\nsecond-order SDE is converted to a first-order system by introducing an\nauxiliary velocity variable, which raises two main challenges. First, the\nsystem is hypoelliptic since the noise affects only the velocity, making the\nEuler-Maruyama estimator ill-conditioned. We propose an estimator based on the\nStrang splitting scheme to overcome this. Second, since the velocity is rarely\nobserved, we adapt the estimator to partial observations. We present four\nestimators for complete and partial observations, using the full\npseudo-likelihood or only the velocity-based partial pseudo-likelihood. These\nestimators are intuitive, easy to implement, and computationally fast, and we\nprove their consistency and asymptotic normality. Our analysis demonstrates\nthat using the full pseudo-likelihood with complete observations reduces the\nasymptotic variance of the diffusion estimator. With partial observations, the\nasymptotic variance increases as a result of information loss but remains\nunaffected by the likelihood choice. However, a numerical study on the Kramers\noscillator reveals that using the partial pseudo-likelihood for partial\nobservations yields less biased estimators. We apply our approach to\npaleoclimate data from the Greenland ice core by fitting the Kramers oscillator\nmodel, capturing transitions between metastable states reflecting observed\nclimatic conditions during glacial eras.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address parameter estimation in second-order stochastic differential\nequations (SDEs), which are prevalent in physics, biology, and ecology. The\nsecond-order SDE is converted to a first-order system by introducing an\nauxiliary velocity variable, which raises two main challenges. First, the\nsystem is hypoelliptic since the noise affects only the velocity, making the\nEuler-Maruyama estimator ill-conditioned. We propose an estimator based on the\nStrang splitting scheme to overcome this. Second, since the velocity is rarely\nobserved, we adapt the estimator to partial observations. We present four\nestimators for complete and partial observations, using the full\npseudo-likelihood or only the velocity-based partial pseudo-likelihood. These\nestimators are intuitive, easy to implement, and computationally fast, and we\nprove their consistency and asymptotic normality. Our analysis demonstrates\nthat using the full pseudo-likelihood with complete observations reduces the\nasymptotic variance of the diffusion estimator. With partial observations, the\nasymptotic variance increases as a result of information loss but remains\nunaffected by the likelihood choice. However, a numerical study on the Kramers\noscillator reveals that using the partial pseudo-likelihood for partial\nobservations yields less biased estimators. We apply our approach to\npaleoclimate data from the Greenland ice core by fitting the Kramers oscillator\nmodel, capturing transitions between metastable states reflecting observed\nclimatic conditions during glacial eras."
                },
                "authors": [
                    {
                        "name": "Predrag Pilipovic"
                    },
                    {
                        "name": "Adeline Samson"
                    },
                    {
                        "name": "Susanne Ditlevsen"
                    }
                ],
                "author_detail": {
                    "name": "Susanne Ditlevsen"
                },
                "author": "Susanne Ditlevsen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.03606v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.03606v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20528v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20528v1",
                "updated": "2025-03-26T13:13:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    13,
                    13,
                    22,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T13:13:22Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    13,
                    13,
                    22,
                    2,
                    85,
                    0
                ],
                "title": "Interpretable Deep Neural Network for Modeling Functional Surrogates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpretable Deep Neural Network for Modeling Functional Surrogates"
                },
                "summary": "Developing surrogates for computer models has become increasingly important\nfor addressing complex problems in science and engineering. This article\nintroduces an artificial intelligent (AI) surrogate, referred to as the\nDeepSurrogate, for analyzing functional outputs with vector-valued inputs. The\nrelationship between the functional output and vector-valued input is modeled\nas an infinite sequence of unknown functions, each representing the\nrelationship at a specific location within the functional domain. These\nspatially indexed functions are expressed through a combination of basis\nfunctions and their corresponding coefficient functions, both of which are\nmodeled using deep neural networks (DNN). The proposed framework accounts for\nspatial dependencies across locations, while capturing the relationship between\nthe functional output and scalar predictors. It also integrates a Monte Carlo\n(MC) dropout strategy to quantify prediction uncertainty, enhancing\nexplainability in the deep neural network architecture. The proposed method\nenables efficient inference on datasets with approximately 50,000 spatial\nlocations and 20 simulations, achieving results in under 10 minutes using\nstandard hardware. The approach is validated on extensive synthetic datasets\nand a large-scale simulation from the Sea Lake and Overland Surge from\nHurricanes (SLOSH) simulator. An open-source Python package implementing the\nmethod is made available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing surrogates for computer models has become increasingly important\nfor addressing complex problems in science and engineering. This article\nintroduces an artificial intelligent (AI) surrogate, referred to as the\nDeepSurrogate, for analyzing functional outputs with vector-valued inputs. The\nrelationship between the functional output and vector-valued input is modeled\nas an infinite sequence of unknown functions, each representing the\nrelationship at a specific location within the functional domain. These\nspatially indexed functions are expressed through a combination of basis\nfunctions and their corresponding coefficient functions, both of which are\nmodeled using deep neural networks (DNN). The proposed framework accounts for\nspatial dependencies across locations, while capturing the relationship between\nthe functional output and scalar predictors. It also integrates a Monte Carlo\n(MC) dropout strategy to quantify prediction uncertainty, enhancing\nexplainability in the deep neural network architecture. The proposed method\nenables efficient inference on datasets with approximately 50,000 spatial\nlocations and 20 simulations, achieving results in under 10 minutes using\nstandard hardware. The approach is validated on extensive synthetic datasets\nand a large-scale simulation from the Sea Lake and Overland Surge from\nHurricanes (SLOSH) simulator. An open-source Python package implementing the\nmethod is made available."
                },
                "authors": [
                    {
                        "name": "Yeseul Jeon"
                    },
                    {
                        "name": "Rajarshi Guhaniyogi"
                    },
                    {
                        "name": "Aaron Scheffler"
                    },
                    {
                        "name": "Devin Francom"
                    },
                    {
                        "name": "Donatella Pasqualini"
                    }
                ],
                "author_detail": {
                    "name": "Donatella Pasqualini"
                },
                "author": "Donatella Pasqualini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20528v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20528v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20527v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20527v1",
                "updated": "2025-03-26T13:13:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    13,
                    13,
                    3,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T13:13:03Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    13,
                    13,
                    3,
                    2,
                    85,
                    0
                ],
                "title": "StableToolBench-MirrorAPI: Modeling Tool Environments as Mirrors of\n  7,000+ Real-World APIs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StableToolBench-MirrorAPI: Modeling Tool Environments as Mirrors of\n  7,000+ Real-World APIs"
                },
                "summary": "The rapid advancement of large language models (LLMs) has spurred significant\ninterest in tool learning, where LLMs are augmented with external tools to\ntackle complex tasks. However, existing tool environments face challenges in\nbalancing stability, scalability, and realness, particularly for benchmarking\npurposes. To address this problem, we propose MirrorAPI, a novel framework that\ntrains specialized LLMs to accurately simulate real API responses, effectively\nacting as \"mirrors\" to tool environments. Using a comprehensive dataset of\nrequest-response pairs from 7,000+ APIs, we employ supervised fine-tuning and\nchain-of-thought reasoning to enhance simulation fidelity. MirrorAPI achieves\nsuperior accuracy and stability compared to state-of-the-art methods, as\ndemonstrated by its performance on the newly constructed MirrorAPI-Bench and\nits integration into StableToolBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) has spurred significant\ninterest in tool learning, where LLMs are augmented with external tools to\ntackle complex tasks. However, existing tool environments face challenges in\nbalancing stability, scalability, and realness, particularly for benchmarking\npurposes. To address this problem, we propose MirrorAPI, a novel framework that\ntrains specialized LLMs to accurately simulate real API responses, effectively\nacting as \"mirrors\" to tool environments. Using a comprehensive dataset of\nrequest-response pairs from 7,000+ APIs, we employ supervised fine-tuning and\nchain-of-thought reasoning to enhance simulation fidelity. MirrorAPI achieves\nsuperior accuracy and stability compared to state-of-the-art methods, as\ndemonstrated by its performance on the newly constructed MirrorAPI-Bench and\nits integration into StableToolBench."
                },
                "authors": [
                    {
                        "name": "Zhicheng Guo"
                    },
                    {
                        "name": "Sijie Cheng"
                    },
                    {
                        "name": "Yuchen Niu"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Sicheng Zhou"
                    },
                    {
                        "name": "Wenbing Huang"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20527v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20527v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15133v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15133v2",
                "updated": "2025-03-26T13:08:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    13,
                    8,
                    41,
                    2,
                    85,
                    0
                ],
                "published": "2024-09-23T15:38:12Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    15,
                    38,
                    12,
                    0,
                    267,
                    0
                ],
                "title": "Don't Use LLMs to Make Relevance Judgments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Don't Use LLMs to Make Relevance Judgments"
                },
                "summary": "Making the relevance judgments for a TREC-style test collection can be\ncomplex and expensive. A typical TREC track usually involves a team of six\ncontractors working for 2-4 weeks. Those contractors need to be trained and\nmonitored. Software has to be written to support recording relevance judgments\ncorrectly and efficiently. The recent advent of large language models that\nproduce astoundingly human-like flowing text output in response to a natural\nlanguage prompt has inspired IR researchers to wonder how those models might be\nused in the relevance judgment collection process. At the ACM SIGIR 2024\nconference, a workshop ``LLM4Eval'' provided a venue for this work, and\nfeatured a data challenge activity where participants reproduced TREC deep\nlearning track judgments, as was done by Thomas et al (arXiv:2408.08896,\narXiv:2309.10621). I was asked to give a keynote at the workshop, and this\npaper presents that keynote in article form. The bottom-line-up-front message\nis, don't use LLMs to create relevance judgments for TREC-style evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Making the relevance judgments for a TREC-style test collection can be\ncomplex and expensive. A typical TREC track usually involves a team of six\ncontractors working for 2-4 weeks. Those contractors need to be trained and\nmonitored. Software has to be written to support recording relevance judgments\ncorrectly and efficiently. The recent advent of large language models that\nproduce astoundingly human-like flowing text output in response to a natural\nlanguage prompt has inspired IR researchers to wonder how those models might be\nused in the relevance judgment collection process. At the ACM SIGIR 2024\nconference, a workshop ``LLM4Eval'' provided a venue for this work, and\nfeatured a data challenge activity where participants reproduced TREC deep\nlearning track judgments, as was done by Thomas et al (arXiv:2408.08896,\narXiv:2309.10621). I was asked to give a keynote at the workshop, and this\npaper presents that keynote in article form. The bottom-line-up-front message\nis, don't use LLMs to create relevance judgments for TREC-style evaluations."
                },
                "authors": [
                    {
                        "name": "Ian Soboroff"
                    }
                ],
                "author_detail": {
                    "name": "Ian Soboroff"
                },
                "author": "Ian Soboroff",
                "arxiv_doi": "10.54195/irrj.19625",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.54195/irrj.19625",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.15133v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15133v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Information Retrieval Research. 1, 1 (Mar. 2025), 29-46",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20519v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20519v1",
                "updated": "2025-03-26T13:00:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    13,
                    0,
                    51,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T13:00:51Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    13,
                    0,
                    51,
                    2,
                    85,
                    0
                ],
                "title": "MAR-3D: Progressive Masked Auto-regressor for High-Resolution 3D\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAR-3D: Progressive Masked Auto-regressor for High-Resolution 3D\n  Generation"
                },
                "summary": "Recent advances in auto-regressive transformers have revolutionized\ngenerative modeling across different domains, from language processing to\nvisual generation, demonstrating remarkable capabilities. However, applying\nthese advances to 3D generation presents three key challenges: the unordered\nnature of 3D data conflicts with sequential next-token prediction paradigm,\nconventional vector quantization approaches incur substantial compression loss\nwhen applied to 3D meshes, and the lack of efficient scaling strategies for\nhigher resolution latent prediction. To address these challenges, we introduce\nMAR-3D, which integrates a pyramid variational autoencoder with a cascaded\nmasked auto-regressive transformer (Cascaded MAR) for progressive latent\nupscaling in the continuous space. Our architecture employs random masking\nduring training and auto-regressive denoising in random order during inference,\nnaturally accommodating the unordered property of 3D latent tokens.\nAdditionally, we propose a cascaded training strategy with condition\naugmentation that enables efficiently up-scale the latent token resolution with\nfast convergence. Extensive experiments demonstrate that MAR-3D not only\nachieves superior performance and generalization capabilities compared to\nexisting methods but also exhibits enhanced scaling capabilities compared to\njoint distribution modeling approaches (e.g., diffusion transformers).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in auto-regressive transformers have revolutionized\ngenerative modeling across different domains, from language processing to\nvisual generation, demonstrating remarkable capabilities. However, applying\nthese advances to 3D generation presents three key challenges: the unordered\nnature of 3D data conflicts with sequential next-token prediction paradigm,\nconventional vector quantization approaches incur substantial compression loss\nwhen applied to 3D meshes, and the lack of efficient scaling strategies for\nhigher resolution latent prediction. To address these challenges, we introduce\nMAR-3D, which integrates a pyramid variational autoencoder with a cascaded\nmasked auto-regressive transformer (Cascaded MAR) for progressive latent\nupscaling in the continuous space. Our architecture employs random masking\nduring training and auto-regressive denoising in random order during inference,\nnaturally accommodating the unordered property of 3D latent tokens.\nAdditionally, we propose a cascaded training strategy with condition\naugmentation that enables efficiently up-scale the latent token resolution with\nfast convergence. Extensive experiments demonstrate that MAR-3D not only\nachieves superior performance and generalization capabilities compared to\nexisting methods but also exhibits enhanced scaling capabilities compared to\njoint distribution modeling approaches (e.g., diffusion transformers)."
                },
                "authors": [
                    {
                        "name": "Jinnan Chen"
                    },
                    {
                        "name": "Lingting Zhu"
                    },
                    {
                        "name": "Zeyu Hu"
                    },
                    {
                        "name": "Shengju Qian"
                    },
                    {
                        "name": "Yugang Chen"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Gim Hee Lee"
                    }
                ],
                "author_detail": {
                    "name": "Gim Hee Lee"
                },
                "author": "Gim Hee Lee",
                "arxiv_comment": "Aceepted to CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20519v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20519v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20518v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20518v1",
                "updated": "2025-03-26T13:00:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    13,
                    0,
                    5,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T13:00:05Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    13,
                    0,
                    5,
                    2,
                    85,
                    0
                ],
                "title": "Exploring the Effect of Robotic Embodiment and Empathetic Tone of LLMs\n  on Empathy Elicitation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Effect of Robotic Embodiment and Empathetic Tone of LLMs\n  on Empathy Elicitation"
                },
                "summary": "This study investigates the elicitation of empathy toward a third party\nthrough interaction with social agents. Participants engaged with either a\nphysical robot or a voice-enabled chatbot, both driven by a large language\nmodel (LLM) programmed to exhibit either an empathetic tone or remain neutral.\nThe interaction is focused on a fictional character, Katie Banks, who is in a\nchallenging situation and in need of financial donations. The willingness to\nhelp Katie, measured by the number of hours participants were willing to\nvolunteer, along with their perceptions of the agent, were assessed for 60\nparticipants. Results indicate that neither robotic embodiment nor empathetic\ntone significantly influenced participants' willingness to volunteer. While the\nLLM effectively simulated human empathy, fostering genuine empathetic responses\nin participants proved challenging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study investigates the elicitation of empathy toward a third party\nthrough interaction with social agents. Participants engaged with either a\nphysical robot or a voice-enabled chatbot, both driven by a large language\nmodel (LLM) programmed to exhibit either an empathetic tone or remain neutral.\nThe interaction is focused on a fictional character, Katie Banks, who is in a\nchallenging situation and in need of financial donations. The willingness to\nhelp Katie, measured by the number of hours participants were willing to\nvolunteer, along with their perceptions of the agent, were assessed for 60\nparticipants. Results indicate that neither robotic embodiment nor empathetic\ntone significantly influenced participants' willingness to volunteer. While the\nLLM effectively simulated human empathy, fostering genuine empathetic responses\nin participants proved challenging."
                },
                "authors": [
                    {
                        "name": "Liza Darwesh"
                    },
                    {
                        "name": "Jaspreet Singh"
                    },
                    {
                        "name": "Marin Marian"
                    },
                    {
                        "name": "Eduard Alexa"
                    },
                    {
                        "name": "Koen Hindriks"
                    },
                    {
                        "name": "Kim Baraka"
                    }
                ],
                "author_detail": {
                    "name": "Kim Baraka"
                },
                "author": "Kim Baraka",
                "arxiv_doi": "10.1007/978-981-96-3525-2_1",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-981-96-3525-2_1",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.20518v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20518v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "*Liza Darwesh, Jaspreet Singh, Marin Marian, and Eduard Alexa\n  contributed equally to this work.*",
                "arxiv_journal_ref": "Proceedings of the International Conference on Social Robotics\n  (ICSR 2024), Springer, 2025, pp. 1-11",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.9, I.2.7, H.5.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20511v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20511v1",
                "updated": "2025-03-26T12:53:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    12,
                    53,
                    31,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T12:53:31Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    12,
                    53,
                    31,
                    2,
                    85,
                    0
                ],
                "title": "From reductionism to realism: Holistic mathematical modelling for\n  complex biological systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From reductionism to realism: Holistic mathematical modelling for\n  complex biological systems"
                },
                "summary": "At its core, the physics paradigm adopts a reductionist approach to\nmodelling, aiming to understand fundamental phenomena by decomposing them into\nsimpler, elementary processes. While this strategy has been tremendously\nsuccessful in physics and is typically considered the pinnacle of scientific\nformulation, it has often fallen short in addressing fundamental questions in\nthe biological sciences. This limitation arises from the inherent complexity of\nbiological systems, characterised by heterogeneity, poly-functionality and\ninteractions across multiple spatial and temporal scales. Nevertheless, the\ntraditional framework of complex systems modelling also falls short, as its\nemphasis on broad theoretical principles has often failed to produce realistic,\npredictive, empirically grounded insights. To advance towards the goal of\nactionable mathematical models in biology, we argue here, using neuroscience as\na case study, that it is necessary to move beyond simple reductionist\napproaches and instead embrace the intrinsic complexity and heterogeneity of\nbiological systems-leveraging the growing availability of high-resolution data\nand recent advances in high-performance computing. In particular, we advocate\nfor a holistic mathematical modelling paradigm that harnesses rich\nrepresentational structures such as annotated and multilayer networks, employs\nagent-based models and simulation-based approaches, and focuses on the inverse\nproblem of inferring system properties from dynamical observations. Finally, we\nemphasise that this approach is fully compatible with the search for\nfundamental principles in biophysics, and we highlight the substantial\npotential it holds to drive progress in mathematical biology over the next two\ndecades.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "At its core, the physics paradigm adopts a reductionist approach to\nmodelling, aiming to understand fundamental phenomena by decomposing them into\nsimpler, elementary processes. While this strategy has been tremendously\nsuccessful in physics and is typically considered the pinnacle of scientific\nformulation, it has often fallen short in addressing fundamental questions in\nthe biological sciences. This limitation arises from the inherent complexity of\nbiological systems, characterised by heterogeneity, poly-functionality and\ninteractions across multiple spatial and temporal scales. Nevertheless, the\ntraditional framework of complex systems modelling also falls short, as its\nemphasis on broad theoretical principles has often failed to produce realistic,\npredictive, empirically grounded insights. To advance towards the goal of\nactionable mathematical models in biology, we argue here, using neuroscience as\na case study, that it is necessary to move beyond simple reductionist\napproaches and instead embrace the intrinsic complexity and heterogeneity of\nbiological systems-leveraging the growing availability of high-resolution data\nand recent advances in high-performance computing. In particular, we advocate\nfor a holistic mathematical modelling paradigm that harnesses rich\nrepresentational structures such as annotated and multilayer networks, employs\nagent-based models and simulation-based approaches, and focuses on the inverse\nproblem of inferring system properties from dynamical observations. Finally, we\nemphasise that this approach is fully compatible with the search for\nfundamental principles in biophysics, and we highlight the substantial\npotential it holds to drive progress in mathematical biology over the next two\ndecades."
                },
                "authors": [
                    {
                        "name": "Ramón Nartallo-Kaluarachchi"
                    },
                    {
                        "name": "Renaud Lambiotte"
                    },
                    {
                        "name": "Alain Goriely"
                    }
                ],
                "author_detail": {
                    "name": "Alain Goriely"
                },
                "author": "Alain Goriely",
                "arxiv_comment": "12 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20511v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20511v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.bio-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20508v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20508v1",
                "updated": "2025-03-26T12:49:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    12,
                    49,
                    35,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T12:49:35Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    12,
                    49,
                    35,
                    2,
                    85,
                    0
                ],
                "title": "Explainable ICD Coding via Entity Linking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explainable ICD Coding via Entity Linking"
                },
                "summary": "Clinical coding is a critical task in healthcare, although traditional\nmethods for automating clinical coding may not provide sufficient explicit\nevidence for coders in production environments. This evidence is crucial, as\nmedical coders have to make sure there exists at least one explicit passage in\nthe input health record that justifies the attribution of a code. We therefore\npropose to reframe the task as an entity linking problem, in which each\ndocument is annotated with its set of codes and respective textual evidence,\nenabling better human-machine collaboration. By leveraging parameter-efficient\nfine-tuning of Large Language Models (LLMs), together with constrained\ndecoding, we introduce three approaches to solve this problem that prove\neffective at disambiguating clinical mentions and that perform well in few-shot\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clinical coding is a critical task in healthcare, although traditional\nmethods for automating clinical coding may not provide sufficient explicit\nevidence for coders in production environments. This evidence is crucial, as\nmedical coders have to make sure there exists at least one explicit passage in\nthe input health record that justifies the attribution of a code. We therefore\npropose to reframe the task as an entity linking problem, in which each\ndocument is annotated with its set of codes and respective textual evidence,\nenabling better human-machine collaboration. By leveraging parameter-efficient\nfine-tuning of Large Language Models (LLMs), together with constrained\ndecoding, we introduce three approaches to solve this problem that prove\neffective at disambiguating clinical mentions and that perform well in few-shot\nscenarios."
                },
                "authors": [
                    {
                        "name": "Leonor Barreiros"
                    },
                    {
                        "name": "Isabel Coutinho"
                    },
                    {
                        "name": "Gonçalo M. Correia"
                    },
                    {
                        "name": "Bruno Martins"
                    }
                ],
                "author_detail": {
                    "name": "Bruno Martins"
                },
                "author": "Bruno Martins",
                "arxiv_comment": "Accepted at CL4Health at NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20508v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20508v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20507v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20507v1",
                "updated": "2025-03-26T12:47:52Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    12,
                    47,
                    52,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T12:47:52Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    12,
                    47,
                    52,
                    2,
                    85,
                    0
                ],
                "title": "Harmonia: A Multi-Agent Reinforcement Learning Approach to Data\n  Placement and Migration in Hybrid Storage Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harmonia: A Multi-Agent Reinforcement Learning Approach to Data\n  Placement and Migration in Hybrid Storage Systems"
                },
                "summary": "Hybrid storage systems (HSS) combine multiple storage devices with diverse\ncharacteristics to achieve high performance and capacity at low cost. The\nperformance of an HSS highly depends on the effectiveness of two key policies:\n(1) the data-placement policy, which determines the best-fit storage device for\nincoming data, and (2) the data-migration policy, which rearranges stored data\nacross the devices to sustain high HSS performance. Prior works focus on\nimproving only data placement or only data migration in HSS, which leads to\nsub-optimal HSS performance. Unfortunately, no prior work tries to optimize\nboth policies together. Our goal is to design a holistic data-management\ntechnique for HSS that optimizes both data-placement and data-migration\npolicies to fully exploit the potential of an HSS. We propose Harmonia, a\nmulti-agent reinforcement learning (RL)-based data-management technique that\nemploys two light-weight autonomous RL agents, a data-placement agent and a\ndata-migration agent, which adapt their policies for the current workload and\nHSS configuration, and coordinate with each other to improve overall HSS\nperformance. We evaluate Harmonia on a real HSS with up to four heterogeneous\nstorage devices with diverse characteristics. Our evaluation using 17\ndata-intensive workloads on performance-optimized (cost-optimized) HSS with two\nstorage devices shows that, on average, Harmonia (1) outperforms the\nbest-performing prior approach by 49.5% (31.7%), (2) bridges the performance\ngap between the best-performing prior work and Oracle by 64.2% (64.3%). On an\nHSS with three (four) devices, Harmonia outperforms the best-performing prior\nwork by 37.0% (42.0%). Harmonia's performance benefits come with low latency\n(240ns for inference) and storage overheads (206 KiB for both RL agents\ntogether). We plan to open-source Harmonia's implementation to aid future\nresearch on HSS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid storage systems (HSS) combine multiple storage devices with diverse\ncharacteristics to achieve high performance and capacity at low cost. The\nperformance of an HSS highly depends on the effectiveness of two key policies:\n(1) the data-placement policy, which determines the best-fit storage device for\nincoming data, and (2) the data-migration policy, which rearranges stored data\nacross the devices to sustain high HSS performance. Prior works focus on\nimproving only data placement or only data migration in HSS, which leads to\nsub-optimal HSS performance. Unfortunately, no prior work tries to optimize\nboth policies together. Our goal is to design a holistic data-management\ntechnique for HSS that optimizes both data-placement and data-migration\npolicies to fully exploit the potential of an HSS. We propose Harmonia, a\nmulti-agent reinforcement learning (RL)-based data-management technique that\nemploys two light-weight autonomous RL agents, a data-placement agent and a\ndata-migration agent, which adapt their policies for the current workload and\nHSS configuration, and coordinate with each other to improve overall HSS\nperformance. We evaluate Harmonia on a real HSS with up to four heterogeneous\nstorage devices with diverse characteristics. Our evaluation using 17\ndata-intensive workloads on performance-optimized (cost-optimized) HSS with two\nstorage devices shows that, on average, Harmonia (1) outperforms the\nbest-performing prior approach by 49.5% (31.7%), (2) bridges the performance\ngap between the best-performing prior work and Oracle by 64.2% (64.3%). On an\nHSS with three (four) devices, Harmonia outperforms the best-performing prior\nwork by 37.0% (42.0%). Harmonia's performance benefits come with low latency\n(240ns for inference) and storage overheads (206 KiB for both RL agents\ntogether). We plan to open-source Harmonia's implementation to aid future\nresearch on HSS."
                },
                "authors": [
                    {
                        "name": "Rakesh Nadig"
                    },
                    {
                        "name": "Vamanan Arulchelvan"
                    },
                    {
                        "name": "Rahul Bera"
                    },
                    {
                        "name": "Taha Shahroodi"
                    },
                    {
                        "name": "Gagandeep Singh"
                    },
                    {
                        "name": "Mohammad Sadrosadati"
                    },
                    {
                        "name": "Jisung Park"
                    },
                    {
                        "name": "Onur Mutlu"
                    }
                ],
                "author_detail": {
                    "name": "Onur Mutlu"
                },
                "author": "Onur Mutlu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20507v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20507v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20504v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20504v1",
                "updated": "2025-03-26T12:45:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    12,
                    45,
                    34,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T12:45:34Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    12,
                    45,
                    34,
                    2,
                    85,
                    0
                ],
                "title": "Vision-Amplified Semantic Entropy for Hallucination Detection in Medical\n  Visual Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Amplified Semantic Entropy for Hallucination Detection in Medical\n  Visual Question Answering"
                },
                "summary": "Multimodal large language models (MLLMs) have demonstrated significant\npotential in medical Visual Question Answering (VQA). Yet, they remain prone to\nhallucinations-incorrect responses that contradict input images, posing\nsubstantial risks in clinical decision-making. Detecting these hallucinations\nis essential for establishing trust in MLLMs among clinicians and patients,\nthereby enabling their real-world adoption. Current hallucination detection\nmethods, especially semantic entropy (SE), have demonstrated promising\nhallucination detection capacity for LLMs. However, adapting SE to medical\nMLLMs by incorporating visual perturbations presents a dilemma. Weak\nperturbations preserve image content and ensure clinical validity, but may be\noverlooked by medical MLLMs, which tend to over rely on language priors. In\ncontrast, strong perturbations can distort essential diagnostic features,\ncompromising clinical interpretation. To address this issue, we propose Vision\nAmplified Semantic Entropy (VASE), which incorporates weak image\ntransformations and amplifies the impact of visual input, to improve\nhallucination detection in medical VQA. We first estimate the semantic\npredictive distribution under weak visual transformations to preserve clinical\nvalidity, and then amplify visual influence by contrasting this distribution\nwith that derived from a distorted image. The entropy of the resulting\ndistribution is estimated as VASE. Experiments on two medical open-ended VQA\ndatasets demonstrate that VASE consistently outperforms existing hallucination\ndetection methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) have demonstrated significant\npotential in medical Visual Question Answering (VQA). Yet, they remain prone to\nhallucinations-incorrect responses that contradict input images, posing\nsubstantial risks in clinical decision-making. Detecting these hallucinations\nis essential for establishing trust in MLLMs among clinicians and patients,\nthereby enabling their real-world adoption. Current hallucination detection\nmethods, especially semantic entropy (SE), have demonstrated promising\nhallucination detection capacity for LLMs. However, adapting SE to medical\nMLLMs by incorporating visual perturbations presents a dilemma. Weak\nperturbations preserve image content and ensure clinical validity, but may be\noverlooked by medical MLLMs, which tend to over rely on language priors. In\ncontrast, strong perturbations can distort essential diagnostic features,\ncompromising clinical interpretation. To address this issue, we propose Vision\nAmplified Semantic Entropy (VASE), which incorporates weak image\ntransformations and amplifies the impact of visual input, to improve\nhallucination detection in medical VQA. We first estimate the semantic\npredictive distribution under weak visual transformations to preserve clinical\nvalidity, and then amplify visual influence by contrasting this distribution\nwith that derived from a distorted image. The entropy of the resulting\ndistribution is estimated as VASE. Experiments on two medical open-ended VQA\ndatasets demonstrate that VASE consistently outperforms existing hallucination\ndetection methods."
                },
                "authors": [
                    {
                        "name": "Zehui Liao"
                    },
                    {
                        "name": "Shishuai Hu"
                    },
                    {
                        "name": "Ke Zou"
                    },
                    {
                        "name": "Huazhu Fu"
                    },
                    {
                        "name": "Liangli Zhen"
                    },
                    {
                        "name": "Yong Xia"
                    }
                ],
                "author_detail": {
                    "name": "Yong Xia"
                },
                "author": "Yong Xia",
                "arxiv_comment": "11 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20504v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20504v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08708v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08708v2",
                "updated": "2025-03-26T12:34:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    12,
                    34,
                    44,
                    2,
                    85,
                    0
                ],
                "published": "2024-10-11T10:53:00Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    10,
                    53,
                    0,
                    4,
                    285,
                    0
                ],
                "title": "Non-linear correlations underlie linear response and causality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-linear correlations underlie linear response and causality"
                },
                "summary": "The inference of causal relationships among observed variables is a pivotal,\nlongstanding problem in the scientific community. An intuitive method for\nquantifying these causal links involves examining the response of one variable\nto perturbations in another. The fluctuation-dissipation theorem elegantly\nconnects this response to the correlation functions of the unperturbed system,\nthereby bridging the concepts of causality and correlation. However, this\nrelationship becomes intricate in nonlinear systems, where knowledge of the\ninvariant measure is required but elusive, especially in high-dimensional\nspaces. In this study, we establish a novel link between the Koopman operator\nof nonlinear stochastic systems and the response function. This connection\nprovides an alternative method for computing the response function using\ngeneralized correlation functions, even when the invariant measure is unknown.\nWe validate our theoretical framework by applying it to a nonlinear\nhigh-dimensional system amenable to exact solutions, demonstrating convergence\nand consistency with established results. Finally, we discuss a significant\ninterplay between the resulting causal network and the relevant time scales of\nthe system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The inference of causal relationships among observed variables is a pivotal,\nlongstanding problem in the scientific community. An intuitive method for\nquantifying these causal links involves examining the response of one variable\nto perturbations in another. The fluctuation-dissipation theorem elegantly\nconnects this response to the correlation functions of the unperturbed system,\nthereby bridging the concepts of causality and correlation. However, this\nrelationship becomes intricate in nonlinear systems, where knowledge of the\ninvariant measure is required but elusive, especially in high-dimensional\nspaces. In this study, we establish a novel link between the Koopman operator\nof nonlinear stochastic systems and the response function. This connection\nprovides an alternative method for computing the response function using\ngeneralized correlation functions, even when the invariant measure is unknown.\nWe validate our theoretical framework by applying it to a nonlinear\nhigh-dimensional system amenable to exact solutions, demonstrating convergence\nand consistency with established results. Finally, we discuss a significant\ninterplay between the resulting causal network and the relevant time scales of\nthe system."
                },
                "authors": [
                    {
                        "name": "Gabriele Di Antonio"
                    },
                    {
                        "name": "Gianni Valerio Vinci"
                    }
                ],
                "author_detail": {
                    "name": "Gianni Valerio Vinci"
                },
                "author": "Gianni Valerio Vinci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08708v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08708v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.stat-mech",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20491v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20491v1",
                "updated": "2025-03-26T12:28:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    12,
                    28,
                    20,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T12:28:20Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    12,
                    28,
                    20,
                    2,
                    85,
                    0
                ],
                "title": "VPO: Aligning Text-to-Video Generation Models with Prompt Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VPO: Aligning Text-to-Video Generation Models with Prompt Optimization"
                },
                "summary": "Video generation models have achieved remarkable progress in text-to-video\ntasks. These models are typically trained on text-video pairs with highly\ndetailed and carefully crafted descriptions, while real-world user inputs\nduring inference are often concise, vague, or poorly structured. This gap makes\nprompt optimization crucial for generating high-quality videos. Current methods\noften rely on large language models (LLMs) to refine prompts through in-context\nlearning, but suffer from several limitations: they may distort user intent,\nomit critical details, or introduce safety risks. Moreover, they optimize\nprompts without considering the impact on the final video quality, which can\nlead to suboptimal results. To address these issues, we introduce VPO, a\nprincipled framework that optimizes prompts based on three core principles:\nharmlessness, accuracy, and helpfulness. The generated prompts faithfully\npreserve user intents and, more importantly, enhance the safety and quality of\ngenerated videos. To achieve this, VPO employs a two-stage optimization\napproach. First, we construct and refine a supervised fine-tuning (SFT) dataset\nbased on principles of safety and alignment. Second, we introduce both\ntext-level and video-level feedback to further optimize the SFT model with\npreference learning. Our extensive experiments demonstrate that VPO\nsignificantly improves safety, alignment, and video quality compared to\nbaseline methods. Moreover, VPO shows strong generalization across video\ngeneration models. Furthermore, we demonstrate that VPO could outperform and be\ncombined with RLHF methods on video generation models, underscoring the\neffectiveness of VPO in aligning video generation models. Our code and data are\npublicly available at https://github.com/thu-coai/VPO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video generation models have achieved remarkable progress in text-to-video\ntasks. These models are typically trained on text-video pairs with highly\ndetailed and carefully crafted descriptions, while real-world user inputs\nduring inference are often concise, vague, or poorly structured. This gap makes\nprompt optimization crucial for generating high-quality videos. Current methods\noften rely on large language models (LLMs) to refine prompts through in-context\nlearning, but suffer from several limitations: they may distort user intent,\nomit critical details, or introduce safety risks. Moreover, they optimize\nprompts without considering the impact on the final video quality, which can\nlead to suboptimal results. To address these issues, we introduce VPO, a\nprincipled framework that optimizes prompts based on three core principles:\nharmlessness, accuracy, and helpfulness. The generated prompts faithfully\npreserve user intents and, more importantly, enhance the safety and quality of\ngenerated videos. To achieve this, VPO employs a two-stage optimization\napproach. First, we construct and refine a supervised fine-tuning (SFT) dataset\nbased on principles of safety and alignment. Second, we introduce both\ntext-level and video-level feedback to further optimize the SFT model with\npreference learning. Our extensive experiments demonstrate that VPO\nsignificantly improves safety, alignment, and video quality compared to\nbaseline methods. Moreover, VPO shows strong generalization across video\ngeneration models. Furthermore, we demonstrate that VPO could outperform and be\ncombined with RLHF methods on video generation models, underscoring the\neffectiveness of VPO in aligning video generation models. Our code and data are\npublicly available at https://github.com/thu-coai/VPO."
                },
                "authors": [
                    {
                        "name": "Jiale Cheng"
                    },
                    {
                        "name": "Ruiliang Lyu"
                    },
                    {
                        "name": "Xiaotao Gu"
                    },
                    {
                        "name": "Xiao Liu"
                    },
                    {
                        "name": "Jiazheng Xu"
                    },
                    {
                        "name": "Yida Lu"
                    },
                    {
                        "name": "Jiayan Teng"
                    },
                    {
                        "name": "Zhuoyi Yang"
                    },
                    {
                        "name": "Yuxiao Dong"
                    },
                    {
                        "name": "Jie Tang"
                    },
                    {
                        "name": "Hongning Wang"
                    },
                    {
                        "name": "Minlie Huang"
                    }
                ],
                "author_detail": {
                    "name": "Minlie Huang"
                },
                "author": "Minlie Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20491v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20491v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14754v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14754v2",
                "updated": "2025-03-26T12:25:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    12,
                    25,
                    3,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-18T21:53:37Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    21,
                    53,
                    37,
                    1,
                    77,
                    0
                ],
                "title": "Bayesian Modeling of Zero-Shot Classifications for Urban Flood Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Modeling of Zero-Shot Classifications for Urban Flood Detection"
                },
                "summary": "Street scene datasets, collected from Street View or dashboard cameras, offer\na promising means of detecting urban objects and incidents like street\nflooding. However, a major challenge in using these datasets is their lack of\nreliable labels: there are myriad types of incidents, many types occur rarely,\nand ground-truth measures of where incidents occur are lacking. Here, we\npropose BayFlood, a two-stage approach which circumvents this difficulty.\nFirst, we perform zero-shot classification of where incidents occur using a\npretrained vision-language model (VLM). Second, we fit a spatial Bayesian model\non the VLM classifications. The zero-shot approach avoids the need to annotate\nlarge training sets, and the Bayesian model provides frequent desiderata in\nurban settings - principled measures of uncertainty, smoothing across\nlocations, and incorporation of external data like stormwater accumulation\nzones. We comprehensively validate this two-stage approach, showing that VLMs\nprovide strong zero-shot signal for floods across multiple cities and time\nperiods, the Bayesian model improves out-of-sample prediction relative to\nbaseline methods, and our inferred flood risk correlates with known external\npredictors of risk. Having validated our approach, we show it can be used to\nimprove urban flood detection: our analysis reveals 113,738 people who are at\nhigh risk of flooding overlooked by current methods, identifies demographic\nbiases in existing methods, and suggests locations for new flood sensors. More\nbroadly, our results showcase how Bayesian modeling of zero-shot LM annotations\nrepresents a promising paradigm because it avoids the need to collect large\nlabeled datasets and leverages the power of foundation models while providing\nthe expressiveness and uncertainty quantification of Bayesian models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Street scene datasets, collected from Street View or dashboard cameras, offer\na promising means of detecting urban objects and incidents like street\nflooding. However, a major challenge in using these datasets is their lack of\nreliable labels: there are myriad types of incidents, many types occur rarely,\nand ground-truth measures of where incidents occur are lacking. Here, we\npropose BayFlood, a two-stage approach which circumvents this difficulty.\nFirst, we perform zero-shot classification of where incidents occur using a\npretrained vision-language model (VLM). Second, we fit a spatial Bayesian model\non the VLM classifications. The zero-shot approach avoids the need to annotate\nlarge training sets, and the Bayesian model provides frequent desiderata in\nurban settings - principled measures of uncertainty, smoothing across\nlocations, and incorporation of external data like stormwater accumulation\nzones. We comprehensively validate this two-stage approach, showing that VLMs\nprovide strong zero-shot signal for floods across multiple cities and time\nperiods, the Bayesian model improves out-of-sample prediction relative to\nbaseline methods, and our inferred flood risk correlates with known external\npredictors of risk. Having validated our approach, we show it can be used to\nimprove urban flood detection: our analysis reveals 113,738 people who are at\nhigh risk of flooding overlooked by current methods, identifies demographic\nbiases in existing methods, and suggests locations for new flood sensors. More\nbroadly, our results showcase how Bayesian modeling of zero-shot LM annotations\nrepresents a promising paradigm because it avoids the need to collect large\nlabeled datasets and leverages the power of foundation models while providing\nthe expressiveness and uncertainty quantification of Bayesian models."
                },
                "authors": [
                    {
                        "name": "Matt Franchi"
                    },
                    {
                        "name": "Nikhil Garg"
                    },
                    {
                        "name": "Wendy Ju"
                    },
                    {
                        "name": "Emma Pierson"
                    }
                ],
                "author_detail": {
                    "name": "Emma Pierson"
                },
                "author": "Emma Pierson",
                "arxiv_comment": "In review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14754v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14754v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.14949v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.14949v2",
                "updated": "2025-03-26T12:16:13Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    12,
                    16,
                    13,
                    2,
                    85,
                    0
                ],
                "published": "2023-09-26T14:06:26Z",
                "published_parsed": [
                    2023,
                    9,
                    26,
                    14,
                    6,
                    26,
                    1,
                    269,
                    0
                ],
                "title": "Towards Real-World Test-Time Adaptation: Tri-Net Self-Training with\n  Balanced Normalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Real-World Test-Time Adaptation: Tri-Net Self-Training with\n  Balanced Normalization"
                },
                "summary": "Test-Time Adaptation aims to adapt source domain model to testing data at\ninference stage with success demonstrated in adapting to unseen corruptions.\nHowever, these attempts may fail under more challenging real-world scenarios.\nExisting works mainly consider real-world test-time adaptation under non-i.i.d.\ndata stream and continual domain shift. In this work, we first complement the\nexisting real-world TTA protocol with a globally class imbalanced testing set.\nWe demonstrate that combining all settings together poses new challenges to\nexisting methods. We argue the failure of state-of-the-art methods is first\ncaused by indiscriminately adapting normalization layers to imbalanced testing\ndata. To remedy this shortcoming, we propose a balanced batchnorm layer to swap\nout the regular batchnorm at inference stage. The new batchnorm layer is\ncapable of adapting without biasing towards majority classes. We are further\ninspired by the success of self-training (ST) in learning from unlabeled data\nand adapt ST for test-time adaptation. However, ST alone is prone to over\nadaption which is responsible for the poor performance under continual domain\nshift. Hence, we propose to improve self-training under continual domain shift\nby regularizing model updates with an anchored loss. The final TTA model,\ntermed as TRIBE, is built upon a tri-net architecture with balanced batchnorm\nlayers. We evaluate TRIBE on four datasets representing real-world TTA\nsettings. TRIBE consistently achieves the state-of-the-art performance across\nmultiple evaluation protocols. The code is available at\nhttps://github.com/Gorilla-Lab-SCUT/TRIBE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-Time Adaptation aims to adapt source domain model to testing data at\ninference stage with success demonstrated in adapting to unseen corruptions.\nHowever, these attempts may fail under more challenging real-world scenarios.\nExisting works mainly consider real-world test-time adaptation under non-i.i.d.\ndata stream and continual domain shift. In this work, we first complement the\nexisting real-world TTA protocol with a globally class imbalanced testing set.\nWe demonstrate that combining all settings together poses new challenges to\nexisting methods. We argue the failure of state-of-the-art methods is first\ncaused by indiscriminately adapting normalization layers to imbalanced testing\ndata. To remedy this shortcoming, we propose a balanced batchnorm layer to swap\nout the regular batchnorm at inference stage. The new batchnorm layer is\ncapable of adapting without biasing towards majority classes. We are further\ninspired by the success of self-training (ST) in learning from unlabeled data\nand adapt ST for test-time adaptation. However, ST alone is prone to over\nadaption which is responsible for the poor performance under continual domain\nshift. Hence, we propose to improve self-training under continual domain shift\nby regularizing model updates with an anchored loss. The final TTA model,\ntermed as TRIBE, is built upon a tri-net architecture with balanced batchnorm\nlayers. We evaluate TRIBE on four datasets representing real-world TTA\nsettings. TRIBE consistently achieves the state-of-the-art performance across\nmultiple evaluation protocols. The code is available at\nhttps://github.com/Gorilla-Lab-SCUT/TRIBE."
                },
                "authors": [
                    {
                        "name": "Yongyi Su"
                    },
                    {
                        "name": "Xun Xu"
                    },
                    {
                        "name": "Kui Jia"
                    }
                ],
                "author_detail": {
                    "name": "Kui Jia"
                },
                "author": "Kui Jia",
                "arxiv_comment": "Accepted by AAAI 2024. 19 pages, 7 figures and 22 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.14949v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.14949v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19385v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19385v2",
                "updated": "2025-03-26T12:12:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    12,
                    12,
                    38,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-25T06:30:45Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    6,
                    30,
                    45,
                    1,
                    84,
                    0
                ],
                "title": "Inference-Time Scaling for Flow Models via Stochastic Generation and\n  Rollover Budget Forcing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-Time Scaling for Flow Models via Stochastic Generation and\n  Rollover Budget Forcing"
                },
                "summary": "We propose an inference-time scaling approach for pretrained flow models.\nRecently, inference-time scaling has gained significant attention in LLMs and\ndiffusion models, improving sample quality or better aligning outputs with user\npreferences by leveraging additional computation. For diffusion models,\nparticle sampling has allowed more efficient scaling due to the stochasticity\nat intermediate denoising steps. On the contrary, while flow models have gained\npopularity as an alternative to diffusion models--offering faster generation\nand high-quality outputs in state-of-the-art image and video generative\nmodels--efficient inference-time scaling methods used for diffusion models\ncannot be directly applied due to their deterministic generative process. To\nenable efficient inference-time scaling for flow models, we propose three key\nideas: 1) SDE-based generation, enabling particle sampling in flow models, 2)\nInterpolant conversion, broadening the search space and enhancing sample\ndiversity, and 3) Rollover Budget Forcing (RBF), an adaptive allocation of\ncomputational resources across timesteps to maximize budget utilization. Our\nexperiments show that SDE-based generation, particularly variance-preserving\n(VP) interpolant-based generation, improves the performance of particle\nsampling methods for inference-time scaling in flow models. Additionally, we\ndemonstrate that RBF with VP-SDE achieves the best performance, outperforming\nall previous inference-time scaling approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose an inference-time scaling approach for pretrained flow models.\nRecently, inference-time scaling has gained significant attention in LLMs and\ndiffusion models, improving sample quality or better aligning outputs with user\npreferences by leveraging additional computation. For diffusion models,\nparticle sampling has allowed more efficient scaling due to the stochasticity\nat intermediate denoising steps. On the contrary, while flow models have gained\npopularity as an alternative to diffusion models--offering faster generation\nand high-quality outputs in state-of-the-art image and video generative\nmodels--efficient inference-time scaling methods used for diffusion models\ncannot be directly applied due to their deterministic generative process. To\nenable efficient inference-time scaling for flow models, we propose three key\nideas: 1) SDE-based generation, enabling particle sampling in flow models, 2)\nInterpolant conversion, broadening the search space and enhancing sample\ndiversity, and 3) Rollover Budget Forcing (RBF), an adaptive allocation of\ncomputational resources across timesteps to maximize budget utilization. Our\nexperiments show that SDE-based generation, particularly variance-preserving\n(VP) interpolant-based generation, improves the performance of particle\nsampling methods for inference-time scaling in flow models. Additionally, we\ndemonstrate that RBF with VP-SDE achieves the best performance, outperforming\nall previous inference-time scaling approaches."
                },
                "authors": [
                    {
                        "name": "Jaihoon Kim"
                    },
                    {
                        "name": "Taehoon Yoon"
                    },
                    {
                        "name": "Jisung Hwang"
                    },
                    {
                        "name": "Minhyuk Sung"
                    }
                ],
                "author_detail": {
                    "name": "Minhyuk Sung"
                },
                "author": "Minhyuk Sung",
                "arxiv_comment": "Project page: https://flow-inference-time-scaling.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19385v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19385v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.19230v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.19230v2",
                "updated": "2025-03-26T11:54:52Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    11,
                    54,
                    52,
                    2,
                    85,
                    0
                ],
                "published": "2024-05-29T16:07:39Z",
                "published_parsed": [
                    2024,
                    5,
                    29,
                    16,
                    7,
                    39,
                    2,
                    150,
                    0
                ],
                "title": "Valid Conformal Prediction for Dynamic GNNs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Valid Conformal Prediction for Dynamic GNNs"
                },
                "summary": "Dynamic graphs provide a flexible data abstraction for modelling many sorts\nof real-world systems, such as transport, trade, and social networks. Graph\nneural networks (GNNs) are powerful tools allowing for different kinds of\nprediction and inference on these systems, but getting a handle on uncertainty,\nespecially in dynamic settings, is a challenging problem. In this work we\npropose to use a dynamic graph representation known in the tensor literature as\nthe unfolding, to achieve valid prediction sets via conformal prediction. This\nrepresentation, a simple graph, can be input to any standard GNN and does not\nrequire any modification to existing GNN architectures or conformal prediction\nroutines. One of our key contributions is a careful mathematical consideration\nof the different inference scenarios which can arise in a dynamic graph\nmodelling context. For a range of practically relevant cases, we obtain valid\nprediction sets with almost no assumptions, even dispensing with\nexchangeability. In a more challenging scenario, which we call the\nsemi-inductive regime, we achieve valid prediction under stronger assumptions,\nakin to stationarity. We provide real data examples demonstrating validity,\nshowing improved accuracy over baselines, and sign-posting different failure\nmodes which can occur when those assumptions are violated.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic graphs provide a flexible data abstraction for modelling many sorts\nof real-world systems, such as transport, trade, and social networks. Graph\nneural networks (GNNs) are powerful tools allowing for different kinds of\nprediction and inference on these systems, but getting a handle on uncertainty,\nespecially in dynamic settings, is a challenging problem. In this work we\npropose to use a dynamic graph representation known in the tensor literature as\nthe unfolding, to achieve valid prediction sets via conformal prediction. This\nrepresentation, a simple graph, can be input to any standard GNN and does not\nrequire any modification to existing GNN architectures or conformal prediction\nroutines. One of our key contributions is a careful mathematical consideration\nof the different inference scenarios which can arise in a dynamic graph\nmodelling context. For a range of practically relevant cases, we obtain valid\nprediction sets with almost no assumptions, even dispensing with\nexchangeability. In a more challenging scenario, which we call the\nsemi-inductive regime, we achieve valid prediction under stronger assumptions,\nakin to stationarity. We provide real data examples demonstrating validity,\nshowing improved accuracy over baselines, and sign-posting different failure\nmodes which can occur when those assumptions are violated."
                },
                "authors": [
                    {
                        "name": "Ed Davis"
                    },
                    {
                        "name": "Ian Gallagher"
                    },
                    {
                        "name": "Daniel John Lawson"
                    },
                    {
                        "name": "Patrick Rubin-Delanchy"
                    }
                ],
                "author_detail": {
                    "name": "Patrick Rubin-Delanchy"
                },
                "author": "Patrick Rubin-Delanchy",
                "arxiv_comment": "25 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.19230v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.19230v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62H30",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20472v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20472v1",
                "updated": "2025-03-26T11:53:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    11,
                    53,
                    3,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T11:53:03Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    11,
                    53,
                    3,
                    2,
                    85,
                    0
                ],
                "title": "From Trial to Triumph: Advancing Long Video Understanding via Visual\n  Context Sample Scaling and Self-reward Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Trial to Triumph: Advancing Long Video Understanding via Visual\n  Context Sample Scaling and Self-reward Alignment"
                },
                "summary": "Multi-modal Large language models (MLLMs) show remarkable ability in video\nunderstanding. Nevertheless, understanding long videos remains challenging as\nthe models can only process a finite number of frames in a single inference,\npotentially omitting crucial visual information. To address the challenge, we\npropose generating multiple predictions through visual context sampling,\nfollowed by a scoring mechanism to select the final prediction. Specifically,\nwe devise a bin-wise sampling strategy that enables MLLMs to generate diverse\nanswers based on various combinations of keyframes, thereby enriching the\nvisual context. To determine the final prediction from the sampled answers, we\nemploy a self-reward by linearly combining three scores: (1) a frequency score\nindicating the prevalence of each option, (2) a marginal confidence score\nreflecting the inter-intra sample certainty of MLLM predictions, and (3) a\nreasoning score for different question types, including clue-guided answering\nfor global questions and temporal self-refocusing for local questions. The\nfrequency score ensures robustness through majority correctness, the\nconfidence-aligned score reflects prediction certainty, and the typed-reasoning\nscore addresses cases with sparse key visual information using tailored\nstrategies. Experiments show that this approach covers the correct answer for a\nhigh percentage of long video questions, on seven datasets show that our method\nimproves the performance of three MLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-modal Large language models (MLLMs) show remarkable ability in video\nunderstanding. Nevertheless, understanding long videos remains challenging as\nthe models can only process a finite number of frames in a single inference,\npotentially omitting crucial visual information. To address the challenge, we\npropose generating multiple predictions through visual context sampling,\nfollowed by a scoring mechanism to select the final prediction. Specifically,\nwe devise a bin-wise sampling strategy that enables MLLMs to generate diverse\nanswers based on various combinations of keyframes, thereby enriching the\nvisual context. To determine the final prediction from the sampled answers, we\nemploy a self-reward by linearly combining three scores: (1) a frequency score\nindicating the prevalence of each option, (2) a marginal confidence score\nreflecting the inter-intra sample certainty of MLLM predictions, and (3) a\nreasoning score for different question types, including clue-guided answering\nfor global questions and temporal self-refocusing for local questions. The\nfrequency score ensures robustness through majority correctness, the\nconfidence-aligned score reflects prediction certainty, and the typed-reasoning\nscore addresses cases with sparse key visual information using tailored\nstrategies. Experiments show that this approach covers the correct answer for a\nhigh percentage of long video questions, on seven datasets show that our method\nimproves the performance of three MLLMs."
                },
                "authors": [
                    {
                        "name": "Yucheng Suo"
                    },
                    {
                        "name": "Fan Ma"
                    },
                    {
                        "name": "Linchao Zhu"
                    },
                    {
                        "name": "Tianyi Wang"
                    },
                    {
                        "name": "Fengyun Rao"
                    },
                    {
                        "name": "Yi Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yi Yang"
                },
                "author": "Yi Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20472v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20472v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20466v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20466v1",
                "updated": "2025-03-26T11:51:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    11,
                    51,
                    23,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T11:51:23Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    11,
                    51,
                    23,
                    2,
                    85,
                    0
                ],
                "title": "Data-driven Seasonal Climate Predictions via Variational Inference and\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-driven Seasonal Climate Predictions via Variational Inference and\n  Transformers"
                },
                "summary": "Most operational climate services providers base their seasonal predictions\non initialised general circulation models (GCMs) or statistical techniques that\nfit past observations. GCMs require substantial computational resources, which\nlimits their capacity. In contrast, statistical methods often lack robustness\ndue to short historical records. Recent works propose machine learning methods\ntrained on climate model output, leveraging larger sample sizes and simulated\nscenarios. Yet, many of these studies focus on prediction tasks that might be\nrestricted in spatial extent or temporal coverage, opening a gap with existing\noperational predictions. Thus, the present study evaluates the effectiveness of\na methodology that combines variational inference with transformer models to\npredict fields of seasonal anomalies. The predictions cover all four seasons\nand are initialised one month before the start of each season. The model was\ntrained on climate model output from CMIP6 and tested using ERA5 reanalysis\ndata. We analyse the method's performance in predicting interannual anomalies\nbeyond the climate change-induced trend. We also test the proposed methodology\nin a regional context with a use case focused on Europe. While climate change\ntrends dominate the skill of temperature predictions, the method presents\nadditional skill over the climatological forecast in regions influenced by\nknown teleconnections. We reach similar conclusions based on the validation of\nprecipitation predictions. Despite underperforming SEAS5 in most tropics, our\nmodel offers added value in numerous extratropical inland regions. This work\ndemonstrates the effectiveness of training generative models on climate model\noutput for seasonal predictions, providing skilful predictions beyond the\ninduced climate change trend at time scales and lead times relevant for user\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most operational climate services providers base their seasonal predictions\non initialised general circulation models (GCMs) or statistical techniques that\nfit past observations. GCMs require substantial computational resources, which\nlimits their capacity. In contrast, statistical methods often lack robustness\ndue to short historical records. Recent works propose machine learning methods\ntrained on climate model output, leveraging larger sample sizes and simulated\nscenarios. Yet, many of these studies focus on prediction tasks that might be\nrestricted in spatial extent or temporal coverage, opening a gap with existing\noperational predictions. Thus, the present study evaluates the effectiveness of\na methodology that combines variational inference with transformer models to\npredict fields of seasonal anomalies. The predictions cover all four seasons\nand are initialised one month before the start of each season. The model was\ntrained on climate model output from CMIP6 and tested using ERA5 reanalysis\ndata. We analyse the method's performance in predicting interannual anomalies\nbeyond the climate change-induced trend. We also test the proposed methodology\nin a regional context with a use case focused on Europe. While climate change\ntrends dominate the skill of temperature predictions, the method presents\nadditional skill over the climatological forecast in regions influenced by\nknown teleconnections. We reach similar conclusions based on the validation of\nprecipitation predictions. Despite underperforming SEAS5 in most tropics, our\nmodel offers added value in numerous extratropical inland regions. This work\ndemonstrates the effectiveness of training generative models on climate model\noutput for seasonal predictions, providing skilful predictions beyond the\ninduced climate change trend at time scales and lead times relevant for user\napplications."
                },
                "authors": [
                    {
                        "name": "Lluís Palma"
                    },
                    {
                        "name": "Alejandro Peraza"
                    },
                    {
                        "name": "David Civantos"
                    },
                    {
                        "name": "Amanda Duarte"
                    },
                    {
                        "name": "Stefano Materia"
                    },
                    {
                        "name": "Ángel G. Muñoz"
                    },
                    {
                        "name": "Jesús Peña"
                    },
                    {
                        "name": "Laia Romero"
                    },
                    {
                        "name": "Albert Soret"
                    },
                    {
                        "name": "Markus G. Donat"
                    }
                ],
                "author_detail": {
                    "name": "Markus G. Donat"
                },
                "author": "Markus G. Donat",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20466v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20466v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ao-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05874v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05874v3",
                "updated": "2025-03-26T11:27:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    11,
                    27,
                    37,
                    2,
                    85,
                    0
                ],
                "published": "2025-02-09T12:23:40Z",
                "published_parsed": [
                    2025,
                    2,
                    9,
                    12,
                    23,
                    40,
                    6,
                    40,
                    0
                ],
                "title": "MMGDreamer: Mixed-Modality Graph for Geometry-Controllable 3D Indoor\n  Scene Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MMGDreamer: Mixed-Modality Graph for Geometry-Controllable 3D Indoor\n  Scene Generation"
                },
                "summary": "Controllable 3D scene generation has extensive applications in virtual\nreality and interior design, where the generated scenes should exhibit high\nlevels of realism and controllability in terms of geometry. Scene graphs\nprovide a suitable data representation that facilitates these applications.\nHowever, current graph-based methods for scene generation are constrained to\ntext-based inputs and exhibit insufficient adaptability to flexible user\ninputs, hindering the ability to precisely control object geometry. To address\nthis issue, we propose MMGDreamer, a dual-branch diffusion model for scene\ngeneration that incorporates a novel Mixed-Modality Graph, visual enhancement\nmodule, and relation predictor. The mixed-modality graph allows object nodes to\nintegrate textual and visual modalities, with optional relationships between\nnodes. It enhances adaptability to flexible user inputs and enables meticulous\ncontrol over the geometry of objects in the generated scenes. The visual\nenhancement module enriches the visual fidelity of text-only nodes by\nconstructing visual representations using text embeddings. Furthermore, our\nrelation predictor leverages node representations to infer absent relationships\nbetween nodes, resulting in more coherent scene layouts. Extensive experimental\nresults demonstrate that MMGDreamer exhibits superior control of object\ngeometry, achieving state-of-the-art scene generation performance. Project\npage: https://yangzhifeio.github.io/project/MMGDreamer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Controllable 3D scene generation has extensive applications in virtual\nreality and interior design, where the generated scenes should exhibit high\nlevels of realism and controllability in terms of geometry. Scene graphs\nprovide a suitable data representation that facilitates these applications.\nHowever, current graph-based methods for scene generation are constrained to\ntext-based inputs and exhibit insufficient adaptability to flexible user\ninputs, hindering the ability to precisely control object geometry. To address\nthis issue, we propose MMGDreamer, a dual-branch diffusion model for scene\ngeneration that incorporates a novel Mixed-Modality Graph, visual enhancement\nmodule, and relation predictor. The mixed-modality graph allows object nodes to\nintegrate textual and visual modalities, with optional relationships between\nnodes. It enhances adaptability to flexible user inputs and enables meticulous\ncontrol over the geometry of objects in the generated scenes. The visual\nenhancement module enriches the visual fidelity of text-only nodes by\nconstructing visual representations using text embeddings. Furthermore, our\nrelation predictor leverages node representations to infer absent relationships\nbetween nodes, resulting in more coherent scene layouts. Extensive experimental\nresults demonstrate that MMGDreamer exhibits superior control of object\ngeometry, achieving state-of-the-art scene generation performance. Project\npage: https://yangzhifeio.github.io/project/MMGDreamer."
                },
                "authors": [
                    {
                        "name": "Zhifei Yang"
                    },
                    {
                        "name": "Keyang Lu"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Jiaxing Qi"
                    },
                    {
                        "name": "Hanqi Jiang"
                    },
                    {
                        "name": "Ruifei Ma"
                    },
                    {
                        "name": "Shenglin Yin"
                    },
                    {
                        "name": "Yifan Xu"
                    },
                    {
                        "name": "Mingzhe Xing"
                    },
                    {
                        "name": "Zhen Xiao"
                    },
                    {
                        "name": "Jieyi Long"
                    },
                    {
                        "name": "Guangyao Zhai"
                    }
                ],
                "author_detail": {
                    "name": "Guangyao Zhai"
                },
                "author": "Guangyao Zhai",
                "arxiv_comment": "Accepted by AAAI 2025 Main Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05874v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05874v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19551v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19551v2",
                "updated": "2025-03-26T11:23:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    11,
                    23,
                    44,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-25T11:07:12Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    11,
                    7,
                    12,
                    1,
                    84,
                    0
                ],
                "title": "Scaling Laws of Synthetic Data for Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Laws of Synthetic Data for Language Models"
                },
                "summary": "Large language models (LLMs) achieve strong performance across diverse tasks,\nlargely driven by high-quality web data used in pre-training. However, recent\nstudies indicate this data source is rapidly depleting. Synthetic data emerges\nas a promising alternative, but it remains unclear whether synthetic datasets\nexhibit predictable scalability comparable to raw pre-training data. In this\nwork, we systematically investigate the scaling laws of synthetic data by\nintroducing SynthLLM, a scalable framework that transforms pre-training corpora\ninto diverse, high-quality synthetic datasets. Our approach achieves this by\nautomatically extracting and recombining high-level concepts across multiple\ndocuments using a graph algorithm. Key findings from our extensive mathematical\nexperiments on SynthLLM include: (1) SynthLLM generates synthetic data that\nreliably adheres to the rectified scaling law across various model sizes; (2)\nPerformance improvements plateau near 300B tokens; and (3) Larger models\napproach optimal performance with fewer training tokens. For instance, an 8B\nmodel peaks at 1T tokens, while a 3B model requires 4T. Moreover, comparisons\nwith existing synthetic data generation and augmentation methods demonstrate\nthat SynthLLM achieves superior performance and scalability. Our findings\nhighlight synthetic data as a scalable and reliable alternative to organic\npre-training corpora, offering a viable path toward continued improvement in\nmodel performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) achieve strong performance across diverse tasks,\nlargely driven by high-quality web data used in pre-training. However, recent\nstudies indicate this data source is rapidly depleting. Synthetic data emerges\nas a promising alternative, but it remains unclear whether synthetic datasets\nexhibit predictable scalability comparable to raw pre-training data. In this\nwork, we systematically investigate the scaling laws of synthetic data by\nintroducing SynthLLM, a scalable framework that transforms pre-training corpora\ninto diverse, high-quality synthetic datasets. Our approach achieves this by\nautomatically extracting and recombining high-level concepts across multiple\ndocuments using a graph algorithm. Key findings from our extensive mathematical\nexperiments on SynthLLM include: (1) SynthLLM generates synthetic data that\nreliably adheres to the rectified scaling law across various model sizes; (2)\nPerformance improvements plateau near 300B tokens; and (3) Larger models\napproach optimal performance with fewer training tokens. For instance, an 8B\nmodel peaks at 1T tokens, while a 3B model requires 4T. Moreover, comparisons\nwith existing synthetic data generation and augmentation methods demonstrate\nthat SynthLLM achieves superior performance and scalability. Our findings\nhighlight synthetic data as a scalable and reliable alternative to organic\npre-training corpora, offering a viable path toward continued improvement in\nmodel performance."
                },
                "authors": [
                    {
                        "name": "Zeyu Qin"
                    },
                    {
                        "name": "Qingxiu Dong"
                    },
                    {
                        "name": "Xingxing Zhang"
                    },
                    {
                        "name": "Li Dong"
                    },
                    {
                        "name": "Xiaolong Huang"
                    },
                    {
                        "name": "Ziyi Yang"
                    },
                    {
                        "name": "Mahmoud Khademi"
                    },
                    {
                        "name": "Dongdong Zhang"
                    },
                    {
                        "name": "Hany Hassan Awadalla"
                    },
                    {
                        "name": "Yi R. Fung"
                    },
                    {
                        "name": "Weizhu Chen"
                    },
                    {
                        "name": "Minhao Cheng"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei",
                "arxiv_comment": "work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19551v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19551v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20436v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20436v1",
                "updated": "2025-03-26T11:10:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    11,
                    10,
                    29,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T11:10:29Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    11,
                    10,
                    29,
                    2,
                    85,
                    0
                ],
                "title": "Siformer: Feature-isolated Transformer for Efficient Skeleton-based Sign\n  Language Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Siformer: Feature-isolated Transformer for Efficient Skeleton-based Sign\n  Language Recognition"
                },
                "summary": "Sign language recognition (SLR) refers to interpreting sign language glosses\nfrom given videos automatically. This research area presents a complex\nchallenge in computer vision because of the rapid and intricate movements\ninherent in sign languages, which encompass hand gestures, body postures, and\neven facial expressions. Recently, skeleton-based action recognition has\nattracted increasing attention due to its ability to handle variations in\nsubjects and backgrounds independently. However, current skeleton-based SLR\nmethods exhibit three limitations: 1) they often neglect the importance of\nrealistic hand poses, where most studies train SLR models on non-realistic\nskeletal representations; 2) they tend to assume complete data availability in\nboth training or inference phases, and capture intricate relationships among\ndifferent body parts collectively; 3) these methods treat all sign glosses\nuniformly, failing to account for differences in complexity levels regarding\nskeletal representations. To enhance the realism of hand skeletal\nrepresentations, we present a kinematic hand pose rectification method for\nenforcing constraints. Mitigating the impact of missing data, we propose a\nfeature-isolated mechanism to focus on capturing local spatial-temporal\ncontext. This method captures the context concurrently and independently from\nindividual features, thus enhancing the robustness of the SLR model.\nAdditionally, to adapt to varying complexity levels of sign glosses, we develop\nan input-adaptive inference approach to optimise computational efficiency and\naccuracy. Experimental results demonstrate the effectiveness of our approach,\nas evidenced by achieving a new state-of-the-art (SOTA) performance on WLASL100\nand LSA64. For WLASL100, we achieve a top-1 accuracy of 86.50\\%, marking a\nrelative improvement of 2.39% over the previous SOTA. For LSA64, we achieve a\ntop-1 accuracy of 99.84%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sign language recognition (SLR) refers to interpreting sign language glosses\nfrom given videos automatically. This research area presents a complex\nchallenge in computer vision because of the rapid and intricate movements\ninherent in sign languages, which encompass hand gestures, body postures, and\neven facial expressions. Recently, skeleton-based action recognition has\nattracted increasing attention due to its ability to handle variations in\nsubjects and backgrounds independently. However, current skeleton-based SLR\nmethods exhibit three limitations: 1) they often neglect the importance of\nrealistic hand poses, where most studies train SLR models on non-realistic\nskeletal representations; 2) they tend to assume complete data availability in\nboth training or inference phases, and capture intricate relationships among\ndifferent body parts collectively; 3) these methods treat all sign glosses\nuniformly, failing to account for differences in complexity levels regarding\nskeletal representations. To enhance the realism of hand skeletal\nrepresentations, we present a kinematic hand pose rectification method for\nenforcing constraints. Mitigating the impact of missing data, we propose a\nfeature-isolated mechanism to focus on capturing local spatial-temporal\ncontext. This method captures the context concurrently and independently from\nindividual features, thus enhancing the robustness of the SLR model.\nAdditionally, to adapt to varying complexity levels of sign glosses, we develop\nan input-adaptive inference approach to optimise computational efficiency and\naccuracy. Experimental results demonstrate the effectiveness of our approach,\nas evidenced by achieving a new state-of-the-art (SOTA) performance on WLASL100\nand LSA64. For WLASL100, we achieve a top-1 accuracy of 86.50\\%, marking a\nrelative improvement of 2.39% over the previous SOTA. For LSA64, we achieve a\ntop-1 accuracy of 99.84%."
                },
                "authors": [
                    {
                        "name": "Muxin Pu"
                    },
                    {
                        "name": "Mei Kuan Lim"
                    },
                    {
                        "name": "Chun Yong Chong"
                    }
                ],
                "author_detail": {
                    "name": "Chun Yong Chong"
                },
                "author": "Chun Yong Chong",
                "arxiv_comment": "10 pages, ACM Multimedia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20436v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20436v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12150v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12150v2",
                "updated": "2025-03-26T11:08:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    11,
                    8,
                    20,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-15T14:13:23Z",
                "published_parsed": [
                    2025,
                    3,
                    15,
                    14,
                    13,
                    23,
                    5,
                    74,
                    0
                ],
                "title": "Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and\n  Generalizable Point Cloud Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and\n  Generalizable Point Cloud Analysis"
                },
                "summary": "This paper proposes a general solution to enable point cloud recognition\nmodels to handle distribution shifts at test time. Unlike prior methods, which\nrely heavily on training data (often inaccessible during online inference) and\nare limited to recognizing a fixed set of point cloud classes predefined during\ntraining, we explore a more practical and challenging scenario: adapting the\nmodel solely based on online test data to recognize both previously seen\nclasses and novel, unseen classes at test time. To this end, we develop\n\\textbf{Point-Cache}, a hierarchical cache model that captures essential clues\nof online test samples, particularly focusing on the global structure of point\nclouds and their local-part details. Point-Cache, which serves as a rich 3D\nknowledge base, is dynamically managed to prioritize the inclusion of\nhigh-quality samples. Designed as a plug-and-play module, our method can be\nflexibly integrated into large multimodal 3D models to support open-vocabulary\npoint cloud recognition. Notably, our solution operates with efficiency\ncomparable to zero-shot inference, as it is entirely training-free. Point-Cache\ndemonstrates substantial gains across 8 challenging benchmarks and 4\nrepresentative large 3D models, highlighting its effectiveness. Code is\navailable at https://github.com/auniquesun/Point-Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a general solution to enable point cloud recognition\nmodels to handle distribution shifts at test time. Unlike prior methods, which\nrely heavily on training data (often inaccessible during online inference) and\nare limited to recognizing a fixed set of point cloud classes predefined during\ntraining, we explore a more practical and challenging scenario: adapting the\nmodel solely based on online test data to recognize both previously seen\nclasses and novel, unseen classes at test time. To this end, we develop\n\\textbf{Point-Cache}, a hierarchical cache model that captures essential clues\nof online test samples, particularly focusing on the global structure of point\nclouds and their local-part details. Point-Cache, which serves as a rich 3D\nknowledge base, is dynamically managed to prioritize the inclusion of\nhigh-quality samples. Designed as a plug-and-play module, our method can be\nflexibly integrated into large multimodal 3D models to support open-vocabulary\npoint cloud recognition. Notably, our solution operates with efficiency\ncomparable to zero-shot inference, as it is entirely training-free. Point-Cache\ndemonstrates substantial gains across 8 challenging benchmarks and 4\nrepresentative large 3D models, highlighting its effectiveness. Code is\navailable at https://github.com/auniquesun/Point-Cache."
                },
                "authors": [
                    {
                        "name": "Hongyu Sun"
                    },
                    {
                        "name": "Qiuhong Ke"
                    },
                    {
                        "name": "Ming Cheng"
                    },
                    {
                        "name": "Yongcai Wang"
                    },
                    {
                        "name": "Deying Li"
                    },
                    {
                        "name": "Chenhui Gou"
                    },
                    {
                        "name": "Jianfei Cai"
                    }
                ],
                "author_detail": {
                    "name": "Jianfei Cai"
                },
                "author": "Jianfei Cai",
                "arxiv_comment": "Accepted by CVPR 2025; 24 pages, 14 figures, 18 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12150v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12150v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17945v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17945v2",
                "updated": "2025-03-26T11:06:10Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    11,
                    6,
                    10,
                    2,
                    85,
                    0
                ],
                "published": "2024-11-26T23:39:43Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    23,
                    39,
                    43,
                    1,
                    331,
                    0
                ],
                "title": "MARVEL-40M+: Multi-Level Visual Elaboration for High-Fidelity Text-to-3D\n  Content Creation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MARVEL-40M+: Multi-Level Visual Elaboration for High-Fidelity Text-to-3D\n  Content Creation"
                },
                "summary": "Generating high-fidelity 3D content from text prompts remains a significant\nchallenge in computer vision due to the limited size, diversity, and annotation\ndepth of the existing datasets. To address this, we introduce MARVEL-40M+, an\nextensive dataset with 40 million text annotations for over 8.9 million 3D\nassets aggregated from seven major 3D datasets. Our contribution is a novel\nmulti-stage annotation pipeline that integrates open-source pretrained\nmulti-view VLMs and LLMs to automatically produce multi-level descriptions,\nranging from detailed (150-200 words) to concise semantic tags (10-20 words).\nThis structure supports both fine-grained 3D reconstruction and rapid\nprototyping. Furthermore, we incorporate human metadata from source datasets\ninto our annotation pipeline to add domain-specific information in our\nannotation and reduce VLM hallucinations. Additionally, we develop MARVEL-FX3D,\na two-stage text-to-3D pipeline. We fine-tune Stable Diffusion with our\nannotations and use a pretrained image-to-3D network to generate 3D textured\nmeshes within 15s. Extensive evaluations show that MARVEL-40M+ significantly\noutperforms existing datasets in annotation quality and linguistic diversity,\nachieving win rates of 72.41% by GPT-4 and 73.40% by human evaluators. Project\npage is available at https://sankalpsinha-cmos.github.io/MARVEL/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating high-fidelity 3D content from text prompts remains a significant\nchallenge in computer vision due to the limited size, diversity, and annotation\ndepth of the existing datasets. To address this, we introduce MARVEL-40M+, an\nextensive dataset with 40 million text annotations for over 8.9 million 3D\nassets aggregated from seven major 3D datasets. Our contribution is a novel\nmulti-stage annotation pipeline that integrates open-source pretrained\nmulti-view VLMs and LLMs to automatically produce multi-level descriptions,\nranging from detailed (150-200 words) to concise semantic tags (10-20 words).\nThis structure supports both fine-grained 3D reconstruction and rapid\nprototyping. Furthermore, we incorporate human metadata from source datasets\ninto our annotation pipeline to add domain-specific information in our\nannotation and reduce VLM hallucinations. Additionally, we develop MARVEL-FX3D,\na two-stage text-to-3D pipeline. We fine-tune Stable Diffusion with our\nannotations and use a pretrained image-to-3D network to generate 3D textured\nmeshes within 15s. Extensive evaluations show that MARVEL-40M+ significantly\noutperforms existing datasets in annotation quality and linguistic diversity,\nachieving win rates of 72.41% by GPT-4 and 73.40% by human evaluators. Project\npage is available at https://sankalpsinha-cmos.github.io/MARVEL/."
                },
                "authors": [
                    {
                        "name": "Sankalp Sinha"
                    },
                    {
                        "name": "Mohammad Sadil Khan"
                    },
                    {
                        "name": "Muhammad Usama"
                    },
                    {
                        "name": "Shino Sam"
                    },
                    {
                        "name": "Didier Stricker"
                    },
                    {
                        "name": "Sk Aziz Ali"
                    },
                    {
                        "name": "Muhammad Zeshan Afzal"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Zeshan Afzal"
                },
                "author": "Muhammad Zeshan Afzal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17945v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17945v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20430v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20430v1",
                "updated": "2025-03-26T11:03:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    11,
                    3,
                    34,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T11:03:34Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    11,
                    3,
                    34,
                    2,
                    85,
                    0
                ],
                "title": "RALLRec+: Retrieval Augmented Large Language Model Recommendation with\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RALLRec+: Retrieval Augmented Large Language Model Recommendation with\n  Reasoning"
                },
                "summary": "Large Language Models (LLMs) have been integrated into recommender systems to\nenhance user behavior comprehension. The Retrieval Augmented Generation (RAG)\ntechnique is further incorporated into these systems to retrieve more relevant\nitems and improve system performance. However, existing RAG methods have two\nshortcomings. \\textit{(i)} In the \\textit{retrieval} stage, they rely primarily\non textual semantics and often fail to incorporate the most relevant items,\nthus constraining system effectiveness. \\textit{(ii)} In the\n\\textit{generation} stage, they lack explicit chain-of-thought reasoning,\nfurther limiting their potential.\n  In this paper, we propose Representation learning and \\textbf{R}easoning\nempowered retrieval-\\textbf{A}ugmented \\textbf{L}arge \\textbf{L}anguage model\n\\textbf{Rec}ommendation (RALLRec+). Specifically, for the retrieval stage, we\nprompt LLMs to generate detailed item descriptions and perform joint\nrepresentation learning, combining textual and collaborative signals extracted\nfrom the LLM and recommendation models, respectively. To account for the\ntime-varying nature of user interests, we propose a simple yet effective\nreranking method to capture preference dynamics. For the generation phase, we\nfirst evaluate reasoning LLMs on recommendation tasks, uncovering valuable\ninsights. Then we introduce knowledge-injected prompting and consistency-based\nmerging approach to integrate reasoning LLMs with general-purpose LLMs,\nenhancing overall performance. Extensive experiments on three real world\ndatasets validate our method's effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been integrated into recommender systems to\nenhance user behavior comprehension. The Retrieval Augmented Generation (RAG)\ntechnique is further incorporated into these systems to retrieve more relevant\nitems and improve system performance. However, existing RAG methods have two\nshortcomings. \\textit{(i)} In the \\textit{retrieval} stage, they rely primarily\non textual semantics and often fail to incorporate the most relevant items,\nthus constraining system effectiveness. \\textit{(ii)} In the\n\\textit{generation} stage, they lack explicit chain-of-thought reasoning,\nfurther limiting their potential.\n  In this paper, we propose Representation learning and \\textbf{R}easoning\nempowered retrieval-\\textbf{A}ugmented \\textbf{L}arge \\textbf{L}anguage model\n\\textbf{Rec}ommendation (RALLRec+). Specifically, for the retrieval stage, we\nprompt LLMs to generate detailed item descriptions and perform joint\nrepresentation learning, combining textual and collaborative signals extracted\nfrom the LLM and recommendation models, respectively. To account for the\ntime-varying nature of user interests, we propose a simple yet effective\nreranking method to capture preference dynamics. For the generation phase, we\nfirst evaluate reasoning LLMs on recommendation tasks, uncovering valuable\ninsights. Then we introduce knowledge-injected prompting and consistency-based\nmerging approach to integrate reasoning LLMs with general-purpose LLMs,\nenhancing overall performance. Extensive experiments on three real world\ndatasets validate our method's effectiveness."
                },
                "authors": [
                    {
                        "name": "Sichun Luo"
                    },
                    {
                        "name": "Jian Xu"
                    },
                    {
                        "name": "Xiaojie Zhang"
                    },
                    {
                        "name": "Linrong Wang"
                    },
                    {
                        "name": "Sicong Liu"
                    },
                    {
                        "name": "Hanxu Hou"
                    },
                    {
                        "name": "Linqi Song"
                    }
                ],
                "author_detail": {
                    "name": "Linqi Song"
                },
                "author": "Linqi Song",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2502.06101",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20430v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20430v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2303.07706v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2303.07706v3",
                "updated": "2025-03-26T11:02:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    11,
                    2,
                    15,
                    2,
                    85,
                    0
                ],
                "published": "2023-03-14T08:43:51Z",
                "published_parsed": [
                    2023,
                    3,
                    14,
                    8,
                    43,
                    51,
                    1,
                    73,
                    0
                ],
                "title": "On the Utility of Equal Batch Sizes for Inference in Stochastic Gradient\n  Descent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Utility of Equal Batch Sizes for Inference in Stochastic Gradient\n  Descent"
                },
                "summary": "Stochastic gradient descent (SGD) is an estimation tool for large data\nemployed in machine learning and statistics. Due to the Markovian nature of the\nSGD process, inference is a challenging problem. An underlying asymptotic\nnormality of the averaged SGD (ASGD) estimator allows for the construction of a\nbatch-means estimator of the asymptotic covariance matrix. Instead of the usual\nincreasing batch-size strategy, we propose a memory efficient equal batch-size\nstrategy and show that under mild conditions, the batch-means estimator is\nconsistent. A key feature of the proposed batching technique is that it allows\nfor bias-correction of the variance, at no additional cost to memory. Further,\nsince joint inference for large dimensional problems may be undesirable, we\npresent marginal-friendly simultaneous confidence intervals, and show through\nan example on how covariance estimators of ASGD can be employed for improved\npredictions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stochastic gradient descent (SGD) is an estimation tool for large data\nemployed in machine learning and statistics. Due to the Markovian nature of the\nSGD process, inference is a challenging problem. An underlying asymptotic\nnormality of the averaged SGD (ASGD) estimator allows for the construction of a\nbatch-means estimator of the asymptotic covariance matrix. Instead of the usual\nincreasing batch-size strategy, we propose a memory efficient equal batch-size\nstrategy and show that under mild conditions, the batch-means estimator is\nconsistent. A key feature of the proposed batching technique is that it allows\nfor bias-correction of the variance, at no additional cost to memory. Further,\nsince joint inference for large dimensional problems may be undesirable, we\npresent marginal-friendly simultaneous confidence intervals, and show through\nan example on how covariance estimators of ASGD can be employed for improved\npredictions."
                },
                "authors": [
                    {
                        "name": "Rahul Singh"
                    },
                    {
                        "name": "Abhinek Shukla"
                    },
                    {
                        "name": "Dootika Vats"
                    }
                ],
                "author_detail": {
                    "name": "Dootika Vats"
                },
                "author": "Dootika Vats",
                "arxiv_comment": "45 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2303.07706v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2303.07706v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20427v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20427v1",
                "updated": "2025-03-26T10:59:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    10,
                    59,
                    27,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T10:59:27Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    10,
                    59,
                    27,
                    2,
                    85,
                    0
                ],
                "title": "Localizing entropy production along non-equilibrium trajectories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Localizing entropy production along non-equilibrium trajectories"
                },
                "summary": "An important open problem in nonequilibrium thermodynamics is the\nquantification and spatiotemporal localization of entropy production in complex\nnanoscale processes from experimental data. Here we address this issue through\na data-driven approach that combines the recently developed short-time\nthermodynamic uncertainty relation based inference scheme with machine learning\ntechniques. Our approach leverages the flexible function representation\nprovided by deep neural networks to achieve accurate reconstruction of\nhigh-dimensional, potentially time-dependent dissipative force fields as well\nas the localization of entropy production in both space and time along\nnonequilibrium trajectories. We demonstrate the versatility of the framework\nthrough applications to diverse systems of fundamental interest and\nexperimental significance, where it successfully addresses distinct challenges\nin localizing entropy production.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An important open problem in nonequilibrium thermodynamics is the\nquantification and spatiotemporal localization of entropy production in complex\nnanoscale processes from experimental data. Here we address this issue through\na data-driven approach that combines the recently developed short-time\nthermodynamic uncertainty relation based inference scheme with machine learning\ntechniques. Our approach leverages the flexible function representation\nprovided by deep neural networks to achieve accurate reconstruction of\nhigh-dimensional, potentially time-dependent dissipative force fields as well\nas the localization of entropy production in both space and time along\nnonequilibrium trajectories. We demonstrate the versatility of the framework\nthrough applications to diverse systems of fundamental interest and\nexperimental significance, where it successfully addresses distinct challenges\nin localizing entropy production."
                },
                "authors": [
                    {
                        "name": "Biswajit Das"
                    },
                    {
                        "name": "Sreekanth K Manikandan"
                    }
                ],
                "author_detail": {
                    "name": "Sreekanth K Manikandan"
                },
                "author": "Sreekanth K Manikandan",
                "arxiv_comment": "12 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20427v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20427v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.stat-mech",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20417v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20417v1",
                "updated": "2025-03-26T10:44:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    10,
                    44,
                    51,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T10:44:51Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    10,
                    44,
                    51,
                    2,
                    85,
                    0
                ],
                "title": "CFunModel: A \"Funny\" Language Model Capable of Chinese Humor Generation\n  and Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CFunModel: A \"Funny\" Language Model Capable of Chinese Humor Generation\n  and Processing"
                },
                "summary": "Humor plays a significant role in daily language communication. With the\nrapid development of large language models (LLMs), natural language processing\nhas made significant strides in understanding and generating various genres of\ntexts. However, most LLMs exhibit poor performance in generating and processing\nChinese humor. In this study, we introduce a comprehensive Chinese\nhumor-related dataset, the Chinese Fun Set (CFunSet). This dataset aggregates\nexisting Chinese humor datasets and includes over 20,000 jokes collected from\nTieba-JokeBar, a Chinese online platform known for joke sharing. The resulting\ncorpus comprises more than 160,000 entries. Leveraging CFunSet, we developed\nthe Chinese Fun Model (CFunModel), the first large language model designed to\nhandle various Chinese humor-related tasks including Crosstalk Response\nSelection, Humor Recognition, Joke Generation, etc. Experimental results\ndemonstrate that CFunModel outperforms popular large language models in these\ntasks. Our CFunSet is available at\nhttps://huggingface.co/datasets/ZhenghanYU/CFunSet and CFunModel is available\nat https://huggingface.co/ZhenghanYU/CFunModel. A demostration video of our\nwork is available at https://youtu.be/MOsISOJ66Ms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humor plays a significant role in daily language communication. With the\nrapid development of large language models (LLMs), natural language processing\nhas made significant strides in understanding and generating various genres of\ntexts. However, most LLMs exhibit poor performance in generating and processing\nChinese humor. In this study, we introduce a comprehensive Chinese\nhumor-related dataset, the Chinese Fun Set (CFunSet). This dataset aggregates\nexisting Chinese humor datasets and includes over 20,000 jokes collected from\nTieba-JokeBar, a Chinese online platform known for joke sharing. The resulting\ncorpus comprises more than 160,000 entries. Leveraging CFunSet, we developed\nthe Chinese Fun Model (CFunModel), the first large language model designed to\nhandle various Chinese humor-related tasks including Crosstalk Response\nSelection, Humor Recognition, Joke Generation, etc. Experimental results\ndemonstrate that CFunModel outperforms popular large language models in these\ntasks. Our CFunSet is available at\nhttps://huggingface.co/datasets/ZhenghanYU/CFunSet and CFunModel is available\nat https://huggingface.co/ZhenghanYU/CFunModel. A demostration video of our\nwork is available at https://youtu.be/MOsISOJ66Ms."
                },
                "authors": [
                    {
                        "name": "Zhenghan Yu"
                    },
                    {
                        "name": "Xinyu Hu"
                    },
                    {
                        "name": "Xiaojun Wan"
                    }
                ],
                "author_detail": {
                    "name": "Xiaojun Wan"
                },
                "author": "Xiaojun Wan",
                "arxiv_comment": "9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20417v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20417v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18147v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18147v2",
                "updated": "2025-03-26T10:42:11Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    10,
                    42,
                    11,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-23T17:24:32Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    17,
                    24,
                    32,
                    6,
                    82,
                    0
                ],
                "title": "PHT-CAD: Efficient CAD Parametric Primitive Analysis with Progressive\n  Hierarchical Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PHT-CAD: Efficient CAD Parametric Primitive Analysis with Progressive\n  Hierarchical Tuning"
                },
                "summary": "Computer-Aided Design (CAD) plays a pivotal role in industrial manufacturing,\nyet 2D Parametric Primitive Analysis (PPA) remains underexplored due to two key\nchallenges: structural constraint reasoning and advanced semantic\nunderstanding. To tackle these challenges, we first propose an Efficient Hybrid\nParametrization (EHP) for better representing 2D engineering drawings. EHP\ncontains four types of atomic component i.e., point, line, circle, and arc).\nAdditionally, we propose PHT-CAD, a novel 2D PPA framework that harnesses the\nmodality alignment and reasoning capabilities of Vision-Language Models (VLMs)\nfor precise engineering drawing analysis. In PHT-CAD, we introduce four\ndedicated regression heads to predict corresponding atomic components. To train\nPHT-CAD, a three-stage training paradigm Progressive Hierarchical Tuning (PHT)\nis proposed to progressively enhance PHT-CAD's capability to perceive\nindividual primitives, infer structural constraints, and align annotation\nlayers with their corresponding geometric representations. Considering that\nexisting datasets lack complete annotation layers and real-world engineering\ndrawings, we introduce ParaCAD, the first large-scale benchmark that explicitly\nintegrates both the geometric and annotation layers. ParaCAD comprises over 10\nmillion annotated drawings for training and 3,000 real-world industrial\ndrawings with complex topological structures and physical constraints for test.\nExtensive experiments demonstrate the effectiveness of PHT-CAD and highlight\nthe practical significance of ParaCAD in advancing 2D PPA research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computer-Aided Design (CAD) plays a pivotal role in industrial manufacturing,\nyet 2D Parametric Primitive Analysis (PPA) remains underexplored due to two key\nchallenges: structural constraint reasoning and advanced semantic\nunderstanding. To tackle these challenges, we first propose an Efficient Hybrid\nParametrization (EHP) for better representing 2D engineering drawings. EHP\ncontains four types of atomic component i.e., point, line, circle, and arc).\nAdditionally, we propose PHT-CAD, a novel 2D PPA framework that harnesses the\nmodality alignment and reasoning capabilities of Vision-Language Models (VLMs)\nfor precise engineering drawing analysis. In PHT-CAD, we introduce four\ndedicated regression heads to predict corresponding atomic components. To train\nPHT-CAD, a three-stage training paradigm Progressive Hierarchical Tuning (PHT)\nis proposed to progressively enhance PHT-CAD's capability to perceive\nindividual primitives, infer structural constraints, and align annotation\nlayers with their corresponding geometric representations. Considering that\nexisting datasets lack complete annotation layers and real-world engineering\ndrawings, we introduce ParaCAD, the first large-scale benchmark that explicitly\nintegrates both the geometric and annotation layers. ParaCAD comprises over 10\nmillion annotated drawings for training and 3,000 real-world industrial\ndrawings with complex topological structures and physical constraints for test.\nExtensive experiments demonstrate the effectiveness of PHT-CAD and highlight\nthe practical significance of ParaCAD in advancing 2D PPA research."
                },
                "authors": [
                    {
                        "name": "Ke Niu"
                    },
                    {
                        "name": "Yuwen Chen"
                    },
                    {
                        "name": "Haiyang Yu"
                    },
                    {
                        "name": "Zhuofan Chen"
                    },
                    {
                        "name": "Xianghui Que"
                    },
                    {
                        "name": "Bin Li"
                    },
                    {
                        "name": "Xiangyang Xue"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyang Xue"
                },
                "author": "Xiangyang Xue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18147v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18147v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19666v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19666v2",
                "updated": "2025-03-26T10:39:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    10,
                    39,
                    33,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-25T13:52:26Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    13,
                    52,
                    26,
                    1,
                    84,
                    0
                ],
                "title": "Towards Efficient Training of Graph Neural Networks: A Multiscale\n  Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Efficient Training of Graph Neural Networks: A Multiscale\n  Approach"
                },
                "summary": "Graph Neural Networks (GNNs) have emerged as a powerful tool for learning and\ninferring from graph-structured data, and are widely used in a variety of\napplications, often considering large amounts of data and large graphs.\nHowever, training on such data requires large memory and extensive\ncomputations. In this paper, we introduce a novel framework for efficient\nmultiscale training of GNNs, designed to integrate information across\nmultiscale representations of a graph. Our approach leverages a hierarchical\ngraph representation, taking advantage of coarse graph scales in the training\nprocess, where each coarse scale graph has fewer nodes and edges. Based on this\napproach, we propose a suite of GNN training methods: such as coarse-to-fine,\nsub-to-full, and multiscale gradient computation. We demonstrate the\neffectiveness of our methods on various datasets and learning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) have emerged as a powerful tool for learning and\ninferring from graph-structured data, and are widely used in a variety of\napplications, often considering large amounts of data and large graphs.\nHowever, training on such data requires large memory and extensive\ncomputations. In this paper, we introduce a novel framework for efficient\nmultiscale training of GNNs, designed to integrate information across\nmultiscale representations of a graph. Our approach leverages a hierarchical\ngraph representation, taking advantage of coarse graph scales in the training\nprocess, where each coarse scale graph has fewer nodes and edges. Based on this\napproach, we propose a suite of GNN training methods: such as coarse-to-fine,\nsub-to-full, and multiscale gradient computation. We demonstrate the\neffectiveness of our methods on various datasets and learning tasks."
                },
                "authors": [
                    {
                        "name": "Eshed Gal"
                    },
                    {
                        "name": "Moshe Eliasof"
                    },
                    {
                        "name": "Carola-Bibiane Schönlieb"
                    },
                    {
                        "name": "Eldad Haber"
                    },
                    {
                        "name": "Eran Treister"
                    }
                ],
                "author_detail": {
                    "name": "Eran Treister"
                },
                "author": "Eran Treister",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19666v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19666v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19527v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19527v2",
                "updated": "2025-03-26T10:28:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    10,
                    28,
                    12,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-25T10:28:51Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    10,
                    28,
                    51,
                    1,
                    84,
                    0
                ],
                "title": "A multi-scale investigation into the diagnostic potential of the\n  HCN/HCO$^+$ ratio for AGN and starburst activity in nearby galaxies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A multi-scale investigation into the diagnostic potential of the\n  HCN/HCO$^+$ ratio for AGN and starburst activity in nearby galaxies"
                },
                "summary": "(Abridged) The identification of AGN and SB regions in galaxies is crucial\nfor understanding the role of various physical processes in galaxy evolution.\nMolecular line ratios, such as the HCN/HCO+ ratio, have been proposed as\npotential tracers of these distinct environments. This paper aims to assess the\nreliability of the HCN/HCO+ ratio, from J = 1-0 to J = 4-3 transitions, as a\ndiagnostic tool for differentiating AGN and SB activity across a diverse sample\nof nearby galaxies. We focus on evaluating the effect of spatial resolution on\nthe robustness of these ratios and investigate the underlying physical\nconditions that drive observed variations. We compile observations of HCN and\nHCO+ lines across multiple J transitions from various sources, covering\ndifferent galaxy types, including Seyferts, starbursts, and (ultra-)luminous\ninfrared galaxies (U/LIRGs). The observations span spatial scales from\ncloud-sized regions to kiloparsec scales. We analyse the behaviour of these\nratios at varying resolutions and employ non-LTE radiative transfer models to\ninfer the physical conditions that drive the observed ratios. We find that the\nHCN/HCO+ ratio from higher J transitions can differentiate between AGN and SB\nactivity when observed at high spatial resolution. This distinction occurs\naround unity. However, at lower resolutions, contamination from multiple\nemission sources and beam averaging effects destroy these distinctions.\nModelling suggests that elevated HCN/HCO+ ratios in AGN-dominated regions are\nlargely driven by an enhancement in HCN abundance relative to HCO+, likely due\nto high-temperature chemistry or increased excitation. Our study confirms that\nthe HCN/HCO+ ratio, particularly of higher J transitions, can be a reliable\ntracer of AGN versus SB activity if observations are conducted at sufficiently\nhigh spatial resolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "(Abridged) The identification of AGN and SB regions in galaxies is crucial\nfor understanding the role of various physical processes in galaxy evolution.\nMolecular line ratios, such as the HCN/HCO+ ratio, have been proposed as\npotential tracers of these distinct environments. This paper aims to assess the\nreliability of the HCN/HCO+ ratio, from J = 1-0 to J = 4-3 transitions, as a\ndiagnostic tool for differentiating AGN and SB activity across a diverse sample\nof nearby galaxies. We focus on evaluating the effect of spatial resolution on\nthe robustness of these ratios and investigate the underlying physical\nconditions that drive observed variations. We compile observations of HCN and\nHCO+ lines across multiple J transitions from various sources, covering\ndifferent galaxy types, including Seyferts, starbursts, and (ultra-)luminous\ninfrared galaxies (U/LIRGs). The observations span spatial scales from\ncloud-sized regions to kiloparsec scales. We analyse the behaviour of these\nratios at varying resolutions and employ non-LTE radiative transfer models to\ninfer the physical conditions that drive the observed ratios. We find that the\nHCN/HCO+ ratio from higher J transitions can differentiate between AGN and SB\nactivity when observed at high spatial resolution. This distinction occurs\naround unity. However, at lower resolutions, contamination from multiple\nemission sources and beam averaging effects destroy these distinctions.\nModelling suggests that elevated HCN/HCO+ ratios in AGN-dominated regions are\nlargely driven by an enhancement in HCN abundance relative to HCO+, likely due\nto high-temperature chemistry or increased excitation. Our study confirms that\nthe HCN/HCO+ ratio, particularly of higher J transitions, can be a reliable\ntracer of AGN versus SB activity if observations are conducted at sufficiently\nhigh spatial resolution."
                },
                "authors": [
                    {
                        "name": "J. Butterworth"
                    },
                    {
                        "name": "S. Viti"
                    },
                    {
                        "name": "Y. Wang"
                    }
                ],
                "author_detail": {
                    "name": "Y. Wang"
                },
                "author": "Y. Wang",
                "arxiv_comment": "11 pages (including appendices), 9 figures (including those within\n  appendices), Accepted for publication to A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19527v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19527v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00004v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00004v2",
                "updated": "2025-03-26T10:27:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    10,
                    27,
                    15,
                    2,
                    85,
                    0
                ],
                "published": "2024-09-12T23:29:33Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    23,
                    29,
                    33,
                    3,
                    256,
                    0
                ],
                "title": "Retro-li: Small-Scale Retrieval Augmented Generation Supporting Noisy\n  Similarity Searches and Domain Shift Generalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retro-li: Small-Scale Retrieval Augmented Generation Supporting Noisy\n  Similarity Searches and Domain Shift Generalization"
                },
                "summary": "The retrieval augmented generation (RAG) system such as Retro has been shown\nto improve language modeling capabilities and reduce toxicity and\nhallucinations by retrieving from a database of non-parametric memory\ncontaining trillions of entries. We introduce Retro-li that shows retrieval can\nalso help using a small-scale database, but it demands more accurate and better\nneighbors when searching in a smaller hence sparser non-parametric memory. This\ncan be met by using a proper semantic similarity search. We further propose\nadding a regularization to the non-parametric memory for the first time: it\nsignificantly reduces perplexity when the neighbor search operations are noisy\nduring inference, and it improves generalization when a domain shift occurs. We\nalso show that Retro-li's non-parametric memory can potentially be implemented\non analog in-memory computing hardware, exhibiting O(1) search time while\ncausing noise in retrieving neighbors, with minimal (<1%) performance loss. Our\ncode is available at:\nhttps://github.com/IBM/Retrieval-Enhanced-Transformer-Little.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The retrieval augmented generation (RAG) system such as Retro has been shown\nto improve language modeling capabilities and reduce toxicity and\nhallucinations by retrieving from a database of non-parametric memory\ncontaining trillions of entries. We introduce Retro-li that shows retrieval can\nalso help using a small-scale database, but it demands more accurate and better\nneighbors when searching in a smaller hence sparser non-parametric memory. This\ncan be met by using a proper semantic similarity search. We further propose\nadding a regularization to the non-parametric memory for the first time: it\nsignificantly reduces perplexity when the neighbor search operations are noisy\nduring inference, and it improves generalization when a domain shift occurs. We\nalso show that Retro-li's non-parametric memory can potentially be implemented\non analog in-memory computing hardware, exhibiting O(1) search time while\ncausing noise in retrieving neighbors, with minimal (<1%) performance loss. Our\ncode is available at:\nhttps://github.com/IBM/Retrieval-Enhanced-Transformer-Little."
                },
                "authors": [
                    {
                        "name": "Gentiana Rashiti"
                    },
                    {
                        "name": "Geethan Karunaratne"
                    },
                    {
                        "name": "Mrinmaya Sachan"
                    },
                    {
                        "name": "Abu Sebastian"
                    },
                    {
                        "name": "Abbas Rahimi"
                    }
                ],
                "author_detail": {
                    "name": "Abbas Rahimi"
                },
                "author": "Abbas Rahimi",
                "arxiv_doi": "10.3233/FAIA240837",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3233/FAIA240837",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.00004v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00004v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Published in: Proceedings of 27TH EUROPEAN CONFERENCE ON\n  ARTIFICIAL INTELLIGENCE, IOS Press, 392, 2024, pp. 2974 - 2982",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10169v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10169v2",
                "updated": "2025-03-26T10:08:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    10,
                    8,
                    23,
                    2,
                    85,
                    0
                ],
                "published": "2024-12-13T14:34:09Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    14,
                    34,
                    9,
                    4,
                    348,
                    0
                ],
                "title": "Recovering Pulsar Braking Index from a Population of Millisecond Pulsars",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recovering Pulsar Braking Index from a Population of Millisecond Pulsars"
                },
                "summary": "The braking index, $n$, of a pulsar is a measure of its angular momentum loss\nand the value it takes corresponds to different spin-down mechanisms. For a\npulsar spinning down due to gravitational wave emission from the principal mass\nquadrupole mode alone, the braking index would equal exactly 5. Unfortunately,\nfor millisecond pulsars, it can be hard to measure observationally due to the\nextremely small second time derivative of the rotation frequency, $\\ddot{f}$.\nThis paper aims to examine whether it could be possible to extract the\ndistribution of $n$ for a whole population of pulsars rather than measuring the\nvalues individually. We use simulated data with an injected $n=5$ signal for 47\nmillisecond pulsars and extract the distribution using hierarchical Bayesian\ninference methods. We find that while possible, observation times of over 20\nyears and RMS noise of the order of $10^{-5}$ ms are needed, which can be\ncompared to the mean noise value of $3\\times10^{-4}$ ms for the recent wideband\n12.5-year NANOGrav sample, which provided the pulsar timing data used in this\npaper.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The braking index, $n$, of a pulsar is a measure of its angular momentum loss\nand the value it takes corresponds to different spin-down mechanisms. For a\npulsar spinning down due to gravitational wave emission from the principal mass\nquadrupole mode alone, the braking index would equal exactly 5. Unfortunately,\nfor millisecond pulsars, it can be hard to measure observationally due to the\nextremely small second time derivative of the rotation frequency, $\\ddot{f}$.\nThis paper aims to examine whether it could be possible to extract the\ndistribution of $n$ for a whole population of pulsars rather than measuring the\nvalues individually. We use simulated data with an injected $n=5$ signal for 47\nmillisecond pulsars and extract the distribution using hierarchical Bayesian\ninference methods. We find that while possible, observation times of over 20\nyears and RMS noise of the order of $10^{-5}$ ms are needed, which can be\ncompared to the mean noise value of $3\\times10^{-4}$ ms for the recent wideband\n12.5-year NANOGrav sample, which provided the pulsar timing data used in this\npaper."
                },
                "authors": [
                    {
                        "name": "A. L. Hewitt"
                    },
                    {
                        "name": "M. Pitkin"
                    },
                    {
                        "name": "I. M. Hook"
                    }
                ],
                "author_detail": {
                    "name": "I. M. Hook"
                },
                "author": "I. M. Hook",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10169v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10169v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20384v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20384v1",
                "updated": "2025-03-26T10:05:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    10,
                    5,
                    38,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T10:05:38Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    10,
                    5,
                    38,
                    2,
                    85,
                    0
                ],
                "title": "MoLe-VLA: Dynamic Layer-skipping Vision Language Action Model via\n  Mixture-of-Layers for Efficient Robot Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoLe-VLA: Dynamic Layer-skipping Vision Language Action Model via\n  Mixture-of-Layers for Efficient Robot Manipulation"
                },
                "summary": "Multimodal Large Language Models (MLLMs) excel in understanding complex\nlanguage and visual data, enabling generalist robotic systems to interpret\ninstructions and perform embodied tasks. Nevertheless, their real-world\ndeployment is hindered by substantial computational and storage demands. Recent\ninsights into the homogeneous patterns in the LLM layer have inspired\nsparsification techniques to address these challenges, such as early exit and\ntoken pruning. However, these methods often neglect the critical role of the\nfinal layers that encode the semantic information most relevant to downstream\nrobotic tasks. Aligning with the recent breakthrough of the Shallow Brain\nHypothesis (SBH) in neuroscience and the mixture of experts in model\nsparsification, we conceptualize each LLM layer as an expert and propose a\nMixture-of-Layers Vision-Language-Action model (MoLe-VLA, or simply MoLe)\narchitecture for dynamic LLM layer activation. We introduce a Spatial-Temporal\nAware Router (STAR) for MoLe to selectively activate only parts of the layers\nbased on the robot's current state, mimicking the brain's distinct signal\npathways specialized for cognition and causal reasoning. Additionally, to\ncompensate for the cognitive ability of LLMs lost in MoLe, we devise a\nCognition Self-Knowledge Distillation (CogKD) framework. CogKD enhances the\nunderstanding of task demands and improves the generation of task-relevant\naction sequences by leveraging cognitive features. Extensive experiments\nconducted in both RLBench simulation and real-world environments demonstrate\nthe superiority of MoLe-VLA in both efficiency and performance. Specifically,\nMoLe-VLA achieves an 8% improvement in the mean success rate across ten tasks\nwhile reducing computational costs by up to x5.6 compared to standard LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) excel in understanding complex\nlanguage and visual data, enabling generalist robotic systems to interpret\ninstructions and perform embodied tasks. Nevertheless, their real-world\ndeployment is hindered by substantial computational and storage demands. Recent\ninsights into the homogeneous patterns in the LLM layer have inspired\nsparsification techniques to address these challenges, such as early exit and\ntoken pruning. However, these methods often neglect the critical role of the\nfinal layers that encode the semantic information most relevant to downstream\nrobotic tasks. Aligning with the recent breakthrough of the Shallow Brain\nHypothesis (SBH) in neuroscience and the mixture of experts in model\nsparsification, we conceptualize each LLM layer as an expert and propose a\nMixture-of-Layers Vision-Language-Action model (MoLe-VLA, or simply MoLe)\narchitecture for dynamic LLM layer activation. We introduce a Spatial-Temporal\nAware Router (STAR) for MoLe to selectively activate only parts of the layers\nbased on the robot's current state, mimicking the brain's distinct signal\npathways specialized for cognition and causal reasoning. Additionally, to\ncompensate for the cognitive ability of LLMs lost in MoLe, we devise a\nCognition Self-Knowledge Distillation (CogKD) framework. CogKD enhances the\nunderstanding of task demands and improves the generation of task-relevant\naction sequences by leveraging cognitive features. Extensive experiments\nconducted in both RLBench simulation and real-world environments demonstrate\nthe superiority of MoLe-VLA in both efficiency and performance. Specifically,\nMoLe-VLA achieves an 8% improvement in the mean success rate across ten tasks\nwhile reducing computational costs by up to x5.6 compared to standard LLMs."
                },
                "authors": [
                    {
                        "name": "Rongyu Zhang"
                    },
                    {
                        "name": "Menghang Dong"
                    },
                    {
                        "name": "Yuan Zhang"
                    },
                    {
                        "name": "Liang Heng"
                    },
                    {
                        "name": "Xiaowei Chi"
                    },
                    {
                        "name": "Gaole Dai"
                    },
                    {
                        "name": "Li Du"
                    },
                    {
                        "name": "Dan Wang"
                    },
                    {
                        "name": "Yuan Du"
                    },
                    {
                        "name": "Shanghang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shanghang Zhang"
                },
                "author": "Shanghang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20384v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20384v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20377v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20377v1",
                "updated": "2025-03-26T09:56:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    9,
                    56,
                    7,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T09:56:07Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    9,
                    56,
                    7,
                    2,
                    85,
                    0
                ],
                "title": "UB-Mesh: a Hierarchically Localized nD-FullMesh Datacenter Network\n  Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UB-Mesh: a Hierarchically Localized nD-FullMesh Datacenter Network\n  Architecture"
                },
                "summary": "As the Large-scale Language Models (LLMs) continue to scale, the requisite\ncomputational power and bandwidth escalate. To address this, we introduce\nUB-Mesh, a novel AI datacenter network architecture designed to enhance\nscalability, performance, cost-efficiency and availability. Unlike traditional\ndatacenters that provide symmetrical node-to-node bandwidth, UB-Mesh employs a\nhierarchically localized nD-FullMesh network topology. This design fully\nleverages the data locality of LLM training, prioritizing short-range, direct\ninterconnects to minimize data movement distance and reduce switch usage.\n  Although UB-Mesh's nD-FullMesh topology offers several theoretical\nadvantages, its concrete architecture design, physical implementation and\nnetworking system optimization present new challenges. For the actual\nconstruction of UB-Mesh, we first design the UB-Mesh-Pod architecture, which is\nbased on a 4D-FullMesh topology. UB-Mesh-Pod is implemented via a suite of\nhardware components that serve as the foundational building blocks, including\nspecifically-designed NPU, CPU, Low-Radix-Switch (LRS), High-Radix-Switch\n(HRS), NICs and others. These components are interconnected via a novel Unified\nBus (UB) technique, which enables flexible IO bandwidth allocation and hardware\nresource pooling. For networking system optimization, we propose advanced\nrouting mechanism named All-Path-Routing (APR) to efficiently manage data\ntraffic. These optimizations, combined with topology-aware performance\nenhancements and robust reliability measures like 64+1 backup design, result in\n2.04x higher cost-efficiency, 7.2% higher network availability compared to\ntraditional Clos architecture and 95%+ linearity in various LLM training tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the Large-scale Language Models (LLMs) continue to scale, the requisite\ncomputational power and bandwidth escalate. To address this, we introduce\nUB-Mesh, a novel AI datacenter network architecture designed to enhance\nscalability, performance, cost-efficiency and availability. Unlike traditional\ndatacenters that provide symmetrical node-to-node bandwidth, UB-Mesh employs a\nhierarchically localized nD-FullMesh network topology. This design fully\nleverages the data locality of LLM training, prioritizing short-range, direct\ninterconnects to minimize data movement distance and reduce switch usage.\n  Although UB-Mesh's nD-FullMesh topology offers several theoretical\nadvantages, its concrete architecture design, physical implementation and\nnetworking system optimization present new challenges. For the actual\nconstruction of UB-Mesh, we first design the UB-Mesh-Pod architecture, which is\nbased on a 4D-FullMesh topology. UB-Mesh-Pod is implemented via a suite of\nhardware components that serve as the foundational building blocks, including\nspecifically-designed NPU, CPU, Low-Radix-Switch (LRS), High-Radix-Switch\n(HRS), NICs and others. These components are interconnected via a novel Unified\nBus (UB) technique, which enables flexible IO bandwidth allocation and hardware\nresource pooling. For networking system optimization, we propose advanced\nrouting mechanism named All-Path-Routing (APR) to efficiently manage data\ntraffic. These optimizations, combined with topology-aware performance\nenhancements and robust reliability measures like 64+1 backup design, result in\n2.04x higher cost-efficiency, 7.2% higher network availability compared to\ntraditional Clos architecture and 95%+ linearity in various LLM training tasks."
                },
                "authors": [
                    {
                        "name": "Heng Liao"
                    },
                    {
                        "name": "Bingyang Liu"
                    },
                    {
                        "name": "Xianping Chen"
                    },
                    {
                        "name": "Zhigang Guo"
                    },
                    {
                        "name": "Chuanning Cheng"
                    },
                    {
                        "name": "Jianbing Wang"
                    },
                    {
                        "name": "Xiangyu Chen"
                    },
                    {
                        "name": "Peng Dong"
                    },
                    {
                        "name": "Rui Meng"
                    },
                    {
                        "name": "Wenjie Liu"
                    },
                    {
                        "name": "Zhe Zhou"
                    },
                    {
                        "name": "Ziyang Zhang"
                    },
                    {
                        "name": "Yuhang Gai"
                    },
                    {
                        "name": "Cunle Qian"
                    },
                    {
                        "name": "Yi Xiong"
                    },
                    {
                        "name": "Zhongwu Cheng"
                    },
                    {
                        "name": "Jing Xia"
                    },
                    {
                        "name": "Yuli Ma"
                    },
                    {
                        "name": "Xi Chen"
                    },
                    {
                        "name": "Wenhua Du"
                    },
                    {
                        "name": "Shizhong Xiao"
                    },
                    {
                        "name": "Chungang Li"
                    },
                    {
                        "name": "Yong Qin"
                    },
                    {
                        "name": "Liudong Xiong"
                    },
                    {
                        "name": "Zhou Yu"
                    },
                    {
                        "name": "Lv Chen"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Buyun Wang"
                    },
                    {
                        "name": "Pei Wu"
                    },
                    {
                        "name": "Junen Gao"
                    },
                    {
                        "name": "Xiaochu Li"
                    },
                    {
                        "name": "Jian He"
                    },
                    {
                        "name": "Shizhuan Yan"
                    },
                    {
                        "name": "Bill McColl"
                    }
                ],
                "author_detail": {
                    "name": "Bill McColl"
                },
                "author": "Bill McColl",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20377v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20377v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20376v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20376v1",
                "updated": "2025-03-26T09:55:00Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    9,
                    55,
                    0,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T09:55:00Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    9,
                    55,
                    0,
                    2,
                    85,
                    0
                ],
                "title": "Dewey Long Context Embedding Model: A Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dewey Long Context Embedding Model: A Technical Report"
                },
                "summary": "This technical report presents the training methodology and evaluation\nresults of the open-source dewey_en_beta embedding model. The increasing demand\nfor retrieval-augmented generation (RAG) systems and the expanding context\nwindow capabilities of large language models (LLMs) have created critical\nchallenges for conventional embedding models. Current approaches often struggle\nto maintain semantic coherence when processing documents exceeding typical\nsequence length limitations, significantly impacting retrieval performance in\nknowledge-intensive applications. This paper presents dewey_en_beta, a novel\ntext embedding model that achieves excellent performance on MTEB (Eng, v2) and\nLongEmbed benchmark while supporting 128K token sequences. Our technical\ncontribution centers on chunk alignment training, an innovative methodology\nthat enables the simultaneous generation of localized chunk embeddings and\nglobal document-level representations through distillation. Information\nregarding the model release can be found at\nhttps://huggingface.co/infgrad/dewey_en_beta.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This technical report presents the training methodology and evaluation\nresults of the open-source dewey_en_beta embedding model. The increasing demand\nfor retrieval-augmented generation (RAG) systems and the expanding context\nwindow capabilities of large language models (LLMs) have created critical\nchallenges for conventional embedding models. Current approaches often struggle\nto maintain semantic coherence when processing documents exceeding typical\nsequence length limitations, significantly impacting retrieval performance in\nknowledge-intensive applications. This paper presents dewey_en_beta, a novel\ntext embedding model that achieves excellent performance on MTEB (Eng, v2) and\nLongEmbed benchmark while supporting 128K token sequences. Our technical\ncontribution centers on chunk alignment training, an innovative methodology\nthat enables the simultaneous generation of localized chunk embeddings and\nglobal document-level representations through distillation. Information\nregarding the model release can be found at\nhttps://huggingface.co/infgrad/dewey_en_beta."
                },
                "authors": [
                    {
                        "name": "Dun Zhang"
                    },
                    {
                        "name": "Panxiang Zou"
                    },
                    {
                        "name": "Yudong Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yudong Zhou"
                },
                "author": "Yudong Zhou",
                "arxiv_comment": "5 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20376v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20376v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20368v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20368v1",
                "updated": "2025-03-26T09:44:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    9,
                    44,
                    40,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T09:44:40Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    9,
                    44,
                    40,
                    2,
                    85,
                    0
                ],
                "title": "Pluggable Style Representation Learning for Multi-Style Transfer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pluggable Style Representation Learning for Multi-Style Transfer"
                },
                "summary": "Due to the high diversity of image styles, the scalability to various styles\nplays a critical role in real-world applications. To accommodate a large amount\nof styles, previous multi-style transfer approaches rely on enlarging the model\nsize while arbitrary-style transfer methods utilize heavy backbones. However,\nthe additional computational cost introduced by more model parameters hinders\nthese methods to be deployed on resource-limited devices. To address this\nchallenge, in this paper, we develop a style transfer framework by decoupling\nthe style modeling and transferring. Specifically, for style modeling, we\npropose a style representation learning scheme to encode the style information\ninto a compact representation. Then, for style transferring, we develop a\nstyle-aware multi-style transfer network (SaMST) to adapt to diverse styles\nusing pluggable style representations. In this way, our framework is able to\naccommodate diverse image styles in the learned style representations without\nintroducing additional overhead during inference, thereby maintaining\nefficiency. Experiments show that our style representation can extract accurate\nstyle information. Moreover, qualitative and quantitative results demonstrate\nthat our method achieves state-of-the-art performance in terms of both accuracy\nand efficiency. The codes are available in\nhttps://github.com/The-Learning-And-Vision-Atelier-LAVA/SaMST.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Due to the high diversity of image styles, the scalability to various styles\nplays a critical role in real-world applications. To accommodate a large amount\nof styles, previous multi-style transfer approaches rely on enlarging the model\nsize while arbitrary-style transfer methods utilize heavy backbones. However,\nthe additional computational cost introduced by more model parameters hinders\nthese methods to be deployed on resource-limited devices. To address this\nchallenge, in this paper, we develop a style transfer framework by decoupling\nthe style modeling and transferring. Specifically, for style modeling, we\npropose a style representation learning scheme to encode the style information\ninto a compact representation. Then, for style transferring, we develop a\nstyle-aware multi-style transfer network (SaMST) to adapt to diverse styles\nusing pluggable style representations. In this way, our framework is able to\naccommodate diverse image styles in the learned style representations without\nintroducing additional overhead during inference, thereby maintaining\nefficiency. Experiments show that our style representation can extract accurate\nstyle information. Moreover, qualitative and quantitative results demonstrate\nthat our method achieves state-of-the-art performance in terms of both accuracy\nand efficiency. The codes are available in\nhttps://github.com/The-Learning-And-Vision-Atelier-LAVA/SaMST."
                },
                "authors": [
                    {
                        "name": "Hongda Liu"
                    },
                    {
                        "name": "Longguang Wang"
                    },
                    {
                        "name": "Weijun Guan"
                    },
                    {
                        "name": "Ye Zhang"
                    },
                    {
                        "name": "Yulan Guo"
                    }
                ],
                "author_detail": {
                    "name": "Yulan Guo"
                },
                "author": "Yulan Guo",
                "arxiv_comment": "18 pages, 13 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20368v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20368v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20362v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20362v1",
                "updated": "2025-03-26T09:39:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    9,
                    39,
                    58,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T09:39:58Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    9,
                    39,
                    58,
                    2,
                    85,
                    0
                ],
                "title": "Self-ReS: Self-Reflection in Large Vision-Language Models for Long Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-ReS: Self-Reflection in Large Vision-Language Models for Long Video\n  Understanding"
                },
                "summary": "Large Vision-Language Models (LVLMs) demonstrate remarkable performance in\nshort-video tasks such as video question answering, but struggle in long-video\nunderstanding. The linear frame sampling strategy, conventionally used by\nLVLMs, fails to account for the non-linear distribution of key events in video\ndata, often introducing redundant or irrelevant information in longer contexts\nwhile risking the omission of critical events in shorter ones. To address this,\nwe propose SelfReS, a non-linear spatiotemporal self-reflective sampling method\nthat dynamically selects key video fragments based on user prompts. Unlike\nprior approaches, SelfReS leverages the inherently sparse attention maps of\nLVLMs to define reflection tokens, enabling relevance-aware token selection\nwithout requiring additional training or external modules. Experiments\ndemonstrate that SelfReS can be seamlessly integrated into strong base LVLMs,\nimproving long-video task accuracy and achieving up to 46% faster inference\nspeed within the same GPU memory budget.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Vision-Language Models (LVLMs) demonstrate remarkable performance in\nshort-video tasks such as video question answering, but struggle in long-video\nunderstanding. The linear frame sampling strategy, conventionally used by\nLVLMs, fails to account for the non-linear distribution of key events in video\ndata, often introducing redundant or irrelevant information in longer contexts\nwhile risking the omission of critical events in shorter ones. To address this,\nwe propose SelfReS, a non-linear spatiotemporal self-reflective sampling method\nthat dynamically selects key video fragments based on user prompts. Unlike\nprior approaches, SelfReS leverages the inherently sparse attention maps of\nLVLMs to define reflection tokens, enabling relevance-aware token selection\nwithout requiring additional training or external modules. Experiments\ndemonstrate that SelfReS can be seamlessly integrated into strong base LVLMs,\nimproving long-video task accuracy and achieving up to 46% faster inference\nspeed within the same GPU memory budget."
                },
                "authors": [
                    {
                        "name": "Joao Pereira"
                    },
                    {
                        "name": "Vasco Lopes"
                    },
                    {
                        "name": "David Semedo"
                    },
                    {
                        "name": "Joao Neves"
                    }
                ],
                "author_detail": {
                    "name": "Joao Neves"
                },
                "author": "Joao Neves",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20362v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20362v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20352v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20352v1",
                "updated": "2025-03-26T09:23:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    9,
                    23,
                    8,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T09:23:08Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    9,
                    23,
                    8,
                    2,
                    85,
                    0
                ],
                "title": "GNSS jammer localization and identification with airborne commercial\n  GNSS receivers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GNSS jammer localization and identification with airborne commercial\n  GNSS receivers"
                },
                "summary": "Global Navigation Satellite Systems (GNSS) are fundamental in ubiquitously\nproviding position and time to a wide gamut of systems. Jamming remains a\nrealistic threat in many deployment settings, civilian and tactical.\nSpecifically, in Unmanned Aerial Vehicles (UAVs) sustained denial raises safety\ncritical concerns. This work presents a strategy that allows detection,\nlocalization, and classification both in the frequency and time domain of\ninterference signals harmful to navigation. A high-performance Vertical Take\nOff and Landing (VTOL) UAV with a single antenna and a commercial GNSS receiver\nis used to geolocate and characterize RF emitters at long range, to infer the\nnavigation impairment. Raw IQ baseband snapshots from the GNSS receiver make\nthe application of spectral correlation methods possible without extra\nsoftware-defined radio payload, paving the way to spectrum identification and\nmonitoring in airborne platforms, aiming at RF situational awareness. Live\ntesting at Jammertest, in Norway, with portable, commercially available GNSS\nmulti-band jammers demonstrates the ability to detect, localize, and\ncharacterize harmful interference. Our system pinpointed the position with an\nerror of a few meters of the transmitter and the extent of the affected area at\nlong range, without entering the denied zone. Additionally, further spectral\ncontent extraction is used to accurately identify the jammer frequency,\nbandwidth, and modulation scheme based on spectral correlation techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Global Navigation Satellite Systems (GNSS) are fundamental in ubiquitously\nproviding position and time to a wide gamut of systems. Jamming remains a\nrealistic threat in many deployment settings, civilian and tactical.\nSpecifically, in Unmanned Aerial Vehicles (UAVs) sustained denial raises safety\ncritical concerns. This work presents a strategy that allows detection,\nlocalization, and classification both in the frequency and time domain of\ninterference signals harmful to navigation. A high-performance Vertical Take\nOff and Landing (VTOL) UAV with a single antenna and a commercial GNSS receiver\nis used to geolocate and characterize RF emitters at long range, to infer the\nnavigation impairment. Raw IQ baseband snapshots from the GNSS receiver make\nthe application of spectral correlation methods possible without extra\nsoftware-defined radio payload, paving the way to spectrum identification and\nmonitoring in airborne platforms, aiming at RF situational awareness. Live\ntesting at Jammertest, in Norway, with portable, commercially available GNSS\nmulti-band jammers demonstrates the ability to detect, localize, and\ncharacterize harmful interference. Our system pinpointed the position with an\nerror of a few meters of the transmitter and the extent of the affected area at\nlong range, without entering the denied zone. Additionally, further spectral\ncontent extraction is used to accurately identify the jammer frequency,\nbandwidth, and modulation scheme based on spectral correlation techniques."
                },
                "authors": [
                    {
                        "name": "Marco Spanghero"
                    },
                    {
                        "name": "Filip Geib"
                    },
                    {
                        "name": "Ronny Panier"
                    },
                    {
                        "name": "Panos Papadimitratos"
                    }
                ],
                "author_detail": {
                    "name": "Panos Papadimitratos"
                },
                "author": "Panos Papadimitratos",
                "arxiv_doi": "10.1109/TIFS.2025.3550050",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TIFS.2025.3550050",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.20352v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20352v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "IEEE Transactions on Information Forensics and Security (2025)",
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20349v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20349v1",
                "updated": "2025-03-26T09:20:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    9,
                    20,
                    42,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T09:20:42Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    9,
                    20,
                    42,
                    2,
                    85,
                    0
                ],
                "title": "Consistency Trajectory Matching for One-Step Generative Super-Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Consistency Trajectory Matching for One-Step Generative Super-Resolution"
                },
                "summary": "Current diffusion-based super-resolution (SR) approaches achieve commendable\nperformance at the cost of high inference overhead. Therefore, distillation\ntechniques are utilized to accelerate the multi-step teacher model into\none-step student model. Nevertheless, these methods significantly raise\ntraining costs and constrain the performance of the student model by the\nteacher model. To overcome these tough challenges, we propose Consistency\nTrajectory Matching for Super-Resolution (CTMSR), a distillation-free strategy\nthat is able to generate photo-realistic SR results in one step. Concretely, we\nfirst formulate a Probability Flow Ordinary Differential Equation (PF-ODE)\ntrajectory to establish a deterministic mapping from low-resolution (LR) images\nwith noise to high-resolution (HR) images. Then we apply the Consistency\nTraining (CT) strategy to directly learn the mapping in one step, eliminating\nthe necessity of pre-trained diffusion model. To further enhance the\nperformance and better leverage the ground-truth during the training process,\nwe aim to align the distribution of SR results more closely with that of the\nnatural images. To this end, we propose to minimize the discrepancy between\ntheir respective PF-ODE trajectories from the LR image distribution by our\nmeticulously designed Distribution Trajectory Matching (DTM) loss, resulting in\nimproved realism of our recovered HR images. Comprehensive experimental results\ndemonstrate that the proposed methods can attain comparable or even superior\ncapabilities on both synthetic and real datasets while maintaining minimal\ninference latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current diffusion-based super-resolution (SR) approaches achieve commendable\nperformance at the cost of high inference overhead. Therefore, distillation\ntechniques are utilized to accelerate the multi-step teacher model into\none-step student model. Nevertheless, these methods significantly raise\ntraining costs and constrain the performance of the student model by the\nteacher model. To overcome these tough challenges, we propose Consistency\nTrajectory Matching for Super-Resolution (CTMSR), a distillation-free strategy\nthat is able to generate photo-realistic SR results in one step. Concretely, we\nfirst formulate a Probability Flow Ordinary Differential Equation (PF-ODE)\ntrajectory to establish a deterministic mapping from low-resolution (LR) images\nwith noise to high-resolution (HR) images. Then we apply the Consistency\nTraining (CT) strategy to directly learn the mapping in one step, eliminating\nthe necessity of pre-trained diffusion model. To further enhance the\nperformance and better leverage the ground-truth during the training process,\nwe aim to align the distribution of SR results more closely with that of the\nnatural images. To this end, we propose to minimize the discrepancy between\ntheir respective PF-ODE trajectories from the LR image distribution by our\nmeticulously designed Distribution Trajectory Matching (DTM) loss, resulting in\nimproved realism of our recovered HR images. Comprehensive experimental results\ndemonstrate that the proposed methods can attain comparable or even superior\ncapabilities on both synthetic and real datasets while maintaining minimal\ninference latency."
                },
                "authors": [
                    {
                        "name": "Weiyi You"
                    },
                    {
                        "name": "Mingyang Zhang"
                    },
                    {
                        "name": "Leheng Zhang"
                    },
                    {
                        "name": "Kexuan Shi"
                    },
                    {
                        "name": "Xingyu Zhou"
                    },
                    {
                        "name": "Shuhang Gu"
                    }
                ],
                "author_detail": {
                    "name": "Shuhang Gu"
                },
                "author": "Shuhang Gu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20349v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20349v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13767v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13767v2",
                "updated": "2025-03-26T09:08:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    9,
                    8,
                    40,
                    2,
                    85,
                    0
                ],
                "published": "2025-02-19T14:28:42Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    14,
                    28,
                    42,
                    2,
                    50,
                    0
                ],
                "title": "Agentic AI Software Engineer: Programming with Trust",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic AI Software Engineer: Programming with Trust"
                },
                "summary": "Large Language Models (LLMs) have shown surprising proficiency in generating\ncode snippets, promising to automate large parts of software engineering via\nartificial intelligence (AI). We argue that successfully deploying AI software\nengineers requires a level of trust equal to or even greater than the trust\nestablished by human-driven software engineering practices. The recent trend\ntoward LLM agents offers a path toward integrating the power of LLMs to create\nnew code with the power of analysis tools to increase trust in the code. This\nopinion piece comments on whether LLM agents could dominate software\nengineering workflows in the future and whether the focus of programming will\nshift from programming at scale to programming with trust.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown surprising proficiency in generating\ncode snippets, promising to automate large parts of software engineering via\nartificial intelligence (AI). We argue that successfully deploying AI software\nengineers requires a level of trust equal to or even greater than the trust\nestablished by human-driven software engineering practices. The recent trend\ntoward LLM agents offers a path toward integrating the power of LLMs to create\nnew code with the power of analysis tools to increase trust in the code. This\nopinion piece comments on whether LLM agents could dominate software\nengineering workflows in the future and whether the focus of programming will\nshift from programming at scale to programming with trust."
                },
                "authors": [
                    {
                        "name": "Abhik Roychoudhury"
                    },
                    {
                        "name": "Corina Pasareanu"
                    },
                    {
                        "name": "Michael Pradel"
                    },
                    {
                        "name": "Baishakhi Ray"
                    }
                ],
                "author_detail": {
                    "name": "Baishakhi Ray"
                },
                "author": "Baishakhi Ray",
                "arxiv_comment": "5 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13767v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13767v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.01586v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.01586v2",
                "updated": "2025-03-26T09:00:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    9,
                    0,
                    8,
                    2,
                    85,
                    0
                ],
                "published": "2024-06-03T17:59:23Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    17,
                    59,
                    23,
                    0,
                    155,
                    0
                ],
                "title": "ManiCM: Real-time 3D Diffusion Policy via Consistency Model for Robotic\n  Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ManiCM: Real-time 3D Diffusion Policy via Consistency Model for Robotic\n  Manipulation"
                },
                "summary": "Diffusion models have been verified to be effective in generating complex\ndistributions from natural images to motion trajectories. Recent\ndiffusion-based methods show impressive performance in 3D robotic manipulation\ntasks, whereas they suffer from severe runtime inefficiency due to multiple\ndenoising steps, especially with high-dimensional observations. To this end, we\npropose a real-time robotic manipulation model named ManiCM that imposes the\nconsistency constraint to the diffusion process, so that the model can generate\nrobot actions in only one-step inference. Specifically, we formulate a\nconsistent diffusion process in the robot action space conditioned on the point\ncloud input, where the original action is required to be directly denoised from\nany point along the ODE trajectory. To model this process, we design a\nconsistency distillation technique to predict the action sample directly\ninstead of predicting the noise within the vision community for fast\nconvergence in the low-dimensional action manifold. We evaluate ManiCM on 31\nrobotic manipulation tasks from Adroit and Metaworld, and the results\ndemonstrate that our approach accelerates the state-of-the-art method by 10\ntimes in average inference speed while maintaining competitive average success\nrate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have been verified to be effective in generating complex\ndistributions from natural images to motion trajectories. Recent\ndiffusion-based methods show impressive performance in 3D robotic manipulation\ntasks, whereas they suffer from severe runtime inefficiency due to multiple\ndenoising steps, especially with high-dimensional observations. To this end, we\npropose a real-time robotic manipulation model named ManiCM that imposes the\nconsistency constraint to the diffusion process, so that the model can generate\nrobot actions in only one-step inference. Specifically, we formulate a\nconsistent diffusion process in the robot action space conditioned on the point\ncloud input, where the original action is required to be directly denoised from\nany point along the ODE trajectory. To model this process, we design a\nconsistency distillation technique to predict the action sample directly\ninstead of predicting the noise within the vision community for fast\nconvergence in the low-dimensional action manifold. We evaluate ManiCM on 31\nrobotic manipulation tasks from Adroit and Metaworld, and the results\ndemonstrate that our approach accelerates the state-of-the-art method by 10\ntimes in average inference speed while maintaining competitive average success\nrate."
                },
                "authors": [
                    {
                        "name": "Guanxing Lu"
                    },
                    {
                        "name": "Zifeng Gao"
                    },
                    {
                        "name": "Tianxing Chen"
                    },
                    {
                        "name": "Wenxun Dai"
                    },
                    {
                        "name": "Ziwei Wang"
                    },
                    {
                        "name": "Wenbo Ding"
                    },
                    {
                        "name": "Yansong Tang"
                    }
                ],
                "author_detail": {
                    "name": "Yansong Tang"
                },
                "author": "Yansong Tang",
                "arxiv_comment": "https://manicm-fast.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.01586v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.01586v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00634v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00634v2",
                "updated": "2025-03-26T08:56:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    8,
                    56,
                    4,
                    2,
                    85,
                    0
                ],
                "published": "2024-11-01T14:45:34Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    14,
                    45,
                    34,
                    4,
                    306,
                    0
                ],
                "title": "Does GenAI Make Usability Testing Obsolete?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does GenAI Make Usability Testing Obsolete?"
                },
                "summary": "Ensuring usability is crucial for the success of mobile apps. Usability\nissues can compromise user experience and negatively impact the perceived app\nquality. This paper presents UX-LLM, a novel tool powered by a Large\nVision-Language Model that predicts usability issues in iOS apps. To evaluate\nthe performance of UX-LLM, we predicted usability issues in two open-source\napps of a medium complexity and asked two usability experts to assess the\npredictions. We also performed traditional usability testing and expert review\nfor both apps and compared the results to those of UX-LLM. UX-LLM demonstrated\nprecision ranging from 0.61 and 0.66 and recall between 0.35 and 0.38,\nindicating its ability to identify valid usability issues, yet failing to\ncapture the majority of issues. Finally, we conducted a focus group with an app\ndevelopment team of a capstone project developing a transit app for visually\nimpaired persons. The focus group expressed positive perceptions of UX-LLM as\nit identified unknown usability issues in their app. However, they also raised\nconcerns about its integration into the development workflow, suggesting\npotential improvements. Our results show that UX-LLM cannot fully replace\ntraditional usability evaluation methods but serves as a valuable supplement\nparticularly for small teams with limited resources, to identify issues in less\ncommon user paths, due to its ability to inspect the source code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring usability is crucial for the success of mobile apps. Usability\nissues can compromise user experience and negatively impact the perceived app\nquality. This paper presents UX-LLM, a novel tool powered by a Large\nVision-Language Model that predicts usability issues in iOS apps. To evaluate\nthe performance of UX-LLM, we predicted usability issues in two open-source\napps of a medium complexity and asked two usability experts to assess the\npredictions. We also performed traditional usability testing and expert review\nfor both apps and compared the results to those of UX-LLM. UX-LLM demonstrated\nprecision ranging from 0.61 and 0.66 and recall between 0.35 and 0.38,\nindicating its ability to identify valid usability issues, yet failing to\ncapture the majority of issues. Finally, we conducted a focus group with an app\ndevelopment team of a capstone project developing a transit app for visually\nimpaired persons. The focus group expressed positive perceptions of UX-LLM as\nit identified unknown usability issues in their app. However, they also raised\nconcerns about its integration into the development workflow, suggesting\npotential improvements. Our results show that UX-LLM cannot fully replace\ntraditional usability evaluation methods but serves as a valuable supplement\nparticularly for small teams with limited resources, to identify issues in less\ncommon user paths, due to its ability to inspect the source code."
                },
                "authors": [
                    {
                        "name": "Ali Ebrahimi Pourasad"
                    },
                    {
                        "name": "Walid Maalej"
                    }
                ],
                "author_detail": {
                    "name": "Walid Maalej"
                },
                "author": "Walid Maalej",
                "arxiv_comment": "Accepted for publication at The 47th IEEE/ACM International\n  Conference on Software Engineering ICSE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00634v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00634v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.18205v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.18205v5",
                "updated": "2025-03-26T08:55:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    8,
                    55,
                    5,
                    2,
                    85,
                    0
                ],
                "published": "2024-02-28T09:51:55Z",
                "published_parsed": [
                    2024,
                    2,
                    28,
                    9,
                    51,
                    55,
                    2,
                    59,
                    0
                ],
                "title": "Lemur: Log Parsing with Entropy Sampling and Chain-of-Thought Merging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lemur: Log Parsing with Entropy Sampling and Chain-of-Thought Merging"
                },
                "summary": "Logs produced by extensive software systems are integral to monitoring system\nbehaviors. Advanced log analysis facilitates the detection, alerting, and\ndiagnosis of system faults. Log parsing, which entails transforming raw log\nmessages into structured templates, constitutes a critical phase in the\nautomation of log analytics. Existing log parsers fail to identify the correct\ntemplates due to reliance on human-made rules. Besides, these methods focus on\nstatistical features while ignoring semantic information in log messages. To\naddress these challenges, we introduce a cutting-edge \\textbf{L}og parsing\nframework with \\textbf{E}ntropy sampling and chain-of-thought \\textbf{M}erging\n(\\model{}). Specifically, to discard the tedious manual rules, we propose a\nnovel sampling method inspired by information entropy, which efficiently\nclusters typical logs. Furthermore, to enhance the merging of log templates, we\ndesign a chain-of-thought method for large language models (LLMs). LLMs exhibit\nexceptional semantic comprehension and deftly distinguish between parameters\nand invariant tokens. We have conducted experiments on large-scale public\ndatasets. Extensive evaluation demonstrates that \\model{} achieves\nstate-of-the-art performance and impressive efficiency. The Code is available\nat https://github.com/zwpride/lemur.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Logs produced by extensive software systems are integral to monitoring system\nbehaviors. Advanced log analysis facilitates the detection, alerting, and\ndiagnosis of system faults. Log parsing, which entails transforming raw log\nmessages into structured templates, constitutes a critical phase in the\nautomation of log analytics. Existing log parsers fail to identify the correct\ntemplates due to reliance on human-made rules. Besides, these methods focus on\nstatistical features while ignoring semantic information in log messages. To\naddress these challenges, we introduce a cutting-edge \\textbf{L}og parsing\nframework with \\textbf{E}ntropy sampling and chain-of-thought \\textbf{M}erging\n(\\model{}). Specifically, to discard the tedious manual rules, we propose a\nnovel sampling method inspired by information entropy, which efficiently\nclusters typical logs. Furthermore, to enhance the merging of log templates, we\ndesign a chain-of-thought method for large language models (LLMs). LLMs exhibit\nexceptional semantic comprehension and deftly distinguish between parameters\nand invariant tokens. We have conducted experiments on large-scale public\ndatasets. Extensive evaluation demonstrates that \\model{} achieves\nstate-of-the-art performance and impressive efficiency. The Code is available\nat https://github.com/zwpride/lemur."
                },
                "authors": [
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Xiangyuan Guan"
                    },
                    {
                        "name": "Lu Yunhong"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Shuangyong Song"
                    },
                    {
                        "name": "Xianfu Cheng"
                    },
                    {
                        "name": "Zhenhe Wu"
                    },
                    {
                        "name": "Zhoujun Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhoujun Li"
                },
                "author": "Zhoujun Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.18205v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.18205v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20320v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20320v1",
                "updated": "2025-03-26T08:40:46Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    8,
                    40,
                    46,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T08:40:46Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    8,
                    40,
                    46,
                    2,
                    85,
                    0
                ],
                "title": "Iterative Prompting with Persuasion Skills in Jailbreaking Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Iterative Prompting with Persuasion Skills in Jailbreaking Large\n  Language Models"
                },
                "summary": "Large language models (LLMs) are designed to align with human values in their\nresponses. This study exploits LLMs with an iterative prompting technique where\neach prompt is systematically modified and refined across multiple iterations\nto enhance its effectiveness in jailbreaking attacks progressively. This\ntechnique involves analyzing the response patterns of LLMs, including GPT-3.5,\nGPT-4, LLaMa2, Vicuna, and ChatGLM, allowing us to adjust and optimize prompts\nto evade the LLMs' ethical and security constraints. Persuasion strategies\nenhance prompt effectiveness while maintaining consistency with malicious\nintent. Our results show that the attack success rates (ASR) increase as the\nattacking prompts become more refined with the highest ASR of 90% for GPT4 and\nChatGLM and the lowest ASR of 68% for LLaMa2. Our technique outperforms\nbaseline techniques (PAIR and PAP) in ASR and shows comparable performance with\nGCG and ArtPrompt.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are designed to align with human values in their\nresponses. This study exploits LLMs with an iterative prompting technique where\neach prompt is systematically modified and refined across multiple iterations\nto enhance its effectiveness in jailbreaking attacks progressively. This\ntechnique involves analyzing the response patterns of LLMs, including GPT-3.5,\nGPT-4, LLaMa2, Vicuna, and ChatGLM, allowing us to adjust and optimize prompts\nto evade the LLMs' ethical and security constraints. Persuasion strategies\nenhance prompt effectiveness while maintaining consistency with malicious\nintent. Our results show that the attack success rates (ASR) increase as the\nattacking prompts become more refined with the highest ASR of 90% for GPT4 and\nChatGLM and the lowest ASR of 68% for LLaMa2. Our technique outperforms\nbaseline techniques (PAIR and PAP) in ASR and shows comparable performance with\nGCG and ArtPrompt."
                },
                "authors": [
                    {
                        "name": "Shih-Wen Ke"
                    },
                    {
                        "name": "Guan-Yu Lai"
                    },
                    {
                        "name": "Guo-Lin Fang"
                    },
                    {
                        "name": "Hsi-Yuan Kao"
                    }
                ],
                "author_detail": {
                    "name": "Hsi-Yuan Kao"
                },
                "author": "Hsi-Yuan Kao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20320v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20320v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2503.20786v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20786v1",
                "updated": "2025-03-26T17:59:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    17,
                    59,
                    56,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T17:59:56Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    17,
                    59,
                    56,
                    2,
                    85,
                    0
                ],
                "title": "Mobile-MMLU: A Mobile Intelligence Language Understanding Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile-MMLU: A Mobile Intelligence Language Understanding Benchmark"
                },
                "summary": "Rapid advancements in large language models (LLMs) have increased interest in\ndeploying them on mobile devices for on-device AI applications. Mobile users\ninteract differently with LLMs compared to desktop users, creating unique\nexpectations and data biases. Current benchmark datasets primarily target at\nserver and desktop environments, and there is a notable lack of extensive\ndatasets specifically designed for mobile contexts. Additionally, mobile\ndevices face strict limitations in storage and computing resources,\nconstraining model size and capabilities, thus requiring optimized efficiency\nand prioritized knowledge. To address these challenges, we introduce\nMobile-MMLU, a large-scale benchmark dataset tailored for mobile intelligence.\nIt consists of 16,186 questions across 80 mobile-related fields, designed to\nevaluate LLM performance in realistic mobile scenarios. A challenging subset,\nMobile-MMLU-Pro, provides advanced evaluation similar in size to MMLU-Pro but\nsignificantly more difficult than our standard full set. Both benchmarks use\nmultiple-choice, order-invariant questions focused on practical mobile\ninteractions, such as recipe suggestions, travel planning, and essential daily\ntasks. The dataset emphasizes critical mobile-specific metrics like inference\nlatency, energy consumption, memory usage, and response quality, offering\ncomprehensive insights into model performance under mobile constraints.\nMoreover, it prioritizes privacy and adaptability, assessing models' ability to\nperform on-device processing, maintain user privacy, and adapt to personalized\nusage patterns. Mobile-MMLU family offers a standardized framework for\ndeveloping and comparing mobile-optimized LLMs, enabling advancements in\nproductivity and decision-making within mobile computing environments. Our code\nand data are available at: https://github.com/VILA-Lab/Mobile-MMLU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid advancements in large language models (LLMs) have increased interest in\ndeploying them on mobile devices for on-device AI applications. Mobile users\ninteract differently with LLMs compared to desktop users, creating unique\nexpectations and data biases. Current benchmark datasets primarily target at\nserver and desktop environments, and there is a notable lack of extensive\ndatasets specifically designed for mobile contexts. Additionally, mobile\ndevices face strict limitations in storage and computing resources,\nconstraining model size and capabilities, thus requiring optimized efficiency\nand prioritized knowledge. To address these challenges, we introduce\nMobile-MMLU, a large-scale benchmark dataset tailored for mobile intelligence.\nIt consists of 16,186 questions across 80 mobile-related fields, designed to\nevaluate LLM performance in realistic mobile scenarios. A challenging subset,\nMobile-MMLU-Pro, provides advanced evaluation similar in size to MMLU-Pro but\nsignificantly more difficult than our standard full set. Both benchmarks use\nmultiple-choice, order-invariant questions focused on practical mobile\ninteractions, such as recipe suggestions, travel planning, and essential daily\ntasks. The dataset emphasizes critical mobile-specific metrics like inference\nlatency, energy consumption, memory usage, and response quality, offering\ncomprehensive insights into model performance under mobile constraints.\nMoreover, it prioritizes privacy and adaptability, assessing models' ability to\nperform on-device processing, maintain user privacy, and adapt to personalized\nusage patterns. Mobile-MMLU family offers a standardized framework for\ndeveloping and comparing mobile-optimized LLMs, enabling advancements in\nproductivity and decision-making within mobile computing environments. Our code\nand data are available at: https://github.com/VILA-Lab/Mobile-MMLU."
                },
                "authors": [
                    {
                        "name": "Sondos Mahmoud Bsharat"
                    },
                    {
                        "name": "Mukul Ranjan"
                    },
                    {
                        "name": "Aidar Myrzakhan"
                    },
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Bowei Guo"
                    },
                    {
                        "name": "Shengkun Tang"
                    },
                    {
                        "name": "Zhuang Liu"
                    },
                    {
                        "name": "Yuanzhi Li"
                    },
                    {
                        "name": "Zhiqiang Shen"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Shen"
                },
                "author": "Zhiqiang Shen",
                "arxiv_comment": "An order-invariant and mobile-centric benchmark. Code and data are\n  available at: https://github.com/VILA-Lab/Mobile-MMLU",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20786v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20786v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20783v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20783v1",
                "updated": "2025-03-26T17:59:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    17,
                    59,
                    14,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T17:59:14Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    17,
                    59,
                    14,
                    2,
                    85,
                    0
                ],
                "title": "Understanding R1-Zero-Like Training: A Critical Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding R1-Zero-Like Training: A Critical Perspective"
                },
                "summary": "DeepSeek-R1-Zero has shown that reinforcement learning (RL) at scale can\ndirectly enhance the reasoning capabilities of LLMs without supervised\nfine-tuning. In this work, we critically examine R1-Zero-like training by\nanalyzing its two core components: base models and RL. We investigate a wide\nrange of base models, including DeepSeek-V3-Base, to understand how pretraining\ncharacteristics influence RL performance. Our analysis reveals that\nDeepSeek-V3-Base already exhibit ''Aha moment'', while Qwen2.5 base models\ndemonstrate strong reasoning capabilities even without prompt templates,\nsuggesting potential pretraining biases. Additionally, we identify an\noptimization bias in Group Relative Policy Optimization (GRPO), which\nartificially increases response length (especially for incorrect outputs)\nduring training. To address this, we introduce Dr. GRPO, an unbiased\noptimization method that improves token efficiency while maintaining reasoning\nperformance. Leveraging these insights, we present a minimalist R1-Zero recipe\nthat achieves 43.3% accuracy on AIME 2024 with a 7B base model, establishing a\nnew state-of-the-art. Our code is available at\nhttps://github.com/sail-sg/understand-r1-zero.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepSeek-R1-Zero has shown that reinforcement learning (RL) at scale can\ndirectly enhance the reasoning capabilities of LLMs without supervised\nfine-tuning. In this work, we critically examine R1-Zero-like training by\nanalyzing its two core components: base models and RL. We investigate a wide\nrange of base models, including DeepSeek-V3-Base, to understand how pretraining\ncharacteristics influence RL performance. Our analysis reveals that\nDeepSeek-V3-Base already exhibit ''Aha moment'', while Qwen2.5 base models\ndemonstrate strong reasoning capabilities even without prompt templates,\nsuggesting potential pretraining biases. Additionally, we identify an\noptimization bias in Group Relative Policy Optimization (GRPO), which\nartificially increases response length (especially for incorrect outputs)\nduring training. To address this, we introduce Dr. GRPO, an unbiased\noptimization method that improves token efficiency while maintaining reasoning\nperformance. Leveraging these insights, we present a minimalist R1-Zero recipe\nthat achieves 43.3% accuracy on AIME 2024 with a 7B base model, establishing a\nnew state-of-the-art. Our code is available at\nhttps://github.com/sail-sg/understand-r1-zero."
                },
                "authors": [
                    {
                        "name": "Zichen Liu"
                    },
                    {
                        "name": "Changyu Chen"
                    },
                    {
                        "name": "Wenjun Li"
                    },
                    {
                        "name": "Penghui Qi"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Wee Sun Lee"
                    },
                    {
                        "name": "Min Lin"
                    }
                ],
                "author_detail": {
                    "name": "Min Lin"
                },
                "author": "Min Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20783v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20783v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20776v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20776v1",
                "updated": "2025-03-26T17:56:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    17,
                    56,
                    16,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T17:56:16Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    17,
                    56,
                    16,
                    2,
                    85,
                    0
                ],
                "title": "Feature4X: Bridging Any Monocular Video to 4D Agentic AI with Versatile\n  Gaussian Feature Fields",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feature4X: Bridging Any Monocular Video to 4D Agentic AI with Versatile\n  Gaussian Feature Fields"
                },
                "summary": "Recent advancements in 2D and multimodal models have achieved remarkable\nsuccess by leveraging large-scale training on extensive datasets. However,\nextending these achievements to enable free-form interactions and high-level\nsemantic operations with complex 3D/4D scenes remains challenging. This\ndifficulty stems from the limited availability of large-scale, annotated 3D/4D\nor multi-view datasets, which are crucial for generalizable vision and language\ntasks such as open-vocabulary and prompt-based segmentation, language-guided\nediting, and visual question answering (VQA). In this paper, we introduce\nFeature4X, a universal framework designed to extend any functionality from 2D\nvision foundation model into the 4D realm, using only monocular video input,\nwhich is widely available from user-generated content. The \"X\" in Feature4X\nrepresents its versatility, enabling any task through adaptable,\nmodel-conditioned 4D feature field distillation. At the core of our framework\nis a dynamic optimization strategy that unifies multiple model capabilities\ninto a single representation. Additionally, to the best of our knowledge,\nFeature4X is the first method to distill and lift the features of video\nfoundation models (e.g. SAM2, InternVideo2) into an explicit 4D feature field\nusing Gaussian Splatting. Our experiments showcase novel view segment anything,\ngeometric and appearance scene editing, and free-form VQA across all time\nsteps, empowered by LLMs in feedback loops. These advancements broaden the\nscope of agentic AI applications by providing a foundation for scalable,\ncontextually and spatiotemporally aware systems capable of immersive dynamic 4D\nscene interaction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in 2D and multimodal models have achieved remarkable\nsuccess by leveraging large-scale training on extensive datasets. However,\nextending these achievements to enable free-form interactions and high-level\nsemantic operations with complex 3D/4D scenes remains challenging. This\ndifficulty stems from the limited availability of large-scale, annotated 3D/4D\nor multi-view datasets, which are crucial for generalizable vision and language\ntasks such as open-vocabulary and prompt-based segmentation, language-guided\nediting, and visual question answering (VQA). In this paper, we introduce\nFeature4X, a universal framework designed to extend any functionality from 2D\nvision foundation model into the 4D realm, using only monocular video input,\nwhich is widely available from user-generated content. The \"X\" in Feature4X\nrepresents its versatility, enabling any task through adaptable,\nmodel-conditioned 4D feature field distillation. At the core of our framework\nis a dynamic optimization strategy that unifies multiple model capabilities\ninto a single representation. Additionally, to the best of our knowledge,\nFeature4X is the first method to distill and lift the features of video\nfoundation models (e.g. SAM2, InternVideo2) into an explicit 4D feature field\nusing Gaussian Splatting. Our experiments showcase novel view segment anything,\ngeometric and appearance scene editing, and free-form VQA across all time\nsteps, empowered by LLMs in feedback loops. These advancements broaden the\nscope of agentic AI applications by providing a foundation for scalable,\ncontextually and spatiotemporally aware systems capable of immersive dynamic 4D\nscene interaction."
                },
                "authors": [
                    {
                        "name": "Shijie Zhou"
                    },
                    {
                        "name": "Hui Ren"
                    },
                    {
                        "name": "Yijia Weng"
                    },
                    {
                        "name": "Shuwang Zhang"
                    },
                    {
                        "name": "Zhen Wang"
                    },
                    {
                        "name": "Dejia Xu"
                    },
                    {
                        "name": "Zhiwen Fan"
                    },
                    {
                        "name": "Suya You"
                    },
                    {
                        "name": "Zhangyang Wang"
                    },
                    {
                        "name": "Leonidas Guibas"
                    },
                    {
                        "name": "Achuta Kadambi"
                    }
                ],
                "author_detail": {
                    "name": "Achuta Kadambi"
                },
                "author": "Achuta Kadambi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20776v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20776v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20771v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20771v1",
                "updated": "2025-03-26T17:53:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    17,
                    53,
                    53,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T17:53:53Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    17,
                    53,
                    53,
                    2,
                    85,
                    0
                ],
                "title": "Disentangled Source-Free Personalization for Facial Expression\n  Recognition with Neutral Target Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disentangled Source-Free Personalization for Facial Expression\n  Recognition with Neutral Target Data"
                },
                "summary": "Facial Expression Recognition (FER) from videos is a crucial task in various\napplication areas, such as human-computer interaction and health monitoring\n(e.g., pain, depression, fatigue, and stress). Beyond the challenges of\nrecognizing subtle emotional or health states, the effectiveness of deep FER\nmodels is often hindered by the considerable variability of expressions among\nsubjects. Source-free domain adaptation (SFDA) methods are employed to adapt a\npre-trained source model using only unlabeled target domain data, thereby\navoiding data privacy and storage issues. Typically, SFDA methods adapt to a\ntarget domain dataset corresponding to an entire population and assume it\nincludes data from all recognition classes. However, collecting such\ncomprehensive target data can be difficult or even impossible for FER in\nhealthcare applications. In many real-world scenarios, it may be feasible to\ncollect a short neutral control video (displaying only neutral expressions) for\ntarget subjects before deployment. These videos can be used to adapt a model to\nbetter handle the variability of expressions among subjects. This paper\nintroduces the Disentangled Source-Free Domain Adaptation (DSFDA) method to\naddress the SFDA challenge posed by missing target expression data. DSFDA\nleverages data from a neutral target control video for end-to-end generation\nand adaptation of target data with missing non-neutral data. Our method learns\nto disentangle features related to expressions and identity while generating\nthe missing non-neutral target data, thereby enhancing model accuracy.\nAdditionally, our self-supervision strategy improves model adaptation by\nreconstructing target images that maintain the same identity and source\nexpression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Facial Expression Recognition (FER) from videos is a crucial task in various\napplication areas, such as human-computer interaction and health monitoring\n(e.g., pain, depression, fatigue, and stress). Beyond the challenges of\nrecognizing subtle emotional or health states, the effectiveness of deep FER\nmodels is often hindered by the considerable variability of expressions among\nsubjects. Source-free domain adaptation (SFDA) methods are employed to adapt a\npre-trained source model using only unlabeled target domain data, thereby\navoiding data privacy and storage issues. Typically, SFDA methods adapt to a\ntarget domain dataset corresponding to an entire population and assume it\nincludes data from all recognition classes. However, collecting such\ncomprehensive target data can be difficult or even impossible for FER in\nhealthcare applications. In many real-world scenarios, it may be feasible to\ncollect a short neutral control video (displaying only neutral expressions) for\ntarget subjects before deployment. These videos can be used to adapt a model to\nbetter handle the variability of expressions among subjects. This paper\nintroduces the Disentangled Source-Free Domain Adaptation (DSFDA) method to\naddress the SFDA challenge posed by missing target expression data. DSFDA\nleverages data from a neutral target control video for end-to-end generation\nand adaptation of target data with missing non-neutral data. Our method learns\nto disentangle features related to expressions and identity while generating\nthe missing non-neutral target data, thereby enhancing model accuracy.\nAdditionally, our self-supervision strategy improves model adaptation by\nreconstructing target images that maintain the same identity and source\nexpression."
                },
                "authors": [
                    {
                        "name": "Masoumeh Sharafi"
                    },
                    {
                        "name": "Emma Ollivier"
                    },
                    {
                        "name": "Muhammad Osama Zeeshan"
                    },
                    {
                        "name": "Soufiane Belharbi"
                    },
                    {
                        "name": "Marco Pedersoli"
                    },
                    {
                        "name": "Alessandro Lameiras Koerich"
                    },
                    {
                        "name": "Simon Bacon"
                    },
                    {
                        "name": "Eric~Granger"
                    }
                ],
                "author_detail": {
                    "name": "Eric~Granger"
                },
                "author": "Eric~Granger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20771v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20771v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20768v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20768v2",
                "updated": "2025-03-27T02:16:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    2,
                    16,
                    6,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-26T17:52:30Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    17,
                    52,
                    30,
                    2,
                    85,
                    0
                ],
                "title": "An Empirical Study of the Impact of Federated Learning on Machine\n  Learning Model Accuracy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Empirical Study of the Impact of Federated Learning on Machine\n  Learning Model Accuracy"
                },
                "summary": "Federated Learning (FL) enables distributed ML model training on private user\ndata at the global scale. Despite the potential of FL demonstrated in many\ndomains, an in-depth view of its impact on model accuracy remains unclear. In\nthis paper, we investigate, systematically, how this learning paradigm can\naffect the accuracy of state-of-the-art ML models for a variety of ML tasks. We\npresent an empirical study that involves various data types: text, image,\naudio, and video, and FL configuration knobs: data distribution, FL scale,\nclient sampling, and local and global computations. Our experiments are\nconducted in a unified FL framework to achieve high fidelity, with substantial\nhuman efforts and resource investments. Based on the results, we perform a\nquantitative analysis of the impact of FL, and highlight challenging scenarios\nwhere applying FL degrades the accuracy of the model drastically and identify\ncases where the impact is negligible. The detailed and extensive findings can\nbenefit practical deployments and future development of FL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) enables distributed ML model training on private user\ndata at the global scale. Despite the potential of FL demonstrated in many\ndomains, an in-depth view of its impact on model accuracy remains unclear. In\nthis paper, we investigate, systematically, how this learning paradigm can\naffect the accuracy of state-of-the-art ML models for a variety of ML tasks. We\npresent an empirical study that involves various data types: text, image,\naudio, and video, and FL configuration knobs: data distribution, FL scale,\nclient sampling, and local and global computations. Our experiments are\nconducted in a unified FL framework to achieve high fidelity, with substantial\nhuman efforts and resource investments. Based on the results, we perform a\nquantitative analysis of the impact of FL, and highlight challenging scenarios\nwhere applying FL degrades the accuracy of the model drastically and identify\ncases where the impact is negligible. The detailed and extensive findings can\nbenefit practical deployments and future development of FL."
                },
                "authors": [
                    {
                        "name": "Haotian Yang"
                    },
                    {
                        "name": "Zhuoran Wang"
                    },
                    {
                        "name": "Benson Chou"
                    },
                    {
                        "name": "Sophie Xu"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Jingxian Wang"
                    },
                    {
                        "name": "Qizhen Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Qizhen Zhang"
                },
                "author": "Qizhen Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20768v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20768v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.2.4; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16974v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16974v2",
                "updated": "2025-03-26T17:48:00Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    17,
                    48,
                    0,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-21T09:43:37Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    9,
                    43,
                    37,
                    4,
                    80,
                    0
                ],
                "title": "Assessing Consistency and Reproducibility in the Outputs of Large\n  Language Models: Evidence Across Diverse Finance and Accounting Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing Consistency and Reproducibility in the Outputs of Large\n  Language Models: Evidence Across Diverse Finance and Accounting Tasks"
                },
                "summary": "This study provides the first comprehensive assessment of consistency and\nreproducibility in Large Language Model (LLM) outputs in finance and accounting\nresearch. We evaluate how consistently LLMs produce outputs given identical\ninputs through extensive experimentation with 50 independent runs across five\ncommon tasks: classification, sentiment analysis, summarization, text\ngeneration, and prediction. Using three OpenAI models (GPT-3.5-turbo,\nGPT-4o-mini, and GPT-4o), we generate over 3.4 million outputs from diverse\nfinancial source texts and data, covering MD&As, FOMC statements, finance news\narticles, earnings call transcripts, and financial statements. Our findings\nreveal substantial but task-dependent consistency, with binary classification\nand sentiment analysis achieving near-perfect reproducibility, while complex\ntasks show greater variability. More advanced models do not consistently\ndemonstrate better consistency and reproducibility, with task-specific patterns\nemerging. LLMs significantly outperform expert human annotators in consistency\nand maintain high agreement even where human experts significantly disagree. We\nfurther find that simple aggregation strategies across 3-5 runs dramatically\nimprove consistency. We also find that aggregation may come with an additional\nbenefit of improved accuracy for sentiment analysis when using newer models.\nSimulation analysis reveals that despite measurable inconsistency in LLM\noutputs, downstream statistical inferences remain remarkably robust. These\nfindings address concerns about what we term \"G-hacking,\" the selective\nreporting of favorable outcomes from multiple Generative AI runs, by\ndemonstrating that such risks are relatively low for finance and accounting\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study provides the first comprehensive assessment of consistency and\nreproducibility in Large Language Model (LLM) outputs in finance and accounting\nresearch. We evaluate how consistently LLMs produce outputs given identical\ninputs through extensive experimentation with 50 independent runs across five\ncommon tasks: classification, sentiment analysis, summarization, text\ngeneration, and prediction. Using three OpenAI models (GPT-3.5-turbo,\nGPT-4o-mini, and GPT-4o), we generate over 3.4 million outputs from diverse\nfinancial source texts and data, covering MD&As, FOMC statements, finance news\narticles, earnings call transcripts, and financial statements. Our findings\nreveal substantial but task-dependent consistency, with binary classification\nand sentiment analysis achieving near-perfect reproducibility, while complex\ntasks show greater variability. More advanced models do not consistently\ndemonstrate better consistency and reproducibility, with task-specific patterns\nemerging. LLMs significantly outperform expert human annotators in consistency\nand maintain high agreement even where human experts significantly disagree. We\nfurther find that simple aggregation strategies across 3-5 runs dramatically\nimprove consistency. We also find that aggregation may come with an additional\nbenefit of improved accuracy for sentiment analysis when using newer models.\nSimulation analysis reveals that despite measurable inconsistency in LLM\noutputs, downstream statistical inferences remain remarkably robust. These\nfindings address concerns about what we term \"G-hacking,\" the selective\nreporting of favorable outcomes from multiple Generative AI runs, by\ndemonstrating that such risks are relatively low for finance and accounting\ntasks."
                },
                "authors": [
                    {
                        "name": "Julian Junyan Wang"
                    },
                    {
                        "name": "Victor Xiaoqi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Victor Xiaoqi Wang"
                },
                "author": "Victor Xiaoqi Wang",
                "arxiv_comment": "97 pages, 20 tables, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16974v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16974v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20757v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20757v1",
                "updated": "2025-03-26T17:46:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    17,
                    46,
                    8,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T17:46:08Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    17,
                    46,
                    8,
                    2,
                    85,
                    0
                ],
                "title": "MCTS-RAG: Enhancing Retrieval-Augmented Generation with Monte Carlo Tree\n  Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MCTS-RAG: Enhancing Retrieval-Augmented Generation with Monte Carlo Tree\n  Search"
                },
                "summary": "We introduce MCTS-RAG, a novel approach that enhances the reasoning\ncapabilities of small language models on knowledge-intensive tasks by\nleveraging retrieval-augmented generation (RAG) to provide relevant context and\nMonte Carlo Tree Search (MCTS) to refine reasoning paths. MCTS-RAG dynamically\nintegrates retrieval and reasoning through an iterative decision-making\nprocess. Unlike standard RAG methods, which typically retrieve information\nindependently from reasoning and thus integrate knowledge suboptimally, or\nconventional MCTS reasoning, which depends solely on internal model knowledge\nwithout external facts, MCTS-RAG combines structured reasoning with adaptive\nretrieval. This integrated approach enhances decision-making, reduces\nhallucinations, and ensures improved factual accuracy and response consistency.\nThe experimental results on multiple reasoning and knowledge-intensive datasets\ndatasets (i.e., ComplexWebQA, GPQA, and FoolMeTwice) show that our method\nenables small-scale LMs to achieve performance comparable to frontier LLMs like\nGPT-4o by effectively scaling inference-time compute, setting a new standard\nfor reasoning in small-scale models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce MCTS-RAG, a novel approach that enhances the reasoning\ncapabilities of small language models on knowledge-intensive tasks by\nleveraging retrieval-augmented generation (RAG) to provide relevant context and\nMonte Carlo Tree Search (MCTS) to refine reasoning paths. MCTS-RAG dynamically\nintegrates retrieval and reasoning through an iterative decision-making\nprocess. Unlike standard RAG methods, which typically retrieve information\nindependently from reasoning and thus integrate knowledge suboptimally, or\nconventional MCTS reasoning, which depends solely on internal model knowledge\nwithout external facts, MCTS-RAG combines structured reasoning with adaptive\nretrieval. This integrated approach enhances decision-making, reduces\nhallucinations, and ensures improved factual accuracy and response consistency.\nThe experimental results on multiple reasoning and knowledge-intensive datasets\ndatasets (i.e., ComplexWebQA, GPQA, and FoolMeTwice) show that our method\nenables small-scale LMs to achieve performance comparable to frontier LLMs like\nGPT-4o by effectively scaling inference-time compute, setting a new standard\nfor reasoning in small-scale models."
                },
                "authors": [
                    {
                        "name": "Yunhai Hu"
                    },
                    {
                        "name": "Yilun Zhao"
                    },
                    {
                        "name": "Chen Zhao"
                    },
                    {
                        "name": "Arman Cohan"
                    }
                ],
                "author_detail": {
                    "name": "Arman Cohan"
                },
                "author": "Arman Cohan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20757v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20757v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11049v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11049v4",
                "updated": "2025-03-26T17:42:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    17,
                    42,
                    17,
                    2,
                    85,
                    0
                ],
                "published": "2024-08-20T17:57:31Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    57,
                    31,
                    1,
                    233,
                    0
                ],
                "title": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding"
                },
                "summary": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency losslessly, but the conventional wisdom suggests\nthat its efficacy is limited to small batch sizes. In MagicDec, we show that\nsurprisingly SD can achieve speedup even for a high throughput inference regime\nfor moderate to long sequences. More interestingly, an intelligent drafting\nstrategy can achieve better speedup with increasing batch size based on our\nrigorous analysis. MagicDec first identifies the bottleneck shifts with\nincreasing batch size and sequence length, and uses these insights to deploy SD\nmore effectively for high throughput inference. We leverage draft model with\nsparse KV cache to address the KV bottleneck, which scales with both sequence\nlength and batch size. Additionally, we propose a theoretical model to select\nthe optimal drafting strategy for maximum speedup. Our work highlights the\nbroad applicability of speculative decoding in long-context serving, as it can\nenhance throughput and reduce latency without compromising accuracy. For\nmoderate to long sequences, we demonstrate up to 2.51x speedup for Llama3.1-8B\nwhen serving batch sizes ranging from 32 to 256 on various types of hardware\nand tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency losslessly, but the conventional wisdom suggests\nthat its efficacy is limited to small batch sizes. In MagicDec, we show that\nsurprisingly SD can achieve speedup even for a high throughput inference regime\nfor moderate to long sequences. More interestingly, an intelligent drafting\nstrategy can achieve better speedup with increasing batch size based on our\nrigorous analysis. MagicDec first identifies the bottleneck shifts with\nincreasing batch size and sequence length, and uses these insights to deploy SD\nmore effectively for high throughput inference. We leverage draft model with\nsparse KV cache to address the KV bottleneck, which scales with both sequence\nlength and batch size. Additionally, we propose a theoretical model to select\nthe optimal drafting strategy for maximum speedup. Our work highlights the\nbroad applicability of speculative decoding in long-context serving, as it can\nenhance throughput and reduce latency without compromising accuracy. For\nmoderate to long sequences, we demonstrate up to 2.51x speedup for Llama3.1-8B\nwhen serving batch sizes ranging from 32 to 256 on various types of hardware\nand tasks."
                },
                "authors": [
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Jian Chen"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Vashisth Tiwari"
                    },
                    {
                        "name": "Ruihang Lai"
                    },
                    {
                        "name": "Jinyuan Shi"
                    },
                    {
                        "name": "Ian En-Hsu Yen"
                    },
                    {
                        "name": "Avner May"
                    },
                    {
                        "name": "Tianqi Chen"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11049v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11049v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20749v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20749v2",
                "updated": "2025-03-27T02:42:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    2,
                    42,
                    3,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-26T17:33:27Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    17,
                    33,
                    27,
                    2,
                    85,
                    0
                ],
                "title": "Beyond Believability: Accurate Human Behavior Simulation with Fine-Tuned\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Believability: Accurate Human Behavior Simulation with Fine-Tuned\n  LLMs"
                },
                "summary": "Recent research shows that LLMs can simulate ``believable'' human behaviors\nto power LLM agents via prompt-only methods. In this work, we focus on\nevaluating and improving LLM's objective ``accuracy'' rather than the\nsubjective ``believability'' in the web action generation task, leveraging a\nlarge-scale, real-world dataset collected from online shopping human actions.\nWe present the first comprehensive quantitative evaluation of state-of-the-art\nLLMs (e.g., DeepSeek-R1, Llama, and Claude) on the task of web action\ngeneration. Our results show that fine-tuning LLMs on real-world behavioral\ndata substantially improves their ability to generate actions compared to\nprompt-only methods. Furthermore, incorporating synthesized reasoning traces\ninto model training leads to additional performance gains, demonstrating the\nvalue of explicit rationale in behavior modeling. This work establishes a new\nbenchmark for evaluating LLMs in behavior simulation and offers actionable\ninsights into how real-world action data and reasoning augmentation can enhance\nthe fidelity of LLM agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research shows that LLMs can simulate ``believable'' human behaviors\nto power LLM agents via prompt-only methods. In this work, we focus on\nevaluating and improving LLM's objective ``accuracy'' rather than the\nsubjective ``believability'' in the web action generation task, leveraging a\nlarge-scale, real-world dataset collected from online shopping human actions.\nWe present the first comprehensive quantitative evaluation of state-of-the-art\nLLMs (e.g., DeepSeek-R1, Llama, and Claude) on the task of web action\ngeneration. Our results show that fine-tuning LLMs on real-world behavioral\ndata substantially improves their ability to generate actions compared to\nprompt-only methods. Furthermore, incorporating synthesized reasoning traces\ninto model training leads to additional performance gains, demonstrating the\nvalue of explicit rationale in behavior modeling. This work establishes a new\nbenchmark for evaluating LLMs in behavior simulation and offers actionable\ninsights into how real-world action data and reasoning augmentation can enhance\nthe fidelity of LLM agents."
                },
                "authors": [
                    {
                        "name": "Yuxuan Lu"
                    },
                    {
                        "name": "Jing Huang"
                    },
                    {
                        "name": "Yan Han"
                    },
                    {
                        "name": "Bennet Bei"
                    },
                    {
                        "name": "Yaochen Xie"
                    },
                    {
                        "name": "Dakuo Wang"
                    },
                    {
                        "name": "Jessie Wang"
                    },
                    {
                        "name": "Qi He"
                    }
                ],
                "author_detail": {
                    "name": "Qi He"
                },
                "author": "Qi He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20749v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20749v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20723v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20723v1",
                "updated": "2025-03-26T17:06:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    17,
                    6,
                    53,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T17:06:53Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    17,
                    6,
                    53,
                    2,
                    85,
                    0
                ],
                "title": "Multi-Robot Coordination Under Physical Limitations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Robot Coordination Under Physical Limitations"
                },
                "summary": "Multi-robot coordination is fundamental to various applications, including\nautonomous exploration, search and rescue, and cooperative transportation. This\npaper presents an optimal consensus framework for multi-robot systems (MRSs)\nthat ensures efficient rendezvous while minimizing energy consumption and\naddressing actuator constraints. A critical challenge in real-world deployments\nis actuator limitations, particularly wheel velocity saturation, which can\nsignificantly degrade control performance. To address this issue, we\nincorporate Pontryagin Minimum Principle (PMP) into the control design,\nfacilitating constrained optimization while ensuring system stability and\nfeasibility. The resulting optimal control policy effectively balances\ncoordination efficiency and energy consumption, even in the presence of\nactuation constraints. The proposed framework is validated through extensive\nnumerical simulations and real-world experiments conducted using a team of\nRobotarium mobile robots. The experimental results confirm that our control\nstrategies achieve reliable and efficient coordinated rendezvous while\naddressing real-world challenges such as communication delays, sensor noise,\nand packet loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-robot coordination is fundamental to various applications, including\nautonomous exploration, search and rescue, and cooperative transportation. This\npaper presents an optimal consensus framework for multi-robot systems (MRSs)\nthat ensures efficient rendezvous while minimizing energy consumption and\naddressing actuator constraints. A critical challenge in real-world deployments\nis actuator limitations, particularly wheel velocity saturation, which can\nsignificantly degrade control performance. To address this issue, we\nincorporate Pontryagin Minimum Principle (PMP) into the control design,\nfacilitating constrained optimization while ensuring system stability and\nfeasibility. The resulting optimal control policy effectively balances\ncoordination efficiency and energy consumption, even in the presence of\nactuation constraints. The proposed framework is validated through extensive\nnumerical simulations and real-world experiments conducted using a team of\nRobotarium mobile robots. The experimental results confirm that our control\nstrategies achieve reliable and efficient coordinated rendezvous while\naddressing real-world challenges such as communication delays, sensor noise,\nand packet loss."
                },
                "authors": [
                    {
                        "name": "Tohid Kargar Tasooji"
                    },
                    {
                        "name": "Sakineh Khodadadi"
                    }
                ],
                "author_detail": {
                    "name": "Sakineh Khodadadi"
                },
                "author": "Sakineh Khodadadi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20723v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20723v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20715v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20715v1",
                "updated": "2025-03-26T16:52:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    16,
                    52,
                    40,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T16:52:40Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    16,
                    52,
                    40,
                    2,
                    85,
                    0
                ],
                "title": "From Annotation to Adaptation: Metrics, Synthetic Data, and Aspect\n  Extraction for Aspect-Based Sentiment Analysis with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Annotation to Adaptation: Metrics, Synthetic Data, and Aspect\n  Extraction for Aspect-Based Sentiment Analysis with Large Language Models"
                },
                "summary": "This study examines the performance of Large Language Models (LLMs) in\nAspect-Based Sentiment Analysis (ABSA), with a focus on implicit aspect\nextraction in a novel domain. Using a synthetic sports feedback dataset, we\nevaluate open-weight LLMs' ability to extract aspect-polarity pairs and propose\na metric to facilitate the evaluation of aspect extraction with generative\nmodels. Our findings highlight both the potential and limitations of LLMs in\nthe ABSA task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study examines the performance of Large Language Models (LLMs) in\nAspect-Based Sentiment Analysis (ABSA), with a focus on implicit aspect\nextraction in a novel domain. Using a synthetic sports feedback dataset, we\nevaluate open-weight LLMs' ability to extract aspect-polarity pairs and propose\na metric to facilitate the evaluation of aspect extraction with generative\nmodels. Our findings highlight both the potential and limitations of LLMs in\nthe ABSA task."
                },
                "authors": [
                    {
                        "name": "Nikita Neveditsin"
                    },
                    {
                        "name": "Pawan Lingras"
                    },
                    {
                        "name": "Vijay Mago"
                    }
                ],
                "author_detail": {
                    "name": "Vijay Mago"
                },
                "author": "Vijay Mago",
                "arxiv_comment": "Accepted to NAACL SRW 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20715v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20715v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.10347v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.10347v3",
                "updated": "2025-03-26T16:44:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    16,
                    44,
                    38,
                    2,
                    85,
                    0
                ],
                "published": "2024-05-16T02:00:44Z",
                "published_parsed": [
                    2024,
                    5,
                    16,
                    2,
                    0,
                    44,
                    3,
                    137,
                    0
                ],
                "title": "Networking Systems for Video Anomaly Detection: A Tutorial and Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Networking Systems for Video Anomaly Detection: A Tutorial and Survey"
                },
                "summary": "The increasing utilization of surveillance cameras in smart cities, coupled\nwith the surge of online video applications, has heightened concerns regarding\npublic security and privacy protection, which propelled automated Video Anomaly\nDetection (VAD) into a fundamental research task within the Artificial\nIntelligence (AI) community. With the advancements in deep learning and edge\ncomputing, VAD has made significant progress and advances synergized with\nemerging applications in smart cities and video internet, which has moved\nbeyond the conventional research scope of algorithm engineering to deployable\nNetworking Systems for VAD (NSVAD), a practical hotspot for intersection\nexploration in the AI, IoVT, and computing fields. In this article, we\ndelineate the foundational assumptions, learning frameworks, and applicable\nscenarios of various deep learning-driven VAD routes, offering an exhaustive\ntutorial for novices in NSVAD. In addition, this article elucidates core\nconcepts by reviewing recent advances and typical solutions and aggregating\navailable research resources accessible at https://github.com/fdjingliu/NSVAD.\nLastly, this article projects future development trends and discusses how the\nintegration of AI and computing technologies can address existing research\nchallenges and promote open opportunities, serving as an insightful guide for\nprospective researchers and engineers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing utilization of surveillance cameras in smart cities, coupled\nwith the surge of online video applications, has heightened concerns regarding\npublic security and privacy protection, which propelled automated Video Anomaly\nDetection (VAD) into a fundamental research task within the Artificial\nIntelligence (AI) community. With the advancements in deep learning and edge\ncomputing, VAD has made significant progress and advances synergized with\nemerging applications in smart cities and video internet, which has moved\nbeyond the conventional research scope of algorithm engineering to deployable\nNetworking Systems for VAD (NSVAD), a practical hotspot for intersection\nexploration in the AI, IoVT, and computing fields. In this article, we\ndelineate the foundational assumptions, learning frameworks, and applicable\nscenarios of various deep learning-driven VAD routes, offering an exhaustive\ntutorial for novices in NSVAD. In addition, this article elucidates core\nconcepts by reviewing recent advances and typical solutions and aggregating\navailable research resources accessible at https://github.com/fdjingliu/NSVAD.\nLastly, this article projects future development trends and discusses how the\nintegration of AI and computing technologies can address existing research\nchallenges and promote open opportunities, serving as an insightful guide for\nprospective researchers and engineers."
                },
                "authors": [
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Jieyu Lin"
                    },
                    {
                        "name": "Jielin Li"
                    },
                    {
                        "name": "Liang Cao"
                    },
                    {
                        "name": "Peng Sun"
                    },
                    {
                        "name": "Bo Hu"
                    },
                    {
                        "name": "Liang Song"
                    },
                    {
                        "name": "Azzedine Boukerche"
                    },
                    {
                        "name": "Victor C. M. Leung"
                    }
                ],
                "author_detail": {
                    "name": "Victor C. M. Leung"
                },
                "author": "Victor C. M. Leung",
                "arxiv_comment": "Revised to ACM Computing Surveys, under review, for more information\n  and supplementary material, please see https://github.com/fdjingliu/NSVAD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.10347v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.10347v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20705v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20705v1",
                "updated": "2025-03-26T16:38:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    16,
                    38,
                    43,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T16:38:43Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    16,
                    38,
                    43,
                    2,
                    85,
                    0
                ],
                "title": "Model-free Vehicle Rollover Prevention: A Data-driven Predictive Control\n  Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model-free Vehicle Rollover Prevention: A Data-driven Predictive Control\n  Approach"
                },
                "summary": "Vehicle rollovers pose a significant safety risk and account for a\ndisproportionately high number of fatalities in road accidents. This paper\naddresses the challenge of rollover prevention using Data-EnablEd Predictive\nControl (DeePC), a data-driven control strategy that directly leverages raw\ninput-output data to maintain vehicle stability without requiring explicit\nsystem modeling. To enhance computational efficiency, we employ a\nreduced-dimension DeePC that utilizes singular value decomposition-based\ndimension reduction to significantly lower computation complexity without\ncompromising control performance. This optimization enables real-time\napplication in scenarios with high-dimensional data, making the approach more\npractical for deployment in real-world vehicles. The proposed approach is\nvalidated through high-fidelity CarSim simulations in both sedan and utility\ntruck scenarios, demonstrating its versatility and ability to maintain vehicle\nstability under challenging driving conditions. Comparative results with Linear\nModel Predictive Control (LMPC) highlight the superior performance of DeePC in\npreventing rollovers while preserving maneuverability. The findings suggest\nthat DeePC offers a robust and adaptable solution for rollover prevention,\ncapable of handling varying road and vehicle conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vehicle rollovers pose a significant safety risk and account for a\ndisproportionately high number of fatalities in road accidents. This paper\naddresses the challenge of rollover prevention using Data-EnablEd Predictive\nControl (DeePC), a data-driven control strategy that directly leverages raw\ninput-output data to maintain vehicle stability without requiring explicit\nsystem modeling. To enhance computational efficiency, we employ a\nreduced-dimension DeePC that utilizes singular value decomposition-based\ndimension reduction to significantly lower computation complexity without\ncompromising control performance. This optimization enables real-time\napplication in scenarios with high-dimensional data, making the approach more\npractical for deployment in real-world vehicles. The proposed approach is\nvalidated through high-fidelity CarSim simulations in both sedan and utility\ntruck scenarios, demonstrating its versatility and ability to maintain vehicle\nstability under challenging driving conditions. Comparative results with Linear\nModel Predictive Control (LMPC) highlight the superior performance of DeePC in\npreventing rollovers while preserving maneuverability. The findings suggest\nthat DeePC offers a robust and adaptable solution for rollover prevention,\ncapable of handling varying road and vehicle conditions."
                },
                "authors": [
                    {
                        "name": "Mohammad R. Hajidavalloo"
                    },
                    {
                        "name": "Kaixiang Zhang"
                    },
                    {
                        "name": "Vaibhav Srivastava"
                    },
                    {
                        "name": "Zhaojian Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhaojian Li"
                },
                "author": "Zhaojian Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20705v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20705v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20701v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20701v1",
                "updated": "2025-03-26T16:33:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    16,
                    33,
                    4,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T16:33:04Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    16,
                    33,
                    4,
                    2,
                    85,
                    0
                ],
                "title": "UniEDU: A Unified Language and Vision Assistant for Education\n  Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniEDU: A Unified Language and Vision Assistant for Education\n  Applications"
                },
                "summary": "Education materials for K-12 students often consist of multiple modalities,\nsuch as text and images, posing challenges for models to fully understand\nnuanced information in these materials. In this paper, we propose a unified\nlanguage and vision assistant UniEDU designed for various educational\napplications, including knowledge recommendation, knowledge tracing, time cost\nprediction, and user answer prediction, all within a single model. Unlike\nconventional task-specific models, UniEDU offers a unified solution that excels\nacross multiple educational tasks while maintaining strong generalization\ncapabilities. Its adaptability makes it well-suited for real-world deployment\nin diverse learning environments. Furthermore, UniEDU is optimized for\nindustry-scale deployment by significantly reducing computational\noverhead-achieving approximately a 300\\% increase in efficiency-while\nmaintaining competitive performance with minimal degradation compared to fully\nfine-tuned models. This work represents a significant step toward creating\nversatile AI systems tailored to the evolving demands of education.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Education materials for K-12 students often consist of multiple modalities,\nsuch as text and images, posing challenges for models to fully understand\nnuanced information in these materials. In this paper, we propose a unified\nlanguage and vision assistant UniEDU designed for various educational\napplications, including knowledge recommendation, knowledge tracing, time cost\nprediction, and user answer prediction, all within a single model. Unlike\nconventional task-specific models, UniEDU offers a unified solution that excels\nacross multiple educational tasks while maintaining strong generalization\ncapabilities. Its adaptability makes it well-suited for real-world deployment\nin diverse learning environments. Furthermore, UniEDU is optimized for\nindustry-scale deployment by significantly reducing computational\noverhead-achieving approximately a 300\\% increase in efficiency-while\nmaintaining competitive performance with minimal degradation compared to fully\nfine-tuned models. This work represents a significant step toward creating\nversatile AI systems tailored to the evolving demands of education."
                },
                "authors": [
                    {
                        "name": "Zhendong Chu"
                    },
                    {
                        "name": "Jian Xie"
                    },
                    {
                        "name": "Shen Wang"
                    },
                    {
                        "name": "Zichao Wang"
                    },
                    {
                        "name": "Qingsong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Qingsong Wen"
                },
                "author": "Qingsong Wen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20701v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20701v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20682v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20682v1",
                "updated": "2025-03-26T16:18:25Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    16,
                    18,
                    25,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T16:18:25Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    16,
                    18,
                    25,
                    2,
                    85,
                    0
                ],
                "title": "GLRD: Global-Local Collaborative Reason and Debate with PSL for 3D\n  Open-Vocabulary Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GLRD: Global-Local Collaborative Reason and Debate with PSL for 3D\n  Open-Vocabulary Detection"
                },
                "summary": "The task of LiDAR-based 3D Open-Vocabulary Detection (3D OVD) requires the\ndetector to learn to detect novel objects from point clouds without\noff-the-shelf training labels. Previous methods focus on the learning of\nobject-level representations and ignore the scene-level information, thus it is\nhard to distinguish objects with similar classes. In this work, we propose a\nGlobal-Local Collaborative Reason and Debate with PSL (GLRD) framework for the\n3D OVD task, considering both local object-level information and global\nscene-level information. Specifically, LLM is utilized to perform common sense\nreasoning based on object-level and scene-level information, where the\ndetection result is refined accordingly. To further boost the LLM's ability of\nprecise decisions, we also design a probabilistic soft logic solver (OV-PSL) to\nsearch for the optimal solution, and a debate scheme to confirm the class of\nconfusable objects. In addition, to alleviate the uneven distribution of\nclasses, a static balance scheme (SBC) and a dynamic balance scheme (DBC) are\ndesigned. In addition, to reduce the influence of noise in data and training,\nwe further propose Reflected Pseudo Labels Generation (RPLG) and\nBackground-Aware Object Localization (BAOL). Extensive experiments conducted on\nScanNet and SUN RGB-D demonstrate the superiority of GLRD, where absolute\nimprovements in mean average precision are $+2.82\\%$ on SUN RGB-D and $+3.72\\%$\non ScanNet in the partial open-vocabulary setting. In the full open-vocabulary\nsetting, the absolute improvements in mean average precision are $+4.03\\%$ on\nScanNet and $+14.11\\%$ on SUN RGB-D.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The task of LiDAR-based 3D Open-Vocabulary Detection (3D OVD) requires the\ndetector to learn to detect novel objects from point clouds without\noff-the-shelf training labels. Previous methods focus on the learning of\nobject-level representations and ignore the scene-level information, thus it is\nhard to distinguish objects with similar classes. In this work, we propose a\nGlobal-Local Collaborative Reason and Debate with PSL (GLRD) framework for the\n3D OVD task, considering both local object-level information and global\nscene-level information. Specifically, LLM is utilized to perform common sense\nreasoning based on object-level and scene-level information, where the\ndetection result is refined accordingly. To further boost the LLM's ability of\nprecise decisions, we also design a probabilistic soft logic solver (OV-PSL) to\nsearch for the optimal solution, and a debate scheme to confirm the class of\nconfusable objects. In addition, to alleviate the uneven distribution of\nclasses, a static balance scheme (SBC) and a dynamic balance scheme (DBC) are\ndesigned. In addition, to reduce the influence of noise in data and training,\nwe further propose Reflected Pseudo Labels Generation (RPLG) and\nBackground-Aware Object Localization (BAOL). Extensive experiments conducted on\nScanNet and SUN RGB-D demonstrate the superiority of GLRD, where absolute\nimprovements in mean average precision are $+2.82\\%$ on SUN RGB-D and $+3.72\\%$\non ScanNet in the partial open-vocabulary setting. In the full open-vocabulary\nsetting, the absolute improvements in mean average precision are $+4.03\\%$ on\nScanNet and $+14.11\\%$ on SUN RGB-D."
                },
                "authors": [
                    {
                        "name": "Xingyu Peng"
                    },
                    {
                        "name": "Si Liu"
                    },
                    {
                        "name": "Chen Gao"
                    },
                    {
                        "name": "Yan Bai"
                    },
                    {
                        "name": "Beipeng Mu"
                    },
                    {
                        "name": "Xiaofei Wang"
                    },
                    {
                        "name": "Huaxia Xia"
                    }
                ],
                "author_detail": {
                    "name": "Huaxia Xia"
                },
                "author": "Huaxia Xia",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20682v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20682v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20680v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20680v1",
                "updated": "2025-03-26T16:15:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    16,
                    15,
                    42,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T16:15:42Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    16,
                    15,
                    42,
                    2,
                    85,
                    0
                ],
                "title": "Vision as LoRA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision as LoRA"
                },
                "summary": "We introduce Vision as LoRA (VoRA), a novel paradigm for transforming an LLM\ninto an MLLM. Unlike prevalent MLLM architectures that rely on external vision\nmodules for vision encoding, VoRA internalizes visual capabilities by\nintegrating vision-specific LoRA layers directly into the LLM. This design\nallows the added parameters to be seamlessly merged into the LLM during\ninference, eliminating structural complexity and minimizing computational\noverhead. Moreover, inheriting the LLM's ability of handling flexible context,\nVoRA can process inputs at arbitrary resolutions.\n  To further strengthen VoRA's visual capabilities, we introduce a block-wise\ndistillation method that transfers visual priors from a pre-trained ViT into\nthe LoRA layers, effectively accelerating training by injecting visual\nknowledge. Additionally, we apply bi-directional attention masks to better\ncapture the context information of an image. We successfully demonstrate that\nwith additional pre-training data, VoRA can perform comparably with\nconventional encode-based MLLMs. All training data, codes, and model weights\nwill be released at https://github.com/Hon-Wong/VoRA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Vision as LoRA (VoRA), a novel paradigm for transforming an LLM\ninto an MLLM. Unlike prevalent MLLM architectures that rely on external vision\nmodules for vision encoding, VoRA internalizes visual capabilities by\nintegrating vision-specific LoRA layers directly into the LLM. This design\nallows the added parameters to be seamlessly merged into the LLM during\ninference, eliminating structural complexity and minimizing computational\noverhead. Moreover, inheriting the LLM's ability of handling flexible context,\nVoRA can process inputs at arbitrary resolutions.\n  To further strengthen VoRA's visual capabilities, we introduce a block-wise\ndistillation method that transfers visual priors from a pre-trained ViT into\nthe LoRA layers, effectively accelerating training by injecting visual\nknowledge. Additionally, we apply bi-directional attention masks to better\ncapture the context information of an image. We successfully demonstrate that\nwith additional pre-training data, VoRA can perform comparably with\nconventional encode-based MLLMs. All training data, codes, and model weights\nwill be released at https://github.com/Hon-Wong/VoRA."
                },
                "authors": [
                    {
                        "name": "Han Wang"
                    },
                    {
                        "name": "Yongjie Ye"
                    },
                    {
                        "name": "Bingru Li"
                    },
                    {
                        "name": "Yuxiang Nie"
                    },
                    {
                        "name": "Jinghui Lu"
                    },
                    {
                        "name": "Jingqun Tang"
                    },
                    {
                        "name": "Yanjie Wang"
                    },
                    {
                        "name": "Can Huang"
                    }
                ],
                "author_detail": {
                    "name": "Can Huang"
                },
                "author": "Can Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20680v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20680v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20666v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20666v1",
                "updated": "2025-03-26T15:58:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    15,
                    58,
                    16,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T15:58:16Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    15,
                    58,
                    16,
                    2,
                    85,
                    0
                ],
                "title": "TAMA: A Human-AI Collaborative Thematic Analysis Framework Using\n  Multi-Agent LLMs for Clinical Interviews",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TAMA: A Human-AI Collaborative Thematic Analysis Framework Using\n  Multi-Agent LLMs for Clinical Interviews"
                },
                "summary": "Thematic analysis (TA) is a widely used qualitative approach for uncovering\nlatent meanings in unstructured text data. TA provides valuable insights in\nhealthcare but is resource-intensive. Large Language Models (LLMs) have been\nintroduced to perform TA, yet their applications in healthcare remain\nunexplored. Here, we propose TAMA: A Human-AI Collaborative Thematic Analysis\nframework using Multi-Agent LLMs for clinical interviews. We leverage the\nscalability and coherence of multi-agent systems through structured\nconversations between agents and coordinate the expertise of cardiac experts in\nTA. Using interview transcripts from parents of children with Anomalous Aortic\nOrigin of a Coronary Artery (AAOCA), a rare congenital heart disease, we\ndemonstrate that TAMA outperforms existing LLM-assisted TA approaches,\nachieving higher thematic hit rate, coverage, and distinctiveness. TAMA\ndemonstrates strong potential for automated TA in clinical settings by\nleveraging multi-agent LLM systems with human-in-the-loop integration by\nenhancing quality while significantly reducing manual workload.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thematic analysis (TA) is a widely used qualitative approach for uncovering\nlatent meanings in unstructured text data. TA provides valuable insights in\nhealthcare but is resource-intensive. Large Language Models (LLMs) have been\nintroduced to perform TA, yet their applications in healthcare remain\nunexplored. Here, we propose TAMA: A Human-AI Collaborative Thematic Analysis\nframework using Multi-Agent LLMs for clinical interviews. We leverage the\nscalability and coherence of multi-agent systems through structured\nconversations between agents and coordinate the expertise of cardiac experts in\nTA. Using interview transcripts from parents of children with Anomalous Aortic\nOrigin of a Coronary Artery (AAOCA), a rare congenital heart disease, we\ndemonstrate that TAMA outperforms existing LLM-assisted TA approaches,\nachieving higher thematic hit rate, coverage, and distinctiveness. TAMA\ndemonstrates strong potential for automated TA in clinical settings by\nleveraging multi-agent LLM systems with human-in-the-loop integration by\nenhancing quality while significantly reducing manual workload."
                },
                "authors": [
                    {
                        "name": "Huimin Xu"
                    },
                    {
                        "name": "Seungjun Yi"
                    },
                    {
                        "name": "Terence Lim"
                    },
                    {
                        "name": "Jiawei Xu"
                    },
                    {
                        "name": "Andrew Well"
                    },
                    {
                        "name": "Carlos Mery"
                    },
                    {
                        "name": "Aidong Zhang"
                    },
                    {
                        "name": "Yuji Zhang"
                    },
                    {
                        "name": "Heng Ji"
                    },
                    {
                        "name": "Keshav Pingali"
                    },
                    {
                        "name": "Yan Leng"
                    },
                    {
                        "name": "Ying Ding"
                    }
                ],
                "author_detail": {
                    "name": "Ying Ding"
                },
                "author": "Ying Ding",
                "arxiv_comment": "Submitted to the American Medical Informatics Association (AMIA) 2025\n  Annual Symposium, 10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20666v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20666v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10013v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10013v2",
                "updated": "2025-03-26T15:56:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    15,
                    56,
                    31,
                    2,
                    85,
                    0
                ],
                "published": "2025-02-14T08:47:10Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    8,
                    47,
                    10,
                    4,
                    45,
                    0
                ],
                "title": "Probabilistic Lexical Manifold Construction in Large Language Models via\n  Hierarchical Vector Field Interpolation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probabilistic Lexical Manifold Construction in Large Language Models via\n  Hierarchical Vector Field Interpolation"
                },
                "summary": "Hierarchical vector field interpolation introduces a structured probabilistic\nframework for lexical representation, ensuring that word embeddings transition\nsmoothly across a continuous manifold rather than being constrained to discrete\ntoken mappings. The proposed methodology constructs a probabilistic function\nspace where word representations adhere to topological consistency, mitigating\nrepresentational discontinuities commonly observed in transformer-based\nembeddings. Empirical evaluations reveal that probabilistic constraints enhance\nlexical coherence by refining contextual relationships, leading to improvements\nin semantic stability across multiple linguistic distributions. The application\nof divergence minimization techniques ensures that interpolated embeddings\nmaintain probabilistic consistency while preserving computational feasibility\nfor large-scale implementations. Experimental findings demonstrate that\ninterpolated lexical manifolds improve representation density alignment,\nreducing anisotropic distortions in contextual embedding distributions.\nComparative analyses with standard transformer-based models highlight that\nstructured interpolation yields more stable representations, particularly in\ntasks requiring fine-grained semantic differentiation. The statistical\nevaluation of embedding divergence confirms that probabilistic lexical\nmanifolds reduce representational inconsistencies while maintaining coherence\nacross varying scales of contextual abstraction. An assessment of computational\nefficiency reveals that while interpolation introduces minor processing\noverhead, the structured representation learning approach remains scalable for\npractical deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical vector field interpolation introduces a structured probabilistic\nframework for lexical representation, ensuring that word embeddings transition\nsmoothly across a continuous manifold rather than being constrained to discrete\ntoken mappings. The proposed methodology constructs a probabilistic function\nspace where word representations adhere to topological consistency, mitigating\nrepresentational discontinuities commonly observed in transformer-based\nembeddings. Empirical evaluations reveal that probabilistic constraints enhance\nlexical coherence by refining contextual relationships, leading to improvements\nin semantic stability across multiple linguistic distributions. The application\nof divergence minimization techniques ensures that interpolated embeddings\nmaintain probabilistic consistency while preserving computational feasibility\nfor large-scale implementations. Experimental findings demonstrate that\ninterpolated lexical manifolds improve representation density alignment,\nreducing anisotropic distortions in contextual embedding distributions.\nComparative analyses with standard transformer-based models highlight that\nstructured interpolation yields more stable representations, particularly in\ntasks requiring fine-grained semantic differentiation. The statistical\nevaluation of embedding divergence confirms that probabilistic lexical\nmanifolds reduce representational inconsistencies while maintaining coherence\nacross varying scales of contextual abstraction. An assessment of computational\nefficiency reveals that while interpolation introduces minor processing\noverhead, the structured representation learning approach remains scalable for\npractical deployment."
                },
                "authors": [
                    {
                        "name": "Clive Pendleton"
                    },
                    {
                        "name": "Ewan Harrington"
                    },
                    {
                        "name": "Giles Fairbrother"
                    },
                    {
                        "name": "Jasper Arkwright"
                    },
                    {
                        "name": "Nigel Fenwick"
                    },
                    {
                        "name": "Richard Katrix"
                    }
                ],
                "author_detail": {
                    "name": "Richard Katrix"
                },
                "author": "Richard Katrix",
                "arxiv_comment": "arXiv admin note: This paper has been withdrawn by arXiv due to\n  disputed and unverifiable authorship",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10013v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10013v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15836v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15836v2",
                "updated": "2025-03-26T15:52:26Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    15,
                    52,
                    26,
                    2,
                    85,
                    0
                ],
                "published": "2025-01-27T07:51:51Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    7,
                    51,
                    51,
                    0,
                    27,
                    0
                ],
                "title": "Intelligent Code Embedding Framework for High-Precision Ransomware\n  Detection via Multimodal Execution Path Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligent Code Embedding Framework for High-Precision Ransomware\n  Detection via Multimodal Execution Path Analysis"
                },
                "summary": "Modern threat landscapes continue to evolve with increasing sophistication,\nchallenging traditional detection methodologies and necessitating innovative\nsolutions capable of addressing complex adversarial tactics. A novel framework\nwas developed to identify ransomware activity through multimodal execution path\nanalysis, integrating high-dimensional embeddings and dynamic heuristic\nderivation mechanisms to capture behavioral patterns across diverse attack\nvariants. The approach demonstrated high adaptability, effectively mitigating\nobfuscation strategies and polymorphic characteristics often employed by\nransomware families to evade detection. Comprehensive experimental evaluations\nrevealed significant advancements in precision, recall, and accuracy metrics\ncompared to baseline techniques, particularly under conditions of variable\nencryption speeds and obfuscated execution flows. The framework achieved\nscalable and computationally efficient performance, ensuring robust\napplicability across a range of system configurations, from\nresource-constrained environments to high-performance infrastructures. Notable\nfindings included reduced false positive rates and enhanced detection latency,\neven for ransomware families employing sophisticated encryption mechanisms. The\nmodular design allowed seamless integration of additional modalities, enabling\nextensibility and future-proofing against emerging threat vectors. Quantitative\nanalyses further highlighted the system's energy efficiency, emphasizing its\npracticality for deployment in environments with stringent operational\nconstraints. The results underline the importance of integrating advanced\ncomputational techniques and dynamic adaptability to safeguard digital\necosystems from increasingly complex threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern threat landscapes continue to evolve with increasing sophistication,\nchallenging traditional detection methodologies and necessitating innovative\nsolutions capable of addressing complex adversarial tactics. A novel framework\nwas developed to identify ransomware activity through multimodal execution path\nanalysis, integrating high-dimensional embeddings and dynamic heuristic\nderivation mechanisms to capture behavioral patterns across diverse attack\nvariants. The approach demonstrated high adaptability, effectively mitigating\nobfuscation strategies and polymorphic characteristics often employed by\nransomware families to evade detection. Comprehensive experimental evaluations\nrevealed significant advancements in precision, recall, and accuracy metrics\ncompared to baseline techniques, particularly under conditions of variable\nencryption speeds and obfuscated execution flows. The framework achieved\nscalable and computationally efficient performance, ensuring robust\napplicability across a range of system configurations, from\nresource-constrained environments to high-performance infrastructures. Notable\nfindings included reduced false positive rates and enhanced detection latency,\neven for ransomware families employing sophisticated encryption mechanisms. The\nmodular design allowed seamless integration of additional modalities, enabling\nextensibility and future-proofing against emerging threat vectors. Quantitative\nanalyses further highlighted the system's energy efficiency, emphasizing its\npracticality for deployment in environments with stringent operational\nconstraints. The results underline the importance of integrating advanced\ncomputational techniques and dynamic adaptability to safeguard digital\necosystems from increasingly complex threats."
                },
                "authors": [
                    {
                        "name": "Levi Gareth"
                    },
                    {
                        "name": "Maximilian Fairbrother"
                    },
                    {
                        "name": "Peregrine Blackwood"
                    },
                    {
                        "name": "Lucasta Underhill"
                    },
                    {
                        "name": "Benedict Ruthermore"
                    }
                ],
                "author_detail": {
                    "name": "Benedict Ruthermore"
                },
                "author": "Benedict Ruthermore",
                "arxiv_comment": "arXiv admin note: This paper has been withdrawn by arXiv due to\n  disputed and unverifiable authorship",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15836v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15836v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09833v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09833v2",
                "updated": "2025-03-26T15:48:36Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    15,
                    48,
                    36,
                    2,
                    85,
                    0
                ],
                "published": "2025-02-14T00:26:10Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    0,
                    26,
                    10,
                    4,
                    45,
                    0
                ],
                "title": "Decentralized Entropy-Based Ransomware Detection Using Autonomous\n  Feature Resonance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralized Entropy-Based Ransomware Detection Using Autonomous\n  Feature Resonance"
                },
                "summary": "The increasing sophistication of cyber threats has necessitated the\ndevelopment of advanced detection mechanisms capable of identifying malicious\nactivities with high precision and efficiency. A novel approach, termed\nAutonomous Feature Resonance, is introduced to address the limitations of\ntraditional ransomware detection methods through the analysis of entropy-based\nfeature interactions within system processes. The proposed method achieves an\noverall detection accuracy of 97.3\\%, with false positive and false negative\nrates of 1.8\\% and 2.1\\%, respectively, outperforming existing techniques such\nas signature-based detection and behavioral analysis. Its decentralized\narchitecture enables local processing of data, reducing latency and improving\nscalability, while a self-learning mechanism ensures continuous adaptation to\nemerging threats. Experimental results demonstrate consistent performance\nacross diverse ransomware families, including LockBit 3.0, BlackCat, and Royal,\nwith low detection latency and efficient resource utilization. The method's\nreliance on entropy as a distinguishing feature provides robustness against\nobfuscation techniques, making it suitable for real-time deployment in\nhigh-throughput environments. These findings highlight the potential of\nentropy-based approaches to enhance cybersecurity frameworks, offering a\nscalable and adaptive solution for modern ransomware detection challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing sophistication of cyber threats has necessitated the\ndevelopment of advanced detection mechanisms capable of identifying malicious\nactivities with high precision and efficiency. A novel approach, termed\nAutonomous Feature Resonance, is introduced to address the limitations of\ntraditional ransomware detection methods through the analysis of entropy-based\nfeature interactions within system processes. The proposed method achieves an\noverall detection accuracy of 97.3\\%, with false positive and false negative\nrates of 1.8\\% and 2.1\\%, respectively, outperforming existing techniques such\nas signature-based detection and behavioral analysis. Its decentralized\narchitecture enables local processing of data, reducing latency and improving\nscalability, while a self-learning mechanism ensures continuous adaptation to\nemerging threats. Experimental results demonstrate consistent performance\nacross diverse ransomware families, including LockBit 3.0, BlackCat, and Royal,\nwith low detection latency and efficient resource utilization. The method's\nreliance on entropy as a distinguishing feature provides robustness against\nobfuscation techniques, making it suitable for real-time deployment in\nhigh-throughput environments. These findings highlight the potential of\nentropy-based approaches to enhance cybersecurity frameworks, offering a\nscalable and adaptive solution for modern ransomware detection challenges."
                },
                "authors": [
                    {
                        "name": "Barnaby Quince"
                    },
                    {
                        "name": "Levi Gareth"
                    },
                    {
                        "name": "Sophie Larkspur"
                    },
                    {
                        "name": "Thaddeus Wobblethorn"
                    },
                    {
                        "name": "Thomas Quibble"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Quibble"
                },
                "author": "Thomas Quibble",
                "arxiv_comment": "arXiv admin note: This paper has been withdrawn by arXiv due to\n  disputed and unverifiable authorship",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09833v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09833v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20648v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20648v1",
                "updated": "2025-03-26T15:40:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    15,
                    40,
                    40,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T15:40:40Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    15,
                    40,
                    40,
                    2,
                    85,
                    0
                ],
                "title": "TN-Eval: Rubric and Evaluation Protocols for Measuring the Quality of\n  Behavioral Therapy Notes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TN-Eval: Rubric and Evaluation Protocols for Measuring the Quality of\n  Behavioral Therapy Notes"
                },
                "summary": "Behavioral therapy notes are important for both legal compliance and patient\ncare. Unlike progress notes in physical health, quality standards for\nbehavioral therapy notes remain underdeveloped. To address this gap, we\ncollaborated with licensed therapists to design a comprehensive rubric for\nevaluating therapy notes across key dimensions: completeness, conciseness, and\nfaithfulness. Further, we extend a public dataset of behavioral health\nconversations with therapist-written notes and LLM-generated notes, and apply\nour evaluation framework to measure their quality. We find that: (1) A\nrubric-based manual evaluation protocol offers more reliable and interpretable\nresults than traditional Likert-scale annotations. (2) LLMs can mimic human\nevaluators in assessing completeness and conciseness but struggle with\nfaithfulness. (3) Therapist-written notes often lack completeness and\nconciseness, while LLM-generated notes contain hallucination. Surprisingly, in\na blind test, therapists prefer and judge LLM-generated notes to be superior to\ntherapist-written notes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Behavioral therapy notes are important for both legal compliance and patient\ncare. Unlike progress notes in physical health, quality standards for\nbehavioral therapy notes remain underdeveloped. To address this gap, we\ncollaborated with licensed therapists to design a comprehensive rubric for\nevaluating therapy notes across key dimensions: completeness, conciseness, and\nfaithfulness. Further, we extend a public dataset of behavioral health\nconversations with therapist-written notes and LLM-generated notes, and apply\nour evaluation framework to measure their quality. We find that: (1) A\nrubric-based manual evaluation protocol offers more reliable and interpretable\nresults than traditional Likert-scale annotations. (2) LLMs can mimic human\nevaluators in assessing completeness and conciseness but struggle with\nfaithfulness. (3) Therapist-written notes often lack completeness and\nconciseness, while LLM-generated notes contain hallucination. Surprisingly, in\na blind test, therapists prefer and judge LLM-generated notes to be superior to\ntherapist-written notes."
                },
                "authors": [
                    {
                        "name": "Raj Sanjay Shah"
                    },
                    {
                        "name": "Lei Xu"
                    },
                    {
                        "name": "Qianchu Liu"
                    },
                    {
                        "name": "Jon Burnsky"
                    },
                    {
                        "name": "Drew Bertagnolli"
                    },
                    {
                        "name": "Chaitanya Shivade"
                    }
                ],
                "author_detail": {
                    "name": "Chaitanya Shivade"
                },
                "author": "Chaitanya Shivade",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20648v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20648v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20641v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20641v1",
                "updated": "2025-03-26T15:34:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    15,
                    34,
                    37,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T15:34:37Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    15,
                    34,
                    37,
                    2,
                    85,
                    0
                ],
                "title": "Unlocking Efficient Long-to-Short LLM Reasoning with Model Merging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking Efficient Long-to-Short LLM Reasoning with Model Merging"
                },
                "summary": "The transition from System 1 to System 2 reasoning in large language models\n(LLMs) has marked significant advancements in handling complex tasks through\ndeliberate, iterative thinking. However, this progress often comes at the cost\nof efficiency, as models tend to overthink, generating redundant reasoning\nsteps without proportional improvements in output quality. Long-to-Short (L2S)\nreasoning has emerged as a promising solution to this challenge, aiming to\nbalance reasoning depth with practical efficiency. While existing approaches,\nsuch as supervised fine-tuning (SFT), reinforcement learning (RL), and prompt\nengineering, have shown potential, they are either computationally expensive or\nunstable. Model merging, on the other hand, offers a cost-effective and robust\nalternative by integrating the quick-thinking capabilities of System 1 models\nwith the methodical reasoning of System 2 models. In this work, we present a\ncomprehensive empirical study on model merging for L2S reasoning, exploring\ndiverse methodologies, including task-vector-based, SVD-based, and\nactivation-informed merging. Our experiments reveal that model merging can\nreduce average response length by up to 55% while preserving or even improving\nbaseline performance. We also identify a strong correlation between model scale\nand merging efficacy with extensive evaluations on 1.5B/7B/14B/32B models.\nFurthermore, we investigate the merged model's ability to self-critique and\nself-correct, as well as its adaptive response length based on task complexity.\nOur findings highlight model merging as a highly efficient and effective\nparadigm for L2S reasoning, offering a practical solution to the overthinking\nproblem while maintaining the robustness of System 2 reasoning. This work can\nbe found on Github https://github.com/hahahawu/Long-to-Short-via-Model-Merging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The transition from System 1 to System 2 reasoning in large language models\n(LLMs) has marked significant advancements in handling complex tasks through\ndeliberate, iterative thinking. However, this progress often comes at the cost\nof efficiency, as models tend to overthink, generating redundant reasoning\nsteps without proportional improvements in output quality. Long-to-Short (L2S)\nreasoning has emerged as a promising solution to this challenge, aiming to\nbalance reasoning depth with practical efficiency. While existing approaches,\nsuch as supervised fine-tuning (SFT), reinforcement learning (RL), and prompt\nengineering, have shown potential, they are either computationally expensive or\nunstable. Model merging, on the other hand, offers a cost-effective and robust\nalternative by integrating the quick-thinking capabilities of System 1 models\nwith the methodical reasoning of System 2 models. In this work, we present a\ncomprehensive empirical study on model merging for L2S reasoning, exploring\ndiverse methodologies, including task-vector-based, SVD-based, and\nactivation-informed merging. Our experiments reveal that model merging can\nreduce average response length by up to 55% while preserving or even improving\nbaseline performance. We also identify a strong correlation between model scale\nand merging efficacy with extensive evaluations on 1.5B/7B/14B/32B models.\nFurthermore, we investigate the merged model's ability to self-critique and\nself-correct, as well as its adaptive response length based on task complexity.\nOur findings highlight model merging as a highly efficient and effective\nparadigm for L2S reasoning, offering a practical solution to the overthinking\nproblem while maintaining the robustness of System 2 reasoning. This work can\nbe found on Github https://github.com/hahahawu/Long-to-Short-via-Model-Merging."
                },
                "authors": [
                    {
                        "name": "Han Wu"
                    },
                    {
                        "name": "Yuxuan Yao"
                    },
                    {
                        "name": "Shuqi Liu"
                    },
                    {
                        "name": "Zehua Liu"
                    },
                    {
                        "name": "Xiaojin Fu"
                    },
                    {
                        "name": "Xiongwei Han"
                    },
                    {
                        "name": "Xing Li"
                    },
                    {
                        "name": "Hui-Ling Zhen"
                    },
                    {
                        "name": "Tao Zhong"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Mingxuan Yuan"
                },
                "author": "Mingxuan Yuan",
                "arxiv_comment": "Work in progress; technical report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20641v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20641v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05223v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05223v2",
                "updated": "2025-03-26T15:18:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    15,
                    18,
                    53,
                    2,
                    85,
                    0
                ],
                "published": "2024-12-06T17:54:54Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    54,
                    54,
                    4,
                    341,
                    0
                ],
                "title": "100% Elimination of Hallucinations on RAGTruth for GPT-4 and GPT-3.5\n  Turbo",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "100% Elimination of Hallucinations on RAGTruth for GPT-4 and GPT-3.5\n  Turbo"
                },
                "summary": "The issue of hallucinations in large language models (LLMs) remains a\ncritical barrier to the adoption of AI in enterprise and other high-stakes\napplications. Despite advancements in retrieval-augmented generation (RAG)\nsystems, current state-of-the-art methods fail to achieve more than 80%\naccuracy in generating faithful and factually correct outputs, even when\nprovided with relevant and accurate context. In this work, we introduce Acurai,\na novel systematic approach that achieves 100% hallucination-free responses in\nLLMs by reformatting queries and context data prior to input. Leveraging a deep\nunderstanding of LLM internal representations, the importance of noun-phrase\ndominance, and the role of discrete functional units (DFUs), Acurai ensures\nalignment between input context and generated output. We validate this method\nusing the RAGTruth corpus, demonstrating its ability to eliminate 100%\nhallucinations for both GPT-4 and GPT-3.5 Turbo. Acurai sets a new standard for\nachieving consistent, accurate, and faithful AI responses, marking a\nsignificant step forward in the development of trustworthy AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The issue of hallucinations in large language models (LLMs) remains a\ncritical barrier to the adoption of AI in enterprise and other high-stakes\napplications. Despite advancements in retrieval-augmented generation (RAG)\nsystems, current state-of-the-art methods fail to achieve more than 80%\naccuracy in generating faithful and factually correct outputs, even when\nprovided with relevant and accurate context. In this work, we introduce Acurai,\na novel systematic approach that achieves 100% hallucination-free responses in\nLLMs by reformatting queries and context data prior to input. Leveraging a deep\nunderstanding of LLM internal representations, the importance of noun-phrase\ndominance, and the role of discrete functional units (DFUs), Acurai ensures\nalignment between input context and generated output. We validate this method\nusing the RAGTruth corpus, demonstrating its ability to eliminate 100%\nhallucinations for both GPT-4 and GPT-3.5 Turbo. Acurai sets a new standard for\nachieving consistent, accurate, and faithful AI responses, marking a\nsignificant step forward in the development of trustworthy AI systems."
                },
                "authors": [
                    {
                        "name": "Michael C. Wood"
                    },
                    {
                        "name": "Adam A. Forbes"
                    }
                ],
                "author_detail": {
                    "name": "Adam A. Forbes"
                },
                "author": "Adam A. Forbes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05223v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05223v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20623v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20623v1",
                "updated": "2025-03-26T15:10:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    15,
                    10,
                    47,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T15:10:47Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    15,
                    10,
                    47,
                    2,
                    85,
                    0
                ],
                "title": "Collaborative Storytelling and LLM: A Linguistic Analysis of\n  Automatically-Generated Role-Playing Game Sessions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collaborative Storytelling and LLM: A Linguistic Analysis of\n  Automatically-Generated Role-Playing Game Sessions"
                },
                "summary": "Role-playing games (RPG) are games in which players interact with one another\nto create narratives. The role of players in the RPG is largely based on the\ninteraction between players and their characters. This emerging form of shared\nnarrative, primarily oral, is receiving increasing attention. In particular,\nmany authors investigated the use of an LLM as an actor in the game. In this\npaper, we aim to discover to what extent the language of Large Language Models\n(LLMs) exhibit oral or written features when asked to generate an RPG session\nwithout human interference. We will conduct a linguistic analysis of the\nlexical and syntactic features of the generated texts and compare the results\nwith analyses of conversations, transcripts of human RPG sessions, and books.\nWe found that LLMs exhibit a pattern that is distinct from all other text\ncategories, including oral conversations, human RPG sessions and books. Our\nanalysis has shown how training influences the way LLMs express themselves and\nprovides important indications of the narrative capabilities of these tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Role-playing games (RPG) are games in which players interact with one another\nto create narratives. The role of players in the RPG is largely based on the\ninteraction between players and their characters. This emerging form of shared\nnarrative, primarily oral, is receiving increasing attention. In particular,\nmany authors investigated the use of an LLM as an actor in the game. In this\npaper, we aim to discover to what extent the language of Large Language Models\n(LLMs) exhibit oral or written features when asked to generate an RPG session\nwithout human interference. We will conduct a linguistic analysis of the\nlexical and syntactic features of the generated texts and compare the results\nwith analyses of conversations, transcripts of human RPG sessions, and books.\nWe found that LLMs exhibit a pattern that is distinct from all other text\ncategories, including oral conversations, human RPG sessions and books. Our\nanalysis has shown how training influences the way LLMs express themselves and\nprovides important indications of the narrative capabilities of these tools."
                },
                "authors": [
                    {
                        "name": "Alessandro Maisto"
                    }
                ],
                "author_detail": {
                    "name": "Alessandro Maisto"
                },
                "author": "Alessandro Maisto",
                "arxiv_comment": "17 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20623v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20623v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20613v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20613v1",
                "updated": "2025-03-26T15:00:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    15,
                    0,
                    7,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T15:00:07Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    15,
                    0,
                    7,
                    2,
                    85,
                    0
                ],
                "title": "State-Aware Perturbation Optimization for Robust Deep Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State-Aware Perturbation Optimization for Robust Deep Reinforcement\n  Learning"
                },
                "summary": "Recently, deep reinforcement learning (DRL) has emerged as a promising\napproach for robotic control. However, the deployment of DRL in real-world\nrobots is hindered by its sensitivity to environmental perturbations. While\nexisting whitebox adversarial attacks rely on local gradient information and\napply uniform perturbations across all states to evaluate DRL robustness, they\nfail to account for temporal dynamics and state-specific vulnerabilities. To\ncombat the above challenge, we first conduct a theoretical analysis of\nwhite-box attacks in DRL by establishing the adversarial victim-dynamics Markov\ndecision process (AVD-MDP), to derive the necessary and sufficient conditions\nfor a successful attack. Based on this, we propose a selective state-aware\nreinforcement adversarial attack method, named STAR, to optimize perturbation\nstealthiness and state visitation dispersion. STAR first employs a soft\nmask-based state-targeting mechanism to minimize redundant perturbations,\nenhancing stealthiness and attack effectiveness. Then, it incorporates an\ninformation-theoretic optimization objective to maximize mutual information\nbetween perturbations, environmental states, and victim actions, ensuring a\ndispersed state-visitation distribution that steers the victim agent into\nvulnerable states for maximum return reduction. Extensive experiments\ndemonstrate that STAR outperforms state-of-the-art benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, deep reinforcement learning (DRL) has emerged as a promising\napproach for robotic control. However, the deployment of DRL in real-world\nrobots is hindered by its sensitivity to environmental perturbations. While\nexisting whitebox adversarial attacks rely on local gradient information and\napply uniform perturbations across all states to evaluate DRL robustness, they\nfail to account for temporal dynamics and state-specific vulnerabilities. To\ncombat the above challenge, we first conduct a theoretical analysis of\nwhite-box attacks in DRL by establishing the adversarial victim-dynamics Markov\ndecision process (AVD-MDP), to derive the necessary and sufficient conditions\nfor a successful attack. Based on this, we propose a selective state-aware\nreinforcement adversarial attack method, named STAR, to optimize perturbation\nstealthiness and state visitation dispersion. STAR first employs a soft\nmask-based state-targeting mechanism to minimize redundant perturbations,\nenhancing stealthiness and attack effectiveness. Then, it incorporates an\ninformation-theoretic optimization objective to maximize mutual information\nbetween perturbations, environmental states, and victim actions, ensuring a\ndispersed state-visitation distribution that steers the victim agent into\nvulnerable states for maximum return reduction. Extensive experiments\ndemonstrate that STAR outperforms state-of-the-art benchmarks."
                },
                "authors": [
                    {
                        "name": "Zongyuan Zhang"
                    },
                    {
                        "name": "Tianyang Duan"
                    },
                    {
                        "name": "Zheng Lin"
                    },
                    {
                        "name": "Dong Huang"
                    },
                    {
                        "name": "Zihan Fang"
                    },
                    {
                        "name": "Zekai Sun"
                    },
                    {
                        "name": "Ling Xiong"
                    },
                    {
                        "name": "Hongbin Liang"
                    },
                    {
                        "name": "Heming Cui"
                    },
                    {
                        "name": "Yong Cui"
                    }
                ],
                "author_detail": {
                    "name": "Yong Cui"
                },
                "author": "Yong Cui",
                "arxiv_comment": "15 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20613v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20613v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20601v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20601v1",
                "updated": "2025-03-26T14:47:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    14,
                    47,
                    22,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T14:47:22Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    14,
                    47,
                    22,
                    2,
                    85,
                    0
                ],
                "title": "Pupillary reactions depend on disgust sensitivity in conceptual\n  pavlovian disgust conditioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pupillary reactions depend on disgust sensitivity in conceptual\n  pavlovian disgust conditioning"
                },
                "summary": "Exposure-based interventions rely on inhibitory learning, often studied\nthrough Pavlovian conditioning. While disgust conditioning is increasingly\nlinked to psychiatric disorders, it has been less researched than fear\nconditioning. In this study, we applied a categorical Pavlovian disgust\nconditioning paradigm with two CS categories (animals and tools) and disgusting\nimages as US (e.g., feces). During categorization, acquisition, and extinction\nphases, we measured eye movements and pupil responses in 44 participants.\nConsistent with previous results, subjective disgust and US expectancy\nincreased from categorization to acquisition for CS+, along with greater pupil\ndilation for CS+ than CS-. Higher disgust sensitivity was associated with more\ngeneralized and longer lasting disgust experiences, as well as higher\nexpectancy for disgusting images. Pupil response during acquisition and\nextinction depended on disgust sensitivity: Participants with lower disgust\nsensitivity showed greater pupil dilation. These findings suggest that\nindividuals with high disgust sensitivity and prolonged expectancy may exhibit\nphysiological differences from less sensitive individuals as early as the\nacquisition phase. This could inform contamination-based OCD treatments by\nintegrating interventions which focus on attentional deployment or\nphysiological reactions. To our knowledge, this is the first study using\npupillometry and eye tracking in categorical disgust conditioning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exposure-based interventions rely on inhibitory learning, often studied\nthrough Pavlovian conditioning. While disgust conditioning is increasingly\nlinked to psychiatric disorders, it has been less researched than fear\nconditioning. In this study, we applied a categorical Pavlovian disgust\nconditioning paradigm with two CS categories (animals and tools) and disgusting\nimages as US (e.g., feces). During categorization, acquisition, and extinction\nphases, we measured eye movements and pupil responses in 44 participants.\nConsistent with previous results, subjective disgust and US expectancy\nincreased from categorization to acquisition for CS+, along with greater pupil\ndilation for CS+ than CS-. Higher disgust sensitivity was associated with more\ngeneralized and longer lasting disgust experiences, as well as higher\nexpectancy for disgusting images. Pupil response during acquisition and\nextinction depended on disgust sensitivity: Participants with lower disgust\nsensitivity showed greater pupil dilation. These findings suggest that\nindividuals with high disgust sensitivity and prolonged expectancy may exhibit\nphysiological differences from less sensitive individuals as early as the\nacquisition phase. This could inform contamination-based OCD treatments by\nintegrating interventions which focus on attentional deployment or\nphysiological reactions. To our knowledge, this is the first study using\npupillometry and eye tracking in categorical disgust conditioning."
                },
                "authors": [
                    {
                        "name": "Lars Rothkegel"
                    },
                    {
                        "name": "Jakob Fink-Lamotte"
                    }
                ],
                "author_detail": {
                    "name": "Jakob Fink-Lamotte"
                },
                "author": "Jakob Fink-Lamotte",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20601v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20601v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.NC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20589v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20589v1",
                "updated": "2025-03-26T14:41:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    14,
                    41,
                    38,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T14:41:38Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    14,
                    41,
                    38,
                    2,
                    85,
                    0
                ],
                "title": "What to Retrieve for Effective Retrieval-Augmented Code Generation? An\n  Empirical Study and Beyond",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What to Retrieve for Effective Retrieval-Augmented Code Generation? An\n  Empirical Study and Beyond"
                },
                "summary": "Repository-level code generation remains challenging due to complex code\ndependencies and the limitations of large language models (LLMs) in processing\nlong contexts. While retrieval-augmented generation (RAG) frameworks are widely\nadopted, the effectiveness of different retrieved information\nsources-contextual code, APIs, and similar snippets-has not been rigorously\nanalyzed. Through an empirical study on two benchmarks, we demonstrate that\nin-context code and potential API information significantly enhance LLM\nperformance, whereas retrieved similar code often introduces noise, degrading\nresults by up to 15%. Based on the preliminary results, we propose\nAllianceCoder, a novel context-integrated method that employs chain-of-thought\nprompting to decompose user queries into implementation steps and retrieves\nAPIs via semantic description matching. Through extensive experiments on\nCoderEval and RepoExec, AllianceCoder achieves state-of-the-art performance,\nimproving Pass@1 by up to 20% over existing approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Repository-level code generation remains challenging due to complex code\ndependencies and the limitations of large language models (LLMs) in processing\nlong contexts. While retrieval-augmented generation (RAG) frameworks are widely\nadopted, the effectiveness of different retrieved information\nsources-contextual code, APIs, and similar snippets-has not been rigorously\nanalyzed. Through an empirical study on two benchmarks, we demonstrate that\nin-context code and potential API information significantly enhance LLM\nperformance, whereas retrieved similar code often introduces noise, degrading\nresults by up to 15%. Based on the preliminary results, we propose\nAllianceCoder, a novel context-integrated method that employs chain-of-thought\nprompting to decompose user queries into implementation steps and retrieves\nAPIs via semantic description matching. Through extensive experiments on\nCoderEval and RepoExec, AllianceCoder achieves state-of-the-art performance,\nimproving Pass@1 by up to 20% over existing approaches."
                },
                "authors": [
                    {
                        "name": "Wenchao Gu"
                    },
                    {
                        "name": "Juntao Chen"
                    },
                    {
                        "name": "Yanlin Wang"
                    },
                    {
                        "name": "Tianyue Jiang"
                    },
                    {
                        "name": "Xingzhe Li"
                    },
                    {
                        "name": "Mingwei Liu"
                    },
                    {
                        "name": "Xilin Liu"
                    },
                    {
                        "name": "Yuchi Ma"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20589v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20589v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20588v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20588v1",
                "updated": "2025-03-26T14:41:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    14,
                    41,
                    4,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T14:41:04Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    14,
                    41,
                    4,
                    2,
                    85,
                    0
                ],
                "title": "Synthetic Data Augmentation for Cross-domain Implicit Discourse Relation\n  Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthetic Data Augmentation for Cross-domain Implicit Discourse Relation\n  Recognition"
                },
                "summary": "Implicit discourse relation recognition (IDRR) -- the task of identifying the\nimplicit coherence relation between two text spans -- requires deep semantic\nunderstanding. Recent studies have shown that zero- or few-shot approaches\nsignificantly lag behind supervised models, but LLMs may be useful for\nsynthetic data augmentation, where LLMs generate a second argument following a\nspecified coherence relation. We applied this approach in a cross-domain\nsetting, generating discourse continuations using unlabelled target-domain data\nto adapt a base model which was trained on source-domain labelled data.\nEvaluations conducted on a large-scale test set revealed that different\nvariations of the approach did not result in any significant improvements. We\nconclude that LLMs often fail to generate useful samples for IDRR, and\nemphasize the importance of considering both statistical significance and\ncomparability when evaluating IDRR models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implicit discourse relation recognition (IDRR) -- the task of identifying the\nimplicit coherence relation between two text spans -- requires deep semantic\nunderstanding. Recent studies have shown that zero- or few-shot approaches\nsignificantly lag behind supervised models, but LLMs may be useful for\nsynthetic data augmentation, where LLMs generate a second argument following a\nspecified coherence relation. We applied this approach in a cross-domain\nsetting, generating discourse continuations using unlabelled target-domain data\nto adapt a base model which was trained on source-domain labelled data.\nEvaluations conducted on a large-scale test set revealed that different\nvariations of the approach did not result in any significant improvements. We\nconclude that LLMs often fail to generate useful samples for IDRR, and\nemphasize the importance of considering both statistical significance and\ncomparability when evaluating IDRR models."
                },
                "authors": [
                    {
                        "name": "Frances Yung"
                    },
                    {
                        "name": "Varsha Suresh"
                    },
                    {
                        "name": "Zaynab Reza"
                    },
                    {
                        "name": "Mansoor Ahmad"
                    },
                    {
                        "name": "Vera Demberg"
                    }
                ],
                "author_detail": {
                    "name": "Vera Demberg"
                },
                "author": "Vera Demberg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20588v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20588v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20579v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20579v1",
                "updated": "2025-03-26T14:25:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    14,
                    25,
                    27,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T14:25:27Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    14,
                    25,
                    27,
                    2,
                    85,
                    0
                ],
                "title": "Is Reuse All You Need? A Systematic Comparison of Regular Expression\n  Composition Strategies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is Reuse All You Need? A Systematic Comparison of Regular Expression\n  Composition Strategies"
                },
                "summary": "Composing regular expressions (regexes) is a common but challenging\nengineering activity. Software engineers struggle with regex complexity,\nleading to defects, performance issues, and security vulnerabilities.\nResearchers have proposed tools to synthesize regexes automatically, and recent\ngenerative AI techniques are also promising. Meanwhile, developers commonly\nreuse existing regexes from Internet sources and codebases. In this study, we\nask a simple question: are regex composition tasks unique enough to merit\ndedicated machinery, or is reuse all we need?\n  We answer this question through a systematic evaluation of state-of-the-art\nregex reuse and synthesis strategies. We begin by collecting a novel dataset of\nregex composition tasks mined from GitHub and RegExLib (55,137 unique tasks\nwith solution regexes). To address the absence of an automated regex reuse\nformulation, we introduce reuse-by-example, a Programming by Example (PbE)\napproach that leverages a curated database of production-ready regexes.\nAlthough all approaches can solve these composition tasks accurately,\nreuse-by-example and LLMs both do far better over the range of metrics we\napplied. Our evaluation then uses multiple dimensions, including a novel\nmetric, to compare reuse-by-example against two synthesis approaches: formal\nregex synthesizers and generative AI (LLMs). Although all approaches can solve\nthese composition tasks accurately, reuse and LLMs both do far better over the\nrange of metrics we applied. Ceteris paribus, prefer the cheaper solution --\nfor regex composition, perhaps reuse is all you need. Our findings provide\nactionable insights for developers selecting regex composition strategies and\ninform the design of future tools to improve regex reliability in software\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Composing regular expressions (regexes) is a common but challenging\nengineering activity. Software engineers struggle with regex complexity,\nleading to defects, performance issues, and security vulnerabilities.\nResearchers have proposed tools to synthesize regexes automatically, and recent\ngenerative AI techniques are also promising. Meanwhile, developers commonly\nreuse existing regexes from Internet sources and codebases. In this study, we\nask a simple question: are regex composition tasks unique enough to merit\ndedicated machinery, or is reuse all we need?\n  We answer this question through a systematic evaluation of state-of-the-art\nregex reuse and synthesis strategies. We begin by collecting a novel dataset of\nregex composition tasks mined from GitHub and RegExLib (55,137 unique tasks\nwith solution regexes). To address the absence of an automated regex reuse\nformulation, we introduce reuse-by-example, a Programming by Example (PbE)\napproach that leverages a curated database of production-ready regexes.\nAlthough all approaches can solve these composition tasks accurately,\nreuse-by-example and LLMs both do far better over the range of metrics we\napplied. Our evaluation then uses multiple dimensions, including a novel\nmetric, to compare reuse-by-example against two synthesis approaches: formal\nregex synthesizers and generative AI (LLMs). Although all approaches can solve\nthese composition tasks accurately, reuse and LLMs both do far better over the\nrange of metrics we applied. Ceteris paribus, prefer the cheaper solution --\nfor regex composition, perhaps reuse is all you need. Our findings provide\nactionable insights for developers selecting regex composition strategies and\ninform the design of future tools to improve regex reliability in software\nsystems."
                },
                "authors": [
                    {
                        "name": "Berk Çakar"
                    },
                    {
                        "name": "Charles M. Sale"
                    },
                    {
                        "name": "Sophie Chen"
                    },
                    {
                        "name": "Ethan H. Burmane"
                    },
                    {
                        "name": "Dongyoon Lee"
                    },
                    {
                        "name": "James C. Davis"
                    }
                ],
                "author_detail": {
                    "name": "James C. Davis"
                },
                "author": "James C. Davis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20579v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20579v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20578v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20578v1",
                "updated": "2025-03-26T14:25:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    14,
                    25,
                    1,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T14:25:01Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    14,
                    25,
                    1,
                    2,
                    85,
                    0
                ],
                "title": "LLPut: Investigating Large Language Models for Bug Report-Based Input\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLPut: Investigating Large Language Models for Bug Report-Based Input\n  Generation"
                },
                "summary": "Failure-inducing inputs play a crucial role in diagnosing and analyzing\nsoftware bugs. Bug reports typically contain these inputs, which developers\nextract to facilitate debugging. Since bug reports are written in natural\nlanguage, prior research has leveraged various Natural Language Processing\n(NLP) techniques for automated input extraction. With the advent of Large\nLanguage Models (LLMs), an important research question arises: how effectively\ncan generative LLMs extract failure-inducing inputs from bug reports? In this\npaper, we propose LLPut, a technique to empirically evaluate the performance of\nthree open-source generative LLMs -- LLaMA, Qwen, and Qwen-Coder -- in\nextracting relevant inputs from bug reports. We conduct an experimental\nevaluation on a dataset of 206 bug reports to assess the accuracy and\neffectiveness of these models. Our findings provide insights into the\ncapabilities and limitations of generative LLMs in automated bug diagnosis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Failure-inducing inputs play a crucial role in diagnosing and analyzing\nsoftware bugs. Bug reports typically contain these inputs, which developers\nextract to facilitate debugging. Since bug reports are written in natural\nlanguage, prior research has leveraged various Natural Language Processing\n(NLP) techniques for automated input extraction. With the advent of Large\nLanguage Models (LLMs), an important research question arises: how effectively\ncan generative LLMs extract failure-inducing inputs from bug reports? In this\npaper, we propose LLPut, a technique to empirically evaluate the performance of\nthree open-source generative LLMs -- LLaMA, Qwen, and Qwen-Coder -- in\nextracting relevant inputs from bug reports. We conduct an experimental\nevaluation on a dataset of 206 bug reports to assess the accuracy and\neffectiveness of these models. Our findings provide insights into the\ncapabilities and limitations of generative LLMs in automated bug diagnosis."
                },
                "authors": [
                    {
                        "name": "Alif Al Hasan"
                    },
                    {
                        "name": "Subarna Saha"
                    },
                    {
                        "name": "Mia Mohammad Imran"
                    },
                    {
                        "name": "Tarannum Shaila Zaman"
                    }
                ],
                "author_detail": {
                    "name": "Tarannum Shaila Zaman"
                },
                "author": "Tarannum Shaila Zaman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20578v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20578v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20576v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20576v1",
                "updated": "2025-03-26T14:23:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    14,
                    23,
                    59,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T14:23:59Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    14,
                    23,
                    59,
                    2,
                    85,
                    0
                ],
                "title": "Optimizing Case-Based Reasoning System for Functional Test Script\n  Generation with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Case-Based Reasoning System for Functional Test Script\n  Generation with Large Language Models"
                },
                "summary": "In this work, we explore the potential of large language models (LLMs) for\ngenerating functional test scripts, which necessitates understanding the\ndynamically evolving code structure of the target software. To achieve this, we\npropose a case-based reasoning (CBR) system utilizing a 4R cycle (i.e.,\nretrieve, reuse, revise, and retain), which maintains and leverages a case bank\nof test intent descriptions and corresponding test scripts to facilitate LLMs\nfor test script generation. To improve user experience further, we introduce\nRe4, an optimization method for the CBR system, comprising reranking-based\nretrieval finetuning and reinforced reuse finetuning. Specifically, we first\nidentify positive examples with high semantic and script similarity, providing\nreliable pseudo-labels for finetuning the retriever model without costly\nlabeling. Then, we apply supervised finetuning, followed by a reinforcement\nlearning finetuning stage, to align LLMs with our production scenarios,\nensuring the faithful reuse of retrieved cases. Extensive experimental results\non two product development units from Huawei Datacom demonstrate the\nsuperiority of the proposed CBR+Re4. Notably, we also show that the proposed\nRe4 method can help alleviate the repetitive generation issues with LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we explore the potential of large language models (LLMs) for\ngenerating functional test scripts, which necessitates understanding the\ndynamically evolving code structure of the target software. To achieve this, we\npropose a case-based reasoning (CBR) system utilizing a 4R cycle (i.e.,\nretrieve, reuse, revise, and retain), which maintains and leverages a case bank\nof test intent descriptions and corresponding test scripts to facilitate LLMs\nfor test script generation. To improve user experience further, we introduce\nRe4, an optimization method for the CBR system, comprising reranking-based\nretrieval finetuning and reinforced reuse finetuning. Specifically, we first\nidentify positive examples with high semantic and script similarity, providing\nreliable pseudo-labels for finetuning the retriever model without costly\nlabeling. Then, we apply supervised finetuning, followed by a reinforcement\nlearning finetuning stage, to align LLMs with our production scenarios,\nensuring the faithful reuse of retrieved cases. Extensive experimental results\non two product development units from Huawei Datacom demonstrate the\nsuperiority of the proposed CBR+Re4. Notably, we also show that the proposed\nRe4 method can help alleviate the repetitive generation issues with LLMs."
                },
                "authors": [
                    {
                        "name": "Siyuan Guo"
                    },
                    {
                        "name": "Huiwu Liu"
                    },
                    {
                        "name": "Xiaolong Chen"
                    },
                    {
                        "name": "Yuming Xie"
                    },
                    {
                        "name": "Liang Zhang"
                    },
                    {
                        "name": "Tao Han"
                    },
                    {
                        "name": "Hechang Chen"
                    },
                    {
                        "name": "Yi Chang"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20576v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20576v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20568v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20568v1",
                "updated": "2025-03-26T14:07:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    14,
                    7,
                    40,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T14:07:40Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    14,
                    7,
                    40,
                    2,
                    85,
                    0
                ],
                "title": "Low-resource Information Extraction with the European Clinical Case\n  Corpus",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-resource Information Extraction with the European Clinical Case\n  Corpus"
                },
                "summary": "We present E3C-3.0, a multilingual dataset in the medical domain, comprising\nclinical cases annotated with diseases and test-result relations. The dataset\nincludes both native texts in five languages (English, French, Italian, Spanish\nand Basque) and texts translated and projected from the English source into\nfive target languages (Greek, Italian, Polish, Slovak, and Slovenian). A\nsemi-automatic approach has been implemented, including automatic annotation\nprojection based on Large Language Models (LLMs) and human revision. We present\nseveral experiments showing that current state-of-the-art LLMs can benefit from\nbeing fine-tuned on the E3C-3.0 dataset. We also show that transfer learning in\ndifferent languages is very effective, mitigating the scarcity of data.\nFinally, we compare performance both on native data and on projected data. We\nrelease the data at\nhttps://huggingface.co/collections/NLP-FBK/e3c-projected-676a7d6221608d60e4e9fd89 .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present E3C-3.0, a multilingual dataset in the medical domain, comprising\nclinical cases annotated with diseases and test-result relations. The dataset\nincludes both native texts in five languages (English, French, Italian, Spanish\nand Basque) and texts translated and projected from the English source into\nfive target languages (Greek, Italian, Polish, Slovak, and Slovenian). A\nsemi-automatic approach has been implemented, including automatic annotation\nprojection based on Large Language Models (LLMs) and human revision. We present\nseveral experiments showing that current state-of-the-art LLMs can benefit from\nbeing fine-tuned on the E3C-3.0 dataset. We also show that transfer learning in\ndifferent languages is very effective, mitigating the scarcity of data.\nFinally, we compare performance both on native data and on projected data. We\nrelease the data at\nhttps://huggingface.co/collections/NLP-FBK/e3c-projected-676a7d6221608d60e4e9fd89 ."
                },
                "authors": [
                    {
                        "name": "Soumitra Ghosh"
                    },
                    {
                        "name": "Begona Altuna"
                    },
                    {
                        "name": "Saeed Farzi"
                    },
                    {
                        "name": "Pietro Ferrazzi"
                    },
                    {
                        "name": "Alberto Lavelli"
                    },
                    {
                        "name": "Giulia Mezzanotte"
                    },
                    {
                        "name": "Manuela Speranza"
                    },
                    {
                        "name": "Bernardo Magnini"
                    }
                ],
                "author_detail": {
                    "name": "Bernardo Magnini"
                },
                "author": "Bernardo Magnini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20568v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20568v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20561v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20561v1",
                "updated": "2025-03-26T13:58:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    13,
                    58,
                    2,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T13:58:02Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    13,
                    58,
                    2,
                    2,
                    85,
                    0
                ],
                "title": "A Theoretical Framework for Prompt Engineering: Approximating Smooth\n  Functions with Transformer Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Theoretical Framework for Prompt Engineering: Approximating Smooth\n  Functions with Transformer Prompts"
                },
                "summary": "Prompt engineering has emerged as a powerful technique for guiding large\nlanguage models (LLMs) toward desired responses, significantly enhancing their\nperformance across diverse tasks. Beyond their role as static predictors, LLMs\nincreasingly function as intelligent agents, capable of reasoning,\ndecision-making, and adapting dynamically to complex environments. However, the\ntheoretical underpinnings of prompt engineering remain largely unexplored. In\nthis paper, we introduce a formal framework demonstrating that transformer\nmodels, when provided with carefully designed prompts, can act as a\nconfigurable computational system by emulating a ``virtual'' neural network\nduring inference. Specifically, input prompts effectively translate into the\ncorresponding network configuration, enabling LLMs to adjust their internal\ncomputations dynamically. Building on this construction, we establish an\napproximation theory for $\\beta$-times differentiable functions, proving that\ntransformers can approximate such functions with arbitrary precision when\nguided by appropriately structured prompts. Moreover, our framework provides\ntheoretical justification for several empirically successful prompt engineering\ntechniques, including the use of longer, structured prompts, filtering\nirrelevant information, enhancing prompt token diversity, and leveraging\nmulti-agent interactions. By framing LLMs as adaptable agents rather than\nstatic models, our findings underscore their potential for autonomous reasoning\nand problem-solving, paving the way for more robust and theoretically grounded\nadvancements in prompt engineering and AI agent design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt engineering has emerged as a powerful technique for guiding large\nlanguage models (LLMs) toward desired responses, significantly enhancing their\nperformance across diverse tasks. Beyond their role as static predictors, LLMs\nincreasingly function as intelligent agents, capable of reasoning,\ndecision-making, and adapting dynamically to complex environments. However, the\ntheoretical underpinnings of prompt engineering remain largely unexplored. In\nthis paper, we introduce a formal framework demonstrating that transformer\nmodels, when provided with carefully designed prompts, can act as a\nconfigurable computational system by emulating a ``virtual'' neural network\nduring inference. Specifically, input prompts effectively translate into the\ncorresponding network configuration, enabling LLMs to adjust their internal\ncomputations dynamically. Building on this construction, we establish an\napproximation theory for $\\beta$-times differentiable functions, proving that\ntransformers can approximate such functions with arbitrary precision when\nguided by appropriately structured prompts. Moreover, our framework provides\ntheoretical justification for several empirically successful prompt engineering\ntechniques, including the use of longer, structured prompts, filtering\nirrelevant information, enhancing prompt token diversity, and leveraging\nmulti-agent interactions. By framing LLMs as adaptable agents rather than\nstatic models, our findings underscore their potential for autonomous reasoning\nand problem-solving, paving the way for more robust and theoretically grounded\nadvancements in prompt engineering and AI agent design."
                },
                "authors": [
                    {
                        "name": "Ryumei Nakada"
                    },
                    {
                        "name": "Wenlong Ji"
                    },
                    {
                        "name": "Tianxi Cai"
                    },
                    {
                        "name": "James Zou"
                    },
                    {
                        "name": "Linjun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linjun Zhang"
                },
                "author": "Linjun Zhang",
                "arxiv_comment": "55 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20561v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20561v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.17790v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.17790v3",
                "updated": "2025-03-26T13:55:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    13,
                    55,
                    18,
                    2,
                    85,
                    0
                ],
                "published": "2024-03-26T15:21:18Z",
                "published_parsed": [
                    2024,
                    3,
                    26,
                    15,
                    21,
                    18,
                    1,
                    86,
                    0
                ],
                "title": "A PAC-Bayesian Framework for Optimal Control with Stability Guarantees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A PAC-Bayesian Framework for Optimal Control with Stability Guarantees"
                },
                "summary": "Stochastic Nonlinear Optimal Control (SNOC) involves minimizing a cost\nfunction that averages out the random uncertainties affecting the dynamics of\nnonlinear systems. For tractability reasons, this problem is typically\naddressed by minimizing an empirical cost, which represents the average cost\nacross a finite dataset of sampled disturbances. However, this approach raises\nthe challenge of quantifying the control performance against out-of-sample\nuncertainties. Particularly, in scenarios where the training dataset is small,\nSNOC policies are prone to overfitting, resulting in significant discrepancies\nbetween the empirical cost and the true cost, i.e., the average SNOC cost\nincurred during control deployment. Therefore, establishing generalization\nbounds on the true cost is crucial for ensuring reliability in real-world\napplications. In this paper, we introduce a novel approach that leverages\nPAC-Bayes theory to provide rigorous generalization bounds for SNOC. Based on\nthese bounds, we propose a new method for designing optimal controllers,\noffering a principled way to incorporate prior knowledge into the synthesis\nprocess, which aids in improving the control policy and mitigating overfitting.\nFurthermore, by leveraging recent parametrizations of stabilizing controllers\nfor nonlinear systems, our framework inherently ensures closed-loop stability.\nThe effectiveness of our proposed method in incorporating prior knowledge and\ncombating overfitting is shown by designing neural network controllers for\ntasks in cooperative robotics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stochastic Nonlinear Optimal Control (SNOC) involves minimizing a cost\nfunction that averages out the random uncertainties affecting the dynamics of\nnonlinear systems. For tractability reasons, this problem is typically\naddressed by minimizing an empirical cost, which represents the average cost\nacross a finite dataset of sampled disturbances. However, this approach raises\nthe challenge of quantifying the control performance against out-of-sample\nuncertainties. Particularly, in scenarios where the training dataset is small,\nSNOC policies are prone to overfitting, resulting in significant discrepancies\nbetween the empirical cost and the true cost, i.e., the average SNOC cost\nincurred during control deployment. Therefore, establishing generalization\nbounds on the true cost is crucial for ensuring reliability in real-world\napplications. In this paper, we introduce a novel approach that leverages\nPAC-Bayes theory to provide rigorous generalization bounds for SNOC. Based on\nthese bounds, we propose a new method for designing optimal controllers,\noffering a principled way to incorporate prior knowledge into the synthesis\nprocess, which aids in improving the control policy and mitigating overfitting.\nFurthermore, by leveraging recent parametrizations of stabilizing controllers\nfor nonlinear systems, our framework inherently ensures closed-loop stability.\nThe effectiveness of our proposed method in incorporating prior knowledge and\ncombating overfitting is shown by designing neural network controllers for\ntasks in cooperative robotics."
                },
                "authors": [
                    {
                        "name": "Mahrokh Ghoddousi Boroujeni"
                    },
                    {
                        "name": "Clara Lucía Galimberti"
                    },
                    {
                        "name": "Andreas Krause"
                    },
                    {
                        "name": "Giancarlo Ferrari-Trecate"
                    }
                ],
                "author_detail": {
                    "name": "Giancarlo Ferrari-Trecate"
                },
                "author": "Giancarlo Ferrari-Trecate",
                "arxiv_doi": "10.1109/CDC56724.2024.10886285",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/CDC56724.2024.10886285",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2403.17790v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.17790v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20554v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20554v1",
                "updated": "2025-03-26T13:49:26Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    13,
                    49,
                    26,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T13:49:26Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    13,
                    49,
                    26,
                    2,
                    85,
                    0
                ],
                "title": "MAnycast Reloaded: a Tool for an Open, Fast, Responsible and Efficient\n  Daily Anycast Census",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAnycast Reloaded: a Tool for an Open, Fast, Responsible and Efficient\n  Daily Anycast Census"
                },
                "summary": "IP anycast is a widely adopted technique in which an address is replicated at\nmultiple locations, to, e.g., reduce latency and enhance resilience. Due to\nanycast's crucial role on the modern Internet, earlier research introduced\ntools to perform anycast censuses. The first, iGreedy, uses latency\nmeasurements from geographically dispersed locations to map anycast\ndeployments. The second, MAnycast2, uses anycast to perform a census of other\nanycast networks. MAnycast2's advantage is speed, performing an Internet-wide\ncensus in 3 hours, but it suffers from problems with accuracy and precision.\nInversely, iGreedy is highly accurate but much slower. On top of that, iGreedy\nhas a much higher probing cost.\n  In this paper we address the shortcomings of both systems and present\nMAnycast Reloaded (MAnycastR). Taking MAnycast2 as a basis, we completely\nredesign its measurement pipeline, and add support for distributed probing,\nadditional protocols (UDP, TCP and IPv6) and latency measurements similar to\niGreedy. We validate MAnycastR on an anycast testbed with 32 globally\ndistributed nodes, compare against an external anycast production deployment\nand extensive latency measurements with RIPE Atlas, and cross-check over 60% of\ndetected anycast prefixes against operator ground truth. This shows that\nMAnycastR achieves high accuracy and precision. We make continual daily\nMAnycastR censuses available to the community and release the source code of\nthe tool under a permissive open source license.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IP anycast is a widely adopted technique in which an address is replicated at\nmultiple locations, to, e.g., reduce latency and enhance resilience. Due to\nanycast's crucial role on the modern Internet, earlier research introduced\ntools to perform anycast censuses. The first, iGreedy, uses latency\nmeasurements from geographically dispersed locations to map anycast\ndeployments. The second, MAnycast2, uses anycast to perform a census of other\nanycast networks. MAnycast2's advantage is speed, performing an Internet-wide\ncensus in 3 hours, but it suffers from problems with accuracy and precision.\nInversely, iGreedy is highly accurate but much slower. On top of that, iGreedy\nhas a much higher probing cost.\n  In this paper we address the shortcomings of both systems and present\nMAnycast Reloaded (MAnycastR). Taking MAnycast2 as a basis, we completely\nredesign its measurement pipeline, and add support for distributed probing,\nadditional protocols (UDP, TCP and IPv6) and latency measurements similar to\niGreedy. We validate MAnycastR on an anycast testbed with 32 globally\ndistributed nodes, compare against an external anycast production deployment\nand extensive latency measurements with RIPE Atlas, and cross-check over 60% of\ndetected anycast prefixes against operator ground truth. This shows that\nMAnycastR achieves high accuracy and precision. We make continual daily\nMAnycastR censuses available to the community and release the source code of\nthe tool under a permissive open source license."
                },
                "authors": [
                    {
                        "name": "Remi Hendriks"
                    },
                    {
                        "name": "Matthew Luckie"
                    },
                    {
                        "name": "Mattijs Jonker"
                    },
                    {
                        "name": "Raffaele Sommese"
                    },
                    {
                        "name": "Roland van Rijswijk-Deij"
                    }
                ],
                "author_detail": {
                    "name": "Roland van Rijswijk-Deij"
                },
                "author": "Roland van Rijswijk-Deij",
                "arxiv_comment": "16 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20554v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20554v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20552v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20552v1",
                "updated": "2025-03-26T13:48:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    13,
                    48,
                    35,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T13:48:35Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    13,
                    48,
                    35,
                    2,
                    85,
                    0
                ],
                "title": "Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and\n  Throughput via Attention Disaggregation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and\n  Throughput via Attention Disaggregation"
                },
                "summary": "In large language model (LLM) serving systems, executing each request\nconsists of two phases: the compute-intensive prefill phase and the\nmemory-intensive decoding phase. To prevent performance interference between\nthe two phases, current LLM serving systems typically adopt prefill-decoding\ndisaggregation, where the two phases are split across separate machines.\nHowever, we observe this approach leads to significant resource\nunderutilization. Specifically, prefill instances that are compute-intensive\nsuffer from low memory utilization, while decoding instances that are\nmemory-intensive experience low compute utilization. To address this problem,\nthis paper proposes Adrenaline, an attention disaggregation and offloading\nmechanism designed to enhance resource utilization and performance in LLM\nserving systems. Adrenaline's key innovation lies in disaggregating part of the\nattention computation in the decoding phase and offloading them to prefill\ninstances. The memory-bound nature of decoding-phase attention computation\ninherently enables an effective offloading strategy, yielding two complementary\nadvantages: 1) improved memory capacity and bandwidth utilization in prefill\ninstances, and 2) increased decoding batch sizes that enhance compute\nutilization in decoding instances, collectively boosting overall system\nperformance. Adrenaline achieves these gains through three key techniques:\nlow-latency decoding synchronization, resource-efficient prefill colocation,\nand load-aware offloading scheduling. Experimental results show that Adrenaline\nachieves 2.28x higher memory capacity and 2.07x better memory bandwidth\nutilization in prefill instances, up to 1.67x improvements in compute\nutilization for decoding instances, and 1.68x higher overall inference\nthroughput compared to state-of-the-art systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In large language model (LLM) serving systems, executing each request\nconsists of two phases: the compute-intensive prefill phase and the\nmemory-intensive decoding phase. To prevent performance interference between\nthe two phases, current LLM serving systems typically adopt prefill-decoding\ndisaggregation, where the two phases are split across separate machines.\nHowever, we observe this approach leads to significant resource\nunderutilization. Specifically, prefill instances that are compute-intensive\nsuffer from low memory utilization, while decoding instances that are\nmemory-intensive experience low compute utilization. To address this problem,\nthis paper proposes Adrenaline, an attention disaggregation and offloading\nmechanism designed to enhance resource utilization and performance in LLM\nserving systems. Adrenaline's key innovation lies in disaggregating part of the\nattention computation in the decoding phase and offloading them to prefill\ninstances. The memory-bound nature of decoding-phase attention computation\ninherently enables an effective offloading strategy, yielding two complementary\nadvantages: 1) improved memory capacity and bandwidth utilization in prefill\ninstances, and 2) increased decoding batch sizes that enhance compute\nutilization in decoding instances, collectively boosting overall system\nperformance. Adrenaline achieves these gains through three key techniques:\nlow-latency decoding synchronization, resource-efficient prefill colocation,\nand load-aware offloading scheduling. Experimental results show that Adrenaline\nachieves 2.28x higher memory capacity and 2.07x better memory bandwidth\nutilization in prefill instances, up to 1.67x improvements in compute\nutilization for decoding instances, and 1.68x higher overall inference\nthroughput compared to state-of-the-art systems."
                },
                "authors": [
                    {
                        "name": "Yunkai Liang"
                    },
                    {
                        "name": "Zhangyu Chen"
                    },
                    {
                        "name": "Pengfei Zuo"
                    },
                    {
                        "name": "Zhi Zhou"
                    },
                    {
                        "name": "Xu Chen"
                    },
                    {
                        "name": "Zhou Yu"
                    }
                ],
                "author_detail": {
                    "name": "Zhou Yu"
                },
                "author": "Zhou Yu",
                "arxiv_comment": "14 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20552v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20552v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20544v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20544v1",
                "updated": "2025-03-26T13:40:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    13,
                    40,
                    8,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T13:40:08Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    13,
                    40,
                    8,
                    2,
                    85,
                    0
                ],
                "title": "Safety integrity framework for automated driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safety integrity framework for automated driving"
                },
                "summary": "This paper describes the comprehensive safety framework that underpinned the\ndevelopment, release process, and regulatory approval of BMW's first SAE Level\n3 Automated Driving System. The framework combines established qualitative and\nquantitative methods from the fields of Systems Engineering, Engineering Risk\nAnalysis, Bayesian Data Analysis, Design of Experiments, and Statistical\nLearning in a novel manner. The approach systematically minimizes the risks\nassociated with hardware and software faults, performance limitations, and\ninsufficient specifications to an acceptable level that achieves a Positive\nRisk Balance. At the core of the framework is the systematic identification and\nquantification of uncertainties associated with hazard scenarios and the\nredundantly designed system based on designed experiments, field data, and\nexpert knowledge. The residual risk of the system is then estimated through\nStochastic Simulation and evaluated by Sensitivity Analysis. By integrating\nthese advanced analytical techniques into the V-Model, the framework fulfills,\nunifies, and complements existing automotive safety standards. It therefore\nprovides a comprehensive, rigorous, and transparent safety assurance process\nfor the development and deployment of Automated Driving Systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper describes the comprehensive safety framework that underpinned the\ndevelopment, release process, and regulatory approval of BMW's first SAE Level\n3 Automated Driving System. The framework combines established qualitative and\nquantitative methods from the fields of Systems Engineering, Engineering Risk\nAnalysis, Bayesian Data Analysis, Design of Experiments, and Statistical\nLearning in a novel manner. The approach systematically minimizes the risks\nassociated with hardware and software faults, performance limitations, and\ninsufficient specifications to an acceptable level that achieves a Positive\nRisk Balance. At the core of the framework is the systematic identification and\nquantification of uncertainties associated with hazard scenarios and the\nredundantly designed system based on designed experiments, field data, and\nexpert knowledge. The residual risk of the system is then estimated through\nStochastic Simulation and evaluated by Sensitivity Analysis. By integrating\nthese advanced analytical techniques into the V-Model, the framework fulfills,\nunifies, and complements existing automotive safety standards. It therefore\nprovides a comprehensive, rigorous, and transparent safety assurance process\nfor the development and deployment of Automated Driving Systems."
                },
                "authors": [
                    {
                        "name": "Moritz Werling"
                    },
                    {
                        "name": "Rainer Faller"
                    },
                    {
                        "name": "Wolfgang Betz"
                    },
                    {
                        "name": "Daniel Straub"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Straub"
                },
                "author": "Daniel Straub",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20544v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20544v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07544v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07544v2",
                "updated": "2025-03-26T13:39:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    13,
                    39,
                    58,
                    2,
                    85,
                    0
                ],
                "published": "2024-12-10T14:28:18Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    14,
                    28,
                    18,
                    1,
                    345,
                    0
                ],
                "title": "Contractive Dynamical Imitation Policies for Efficient Out-of-Sample\n  Recovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contractive Dynamical Imitation Policies for Efficient Out-of-Sample\n  Recovery"
                },
                "summary": "Imitation learning is a data-driven approach to learning policies from expert\nbehavior, but it is prone to unreliable outcomes in out-of-sample (OOS)\nregions. While previous research relying on stable dynamical systems guarantees\nconvergence to a desired state, it often overlooks transient behavior. We\npropose a framework for learning policies modeled by contractive dynamical\nsystems, ensuring that all policy rollouts converge regardless of\nperturbations, and in turn, enable efficient OOS recovery. By leveraging\nrecurrent equilibrium networks and coupling layers, the policy structure\nguarantees contractivity for any parameter choice, which facilitates\nunconstrained optimization. We also provide theoretical upper bounds for\nworst-case and expected loss to rigorously establish the reliability of our\nmethod in deployment. Empirically, we demonstrate substantial OOS performance\nimprovements for simulated robotic manipulation and navigation tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Imitation learning is a data-driven approach to learning policies from expert\nbehavior, but it is prone to unreliable outcomes in out-of-sample (OOS)\nregions. While previous research relying on stable dynamical systems guarantees\nconvergence to a desired state, it often overlooks transient behavior. We\npropose a framework for learning policies modeled by contractive dynamical\nsystems, ensuring that all policy rollouts converge regardless of\nperturbations, and in turn, enable efficient OOS recovery. By leveraging\nrecurrent equilibrium networks and coupling layers, the policy structure\nguarantees contractivity for any parameter choice, which facilitates\nunconstrained optimization. We also provide theoretical upper bounds for\nworst-case and expected loss to rigorously establish the reliability of our\nmethod in deployment. Empirically, we demonstrate substantial OOS performance\nimprovements for simulated robotic manipulation and navigation tasks."
                },
                "authors": [
                    {
                        "name": "Amin Abyaneh"
                    },
                    {
                        "name": "Mahrokh G. Boroujeni"
                    },
                    {
                        "name": "Hsiu-Chin Lin"
                    },
                    {
                        "name": "Giancarlo Ferrari-Trecate"
                    }
                ],
                "author_detail": {
                    "name": "Giancarlo Ferrari-Trecate"
                },
                "author": "Giancarlo Ferrari-Trecate",
                "arxiv_comment": "International Conference on Learning Representations",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07544v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07544v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18227v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18227v3",
                "updated": "2025-03-26T13:38:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    13,
                    38,
                    40,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-23T22:06:07Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    22,
                    6,
                    7,
                    6,
                    82,
                    0
                ],
                "title": "PG-SAM: Prior-Guided SAM with Medical for Multi-organ Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PG-SAM: Prior-Guided SAM with Medical for Multi-organ Segmentation"
                },
                "summary": "Segment Anything Model (SAM) demonstrates powerful zero-shot capabilities;\nhowever, its accuracy and robustness significantly decrease when applied to\nmedical image segmentation. Existing methods address this issue through\nmodality fusion, integrating textual and image information to provide more\ndetailed priors. In this study, we argue that the granularity of text and the\ndomain gap affect the accuracy of the priors. Furthermore, the discrepancy\nbetween high-level abstract semantics and pixel-level boundary details in\nimages can introduce noise into the fusion process. To address this, we propose\nPrior-Guided SAM (PG-SAM), which employs a fine-grained modality prior aligner\nto leverage specialized medical knowledge for better modality alignment. The\ncore of our method lies in efficiently addressing the domain gap with\nfine-grained text from a medical LLM. Meanwhile, it also enhances the priors'\nquality after modality alignment, ensuring more accurate segmentation. In\naddition, our decoder enhances the model's expressive capabilities through\nmulti-level feature fusion and iterative mask optimizer operations, supporting\nunprompted learning. We also propose a unified pipeline that effectively\nsupplies high-quality semantic information to SAM. Extensive experiments on the\nSynapse dataset demonstrate that the proposed PG-SAM achieves state-of-the-art\nperformance. Our code is released at https://github.com/logan-0623/PG-SAM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Segment Anything Model (SAM) demonstrates powerful zero-shot capabilities;\nhowever, its accuracy and robustness significantly decrease when applied to\nmedical image segmentation. Existing methods address this issue through\nmodality fusion, integrating textual and image information to provide more\ndetailed priors. In this study, we argue that the granularity of text and the\ndomain gap affect the accuracy of the priors. Furthermore, the discrepancy\nbetween high-level abstract semantics and pixel-level boundary details in\nimages can introduce noise into the fusion process. To address this, we propose\nPrior-Guided SAM (PG-SAM), which employs a fine-grained modality prior aligner\nto leverage specialized medical knowledge for better modality alignment. The\ncore of our method lies in efficiently addressing the domain gap with\nfine-grained text from a medical LLM. Meanwhile, it also enhances the priors'\nquality after modality alignment, ensuring more accurate segmentation. In\naddition, our decoder enhances the model's expressive capabilities through\nmulti-level feature fusion and iterative mask optimizer operations, supporting\nunprompted learning. We also propose a unified pipeline that effectively\nsupplies high-quality semantic information to SAM. Extensive experiments on the\nSynapse dataset demonstrate that the proposed PG-SAM achieves state-of-the-art\nperformance. Our code is released at https://github.com/logan-0623/PG-SAM."
                },
                "authors": [
                    {
                        "name": "Yiheng Zhong"
                    },
                    {
                        "name": "Zihong Luo"
                    },
                    {
                        "name": "Chengzhi Liu"
                    },
                    {
                        "name": "Feilong Tang"
                    },
                    {
                        "name": "Zelin Peng"
                    },
                    {
                        "name": "Ming Hu"
                    },
                    {
                        "name": "Yingzhen Hu"
                    },
                    {
                        "name": "Jionglong Su"
                    },
                    {
                        "name": "Zongyuan Ge"
                    },
                    {
                        "name": "Imran Razzak"
                    }
                ],
                "author_detail": {
                    "name": "Imran Razzak"
                },
                "author": "Imran Razzak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18227v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18227v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20536v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20536v1",
                "updated": "2025-03-26T13:35:10Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    13,
                    35,
                    10,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T13:35:10Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    13,
                    35,
                    10,
                    2,
                    85,
                    0
                ],
                "title": "Knowledge-Based Multi-Agent Framework for Automated Software\n  Architecture Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge-Based Multi-Agent Framework for Automated Software\n  Architecture Design"
                },
                "summary": "Architecture design is a critical step in software development. However,\ncreating a high-quality architecture is often costly due to the significant\nneed for human expertise and manual effort. Recently, agents built upon Large\nLanguage Models (LLMs) have achieved remarkable success in various software\nengineering tasks. Despite this progress, the use of agents to automate the\narchitecture design process remains largely unexplored. To address this gap, we\nenvision a Knowledge-based Multi-Agent Architecture Design (MAAD) framework.\nMAAD uses agents to simulate human roles in the traditional software\narchitecture design process, thereby automating the design process. To empower\nthese agents, MAAD incorporates knowledge extracted from three key sources: 1)\nexisting system designs, 2) authoritative literature, and 3) architecture\nexperts. By envisioning the MAAD framework, we aim to advance the full\nautomation of application-level system development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Architecture design is a critical step in software development. However,\ncreating a high-quality architecture is often costly due to the significant\nneed for human expertise and manual effort. Recently, agents built upon Large\nLanguage Models (LLMs) have achieved remarkable success in various software\nengineering tasks. Despite this progress, the use of agents to automate the\narchitecture design process remains largely unexplored. To address this gap, we\nenvision a Knowledge-based Multi-Agent Architecture Design (MAAD) framework.\nMAAD uses agents to simulate human roles in the traditional software\narchitecture design process, thereby automating the design process. To empower\nthese agents, MAAD incorporates knowledge extracted from three key sources: 1)\nexisting system designs, 2) authoritative literature, and 3) architecture\nexperts. By envisioning the MAAD framework, we aim to advance the full\nautomation of application-level system development."
                },
                "authors": [
                    {
                        "name": "Yiran Zhang"
                    },
                    {
                        "name": "Ruiyin Li"
                    },
                    {
                        "name": "Peng Liang"
                    },
                    {
                        "name": "Weisong Sun"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20536v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20536v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02550v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02550v3",
                "updated": "2025-03-26T13:27:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    13,
                    27,
                    14,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-04T12:21:28Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    12,
                    21,
                    28,
                    1,
                    63,
                    0
                ],
                "title": "SpecInF: Exploiting Idle GPU Resources in Distributed DL Training via\n  Speculative Inference Filling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecInF: Exploiting Idle GPU Resources in Distributed DL Training via\n  Speculative Inference Filling"
                },
                "summary": "Deep Learning (DL), especially with Large Language Models (LLMs), brings\nbenefits to various areas. However, DL training systems usually yield prominent\nidling GPU resources due to many factors, such as resource allocation and\ncollective communication. To improve GPU utilization, we present SpecInF, which\nadopts a Speculative Inference Filling method to exploit idle GPU resources. It\ncollocates each primary training instance with additional inference instances\non the same GPU, detects the training bubbles and adaptively fills with online\nor offline inference workloads. Our results show that SpecInF can effectively\nenhance GPU utilization under mainstream parallel training modes, delivering\nadditional up to 14$\\times$ offline inference throughputs than TGS and 67\\%\nreduction in online inference p95 latency than MPS, while guaranteeing\ncollocated training throughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning (DL), especially with Large Language Models (LLMs), brings\nbenefits to various areas. However, DL training systems usually yield prominent\nidling GPU resources due to many factors, such as resource allocation and\ncollective communication. To improve GPU utilization, we present SpecInF, which\nadopts a Speculative Inference Filling method to exploit idle GPU resources. It\ncollocates each primary training instance with additional inference instances\non the same GPU, detects the training bubbles and adaptively fills with online\nor offline inference workloads. Our results show that SpecInF can effectively\nenhance GPU utilization under mainstream parallel training modes, delivering\nadditional up to 14$\\times$ offline inference throughputs than TGS and 67\\%\nreduction in online inference p95 latency than MPS, while guaranteeing\ncollocated training throughput."
                },
                "authors": [
                    {
                        "name": "Cunchi Lv"
                    },
                    {
                        "name": "Xiao Shi"
                    },
                    {
                        "name": "Dong Liang"
                    },
                    {
                        "name": "Wenting Tan"
                    },
                    {
                        "name": "Xiaofang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Xiaofang Zhao"
                },
                "author": "Xiaofang Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02550v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02550v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10927v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10927v2",
                "updated": "2025-03-26T13:24:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    13,
                    24,
                    43,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-13T22:28:38Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    22,
                    28,
                    38,
                    3,
                    72,
                    0
                ],
                "title": "OASST-ETC Dataset: Alignment Signals from Eye-tracking Analysis of LLM\n  Responses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OASST-ETC Dataset: Alignment Signals from Eye-tracking Analysis of LLM\n  Responses"
                },
                "summary": "While Large Language Models (LLMs) have significantly advanced natural\nlanguage processing, aligning them with human preferences remains an open\nchallenge. Although current alignment methods rely primarily on explicit\nfeedback, eye-tracking (ET) data offers insights into real-time cognitive\nprocessing during reading. In this paper, we present OASST-ETC, a novel\neye-tracking corpus capturing reading patterns from 24 participants, while\nevaluating LLM-generated responses from the OASST1 dataset. Our analysis\nreveals distinct reading patterns between preferred and non-preferred\nresponses, which we compare with synthetic eye-tracking data. Furthermore, we\nexamine the correlation between human reading measures and attention patterns\nfrom various transformer-based models, discovering stronger correlations in\npreferred responses. This work introduces a unique resource for studying human\ncognitive processing in LLM evaluation and suggests promising directions for\nincorporating eye-tracking data into alignment methods. The dataset and\nanalysis code are publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large Language Models (LLMs) have significantly advanced natural\nlanguage processing, aligning them with human preferences remains an open\nchallenge. Although current alignment methods rely primarily on explicit\nfeedback, eye-tracking (ET) data offers insights into real-time cognitive\nprocessing during reading. In this paper, we present OASST-ETC, a novel\neye-tracking corpus capturing reading patterns from 24 participants, while\nevaluating LLM-generated responses from the OASST1 dataset. Our analysis\nreveals distinct reading patterns between preferred and non-preferred\nresponses, which we compare with synthetic eye-tracking data. Furthermore, we\nexamine the correlation between human reading measures and attention patterns\nfrom various transformer-based models, discovering stronger correlations in\npreferred responses. This work introduces a unique resource for studying human\ncognitive processing in LLM evaluation and suggests promising directions for\nincorporating eye-tracking data into alignment methods. The dataset and\nanalysis code are publicly available."
                },
                "authors": [
                    {
                        "name": "Angela Lopez-Cardona"
                    },
                    {
                        "name": "Sebastian Idesis"
                    },
                    {
                        "name": "Miguel Barreda-Ángeles"
                    },
                    {
                        "name": "Sergi Abadal"
                    },
                    {
                        "name": "Ioannis Arapakis"
                    }
                ],
                "author_detail": {
                    "name": "Ioannis Arapakis"
                },
                "author": "Ioannis Arapakis",
                "arxiv_doi": "10.1145/3725840",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3725840",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.10927v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10927v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This paper has been accepted to ACM ETRA 2025 and published on\n  PACMHCI",
                "arxiv_journal_ref": "Proceedings of the ACM on Human-Computer Interaction. 2025",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05167v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05167v2",
                "updated": "2025-03-26T13:23:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    13,
                    23,
                    30,
                    2,
                    85,
                    0
                ],
                "published": "2025-02-07T18:49:46Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    18,
                    49,
                    46,
                    4,
                    38,
                    0
                ],
                "title": "NoLiMa: Long-Context Evaluation Beyond Literal Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NoLiMa: Long-Context Evaluation Beyond Literal Matching"
                },
                "summary": "Recent large language models (LLMs) support long contexts ranging from 128K\nto 1M tokens. A popular method for evaluating these capabilities is the\nneedle-in-a-haystack (NIAH) test, which involves retrieving a \"needle\"\n(relevant information) from a \"haystack\" (long irrelevant context). Extensions\nof this approach include increasing distractors, fact chaining, and in-context\nreasoning. However, in these benchmarks, models can exploit existing literal\nmatches between the needle and haystack to simplify the task. To address this,\nwe introduce NoLiMa, a benchmark extending NIAH with a carefully designed\nneedle set, where questions and needles have minimal lexical overlap, requiring\nmodels to infer latent associations to locate the needle within the haystack.\nWe evaluate 12 popular LLMs that claim to support contexts of at least 128K\ntokens. While they perform well in short contexts (<1K), performance degrades\nsignificantly as context length increases. At 32K, for instance, 10 models drop\nbelow 50% of their strong short-length baselines. Even GPT-4o, one of the\ntop-performing exceptions, experiences a reduction from an almost-perfect\nbaseline of 99.3% to 69.7%. Our analysis suggests these declines stem from the\nincreased difficulty the attention mechanism faces in longer contexts when\nliteral matches are absent, making it harder to retrieve relevant information.\nWe publicly release the dataset and evaluation code at\nhttps://github.com/adobe-research/NoLiMa.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language models (LLMs) support long contexts ranging from 128K\nto 1M tokens. A popular method for evaluating these capabilities is the\nneedle-in-a-haystack (NIAH) test, which involves retrieving a \"needle\"\n(relevant information) from a \"haystack\" (long irrelevant context). Extensions\nof this approach include increasing distractors, fact chaining, and in-context\nreasoning. However, in these benchmarks, models can exploit existing literal\nmatches between the needle and haystack to simplify the task. To address this,\nwe introduce NoLiMa, a benchmark extending NIAH with a carefully designed\nneedle set, where questions and needles have minimal lexical overlap, requiring\nmodels to infer latent associations to locate the needle within the haystack.\nWe evaluate 12 popular LLMs that claim to support contexts of at least 128K\ntokens. While they perform well in short contexts (<1K), performance degrades\nsignificantly as context length increases. At 32K, for instance, 10 models drop\nbelow 50% of their strong short-length baselines. Even GPT-4o, one of the\ntop-performing exceptions, experiences a reduction from an almost-perfect\nbaseline of 99.3% to 69.7%. Our analysis suggests these declines stem from the\nincreased difficulty the attention mechanism faces in longer contexts when\nliteral matches are absent, making it harder to retrieve relevant information.\nWe publicly release the dataset and evaluation code at\nhttps://github.com/adobe-research/NoLiMa."
                },
                "authors": [
                    {
                        "name": "Ali Modarressi"
                    },
                    {
                        "name": "Hanieh Deilamsalehy"
                    },
                    {
                        "name": "Franck Dernoncourt"
                    },
                    {
                        "name": "Trung Bui"
                    },
                    {
                        "name": "Ryan A. Rossi"
                    },
                    {
                        "name": "Seunghyun Yoon"
                    },
                    {
                        "name": "Hinrich Schütze"
                    }
                ],
                "author_detail": {
                    "name": "Hinrich Schütze"
                },
                "author": "Hinrich Schütze",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05167v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05167v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20527v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20527v1",
                "updated": "2025-03-26T13:13:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    13,
                    13,
                    3,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T13:13:03Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    13,
                    13,
                    3,
                    2,
                    85,
                    0
                ],
                "title": "StableToolBench-MirrorAPI: Modeling Tool Environments as Mirrors of\n  7,000+ Real-World APIs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StableToolBench-MirrorAPI: Modeling Tool Environments as Mirrors of\n  7,000+ Real-World APIs"
                },
                "summary": "The rapid advancement of large language models (LLMs) has spurred significant\ninterest in tool learning, where LLMs are augmented with external tools to\ntackle complex tasks. However, existing tool environments face challenges in\nbalancing stability, scalability, and realness, particularly for benchmarking\npurposes. To address this problem, we propose MirrorAPI, a novel framework that\ntrains specialized LLMs to accurately simulate real API responses, effectively\nacting as \"mirrors\" to tool environments. Using a comprehensive dataset of\nrequest-response pairs from 7,000+ APIs, we employ supervised fine-tuning and\nchain-of-thought reasoning to enhance simulation fidelity. MirrorAPI achieves\nsuperior accuracy and stability compared to state-of-the-art methods, as\ndemonstrated by its performance on the newly constructed MirrorAPI-Bench and\nits integration into StableToolBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) has spurred significant\ninterest in tool learning, where LLMs are augmented with external tools to\ntackle complex tasks. However, existing tool environments face challenges in\nbalancing stability, scalability, and realness, particularly for benchmarking\npurposes. To address this problem, we propose MirrorAPI, a novel framework that\ntrains specialized LLMs to accurately simulate real API responses, effectively\nacting as \"mirrors\" to tool environments. Using a comprehensive dataset of\nrequest-response pairs from 7,000+ APIs, we employ supervised fine-tuning and\nchain-of-thought reasoning to enhance simulation fidelity. MirrorAPI achieves\nsuperior accuracy and stability compared to state-of-the-art methods, as\ndemonstrated by its performance on the newly constructed MirrorAPI-Bench and\nits integration into StableToolBench."
                },
                "authors": [
                    {
                        "name": "Zhicheng Guo"
                    },
                    {
                        "name": "Sijie Cheng"
                    },
                    {
                        "name": "Yuchen Niu"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Sicheng Zhou"
                    },
                    {
                        "name": "Wenbing Huang"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20527v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20527v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15133v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15133v2",
                "updated": "2025-03-26T13:08:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    13,
                    8,
                    41,
                    2,
                    85,
                    0
                ],
                "published": "2024-09-23T15:38:12Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    15,
                    38,
                    12,
                    0,
                    267,
                    0
                ],
                "title": "Don't Use LLMs to Make Relevance Judgments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Don't Use LLMs to Make Relevance Judgments"
                },
                "summary": "Making the relevance judgments for a TREC-style test collection can be\ncomplex and expensive. A typical TREC track usually involves a team of six\ncontractors working for 2-4 weeks. Those contractors need to be trained and\nmonitored. Software has to be written to support recording relevance judgments\ncorrectly and efficiently. The recent advent of large language models that\nproduce astoundingly human-like flowing text output in response to a natural\nlanguage prompt has inspired IR researchers to wonder how those models might be\nused in the relevance judgment collection process. At the ACM SIGIR 2024\nconference, a workshop ``LLM4Eval'' provided a venue for this work, and\nfeatured a data challenge activity where participants reproduced TREC deep\nlearning track judgments, as was done by Thomas et al (arXiv:2408.08896,\narXiv:2309.10621). I was asked to give a keynote at the workshop, and this\npaper presents that keynote in article form. The bottom-line-up-front message\nis, don't use LLMs to create relevance judgments for TREC-style evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Making the relevance judgments for a TREC-style test collection can be\ncomplex and expensive. A typical TREC track usually involves a team of six\ncontractors working for 2-4 weeks. Those contractors need to be trained and\nmonitored. Software has to be written to support recording relevance judgments\ncorrectly and efficiently. The recent advent of large language models that\nproduce astoundingly human-like flowing text output in response to a natural\nlanguage prompt has inspired IR researchers to wonder how those models might be\nused in the relevance judgment collection process. At the ACM SIGIR 2024\nconference, a workshop ``LLM4Eval'' provided a venue for this work, and\nfeatured a data challenge activity where participants reproduced TREC deep\nlearning track judgments, as was done by Thomas et al (arXiv:2408.08896,\narXiv:2309.10621). I was asked to give a keynote at the workshop, and this\npaper presents that keynote in article form. The bottom-line-up-front message\nis, don't use LLMs to create relevance judgments for TREC-style evaluations."
                },
                "authors": [
                    {
                        "name": "Ian Soboroff"
                    }
                ],
                "author_detail": {
                    "name": "Ian Soboroff"
                },
                "author": "Ian Soboroff",
                "arxiv_doi": "10.54195/irrj.19625",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.54195/irrj.19625",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.15133v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15133v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Information Retrieval Research. 1, 1 (Mar. 2025), 29-46",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20518v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20518v1",
                "updated": "2025-03-26T13:00:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    13,
                    0,
                    5,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T13:00:05Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    13,
                    0,
                    5,
                    2,
                    85,
                    0
                ],
                "title": "Exploring the Effect of Robotic Embodiment and Empathetic Tone of LLMs\n  on Empathy Elicitation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Effect of Robotic Embodiment and Empathetic Tone of LLMs\n  on Empathy Elicitation"
                },
                "summary": "This study investigates the elicitation of empathy toward a third party\nthrough interaction with social agents. Participants engaged with either a\nphysical robot or a voice-enabled chatbot, both driven by a large language\nmodel (LLM) programmed to exhibit either an empathetic tone or remain neutral.\nThe interaction is focused on a fictional character, Katie Banks, who is in a\nchallenging situation and in need of financial donations. The willingness to\nhelp Katie, measured by the number of hours participants were willing to\nvolunteer, along with their perceptions of the agent, were assessed for 60\nparticipants. Results indicate that neither robotic embodiment nor empathetic\ntone significantly influenced participants' willingness to volunteer. While the\nLLM effectively simulated human empathy, fostering genuine empathetic responses\nin participants proved challenging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study investigates the elicitation of empathy toward a third party\nthrough interaction with social agents. Participants engaged with either a\nphysical robot or a voice-enabled chatbot, both driven by a large language\nmodel (LLM) programmed to exhibit either an empathetic tone or remain neutral.\nThe interaction is focused on a fictional character, Katie Banks, who is in a\nchallenging situation and in need of financial donations. The willingness to\nhelp Katie, measured by the number of hours participants were willing to\nvolunteer, along with their perceptions of the agent, were assessed for 60\nparticipants. Results indicate that neither robotic embodiment nor empathetic\ntone significantly influenced participants' willingness to volunteer. While the\nLLM effectively simulated human empathy, fostering genuine empathetic responses\nin participants proved challenging."
                },
                "authors": [
                    {
                        "name": "Liza Darwesh"
                    },
                    {
                        "name": "Jaspreet Singh"
                    },
                    {
                        "name": "Marin Marian"
                    },
                    {
                        "name": "Eduard Alexa"
                    },
                    {
                        "name": "Koen Hindriks"
                    },
                    {
                        "name": "Kim Baraka"
                    }
                ],
                "author_detail": {
                    "name": "Kim Baraka"
                },
                "author": "Kim Baraka",
                "arxiv_doi": "10.1007/978-981-96-3525-2_1",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-981-96-3525-2_1",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.20518v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20518v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "*Liza Darwesh, Jaspreet Singh, Marin Marian, and Eduard Alexa\n  contributed equally to this work.*",
                "arxiv_journal_ref": "Proceedings of the International Conference on Social Robotics\n  (ICSR 2024), Springer, 2025, pp. 1-11",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.9, I.2.7, H.5.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20508v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20508v1",
                "updated": "2025-03-26T12:49:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    12,
                    49,
                    35,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T12:49:35Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    12,
                    49,
                    35,
                    2,
                    85,
                    0
                ],
                "title": "Explainable ICD Coding via Entity Linking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explainable ICD Coding via Entity Linking"
                },
                "summary": "Clinical coding is a critical task in healthcare, although traditional\nmethods for automating clinical coding may not provide sufficient explicit\nevidence for coders in production environments. This evidence is crucial, as\nmedical coders have to make sure there exists at least one explicit passage in\nthe input health record that justifies the attribution of a code. We therefore\npropose to reframe the task as an entity linking problem, in which each\ndocument is annotated with its set of codes and respective textual evidence,\nenabling better human-machine collaboration. By leveraging parameter-efficient\nfine-tuning of Large Language Models (LLMs), together with constrained\ndecoding, we introduce three approaches to solve this problem that prove\neffective at disambiguating clinical mentions and that perform well in few-shot\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clinical coding is a critical task in healthcare, although traditional\nmethods for automating clinical coding may not provide sufficient explicit\nevidence for coders in production environments. This evidence is crucial, as\nmedical coders have to make sure there exists at least one explicit passage in\nthe input health record that justifies the attribution of a code. We therefore\npropose to reframe the task as an entity linking problem, in which each\ndocument is annotated with its set of codes and respective textual evidence,\nenabling better human-machine collaboration. By leveraging parameter-efficient\nfine-tuning of Large Language Models (LLMs), together with constrained\ndecoding, we introduce three approaches to solve this problem that prove\neffective at disambiguating clinical mentions and that perform well in few-shot\nscenarios."
                },
                "authors": [
                    {
                        "name": "Leonor Barreiros"
                    },
                    {
                        "name": "Isabel Coutinho"
                    },
                    {
                        "name": "Gonçalo M. Correia"
                    },
                    {
                        "name": "Bruno Martins"
                    }
                ],
                "author_detail": {
                    "name": "Bruno Martins"
                },
                "author": "Bruno Martins",
                "arxiv_comment": "Accepted at CL4Health at NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20508v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20508v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20504v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20504v1",
                "updated": "2025-03-26T12:45:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    12,
                    45,
                    34,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T12:45:34Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    12,
                    45,
                    34,
                    2,
                    85,
                    0
                ],
                "title": "Vision-Amplified Semantic Entropy for Hallucination Detection in Medical\n  Visual Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Amplified Semantic Entropy for Hallucination Detection in Medical\n  Visual Question Answering"
                },
                "summary": "Multimodal large language models (MLLMs) have demonstrated significant\npotential in medical Visual Question Answering (VQA). Yet, they remain prone to\nhallucinations-incorrect responses that contradict input images, posing\nsubstantial risks in clinical decision-making. Detecting these hallucinations\nis essential for establishing trust in MLLMs among clinicians and patients,\nthereby enabling their real-world adoption. Current hallucination detection\nmethods, especially semantic entropy (SE), have demonstrated promising\nhallucination detection capacity for LLMs. However, adapting SE to medical\nMLLMs by incorporating visual perturbations presents a dilemma. Weak\nperturbations preserve image content and ensure clinical validity, but may be\noverlooked by medical MLLMs, which tend to over rely on language priors. In\ncontrast, strong perturbations can distort essential diagnostic features,\ncompromising clinical interpretation. To address this issue, we propose Vision\nAmplified Semantic Entropy (VASE), which incorporates weak image\ntransformations and amplifies the impact of visual input, to improve\nhallucination detection in medical VQA. We first estimate the semantic\npredictive distribution under weak visual transformations to preserve clinical\nvalidity, and then amplify visual influence by contrasting this distribution\nwith that derived from a distorted image. The entropy of the resulting\ndistribution is estimated as VASE. Experiments on two medical open-ended VQA\ndatasets demonstrate that VASE consistently outperforms existing hallucination\ndetection methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) have demonstrated significant\npotential in medical Visual Question Answering (VQA). Yet, they remain prone to\nhallucinations-incorrect responses that contradict input images, posing\nsubstantial risks in clinical decision-making. Detecting these hallucinations\nis essential for establishing trust in MLLMs among clinicians and patients,\nthereby enabling their real-world adoption. Current hallucination detection\nmethods, especially semantic entropy (SE), have demonstrated promising\nhallucination detection capacity for LLMs. However, adapting SE to medical\nMLLMs by incorporating visual perturbations presents a dilemma. Weak\nperturbations preserve image content and ensure clinical validity, but may be\noverlooked by medical MLLMs, which tend to over rely on language priors. In\ncontrast, strong perturbations can distort essential diagnostic features,\ncompromising clinical interpretation. To address this issue, we propose Vision\nAmplified Semantic Entropy (VASE), which incorporates weak image\ntransformations and amplifies the impact of visual input, to improve\nhallucination detection in medical VQA. We first estimate the semantic\npredictive distribution under weak visual transformations to preserve clinical\nvalidity, and then amplify visual influence by contrasting this distribution\nwith that derived from a distorted image. The entropy of the resulting\ndistribution is estimated as VASE. Experiments on two medical open-ended VQA\ndatasets demonstrate that VASE consistently outperforms existing hallucination\ndetection methods."
                },
                "authors": [
                    {
                        "name": "Zehui Liao"
                    },
                    {
                        "name": "Shishuai Hu"
                    },
                    {
                        "name": "Ke Zou"
                    },
                    {
                        "name": "Huazhu Fu"
                    },
                    {
                        "name": "Liangli Zhen"
                    },
                    {
                        "name": "Yong Xia"
                    }
                ],
                "author_detail": {
                    "name": "Yong Xia"
                },
                "author": "Yong Xia",
                "arxiv_comment": "11 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20504v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20504v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20492v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20492v1",
                "updated": "2025-03-26T12:31:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    12,
                    31,
                    4,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T12:31:04Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    12,
                    31,
                    4,
                    2,
                    85,
                    0
                ],
                "title": "Towards Efficient and General-Purpose Few-Shot Misclassification\n  Detection for Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Efficient and General-Purpose Few-Shot Misclassification\n  Detection for Vision-Language Models"
                },
                "summary": "Reliable prediction by classifiers is crucial for their deployment in high\nsecurity and dynamically changing situations. However, modern neural networks\noften exhibit overconfidence for misclassified predictions, highlighting the\nneed for confidence estimation to detect errors. Despite the achievements\nobtained by existing methods on small-scale datasets, they all require training\nfrom scratch and there are no efficient and effective misclassification\ndetection (MisD) methods, hindering practical application towards large-scale\nand ever-changing datasets. In this paper, we pave the way to exploit vision\nlanguage model (VLM) leveraging text information to establish an efficient and\ngeneral-purpose misclassification detection framework. By harnessing the power\nof VLM, we construct FSMisD, a Few-Shot prompt learning framework for MisD to\nrefrain from training from scratch and therefore improve tuning efficiency. To\nenhance misclassification detection ability, we use adaptive pseudo sample\ngeneration and a novel negative loss to mitigate the issue of overconfidence by\npushing category prompts away from pseudo features. We conduct comprehensive\nexperiments with prompt learning methods and validate the generalization\nability across various datasets with domain shift. Significant and consistent\nimprovement demonstrates the effectiveness, efficiency and generalizability of\nour approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliable prediction by classifiers is crucial for their deployment in high\nsecurity and dynamically changing situations. However, modern neural networks\noften exhibit overconfidence for misclassified predictions, highlighting the\nneed for confidence estimation to detect errors. Despite the achievements\nobtained by existing methods on small-scale datasets, they all require training\nfrom scratch and there are no efficient and effective misclassification\ndetection (MisD) methods, hindering practical application towards large-scale\nand ever-changing datasets. In this paper, we pave the way to exploit vision\nlanguage model (VLM) leveraging text information to establish an efficient and\ngeneral-purpose misclassification detection framework. By harnessing the power\nof VLM, we construct FSMisD, a Few-Shot prompt learning framework for MisD to\nrefrain from training from scratch and therefore improve tuning efficiency. To\nenhance misclassification detection ability, we use adaptive pseudo sample\ngeneration and a novel negative loss to mitigate the issue of overconfidence by\npushing category prompts away from pseudo features. We conduct comprehensive\nexperiments with prompt learning methods and validate the generalization\nability across various datasets with domain shift. Significant and consistent\nimprovement demonstrates the effectiveness, efficiency and generalizability of\nour approach."
                },
                "authors": [
                    {
                        "name": "Fanhu Zeng"
                    },
                    {
                        "name": "Zhen Cheng"
                    },
                    {
                        "name": "Fei Zhu"
                    },
                    {
                        "name": "Xu-Yao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xu-Yao Zhang"
                },
                "author": "Xu-Yao Zhang",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20492v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20492v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20491v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20491v1",
                "updated": "2025-03-26T12:28:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    12,
                    28,
                    20,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T12:28:20Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    12,
                    28,
                    20,
                    2,
                    85,
                    0
                ],
                "title": "VPO: Aligning Text-to-Video Generation Models with Prompt Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VPO: Aligning Text-to-Video Generation Models with Prompt Optimization"
                },
                "summary": "Video generation models have achieved remarkable progress in text-to-video\ntasks. These models are typically trained on text-video pairs with highly\ndetailed and carefully crafted descriptions, while real-world user inputs\nduring inference are often concise, vague, or poorly structured. This gap makes\nprompt optimization crucial for generating high-quality videos. Current methods\noften rely on large language models (LLMs) to refine prompts through in-context\nlearning, but suffer from several limitations: they may distort user intent,\nomit critical details, or introduce safety risks. Moreover, they optimize\nprompts without considering the impact on the final video quality, which can\nlead to suboptimal results. To address these issues, we introduce VPO, a\nprincipled framework that optimizes prompts based on three core principles:\nharmlessness, accuracy, and helpfulness. The generated prompts faithfully\npreserve user intents and, more importantly, enhance the safety and quality of\ngenerated videos. To achieve this, VPO employs a two-stage optimization\napproach. First, we construct and refine a supervised fine-tuning (SFT) dataset\nbased on principles of safety and alignment. Second, we introduce both\ntext-level and video-level feedback to further optimize the SFT model with\npreference learning. Our extensive experiments demonstrate that VPO\nsignificantly improves safety, alignment, and video quality compared to\nbaseline methods. Moreover, VPO shows strong generalization across video\ngeneration models. Furthermore, we demonstrate that VPO could outperform and be\ncombined with RLHF methods on video generation models, underscoring the\neffectiveness of VPO in aligning video generation models. Our code and data are\npublicly available at https://github.com/thu-coai/VPO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video generation models have achieved remarkable progress in text-to-video\ntasks. These models are typically trained on text-video pairs with highly\ndetailed and carefully crafted descriptions, while real-world user inputs\nduring inference are often concise, vague, or poorly structured. This gap makes\nprompt optimization crucial for generating high-quality videos. Current methods\noften rely on large language models (LLMs) to refine prompts through in-context\nlearning, but suffer from several limitations: they may distort user intent,\nomit critical details, or introduce safety risks. Moreover, they optimize\nprompts without considering the impact on the final video quality, which can\nlead to suboptimal results. To address these issues, we introduce VPO, a\nprincipled framework that optimizes prompts based on three core principles:\nharmlessness, accuracy, and helpfulness. The generated prompts faithfully\npreserve user intents and, more importantly, enhance the safety and quality of\ngenerated videos. To achieve this, VPO employs a two-stage optimization\napproach. First, we construct and refine a supervised fine-tuning (SFT) dataset\nbased on principles of safety and alignment. Second, we introduce both\ntext-level and video-level feedback to further optimize the SFT model with\npreference learning. Our extensive experiments demonstrate that VPO\nsignificantly improves safety, alignment, and video quality compared to\nbaseline methods. Moreover, VPO shows strong generalization across video\ngeneration models. Furthermore, we demonstrate that VPO could outperform and be\ncombined with RLHF methods on video generation models, underscoring the\neffectiveness of VPO in aligning video generation models. Our code and data are\npublicly available at https://github.com/thu-coai/VPO."
                },
                "authors": [
                    {
                        "name": "Jiale Cheng"
                    },
                    {
                        "name": "Ruiliang Lyu"
                    },
                    {
                        "name": "Xiaotao Gu"
                    },
                    {
                        "name": "Xiao Liu"
                    },
                    {
                        "name": "Jiazheng Xu"
                    },
                    {
                        "name": "Yida Lu"
                    },
                    {
                        "name": "Jiayan Teng"
                    },
                    {
                        "name": "Zhuoyi Yang"
                    },
                    {
                        "name": "Yuxiao Dong"
                    },
                    {
                        "name": "Jie Tang"
                    },
                    {
                        "name": "Hongning Wang"
                    },
                    {
                        "name": "Minlie Huang"
                    }
                ],
                "author_detail": {
                    "name": "Minlie Huang"
                },
                "author": "Minlie Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20491v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20491v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14526v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14526v2",
                "updated": "2025-03-26T12:21:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    12,
                    21,
                    42,
                    2,
                    85,
                    0
                ],
                "published": "2024-06-20T17:38:16Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    17,
                    38,
                    16,
                    3,
                    172,
                    0
                ],
                "title": "Fantastic Copyrighted Beasts and How (Not) to Generate Them",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fantastic Copyrighted Beasts and How (Not) to Generate Them"
                },
                "summary": "Recent studies show that image and video generation models can be prompted to\nreproduce copyrighted content from their training data, raising serious legal\nconcerns about copyright infringement. Copyrighted characters (e.g., Mario,\nBatman) present a significant challenge: at least one lawsuit has already\nawarded damages based on the generation of such characters. Consequently,\ncommercial services like DALL-E have started deploying interventions. However,\nlittle research has systematically examined these problems: (1) Can users\neasily prompt models to generate copyrighted characters, even if it is\nunintentional?; (2) How effective are the existing mitigation strategies? To\naddress these questions, we introduce a novel evaluation framework with metrics\nthat assess both the generated image's similarity to copyrighted characters and\nits consistency with user intent, grounded in a set of popular copyrighted\ncharacters from diverse studios and regions. We show that state-of-the-art\nimage and video generation models can still generate characters even if\ncharacters' names are not explicitly mentioned, sometimes with only two generic\nkeywords (e.g., prompting with \"videogame, plumber\" consistently generates\nNintendo's Mario character). We also introduce semi-automatic techniques to\nidentify such keywords or descriptions that trigger character generation. Using\nthis framework, we evaluate mitigation strategies, including prompt rewriting\nand new approaches we propose. Our findings reveal that common methods, such as\nDALL-E's prompt rewriting, are insufficient alone and require supplementary\nstrategies like negative prompting. Our work provides empirical grounding for\ndiscussions on copyright mitigation strategies and offers actionable insights\nfor model deployers implementing these safeguards.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies show that image and video generation models can be prompted to\nreproduce copyrighted content from their training data, raising serious legal\nconcerns about copyright infringement. Copyrighted characters (e.g., Mario,\nBatman) present a significant challenge: at least one lawsuit has already\nawarded damages based on the generation of such characters. Consequently,\ncommercial services like DALL-E have started deploying interventions. However,\nlittle research has systematically examined these problems: (1) Can users\neasily prompt models to generate copyrighted characters, even if it is\nunintentional?; (2) How effective are the existing mitigation strategies? To\naddress these questions, we introduce a novel evaluation framework with metrics\nthat assess both the generated image's similarity to copyrighted characters and\nits consistency with user intent, grounded in a set of popular copyrighted\ncharacters from diverse studios and regions. We show that state-of-the-art\nimage and video generation models can still generate characters even if\ncharacters' names are not explicitly mentioned, sometimes with only two generic\nkeywords (e.g., prompting with \"videogame, plumber\" consistently generates\nNintendo's Mario character). We also introduce semi-automatic techniques to\nidentify such keywords or descriptions that trigger character generation. Using\nthis framework, we evaluate mitigation strategies, including prompt rewriting\nand new approaches we propose. Our findings reveal that common methods, such as\nDALL-E's prompt rewriting, are insufficient alone and require supplementary\nstrategies like negative prompting. Our work provides empirical grounding for\ndiscussions on copyright mitigation strategies and offers actionable insights\nfor model deployers implementing these safeguards."
                },
                "authors": [
                    {
                        "name": "Luxi He"
                    },
                    {
                        "name": "Yangsibo Huang"
                    },
                    {
                        "name": "Weijia Shi"
                    },
                    {
                        "name": "Tinghao Xie"
                    },
                    {
                        "name": "Haotian Liu"
                    },
                    {
                        "name": "Yue Wang"
                    },
                    {
                        "name": "Luke Zettlemoyer"
                    },
                    {
                        "name": "Chiyuan Zhang"
                    },
                    {
                        "name": "Danqi Chen"
                    },
                    {
                        "name": "Peter Henderson"
                    }
                ],
                "author_detail": {
                    "name": "Peter Henderson"
                },
                "author": "Peter Henderson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14526v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14526v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19385v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19385v2",
                "updated": "2025-03-26T12:12:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    12,
                    12,
                    38,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-25T06:30:45Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    6,
                    30,
                    45,
                    1,
                    84,
                    0
                ],
                "title": "Inference-Time Scaling for Flow Models via Stochastic Generation and\n  Rollover Budget Forcing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-Time Scaling for Flow Models via Stochastic Generation and\n  Rollover Budget Forcing"
                },
                "summary": "We propose an inference-time scaling approach for pretrained flow models.\nRecently, inference-time scaling has gained significant attention in LLMs and\ndiffusion models, improving sample quality or better aligning outputs with user\npreferences by leveraging additional computation. For diffusion models,\nparticle sampling has allowed more efficient scaling due to the stochasticity\nat intermediate denoising steps. On the contrary, while flow models have gained\npopularity as an alternative to diffusion models--offering faster generation\nand high-quality outputs in state-of-the-art image and video generative\nmodels--efficient inference-time scaling methods used for diffusion models\ncannot be directly applied due to their deterministic generative process. To\nenable efficient inference-time scaling for flow models, we propose three key\nideas: 1) SDE-based generation, enabling particle sampling in flow models, 2)\nInterpolant conversion, broadening the search space and enhancing sample\ndiversity, and 3) Rollover Budget Forcing (RBF), an adaptive allocation of\ncomputational resources across timesteps to maximize budget utilization. Our\nexperiments show that SDE-based generation, particularly variance-preserving\n(VP) interpolant-based generation, improves the performance of particle\nsampling methods for inference-time scaling in flow models. Additionally, we\ndemonstrate that RBF with VP-SDE achieves the best performance, outperforming\nall previous inference-time scaling approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose an inference-time scaling approach for pretrained flow models.\nRecently, inference-time scaling has gained significant attention in LLMs and\ndiffusion models, improving sample quality or better aligning outputs with user\npreferences by leveraging additional computation. For diffusion models,\nparticle sampling has allowed more efficient scaling due to the stochasticity\nat intermediate denoising steps. On the contrary, while flow models have gained\npopularity as an alternative to diffusion models--offering faster generation\nand high-quality outputs in state-of-the-art image and video generative\nmodels--efficient inference-time scaling methods used for diffusion models\ncannot be directly applied due to their deterministic generative process. To\nenable efficient inference-time scaling for flow models, we propose three key\nideas: 1) SDE-based generation, enabling particle sampling in flow models, 2)\nInterpolant conversion, broadening the search space and enhancing sample\ndiversity, and 3) Rollover Budget Forcing (RBF), an adaptive allocation of\ncomputational resources across timesteps to maximize budget utilization. Our\nexperiments show that SDE-based generation, particularly variance-preserving\n(VP) interpolant-based generation, improves the performance of particle\nsampling methods for inference-time scaling in flow models. Additionally, we\ndemonstrate that RBF with VP-SDE achieves the best performance, outperforming\nall previous inference-time scaling approaches."
                },
                "authors": [
                    {
                        "name": "Jaihoon Kim"
                    },
                    {
                        "name": "Taehoon Yoon"
                    },
                    {
                        "name": "Jisung Hwang"
                    },
                    {
                        "name": "Minhyuk Sung"
                    }
                ],
                "author_detail": {
                    "name": "Minhyuk Sung"
                },
                "author": "Minhyuk Sung",
                "arxiv_comment": "Project page: https://flow-inference-time-scaling.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19385v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19385v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19551v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19551v2",
                "updated": "2025-03-26T11:23:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    11,
                    23,
                    44,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-25T11:07:12Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    11,
                    7,
                    12,
                    1,
                    84,
                    0
                ],
                "title": "Scaling Laws of Synthetic Data for Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Laws of Synthetic Data for Language Models"
                },
                "summary": "Large language models (LLMs) achieve strong performance across diverse tasks,\nlargely driven by high-quality web data used in pre-training. However, recent\nstudies indicate this data source is rapidly depleting. Synthetic data emerges\nas a promising alternative, but it remains unclear whether synthetic datasets\nexhibit predictable scalability comparable to raw pre-training data. In this\nwork, we systematically investigate the scaling laws of synthetic data by\nintroducing SynthLLM, a scalable framework that transforms pre-training corpora\ninto diverse, high-quality synthetic datasets. Our approach achieves this by\nautomatically extracting and recombining high-level concepts across multiple\ndocuments using a graph algorithm. Key findings from our extensive mathematical\nexperiments on SynthLLM include: (1) SynthLLM generates synthetic data that\nreliably adheres to the rectified scaling law across various model sizes; (2)\nPerformance improvements plateau near 300B tokens; and (3) Larger models\napproach optimal performance with fewer training tokens. For instance, an 8B\nmodel peaks at 1T tokens, while a 3B model requires 4T. Moreover, comparisons\nwith existing synthetic data generation and augmentation methods demonstrate\nthat SynthLLM achieves superior performance and scalability. Our findings\nhighlight synthetic data as a scalable and reliable alternative to organic\npre-training corpora, offering a viable path toward continued improvement in\nmodel performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) achieve strong performance across diverse tasks,\nlargely driven by high-quality web data used in pre-training. However, recent\nstudies indicate this data source is rapidly depleting. Synthetic data emerges\nas a promising alternative, but it remains unclear whether synthetic datasets\nexhibit predictable scalability comparable to raw pre-training data. In this\nwork, we systematically investigate the scaling laws of synthetic data by\nintroducing SynthLLM, a scalable framework that transforms pre-training corpora\ninto diverse, high-quality synthetic datasets. Our approach achieves this by\nautomatically extracting and recombining high-level concepts across multiple\ndocuments using a graph algorithm. Key findings from our extensive mathematical\nexperiments on SynthLLM include: (1) SynthLLM generates synthetic data that\nreliably adheres to the rectified scaling law across various model sizes; (2)\nPerformance improvements plateau near 300B tokens; and (3) Larger models\napproach optimal performance with fewer training tokens. For instance, an 8B\nmodel peaks at 1T tokens, while a 3B model requires 4T. Moreover, comparisons\nwith existing synthetic data generation and augmentation methods demonstrate\nthat SynthLLM achieves superior performance and scalability. Our findings\nhighlight synthetic data as a scalable and reliable alternative to organic\npre-training corpora, offering a viable path toward continued improvement in\nmodel performance."
                },
                "authors": [
                    {
                        "name": "Zeyu Qin"
                    },
                    {
                        "name": "Qingxiu Dong"
                    },
                    {
                        "name": "Xingxing Zhang"
                    },
                    {
                        "name": "Li Dong"
                    },
                    {
                        "name": "Xiaolong Huang"
                    },
                    {
                        "name": "Ziyi Yang"
                    },
                    {
                        "name": "Mahmoud Khademi"
                    },
                    {
                        "name": "Dongdong Zhang"
                    },
                    {
                        "name": "Hany Hassan Awadalla"
                    },
                    {
                        "name": "Yi R. Fung"
                    },
                    {
                        "name": "Weizhu Chen"
                    },
                    {
                        "name": "Minhao Cheng"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei",
                "arxiv_comment": "work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19551v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19551v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17945v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17945v2",
                "updated": "2025-03-26T11:06:10Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    11,
                    6,
                    10,
                    2,
                    85,
                    0
                ],
                "published": "2024-11-26T23:39:43Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    23,
                    39,
                    43,
                    1,
                    331,
                    0
                ],
                "title": "MARVEL-40M+: Multi-Level Visual Elaboration for High-Fidelity Text-to-3D\n  Content Creation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MARVEL-40M+: Multi-Level Visual Elaboration for High-Fidelity Text-to-3D\n  Content Creation"
                },
                "summary": "Generating high-fidelity 3D content from text prompts remains a significant\nchallenge in computer vision due to the limited size, diversity, and annotation\ndepth of the existing datasets. To address this, we introduce MARVEL-40M+, an\nextensive dataset with 40 million text annotations for over 8.9 million 3D\nassets aggregated from seven major 3D datasets. Our contribution is a novel\nmulti-stage annotation pipeline that integrates open-source pretrained\nmulti-view VLMs and LLMs to automatically produce multi-level descriptions,\nranging from detailed (150-200 words) to concise semantic tags (10-20 words).\nThis structure supports both fine-grained 3D reconstruction and rapid\nprototyping. Furthermore, we incorporate human metadata from source datasets\ninto our annotation pipeline to add domain-specific information in our\nannotation and reduce VLM hallucinations. Additionally, we develop MARVEL-FX3D,\na two-stage text-to-3D pipeline. We fine-tune Stable Diffusion with our\nannotations and use a pretrained image-to-3D network to generate 3D textured\nmeshes within 15s. Extensive evaluations show that MARVEL-40M+ significantly\noutperforms existing datasets in annotation quality and linguistic diversity,\nachieving win rates of 72.41% by GPT-4 and 73.40% by human evaluators. Project\npage is available at https://sankalpsinha-cmos.github.io/MARVEL/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating high-fidelity 3D content from text prompts remains a significant\nchallenge in computer vision due to the limited size, diversity, and annotation\ndepth of the existing datasets. To address this, we introduce MARVEL-40M+, an\nextensive dataset with 40 million text annotations for over 8.9 million 3D\nassets aggregated from seven major 3D datasets. Our contribution is a novel\nmulti-stage annotation pipeline that integrates open-source pretrained\nmulti-view VLMs and LLMs to automatically produce multi-level descriptions,\nranging from detailed (150-200 words) to concise semantic tags (10-20 words).\nThis structure supports both fine-grained 3D reconstruction and rapid\nprototyping. Furthermore, we incorporate human metadata from source datasets\ninto our annotation pipeline to add domain-specific information in our\nannotation and reduce VLM hallucinations. Additionally, we develop MARVEL-FX3D,\na two-stage text-to-3D pipeline. We fine-tune Stable Diffusion with our\nannotations and use a pretrained image-to-3D network to generate 3D textured\nmeshes within 15s. Extensive evaluations show that MARVEL-40M+ significantly\noutperforms existing datasets in annotation quality and linguistic diversity,\nachieving win rates of 72.41% by GPT-4 and 73.40% by human evaluators. Project\npage is available at https://sankalpsinha-cmos.github.io/MARVEL/."
                },
                "authors": [
                    {
                        "name": "Sankalp Sinha"
                    },
                    {
                        "name": "Mohammad Sadil Khan"
                    },
                    {
                        "name": "Muhammad Usama"
                    },
                    {
                        "name": "Shino Sam"
                    },
                    {
                        "name": "Didier Stricker"
                    },
                    {
                        "name": "Sk Aziz Ali"
                    },
                    {
                        "name": "Muhammad Zeshan Afzal"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Zeshan Afzal"
                },
                "author": "Muhammad Zeshan Afzal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17945v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17945v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20430v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20430v1",
                "updated": "2025-03-26T11:03:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    11,
                    3,
                    34,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T11:03:34Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    11,
                    3,
                    34,
                    2,
                    85,
                    0
                ],
                "title": "RALLRec+: Retrieval Augmented Large Language Model Recommendation with\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RALLRec+: Retrieval Augmented Large Language Model Recommendation with\n  Reasoning"
                },
                "summary": "Large Language Models (LLMs) have been integrated into recommender systems to\nenhance user behavior comprehension. The Retrieval Augmented Generation (RAG)\ntechnique is further incorporated into these systems to retrieve more relevant\nitems and improve system performance. However, existing RAG methods have two\nshortcomings. \\textit{(i)} In the \\textit{retrieval} stage, they rely primarily\non textual semantics and often fail to incorporate the most relevant items,\nthus constraining system effectiveness. \\textit{(ii)} In the\n\\textit{generation} stage, they lack explicit chain-of-thought reasoning,\nfurther limiting their potential.\n  In this paper, we propose Representation learning and \\textbf{R}easoning\nempowered retrieval-\\textbf{A}ugmented \\textbf{L}arge \\textbf{L}anguage model\n\\textbf{Rec}ommendation (RALLRec+). Specifically, for the retrieval stage, we\nprompt LLMs to generate detailed item descriptions and perform joint\nrepresentation learning, combining textual and collaborative signals extracted\nfrom the LLM and recommendation models, respectively. To account for the\ntime-varying nature of user interests, we propose a simple yet effective\nreranking method to capture preference dynamics. For the generation phase, we\nfirst evaluate reasoning LLMs on recommendation tasks, uncovering valuable\ninsights. Then we introduce knowledge-injected prompting and consistency-based\nmerging approach to integrate reasoning LLMs with general-purpose LLMs,\nenhancing overall performance. Extensive experiments on three real world\ndatasets validate our method's effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been integrated into recommender systems to\nenhance user behavior comprehension. The Retrieval Augmented Generation (RAG)\ntechnique is further incorporated into these systems to retrieve more relevant\nitems and improve system performance. However, existing RAG methods have two\nshortcomings. \\textit{(i)} In the \\textit{retrieval} stage, they rely primarily\non textual semantics and often fail to incorporate the most relevant items,\nthus constraining system effectiveness. \\textit{(ii)} In the\n\\textit{generation} stage, they lack explicit chain-of-thought reasoning,\nfurther limiting their potential.\n  In this paper, we propose Representation learning and \\textbf{R}easoning\nempowered retrieval-\\textbf{A}ugmented \\textbf{L}arge \\textbf{L}anguage model\n\\textbf{Rec}ommendation (RALLRec+). Specifically, for the retrieval stage, we\nprompt LLMs to generate detailed item descriptions and perform joint\nrepresentation learning, combining textual and collaborative signals extracted\nfrom the LLM and recommendation models, respectively. To account for the\ntime-varying nature of user interests, we propose a simple yet effective\nreranking method to capture preference dynamics. For the generation phase, we\nfirst evaluate reasoning LLMs on recommendation tasks, uncovering valuable\ninsights. Then we introduce knowledge-injected prompting and consistency-based\nmerging approach to integrate reasoning LLMs with general-purpose LLMs,\nenhancing overall performance. Extensive experiments on three real world\ndatasets validate our method's effectiveness."
                },
                "authors": [
                    {
                        "name": "Sichun Luo"
                    },
                    {
                        "name": "Jian Xu"
                    },
                    {
                        "name": "Xiaojie Zhang"
                    },
                    {
                        "name": "Linrong Wang"
                    },
                    {
                        "name": "Sicong Liu"
                    },
                    {
                        "name": "Hanxu Hou"
                    },
                    {
                        "name": "Linqi Song"
                    }
                ],
                "author_detail": {
                    "name": "Linqi Song"
                },
                "author": "Linqi Song",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2502.06101",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20430v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20430v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20417v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20417v1",
                "updated": "2025-03-26T10:44:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    10,
                    44,
                    51,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T10:44:51Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    10,
                    44,
                    51,
                    2,
                    85,
                    0
                ],
                "title": "CFunModel: A \"Funny\" Language Model Capable of Chinese Humor Generation\n  and Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CFunModel: A \"Funny\" Language Model Capable of Chinese Humor Generation\n  and Processing"
                },
                "summary": "Humor plays a significant role in daily language communication. With the\nrapid development of large language models (LLMs), natural language processing\nhas made significant strides in understanding and generating various genres of\ntexts. However, most LLMs exhibit poor performance in generating and processing\nChinese humor. In this study, we introduce a comprehensive Chinese\nhumor-related dataset, the Chinese Fun Set (CFunSet). This dataset aggregates\nexisting Chinese humor datasets and includes over 20,000 jokes collected from\nTieba-JokeBar, a Chinese online platform known for joke sharing. The resulting\ncorpus comprises more than 160,000 entries. Leveraging CFunSet, we developed\nthe Chinese Fun Model (CFunModel), the first large language model designed to\nhandle various Chinese humor-related tasks including Crosstalk Response\nSelection, Humor Recognition, Joke Generation, etc. Experimental results\ndemonstrate that CFunModel outperforms popular large language models in these\ntasks. Our CFunSet is available at\nhttps://huggingface.co/datasets/ZhenghanYU/CFunSet and CFunModel is available\nat https://huggingface.co/ZhenghanYU/CFunModel. A demostration video of our\nwork is available at https://youtu.be/MOsISOJ66Ms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humor plays a significant role in daily language communication. With the\nrapid development of large language models (LLMs), natural language processing\nhas made significant strides in understanding and generating various genres of\ntexts. However, most LLMs exhibit poor performance in generating and processing\nChinese humor. In this study, we introduce a comprehensive Chinese\nhumor-related dataset, the Chinese Fun Set (CFunSet). This dataset aggregates\nexisting Chinese humor datasets and includes over 20,000 jokes collected from\nTieba-JokeBar, a Chinese online platform known for joke sharing. The resulting\ncorpus comprises more than 160,000 entries. Leveraging CFunSet, we developed\nthe Chinese Fun Model (CFunModel), the first large language model designed to\nhandle various Chinese humor-related tasks including Crosstalk Response\nSelection, Humor Recognition, Joke Generation, etc. Experimental results\ndemonstrate that CFunModel outperforms popular large language models in these\ntasks. Our CFunSet is available at\nhttps://huggingface.co/datasets/ZhenghanYU/CFunSet and CFunModel is available\nat https://huggingface.co/ZhenghanYU/CFunModel. A demostration video of our\nwork is available at https://youtu.be/MOsISOJ66Ms."
                },
                "authors": [
                    {
                        "name": "Zhenghan Yu"
                    },
                    {
                        "name": "Xinyu Hu"
                    },
                    {
                        "name": "Xiaojun Wan"
                    }
                ],
                "author_detail": {
                    "name": "Xiaojun Wan"
                },
                "author": "Xiaojun Wan",
                "arxiv_comment": "9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20417v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20417v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20384v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20384v1",
                "updated": "2025-03-26T10:05:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    10,
                    5,
                    38,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T10:05:38Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    10,
                    5,
                    38,
                    2,
                    85,
                    0
                ],
                "title": "MoLe-VLA: Dynamic Layer-skipping Vision Language Action Model via\n  Mixture-of-Layers for Efficient Robot Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoLe-VLA: Dynamic Layer-skipping Vision Language Action Model via\n  Mixture-of-Layers for Efficient Robot Manipulation"
                },
                "summary": "Multimodal Large Language Models (MLLMs) excel in understanding complex\nlanguage and visual data, enabling generalist robotic systems to interpret\ninstructions and perform embodied tasks. Nevertheless, their real-world\ndeployment is hindered by substantial computational and storage demands. Recent\ninsights into the homogeneous patterns in the LLM layer have inspired\nsparsification techniques to address these challenges, such as early exit and\ntoken pruning. However, these methods often neglect the critical role of the\nfinal layers that encode the semantic information most relevant to downstream\nrobotic tasks. Aligning with the recent breakthrough of the Shallow Brain\nHypothesis (SBH) in neuroscience and the mixture of experts in model\nsparsification, we conceptualize each LLM layer as an expert and propose a\nMixture-of-Layers Vision-Language-Action model (MoLe-VLA, or simply MoLe)\narchitecture for dynamic LLM layer activation. We introduce a Spatial-Temporal\nAware Router (STAR) for MoLe to selectively activate only parts of the layers\nbased on the robot's current state, mimicking the brain's distinct signal\npathways specialized for cognition and causal reasoning. Additionally, to\ncompensate for the cognitive ability of LLMs lost in MoLe, we devise a\nCognition Self-Knowledge Distillation (CogKD) framework. CogKD enhances the\nunderstanding of task demands and improves the generation of task-relevant\naction sequences by leveraging cognitive features. Extensive experiments\nconducted in both RLBench simulation and real-world environments demonstrate\nthe superiority of MoLe-VLA in both efficiency and performance. Specifically,\nMoLe-VLA achieves an 8% improvement in the mean success rate across ten tasks\nwhile reducing computational costs by up to x5.6 compared to standard LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) excel in understanding complex\nlanguage and visual data, enabling generalist robotic systems to interpret\ninstructions and perform embodied tasks. Nevertheless, their real-world\ndeployment is hindered by substantial computational and storage demands. Recent\ninsights into the homogeneous patterns in the LLM layer have inspired\nsparsification techniques to address these challenges, such as early exit and\ntoken pruning. However, these methods often neglect the critical role of the\nfinal layers that encode the semantic information most relevant to downstream\nrobotic tasks. Aligning with the recent breakthrough of the Shallow Brain\nHypothesis (SBH) in neuroscience and the mixture of experts in model\nsparsification, we conceptualize each LLM layer as an expert and propose a\nMixture-of-Layers Vision-Language-Action model (MoLe-VLA, or simply MoLe)\narchitecture for dynamic LLM layer activation. We introduce a Spatial-Temporal\nAware Router (STAR) for MoLe to selectively activate only parts of the layers\nbased on the robot's current state, mimicking the brain's distinct signal\npathways specialized for cognition and causal reasoning. Additionally, to\ncompensate for the cognitive ability of LLMs lost in MoLe, we devise a\nCognition Self-Knowledge Distillation (CogKD) framework. CogKD enhances the\nunderstanding of task demands and improves the generation of task-relevant\naction sequences by leveraging cognitive features. Extensive experiments\nconducted in both RLBench simulation and real-world environments demonstrate\nthe superiority of MoLe-VLA in both efficiency and performance. Specifically,\nMoLe-VLA achieves an 8% improvement in the mean success rate across ten tasks\nwhile reducing computational costs by up to x5.6 compared to standard LLMs."
                },
                "authors": [
                    {
                        "name": "Rongyu Zhang"
                    },
                    {
                        "name": "Menghang Dong"
                    },
                    {
                        "name": "Yuan Zhang"
                    },
                    {
                        "name": "Liang Heng"
                    },
                    {
                        "name": "Xiaowei Chi"
                    },
                    {
                        "name": "Gaole Dai"
                    },
                    {
                        "name": "Li Du"
                    },
                    {
                        "name": "Dan Wang"
                    },
                    {
                        "name": "Yuan Du"
                    },
                    {
                        "name": "Shanghang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shanghang Zhang"
                },
                "author": "Shanghang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20384v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20384v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20377v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20377v1",
                "updated": "2025-03-26T09:56:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    9,
                    56,
                    7,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T09:56:07Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    9,
                    56,
                    7,
                    2,
                    85,
                    0
                ],
                "title": "UB-Mesh: a Hierarchically Localized nD-FullMesh Datacenter Network\n  Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UB-Mesh: a Hierarchically Localized nD-FullMesh Datacenter Network\n  Architecture"
                },
                "summary": "As the Large-scale Language Models (LLMs) continue to scale, the requisite\ncomputational power and bandwidth escalate. To address this, we introduce\nUB-Mesh, a novel AI datacenter network architecture designed to enhance\nscalability, performance, cost-efficiency and availability. Unlike traditional\ndatacenters that provide symmetrical node-to-node bandwidth, UB-Mesh employs a\nhierarchically localized nD-FullMesh network topology. This design fully\nleverages the data locality of LLM training, prioritizing short-range, direct\ninterconnects to minimize data movement distance and reduce switch usage.\n  Although UB-Mesh's nD-FullMesh topology offers several theoretical\nadvantages, its concrete architecture design, physical implementation and\nnetworking system optimization present new challenges. For the actual\nconstruction of UB-Mesh, we first design the UB-Mesh-Pod architecture, which is\nbased on a 4D-FullMesh topology. UB-Mesh-Pod is implemented via a suite of\nhardware components that serve as the foundational building blocks, including\nspecifically-designed NPU, CPU, Low-Radix-Switch (LRS), High-Radix-Switch\n(HRS), NICs and others. These components are interconnected via a novel Unified\nBus (UB) technique, which enables flexible IO bandwidth allocation and hardware\nresource pooling. For networking system optimization, we propose advanced\nrouting mechanism named All-Path-Routing (APR) to efficiently manage data\ntraffic. These optimizations, combined with topology-aware performance\nenhancements and robust reliability measures like 64+1 backup design, result in\n2.04x higher cost-efficiency, 7.2% higher network availability compared to\ntraditional Clos architecture and 95%+ linearity in various LLM training tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the Large-scale Language Models (LLMs) continue to scale, the requisite\ncomputational power and bandwidth escalate. To address this, we introduce\nUB-Mesh, a novel AI datacenter network architecture designed to enhance\nscalability, performance, cost-efficiency and availability. Unlike traditional\ndatacenters that provide symmetrical node-to-node bandwidth, UB-Mesh employs a\nhierarchically localized nD-FullMesh network topology. This design fully\nleverages the data locality of LLM training, prioritizing short-range, direct\ninterconnects to minimize data movement distance and reduce switch usage.\n  Although UB-Mesh's nD-FullMesh topology offers several theoretical\nadvantages, its concrete architecture design, physical implementation and\nnetworking system optimization present new challenges. For the actual\nconstruction of UB-Mesh, we first design the UB-Mesh-Pod architecture, which is\nbased on a 4D-FullMesh topology. UB-Mesh-Pod is implemented via a suite of\nhardware components that serve as the foundational building blocks, including\nspecifically-designed NPU, CPU, Low-Radix-Switch (LRS), High-Radix-Switch\n(HRS), NICs and others. These components are interconnected via a novel Unified\nBus (UB) technique, which enables flexible IO bandwidth allocation and hardware\nresource pooling. For networking system optimization, we propose advanced\nrouting mechanism named All-Path-Routing (APR) to efficiently manage data\ntraffic. These optimizations, combined with topology-aware performance\nenhancements and robust reliability measures like 64+1 backup design, result in\n2.04x higher cost-efficiency, 7.2% higher network availability compared to\ntraditional Clos architecture and 95%+ linearity in various LLM training tasks."
                },
                "authors": [
                    {
                        "name": "Heng Liao"
                    },
                    {
                        "name": "Bingyang Liu"
                    },
                    {
                        "name": "Xianping Chen"
                    },
                    {
                        "name": "Zhigang Guo"
                    },
                    {
                        "name": "Chuanning Cheng"
                    },
                    {
                        "name": "Jianbing Wang"
                    },
                    {
                        "name": "Xiangyu Chen"
                    },
                    {
                        "name": "Peng Dong"
                    },
                    {
                        "name": "Rui Meng"
                    },
                    {
                        "name": "Wenjie Liu"
                    },
                    {
                        "name": "Zhe Zhou"
                    },
                    {
                        "name": "Ziyang Zhang"
                    },
                    {
                        "name": "Yuhang Gai"
                    },
                    {
                        "name": "Cunle Qian"
                    },
                    {
                        "name": "Yi Xiong"
                    },
                    {
                        "name": "Zhongwu Cheng"
                    },
                    {
                        "name": "Jing Xia"
                    },
                    {
                        "name": "Yuli Ma"
                    },
                    {
                        "name": "Xi Chen"
                    },
                    {
                        "name": "Wenhua Du"
                    },
                    {
                        "name": "Shizhong Xiao"
                    },
                    {
                        "name": "Chungang Li"
                    },
                    {
                        "name": "Yong Qin"
                    },
                    {
                        "name": "Liudong Xiong"
                    },
                    {
                        "name": "Zhou Yu"
                    },
                    {
                        "name": "Lv Chen"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Buyun Wang"
                    },
                    {
                        "name": "Pei Wu"
                    },
                    {
                        "name": "Junen Gao"
                    },
                    {
                        "name": "Xiaochu Li"
                    },
                    {
                        "name": "Jian He"
                    },
                    {
                        "name": "Shizhuan Yan"
                    },
                    {
                        "name": "Bill McColl"
                    }
                ],
                "author_detail": {
                    "name": "Bill McColl"
                },
                "author": "Bill McColl",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20377v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20377v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20376v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20376v1",
                "updated": "2025-03-26T09:55:00Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    9,
                    55,
                    0,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T09:55:00Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    9,
                    55,
                    0,
                    2,
                    85,
                    0
                ],
                "title": "Dewey Long Context Embedding Model: A Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dewey Long Context Embedding Model: A Technical Report"
                },
                "summary": "This technical report presents the training methodology and evaluation\nresults of the open-source dewey_en_beta embedding model. The increasing demand\nfor retrieval-augmented generation (RAG) systems and the expanding context\nwindow capabilities of large language models (LLMs) have created critical\nchallenges for conventional embedding models. Current approaches often struggle\nto maintain semantic coherence when processing documents exceeding typical\nsequence length limitations, significantly impacting retrieval performance in\nknowledge-intensive applications. This paper presents dewey_en_beta, a novel\ntext embedding model that achieves excellent performance on MTEB (Eng, v2) and\nLongEmbed benchmark while supporting 128K token sequences. Our technical\ncontribution centers on chunk alignment training, an innovative methodology\nthat enables the simultaneous generation of localized chunk embeddings and\nglobal document-level representations through distillation. Information\nregarding the model release can be found at\nhttps://huggingface.co/infgrad/dewey_en_beta.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This technical report presents the training methodology and evaluation\nresults of the open-source dewey_en_beta embedding model. The increasing demand\nfor retrieval-augmented generation (RAG) systems and the expanding context\nwindow capabilities of large language models (LLMs) have created critical\nchallenges for conventional embedding models. Current approaches often struggle\nto maintain semantic coherence when processing documents exceeding typical\nsequence length limitations, significantly impacting retrieval performance in\nknowledge-intensive applications. This paper presents dewey_en_beta, a novel\ntext embedding model that achieves excellent performance on MTEB (Eng, v2) and\nLongEmbed benchmark while supporting 128K token sequences. Our technical\ncontribution centers on chunk alignment training, an innovative methodology\nthat enables the simultaneous generation of localized chunk embeddings and\nglobal document-level representations through distillation. Information\nregarding the model release can be found at\nhttps://huggingface.co/infgrad/dewey_en_beta."
                },
                "authors": [
                    {
                        "name": "Dun Zhang"
                    },
                    {
                        "name": "Panxiang Zou"
                    },
                    {
                        "name": "Yudong Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yudong Zhou"
                },
                "author": "Yudong Zhou",
                "arxiv_comment": "5 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20376v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20376v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20354v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20354v1",
                "updated": "2025-03-26T09:27:09Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    9,
                    27,
                    9,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T09:27:09Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    9,
                    27,
                    9,
                    2,
                    85,
                    0
                ],
                "title": "SURGEON: Memory-Adaptive Fully Test-Time Adaptation via Dynamic\n  Activation Sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SURGEON: Memory-Adaptive Fully Test-Time Adaptation via Dynamic\n  Activation Sparsity"
                },
                "summary": "Despite the growing integration of deep models into mobile terminals, the\naccuracy of these models declines significantly due to various deployment\ninterferences. Test-time adaptation (TTA) has emerged to improve the\nperformance of deep models by adapting them to unlabeled target data online.\nYet, the significant memory cost, particularly in resource-constrained\nterminals, impedes the effective deployment of most backward-propagation-based\nTTA methods. To tackle memory constraints, we introduce SURGEON, a method that\nsubstantially reduces memory cost while preserving comparable accuracy\nimprovements during fully test-time adaptation (FTTA) without relying on\nspecific network architectures or modifications to the original training\nprocedure. Specifically, we propose a novel dynamic activation sparsity\nstrategy that directly prunes activations at layer-specific dynamic ratios\nduring adaptation, allowing for flexible control of learning ability and memory\ncost in a data-sensitive manner. Among this, two metrics, Gradient Importance\nand Layer Activation Memory, are considered to determine the layer-wise pruning\nratios, reflecting accuracy contribution and memory efficiency, respectively.\nExperimentally, our method surpasses the baselines by not only reducing memory\nusage but also achieving superior accuracy, delivering SOTA performance across\ndiverse datasets, architectures, and tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the growing integration of deep models into mobile terminals, the\naccuracy of these models declines significantly due to various deployment\ninterferences. Test-time adaptation (TTA) has emerged to improve the\nperformance of deep models by adapting them to unlabeled target data online.\nYet, the significant memory cost, particularly in resource-constrained\nterminals, impedes the effective deployment of most backward-propagation-based\nTTA methods. To tackle memory constraints, we introduce SURGEON, a method that\nsubstantially reduces memory cost while preserving comparable accuracy\nimprovements during fully test-time adaptation (FTTA) without relying on\nspecific network architectures or modifications to the original training\nprocedure. Specifically, we propose a novel dynamic activation sparsity\nstrategy that directly prunes activations at layer-specific dynamic ratios\nduring adaptation, allowing for flexible control of learning ability and memory\ncost in a data-sensitive manner. Among this, two metrics, Gradient Importance\nand Layer Activation Memory, are considered to determine the layer-wise pruning\nratios, reflecting accuracy contribution and memory efficiency, respectively.\nExperimentally, our method surpasses the baselines by not only reducing memory\nusage but also achieving superior accuracy, delivering SOTA performance across\ndiverse datasets, architectures, and tasks."
                },
                "authors": [
                    {
                        "name": "Ke Ma"
                    },
                    {
                        "name": "Jiaqi Tang"
                    },
                    {
                        "name": "Bin Guo"
                    },
                    {
                        "name": "Fan Dang"
                    },
                    {
                        "name": "Sicong Liu"
                    },
                    {
                        "name": "Zhui Zhu"
                    },
                    {
                        "name": "Lei Wu"
                    },
                    {
                        "name": "Cheng Fang"
                    },
                    {
                        "name": "Ying-Cong Chen"
                    },
                    {
                        "name": "Zhiwen Yu"
                    },
                    {
                        "name": "Yunhao Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yunhao Liu"
                },
                "author": "Yunhao Liu",
                "arxiv_comment": "Accepted to CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20354v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20354v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20352v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20352v1",
                "updated": "2025-03-26T09:23:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    9,
                    23,
                    8,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T09:23:08Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    9,
                    23,
                    8,
                    2,
                    85,
                    0
                ],
                "title": "GNSS jammer localization and identification with airborne commercial\n  GNSS receivers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GNSS jammer localization and identification with airborne commercial\n  GNSS receivers"
                },
                "summary": "Global Navigation Satellite Systems (GNSS) are fundamental in ubiquitously\nproviding position and time to a wide gamut of systems. Jamming remains a\nrealistic threat in many deployment settings, civilian and tactical.\nSpecifically, in Unmanned Aerial Vehicles (UAVs) sustained denial raises safety\ncritical concerns. This work presents a strategy that allows detection,\nlocalization, and classification both in the frequency and time domain of\ninterference signals harmful to navigation. A high-performance Vertical Take\nOff and Landing (VTOL) UAV with a single antenna and a commercial GNSS receiver\nis used to geolocate and characterize RF emitters at long range, to infer the\nnavigation impairment. Raw IQ baseband snapshots from the GNSS receiver make\nthe application of spectral correlation methods possible without extra\nsoftware-defined radio payload, paving the way to spectrum identification and\nmonitoring in airborne platforms, aiming at RF situational awareness. Live\ntesting at Jammertest, in Norway, with portable, commercially available GNSS\nmulti-band jammers demonstrates the ability to detect, localize, and\ncharacterize harmful interference. Our system pinpointed the position with an\nerror of a few meters of the transmitter and the extent of the affected area at\nlong range, without entering the denied zone. Additionally, further spectral\ncontent extraction is used to accurately identify the jammer frequency,\nbandwidth, and modulation scheme based on spectral correlation techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Global Navigation Satellite Systems (GNSS) are fundamental in ubiquitously\nproviding position and time to a wide gamut of systems. Jamming remains a\nrealistic threat in many deployment settings, civilian and tactical.\nSpecifically, in Unmanned Aerial Vehicles (UAVs) sustained denial raises safety\ncritical concerns. This work presents a strategy that allows detection,\nlocalization, and classification both in the frequency and time domain of\ninterference signals harmful to navigation. A high-performance Vertical Take\nOff and Landing (VTOL) UAV with a single antenna and a commercial GNSS receiver\nis used to geolocate and characterize RF emitters at long range, to infer the\nnavigation impairment. Raw IQ baseband snapshots from the GNSS receiver make\nthe application of spectral correlation methods possible without extra\nsoftware-defined radio payload, paving the way to spectrum identification and\nmonitoring in airborne platforms, aiming at RF situational awareness. Live\ntesting at Jammertest, in Norway, with portable, commercially available GNSS\nmulti-band jammers demonstrates the ability to detect, localize, and\ncharacterize harmful interference. Our system pinpointed the position with an\nerror of a few meters of the transmitter and the extent of the affected area at\nlong range, without entering the denied zone. Additionally, further spectral\ncontent extraction is used to accurately identify the jammer frequency,\nbandwidth, and modulation scheme based on spectral correlation techniques."
                },
                "authors": [
                    {
                        "name": "Marco Spanghero"
                    },
                    {
                        "name": "Filip Geib"
                    },
                    {
                        "name": "Ronny Panier"
                    },
                    {
                        "name": "Panos Papadimitratos"
                    }
                ],
                "author_detail": {
                    "name": "Panos Papadimitratos"
                },
                "author": "Panos Papadimitratos",
                "arxiv_doi": "10.1109/TIFS.2025.3550050",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TIFS.2025.3550050",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.20352v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20352v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "IEEE Transactions on Information Forensics and Security (2025)",
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20344v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20344v1",
                "updated": "2025-03-26T09:17:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    9,
                    17,
                    40,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T09:17:40Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    9,
                    17,
                    40,
                    2,
                    85,
                    0
                ],
                "title": "GeoNimbus: A serverless framework to build earth observation and\n  environmental services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GeoNimbus: A serverless framework to build earth observation and\n  environmental services"
                },
                "summary": "Cloud computing has become a popular solution for organizations implementing\nEarth Observation Systems (EOS). However, this produces a dependency on\nprovider resources. Moreover, managing and executing tasks and data in these\nenvironments are challenges that commonly arise when building an EOS. This\npaper presents GeoNimbus, a serverless framework for composing and deploying\nspatio-temporal EOS on multiple infrastructures, e.g., on-premise resources and\npublic or private clouds. This framework organizes EOS tasks as functions and\nautomatically manages their deployment, invocation, scalability, and monitoring\nin the cloud. GeoNimbus framework enables organizations to reuse and share\navailable functions to compose multiple EOS. We use this framework to implement\nEOS as a service for conducting a case study focused on measuring water\nresource changes in a lake in the south of Mexico. The experimental evaluation\nrevealed the feasibility and efficiency of using GeoNimbus to build different\nearth observation studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud computing has become a popular solution for organizations implementing\nEarth Observation Systems (EOS). However, this produces a dependency on\nprovider resources. Moreover, managing and executing tasks and data in these\nenvironments are challenges that commonly arise when building an EOS. This\npaper presents GeoNimbus, a serverless framework for composing and deploying\nspatio-temporal EOS on multiple infrastructures, e.g., on-premise resources and\npublic or private clouds. This framework organizes EOS tasks as functions and\nautomatically manages their deployment, invocation, scalability, and monitoring\nin the cloud. GeoNimbus framework enables organizations to reuse and share\navailable functions to compose multiple EOS. We use this framework to implement\nEOS as a service for conducting a case study focused on measuring water\nresource changes in a lake in the south of Mexico. The experimental evaluation\nrevealed the feasibility and efficiency of using GeoNimbus to build different\nearth observation studies."
                },
                "authors": [
                    {
                        "name": "Dante D. Sánchez-Gallegos"
                    },
                    {
                        "name": "Diana Carrizales-Espinoza"
                    },
                    {
                        "name": "Alejandro Zequeira"
                    },
                    {
                        "name": "Catherine Torres-Charles"
                    },
                    {
                        "name": "J. L. Gonzalez-Compean"
                    },
                    {
                        "name": "Jesus Carretero"
                    }
                ],
                "author_detail": {
                    "name": "Jesus Carretero"
                },
                "author": "Jesus Carretero",
                "arxiv_comment": "12 pages, 10 images. Presented at the 1st workshop about\n  High-Performance e-Science in the EuroPar2024 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20344v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20344v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13767v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13767v2",
                "updated": "2025-03-26T09:08:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    9,
                    8,
                    40,
                    2,
                    85,
                    0
                ],
                "published": "2025-02-19T14:28:42Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    14,
                    28,
                    42,
                    2,
                    50,
                    0
                ],
                "title": "Agentic AI Software Engineer: Programming with Trust",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic AI Software Engineer: Programming with Trust"
                },
                "summary": "Large Language Models (LLMs) have shown surprising proficiency in generating\ncode snippets, promising to automate large parts of software engineering via\nartificial intelligence (AI). We argue that successfully deploying AI software\nengineers requires a level of trust equal to or even greater than the trust\nestablished by human-driven software engineering practices. The recent trend\ntoward LLM agents offers a path toward integrating the power of LLMs to create\nnew code with the power of analysis tools to increase trust in the code. This\nopinion piece comments on whether LLM agents could dominate software\nengineering workflows in the future and whether the focus of programming will\nshift from programming at scale to programming with trust.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown surprising proficiency in generating\ncode snippets, promising to automate large parts of software engineering via\nartificial intelligence (AI). We argue that successfully deploying AI software\nengineers requires a level of trust equal to or even greater than the trust\nestablished by human-driven software engineering practices. The recent trend\ntoward LLM agents offers a path toward integrating the power of LLMs to create\nnew code with the power of analysis tools to increase trust in the code. This\nopinion piece comments on whether LLM agents could dominate software\nengineering workflows in the future and whether the focus of programming will\nshift from programming at scale to programming with trust."
                },
                "authors": [
                    {
                        "name": "Abhik Roychoudhury"
                    },
                    {
                        "name": "Corina Pasareanu"
                    },
                    {
                        "name": "Michael Pradel"
                    },
                    {
                        "name": "Baishakhi Ray"
                    }
                ],
                "author_detail": {
                    "name": "Baishakhi Ray"
                },
                "author": "Baishakhi Ray",
                "arxiv_comment": "5 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13767v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13767v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03907v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03907v3",
                "updated": "2025-03-26T09:06:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    9,
                    6,
                    21,
                    2,
                    85,
                    0
                ],
                "published": "2024-12-05T06:26:32Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    6,
                    26,
                    32,
                    3,
                    340,
                    0
                ],
                "title": "ONER: Online Experience Replay for Incremental Anomaly Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ONER: Online Experience Replay for Incremental Anomaly Detection"
                },
                "summary": "Incremental anomaly detection aims to sequentially identify defects in\nindustrial product lines but suffers from catastrophic forgetting, primarily\ndue to knowledge overwriting during parameter updates and feature conflicts\nbetween tasks. In this work, We propose ONER (ONline Experience Replay), an\nend-to-end framework that addresses these issues by synergistically integrating\ntwo types of experience: (1) decomposed prompts, which dynamically generate\nimage-conditioned prompts from reusable modules to retain prior knowledge thus\nprevent knowledge overwriting, and (2) semantic prototypes, which enforce\nseparability in latent feature spaces at pixel and image levels to mitigate\ncross-task feature conflicts. Extensive experiments demonstrate the superiority\nof ONER, achieving state-of-the-art performance with +4.4% Pixel AUROC and\n+28.3% Pixel AUPR improvements on the MVTec AD dataset over prior methods.\nRemarkably, ONER achieves this with only 0.019M parameters and 5 training\nepochs per task, confirming its efficiency and stability for real-world\nindustrial deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Incremental anomaly detection aims to sequentially identify defects in\nindustrial product lines but suffers from catastrophic forgetting, primarily\ndue to knowledge overwriting during parameter updates and feature conflicts\nbetween tasks. In this work, We propose ONER (ONline Experience Replay), an\nend-to-end framework that addresses these issues by synergistically integrating\ntwo types of experience: (1) decomposed prompts, which dynamically generate\nimage-conditioned prompts from reusable modules to retain prior knowledge thus\nprevent knowledge overwriting, and (2) semantic prototypes, which enforce\nseparability in latent feature spaces at pixel and image levels to mitigate\ncross-task feature conflicts. Extensive experiments demonstrate the superiority\nof ONER, achieving state-of-the-art performance with +4.4% Pixel AUROC and\n+28.3% Pixel AUPR improvements on the MVTec AD dataset over prior methods.\nRemarkably, ONER achieves this with only 0.019M parameters and 5 training\nepochs per task, confirming its efficiency and stability for real-world\nindustrial deployment."
                },
                "authors": [
                    {
                        "name": "Yizhou Jin"
                    },
                    {
                        "name": "Jiahui Zhu"
                    },
                    {
                        "name": "Guodong Wang"
                    },
                    {
                        "name": "Shiwei Li"
                    },
                    {
                        "name": "Jinjin Zhang"
                    },
                    {
                        "name": "Xinyue Liu"
                    },
                    {
                        "name": "Qingjie Liu"
                    },
                    {
                        "name": "Yunhong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yunhong Wang"
                },
                "author": "Yunhong Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03907v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03907v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03000v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03000v3",
                "updated": "2025-03-26T09:02:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    9,
                    2,
                    8,
                    2,
                    85,
                    0
                ],
                "published": "2025-02-05T08:52:37Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    8,
                    52,
                    37,
                    2,
                    36,
                    0
                ],
                "title": "Armadillo: An Efficient Framework for Numerical Linear Algebra",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Armadillo: An Efficient Framework for Numerical Linear Algebra"
                },
                "summary": "A major challenge in the deployment of scientific software solutions is the\nadaptation of research prototypes to production-grade code. While high-level\nlanguages like MATLAB are useful for rapid prototyping, they lack the resource\nefficiency required for scalable production applications, necessitating\ntranslation into lower level languages like C++. Further, for machine learning\nand signal processing applications, the underlying linear algebra primitives,\ngenerally provided by the standard BLAS and LAPACK libraries, are unwieldy and\ndifficult to use, requiring manual memory management and other tedium. To\naddress this challenge, the Armadillo C++ linear algebra library provides an\nintuitive interface for writing linear algebra expressions that are easily\ncompiled into efficient production-grade implementations. We describe the\nexpression optimisations we have implemented in Armadillo, exploiting template\nmetaprogramming. We demonstrate that these optimisations result in considerable\nefficiency gains on a variety of benchmark linear algebra expressions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A major challenge in the deployment of scientific software solutions is the\nadaptation of research prototypes to production-grade code. While high-level\nlanguages like MATLAB are useful for rapid prototyping, they lack the resource\nefficiency required for scalable production applications, necessitating\ntranslation into lower level languages like C++. Further, for machine learning\nand signal processing applications, the underlying linear algebra primitives,\ngenerally provided by the standard BLAS and LAPACK libraries, are unwieldy and\ndifficult to use, requiring manual memory management and other tedium. To\naddress this challenge, the Armadillo C++ linear algebra library provides an\nintuitive interface for writing linear algebra expressions that are easily\ncompiled into efficient production-grade implementations. We describe the\nexpression optimisations we have implemented in Armadillo, exploiting template\nmetaprogramming. We demonstrate that these optimisations result in considerable\nefficiency gains on a variety of benchmark linear algebra expressions."
                },
                "authors": [
                    {
                        "name": "Conrad Sanderson"
                    },
                    {
                        "name": "Ryan Curtin"
                    }
                ],
                "author_detail": {
                    "name": "Ryan Curtin"
                },
                "author": "Ryan Curtin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03000v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03000v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68N99, 65Y04, 65Y15, 65F45",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.4; G.1.3; D.2.3; F.2.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00634v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00634v2",
                "updated": "2025-03-26T08:56:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    8,
                    56,
                    4,
                    2,
                    85,
                    0
                ],
                "published": "2024-11-01T14:45:34Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    14,
                    45,
                    34,
                    4,
                    306,
                    0
                ],
                "title": "Does GenAI Make Usability Testing Obsolete?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does GenAI Make Usability Testing Obsolete?"
                },
                "summary": "Ensuring usability is crucial for the success of mobile apps. Usability\nissues can compromise user experience and negatively impact the perceived app\nquality. This paper presents UX-LLM, a novel tool powered by a Large\nVision-Language Model that predicts usability issues in iOS apps. To evaluate\nthe performance of UX-LLM, we predicted usability issues in two open-source\napps of a medium complexity and asked two usability experts to assess the\npredictions. We also performed traditional usability testing and expert review\nfor both apps and compared the results to those of UX-LLM. UX-LLM demonstrated\nprecision ranging from 0.61 and 0.66 and recall between 0.35 and 0.38,\nindicating its ability to identify valid usability issues, yet failing to\ncapture the majority of issues. Finally, we conducted a focus group with an app\ndevelopment team of a capstone project developing a transit app for visually\nimpaired persons. The focus group expressed positive perceptions of UX-LLM as\nit identified unknown usability issues in their app. However, they also raised\nconcerns about its integration into the development workflow, suggesting\npotential improvements. Our results show that UX-LLM cannot fully replace\ntraditional usability evaluation methods but serves as a valuable supplement\nparticularly for small teams with limited resources, to identify issues in less\ncommon user paths, due to its ability to inspect the source code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring usability is crucial for the success of mobile apps. Usability\nissues can compromise user experience and negatively impact the perceived app\nquality. This paper presents UX-LLM, a novel tool powered by a Large\nVision-Language Model that predicts usability issues in iOS apps. To evaluate\nthe performance of UX-LLM, we predicted usability issues in two open-source\napps of a medium complexity and asked two usability experts to assess the\npredictions. We also performed traditional usability testing and expert review\nfor both apps and compared the results to those of UX-LLM. UX-LLM demonstrated\nprecision ranging from 0.61 and 0.66 and recall between 0.35 and 0.38,\nindicating its ability to identify valid usability issues, yet failing to\ncapture the majority of issues. Finally, we conducted a focus group with an app\ndevelopment team of a capstone project developing a transit app for visually\nimpaired persons. The focus group expressed positive perceptions of UX-LLM as\nit identified unknown usability issues in their app. However, they also raised\nconcerns about its integration into the development workflow, suggesting\npotential improvements. Our results show that UX-LLM cannot fully replace\ntraditional usability evaluation methods but serves as a valuable supplement\nparticularly for small teams with limited resources, to identify issues in less\ncommon user paths, due to its ability to inspect the source code."
                },
                "authors": [
                    {
                        "name": "Ali Ebrahimi Pourasad"
                    },
                    {
                        "name": "Walid Maalej"
                    }
                ],
                "author_detail": {
                    "name": "Walid Maalej"
                },
                "author": "Walid Maalej",
                "arxiv_comment": "Accepted for publication at The 47th IEEE/ACM International\n  Conference on Software Engineering ICSE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00634v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00634v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.18205v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.18205v5",
                "updated": "2025-03-26T08:55:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    8,
                    55,
                    5,
                    2,
                    85,
                    0
                ],
                "published": "2024-02-28T09:51:55Z",
                "published_parsed": [
                    2024,
                    2,
                    28,
                    9,
                    51,
                    55,
                    2,
                    59,
                    0
                ],
                "title": "Lemur: Log Parsing with Entropy Sampling and Chain-of-Thought Merging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lemur: Log Parsing with Entropy Sampling and Chain-of-Thought Merging"
                },
                "summary": "Logs produced by extensive software systems are integral to monitoring system\nbehaviors. Advanced log analysis facilitates the detection, alerting, and\ndiagnosis of system faults. Log parsing, which entails transforming raw log\nmessages into structured templates, constitutes a critical phase in the\nautomation of log analytics. Existing log parsers fail to identify the correct\ntemplates due to reliance on human-made rules. Besides, these methods focus on\nstatistical features while ignoring semantic information in log messages. To\naddress these challenges, we introduce a cutting-edge \\textbf{L}og parsing\nframework with \\textbf{E}ntropy sampling and chain-of-thought \\textbf{M}erging\n(\\model{}). Specifically, to discard the tedious manual rules, we propose a\nnovel sampling method inspired by information entropy, which efficiently\nclusters typical logs. Furthermore, to enhance the merging of log templates, we\ndesign a chain-of-thought method for large language models (LLMs). LLMs exhibit\nexceptional semantic comprehension and deftly distinguish between parameters\nand invariant tokens. We have conducted experiments on large-scale public\ndatasets. Extensive evaluation demonstrates that \\model{} achieves\nstate-of-the-art performance and impressive efficiency. The Code is available\nat https://github.com/zwpride/lemur.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Logs produced by extensive software systems are integral to monitoring system\nbehaviors. Advanced log analysis facilitates the detection, alerting, and\ndiagnosis of system faults. Log parsing, which entails transforming raw log\nmessages into structured templates, constitutes a critical phase in the\nautomation of log analytics. Existing log parsers fail to identify the correct\ntemplates due to reliance on human-made rules. Besides, these methods focus on\nstatistical features while ignoring semantic information in log messages. To\naddress these challenges, we introduce a cutting-edge \\textbf{L}og parsing\nframework with \\textbf{E}ntropy sampling and chain-of-thought \\textbf{M}erging\n(\\model{}). Specifically, to discard the tedious manual rules, we propose a\nnovel sampling method inspired by information entropy, which efficiently\nclusters typical logs. Furthermore, to enhance the merging of log templates, we\ndesign a chain-of-thought method for large language models (LLMs). LLMs exhibit\nexceptional semantic comprehension and deftly distinguish between parameters\nand invariant tokens. We have conducted experiments on large-scale public\ndatasets. Extensive evaluation demonstrates that \\model{} achieves\nstate-of-the-art performance and impressive efficiency. The Code is available\nat https://github.com/zwpride/lemur."
                },
                "authors": [
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Xiangyuan Guan"
                    },
                    {
                        "name": "Lu Yunhong"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Shuangyong Song"
                    },
                    {
                        "name": "Xianfu Cheng"
                    },
                    {
                        "name": "Zhenhe Wu"
                    },
                    {
                        "name": "Zhoujun Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhoujun Li"
                },
                "author": "Zhoujun Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.18205v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.18205v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20320v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20320v1",
                "updated": "2025-03-26T08:40:46Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    8,
                    40,
                    46,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T08:40:46Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    8,
                    40,
                    46,
                    2,
                    85,
                    0
                ],
                "title": "Iterative Prompting with Persuasion Skills in Jailbreaking Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Iterative Prompting with Persuasion Skills in Jailbreaking Large\n  Language Models"
                },
                "summary": "Large language models (LLMs) are designed to align with human values in their\nresponses. This study exploits LLMs with an iterative prompting technique where\neach prompt is systematically modified and refined across multiple iterations\nto enhance its effectiveness in jailbreaking attacks progressively. This\ntechnique involves analyzing the response patterns of LLMs, including GPT-3.5,\nGPT-4, LLaMa2, Vicuna, and ChatGLM, allowing us to adjust and optimize prompts\nto evade the LLMs' ethical and security constraints. Persuasion strategies\nenhance prompt effectiveness while maintaining consistency with malicious\nintent. Our results show that the attack success rates (ASR) increase as the\nattacking prompts become more refined with the highest ASR of 90% for GPT4 and\nChatGLM and the lowest ASR of 68% for LLaMa2. Our technique outperforms\nbaseline techniques (PAIR and PAP) in ASR and shows comparable performance with\nGCG and ArtPrompt.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are designed to align with human values in their\nresponses. This study exploits LLMs with an iterative prompting technique where\neach prompt is systematically modified and refined across multiple iterations\nto enhance its effectiveness in jailbreaking attacks progressively. This\ntechnique involves analyzing the response patterns of LLMs, including GPT-3.5,\nGPT-4, LLaMa2, Vicuna, and ChatGLM, allowing us to adjust and optimize prompts\nto evade the LLMs' ethical and security constraints. Persuasion strategies\nenhance prompt effectiveness while maintaining consistency with malicious\nintent. Our results show that the attack success rates (ASR) increase as the\nattacking prompts become more refined with the highest ASR of 90% for GPT4 and\nChatGLM and the lowest ASR of 68% for LLaMa2. Our technique outperforms\nbaseline techniques (PAIR and PAP) in ASR and shows comparable performance with\nGCG and ArtPrompt."
                },
                "authors": [
                    {
                        "name": "Shih-Wen Ke"
                    },
                    {
                        "name": "Guan-Yu Lai"
                    },
                    {
                        "name": "Guo-Lin Fang"
                    },
                    {
                        "name": "Hsi-Yuan Kao"
                    }
                ],
                "author_detail": {
                    "name": "Hsi-Yuan Kao"
                },
                "author": "Hsi-Yuan Kao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20320v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20320v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20316v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20316v1",
                "updated": "2025-03-26T08:33:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    8,
                    33,
                    3,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T08:33:03Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    8,
                    33,
                    3,
                    2,
                    85,
                    0
                ],
                "title": "AI-Driven MRI Spine Pathology Detection: A Comprehensive Deep Learning\n  Approach for Automated Diagnosis in Diverse Clinical Settings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-Driven MRI Spine Pathology Detection: A Comprehensive Deep Learning\n  Approach for Automated Diagnosis in Diverse Clinical Settings"
                },
                "summary": "Study Design This study presents the development of an autonomous AI system\nfor MRI spine pathology detection, trained on a dataset of 2 million MRI spine\nscans sourced from diverse healthcare facilities across India. The AI system\nintegrates advanced architectures, including Vision Transformers, U-Net with\ncross-attention, MedSAM, and Cascade R-CNN, enabling comprehensive\nclassification, segmentation, and detection of 43 distinct spinal pathologies.\nThe dataset is balanced across age groups, genders, and scanner manufacturers\nto ensure robustness and adaptability. Subgroup analyses were conducted to\nvalidate the model's performance across different patient demographics, imaging\nconditions, and equipment types.\n  Performance The AI system achieved up to 97.9 percent multi-pathology\ndetection, demonstrating consistent performance across age, gender, and\nmanufacturer subgroups. The normal vs. abnormal classification achieved 98.0\npercent accuracy, and the system was deployed across 13 major healthcare\nenterprises in India, encompassing diagnostic centers, large hospitals, and\ngovernment facilities. During deployment, it processed approximately 100,000\nplus MRI spine scans, leading to reduced reporting times and increased\ndiagnostic efficiency by automating the identification of common spinal\nconditions.\n  Conclusion The AI system's high precision and recall validate its capability\nas a reliable tool for autonomous normal/abnormal classification, pathology\nsegmentation, and detection. Its scalability and adaptability address critical\ndiagnostic gaps, optimize radiology workflows, and improve patient care across\nvaried healthcare environments in India.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Study Design This study presents the development of an autonomous AI system\nfor MRI spine pathology detection, trained on a dataset of 2 million MRI spine\nscans sourced from diverse healthcare facilities across India. The AI system\nintegrates advanced architectures, including Vision Transformers, U-Net with\ncross-attention, MedSAM, and Cascade R-CNN, enabling comprehensive\nclassification, segmentation, and detection of 43 distinct spinal pathologies.\nThe dataset is balanced across age groups, genders, and scanner manufacturers\nto ensure robustness and adaptability. Subgroup analyses were conducted to\nvalidate the model's performance across different patient demographics, imaging\nconditions, and equipment types.\n  Performance The AI system achieved up to 97.9 percent multi-pathology\ndetection, demonstrating consistent performance across age, gender, and\nmanufacturer subgroups. The normal vs. abnormal classification achieved 98.0\npercent accuracy, and the system was deployed across 13 major healthcare\nenterprises in India, encompassing diagnostic centers, large hospitals, and\ngovernment facilities. During deployment, it processed approximately 100,000\nplus MRI spine scans, leading to reduced reporting times and increased\ndiagnostic efficiency by automating the identification of common spinal\nconditions.\n  Conclusion The AI system's high precision and recall validate its capability\nas a reliable tool for autonomous normal/abnormal classification, pathology\nsegmentation, and detection. Its scalability and adaptability address critical\ndiagnostic gaps, optimize radiology workflows, and improve patient care across\nvaried healthcare environments in India."
                },
                "authors": [
                    {
                        "name": "Bargava Subramanian"
                    },
                    {
                        "name": "Naveen Kumarasami"
                    },
                    {
                        "name": "Praveen Shastry"
                    },
                    {
                        "name": "Raghotham Sripadraj"
                    },
                    {
                        "name": "Kalyan Sivasailam"
                    },
                    {
                        "name": "Anandakumar D"
                    },
                    {
                        "name": "Abinaya Ramachandran"
                    },
                    {
                        "name": "Sudhir MP"
                    },
                    {
                        "name": "Gunakutti G"
                    },
                    {
                        "name": "Kishore Prasath Venkatesh"
                    }
                ],
                "author_detail": {
                    "name": "Kishore Prasath Venkatesh"
                },
                "author": "Kishore Prasath Venkatesh",
                "arxiv_comment": "20 pages , 3 figurea",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20316v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20316v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20302v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20302v1",
                "updated": "2025-03-26T08:01:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    8,
                    1,
                    35,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T08:01:35Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    8,
                    1,
                    35,
                    2,
                    85,
                    0
                ],
                "title": "A Multilingual, Culture-First Approach to Addressing Misgendering in LLM\n  Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Multilingual, Culture-First Approach to Addressing Misgendering in LLM\n  Applications"
                },
                "summary": "Misgendering is the act of referring to someone by a gender that does not\nmatch their chosen identity. It marginalizes and undermines a person's sense of\nself, causing significant harm. English-based approaches have clear-cut\napproaches to avoiding misgendering, such as the use of the pronoun ``they''.\nHowever, other languages pose unique challenges due to both grammatical and\ncultural constructs. In this work we develop methodologies to assess and\nmitigate misgendering across 42 languages and dialects using a\nparticipatory-design approach to design effective and appropriate guardrails\nacross all languages. We test these guardrails in a standard large language\nmodel-based application (meeting transcript summarization), where both the data\ngeneration and the annotation steps followed a human-in-the-loop approach. We\nfind that the proposed guardrails are very effective in reducing misgendering\nrates across all languages in the summaries generated, and without incurring\nloss of quality. Our human-in-the-loop approach demonstrates a method to\nfeasibly scale inclusive and responsible AI-based solutions across multiple\nlanguages and cultures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Misgendering is the act of referring to someone by a gender that does not\nmatch their chosen identity. It marginalizes and undermines a person's sense of\nself, causing significant harm. English-based approaches have clear-cut\napproaches to avoiding misgendering, such as the use of the pronoun ``they''.\nHowever, other languages pose unique challenges due to both grammatical and\ncultural constructs. In this work we develop methodologies to assess and\nmitigate misgendering across 42 languages and dialects using a\nparticipatory-design approach to design effective and appropriate guardrails\nacross all languages. We test these guardrails in a standard large language\nmodel-based application (meeting transcript summarization), where both the data\ngeneration and the annotation steps followed a human-in-the-loop approach. We\nfind that the proposed guardrails are very effective in reducing misgendering\nrates across all languages in the summaries generated, and without incurring\nloss of quality. Our human-in-the-loop approach demonstrates a method to\nfeasibly scale inclusive and responsible AI-based solutions across multiple\nlanguages and cultures."
                },
                "authors": [
                    {
                        "name": "Sunayana Sitaram"
                    },
                    {
                        "name": "Adrian de Wynter"
                    },
                    {
                        "name": "Isobel McCrum"
                    },
                    {
                        "name": "Qilong Gu"
                    },
                    {
                        "name": "Si-Qing Chen"
                    }
                ],
                "author_detail": {
                    "name": "Si-Qing Chen"
                },
                "author": "Si-Qing Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20302v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20302v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.15190v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.15190v2",
                "updated": "2025-03-26T07:42:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    7,
                    42,
                    56,
                    2,
                    85,
                    0
                ],
                "published": "2024-04-21T08:10:20Z",
                "published_parsed": [
                    2024,
                    4,
                    21,
                    8,
                    10,
                    20,
                    6,
                    112,
                    0
                ],
                "title": "Socratic Planner: Self-QA-Based Zero-Shot Planning for Embodied\n  Instruction Following",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Socratic Planner: Self-QA-Based Zero-Shot Planning for Embodied\n  Instruction Following"
                },
                "summary": "Embodied Instruction Following (EIF) is the task of executing natural\nlanguage instructions by navigating and interacting with objects in interactive\nenvironments. A key challenge in EIF is compositional task planning, typically\naddressed through supervised learning or few-shot in-context learning with\nlabeled data. To this end, we introduce the Socratic Planner, a self-QA-based\nzero-shot planning method that infers an appropriate plan without any further\ntraining. The Socratic Planner first facilitates self-questioning and answering\nby the Large Language Model (LLM), which in turn helps generate a sequence of\nsubgoals. While executing the subgoals, an embodied agent may encounter\nunexpected situations, such as unforeseen obstacles. The Socratic Planner then\nadjusts plans based on dense visual feedback through a visually-grounded\nre-planning mechanism. Experiments demonstrate the effectiveness of the\nSocratic Planner, outperforming current state-of-the-art planning models on the\nALFRED benchmark across all metrics, particularly excelling in long-horizon\ntasks that demand complex inference. We further demonstrate its real-world\napplicability through deployment on a physical robot for long-horizon tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied Instruction Following (EIF) is the task of executing natural\nlanguage instructions by navigating and interacting with objects in interactive\nenvironments. A key challenge in EIF is compositional task planning, typically\naddressed through supervised learning or few-shot in-context learning with\nlabeled data. To this end, we introduce the Socratic Planner, a self-QA-based\nzero-shot planning method that infers an appropriate plan without any further\ntraining. The Socratic Planner first facilitates self-questioning and answering\nby the Large Language Model (LLM), which in turn helps generate a sequence of\nsubgoals. While executing the subgoals, an embodied agent may encounter\nunexpected situations, such as unforeseen obstacles. The Socratic Planner then\nadjusts plans based on dense visual feedback through a visually-grounded\nre-planning mechanism. Experiments demonstrate the effectiveness of the\nSocratic Planner, outperforming current state-of-the-art planning models on the\nALFRED benchmark across all metrics, particularly excelling in long-horizon\ntasks that demand complex inference. We further demonstrate its real-world\napplicability through deployment on a physical robot for long-horizon tasks."
                },
                "authors": [
                    {
                        "name": "Suyeon Shin"
                    },
                    {
                        "name": "Sujin jeon"
                    },
                    {
                        "name": "Junghyun Kim"
                    },
                    {
                        "name": "Gi-Cheon Kang"
                    },
                    {
                        "name": "Byoung-Tak Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Byoung-Tak Zhang"
                },
                "author": "Byoung-Tak Zhang",
                "arxiv_comment": "8 pages, 6 figures, published to ICRA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.15190v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.15190v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T01 (Primary) 68T40, 68T50, 68T45 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20290v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20290v1",
                "updated": "2025-03-26T07:32:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    7,
                    32,
                    20,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T07:32:20Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    7,
                    32,
                    20,
                    2,
                    85,
                    0
                ],
                "title": "QualiSpeech: A Speech Quality Assessment Dataset with Natural Language\n  Reasoning and Descriptions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QualiSpeech: A Speech Quality Assessment Dataset with Natural Language\n  Reasoning and Descriptions"
                },
                "summary": "This paper explores a novel perspective to speech quality assessment by\nleveraging natural language descriptions, offering richer, more nuanced\ninsights than traditional numerical scoring methods. Natural language feedback\nprovides instructive recommendations and detailed evaluations, yet existing\ndatasets lack the comprehensive annotations needed for this approach. To bridge\nthis gap, we introduce QualiSpeech, a comprehensive low-level speech quality\nassessment dataset encompassing 11 key aspects and detailed natural language\ncomments that include reasoning and contextual insights. Additionally, we\npropose the QualiSpeech Benchmark to evaluate the low-level speech\nunderstanding capabilities of auditory large language models (LLMs).\nExperimental results demonstrate that finetuned auditory LLMs can reliably\ngenerate detailed descriptions of noise and distortion, effectively identifying\ntheir types and temporal characteristics. The results further highlight the\npotential for incorporating reasoning to enhance the accuracy and reliability\nof quality assessments. The dataset will be released at\nhttps://huggingface.co/datasets/tsinghua-ee/QualiSpeech.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores a novel perspective to speech quality assessment by\nleveraging natural language descriptions, offering richer, more nuanced\ninsights than traditional numerical scoring methods. Natural language feedback\nprovides instructive recommendations and detailed evaluations, yet existing\ndatasets lack the comprehensive annotations needed for this approach. To bridge\nthis gap, we introduce QualiSpeech, a comprehensive low-level speech quality\nassessment dataset encompassing 11 key aspects and detailed natural language\ncomments that include reasoning and contextual insights. Additionally, we\npropose the QualiSpeech Benchmark to evaluate the low-level speech\nunderstanding capabilities of auditory large language models (LLMs).\nExperimental results demonstrate that finetuned auditory LLMs can reliably\ngenerate detailed descriptions of noise and distortion, effectively identifying\ntheir types and temporal characteristics. The results further highlight the\npotential for incorporating reasoning to enhance the accuracy and reliability\nof quality assessments. The dataset will be released at\nhttps://huggingface.co/datasets/tsinghua-ee/QualiSpeech."
                },
                "authors": [
                    {
                        "name": "Siyin Wang"
                    },
                    {
                        "name": "Wenyi Yu"
                    },
                    {
                        "name": "Xianzhao Chen"
                    },
                    {
                        "name": "Xiaohai Tian"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Yu Tsao"
                    },
                    {
                        "name": "Junichi Yamagishi"
                    },
                    {
                        "name": "Yuxuan Wang"
                    },
                    {
                        "name": "Chao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Zhang"
                },
                "author": "Chao Zhang",
                "arxiv_comment": "23 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20290v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20290v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16425v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16425v2",
                "updated": "2025-03-26T07:26:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    7,
                    26,
                    43,
                    2,
                    85,
                    0
                ],
                "published": "2024-11-25T14:27:55Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    14,
                    27,
                    55,
                    0,
                    330,
                    0
                ],
                "title": "TopV-Nav: Unlocking the Top-View Spatial Reasoning Potential of MLLM for\n  Zero-shot Object Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TopV-Nav: Unlocking the Top-View Spatial Reasoning Potential of MLLM for\n  Zero-shot Object Navigation"
                },
                "summary": "The Zero-Shot Object Navigation (ZSON) task requires embodied agents to find\na previously unseen object by navigating in unfamiliar environments. Such a\ngoal-oriented exploration heavily relies on the ability to perceive,\nunderstand, and reason based on the spatial information of the environment.\nHowever, current LLM-based approaches convert visual observations to language\ndescriptions and reason in the linguistic space, leading to the loss of spatial\ninformation. In this paper, we introduce TopV-Nav, an MLLM-based method that\ndirectly reasons on the top-view map with sufficient spatial information. To\nfully unlock the MLLM's spatial reasoning potential in top-view perspective, we\npropose the Adaptive Visual Prompt Generation (AVPG) method to adaptively\nconstruct semantically-rich top-view map. It enables the agent to directly\nutilize spatial information contained in the top-view map to conduct thorough\nreasoning. Besides, we design a Dynamic Map Scaling (DMS) mechanism to\ndynamically zoom top-view map at preferred scales, enhancing local fine-grained\nreasoning. Additionally, we devise a Potential Target Driven (PTD) mechanism to\npredict and to utilize target locations, facilitating global and human-like\nexploration. Experiments on MP3D and HM3D datasets demonstrate the superiority\nof our TopV-Nav.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Zero-Shot Object Navigation (ZSON) task requires embodied agents to find\na previously unseen object by navigating in unfamiliar environments. Such a\ngoal-oriented exploration heavily relies on the ability to perceive,\nunderstand, and reason based on the spatial information of the environment.\nHowever, current LLM-based approaches convert visual observations to language\ndescriptions and reason in the linguistic space, leading to the loss of spatial\ninformation. In this paper, we introduce TopV-Nav, an MLLM-based method that\ndirectly reasons on the top-view map with sufficient spatial information. To\nfully unlock the MLLM's spatial reasoning potential in top-view perspective, we\npropose the Adaptive Visual Prompt Generation (AVPG) method to adaptively\nconstruct semantically-rich top-view map. It enables the agent to directly\nutilize spatial information contained in the top-view map to conduct thorough\nreasoning. Besides, we design a Dynamic Map Scaling (DMS) mechanism to\ndynamically zoom top-view map at preferred scales, enhancing local fine-grained\nreasoning. Additionally, we devise a Potential Target Driven (PTD) mechanism to\npredict and to utilize target locations, facilitating global and human-like\nexploration. Experiments on MP3D and HM3D datasets demonstrate the superiority\nof our TopV-Nav."
                },
                "authors": [
                    {
                        "name": "Linqing Zhong"
                    },
                    {
                        "name": "Chen Gao"
                    },
                    {
                        "name": "Zihan Ding"
                    },
                    {
                        "name": "Yue Liao"
                    },
                    {
                        "name": "Huimin Ma"
                    },
                    {
                        "name": "Shifeng Zhang"
                    },
                    {
                        "name": "Xu Zhou"
                    },
                    {
                        "name": "Si Liu"
                    }
                ],
                "author_detail": {
                    "name": "Si Liu"
                },
                "author": "Si Liu",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16425v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16425v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20279v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20279v1",
                "updated": "2025-03-26T07:08:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    7,
                    8,
                    15,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T07:08:15Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    7,
                    8,
                    15,
                    2,
                    85,
                    0
                ],
                "title": "sudo rm -rf agentic_security",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "sudo rm -rf agentic_security"
                },
                "summary": "Large Language Models (LLMs) are increasingly deployed as computer-use\nagents, autonomously performing tasks within real desktop or web environments.\nWhile this evolution greatly expands practical use cases for humans, it also\ncreates serious security exposures. We present SUDO (Screen-based Universal\nDetox2Tox Offense), a novel attack framework that systematically bypasses\nrefusal trained safeguards in commercial computer-use agents, such as Claude\nComputer Use. The core mechanism, Detox2Tox, transforms harmful requests (that\nagents initially reject) into seemingly benign requests via detoxification,\nsecures detailed instructions from advanced vision language models (VLMs), and\nthen reintroduces malicious content via toxification just before execution.\nUnlike conventional jailbreaks, SUDO iteratively refines its attacks based on a\nbuilt-in refusal feedback, making it increasingly effective against robust\npolicy filters. In extensive tests spanning 50 real-world tasks and multiple\nstate-of-the-art VLMs, SUDO achieves a stark attack success rate of 24% (with\nno refinement), and up to 41% (by its iterative refinement) in Claude Computer\nUse. By revealing these vulnerabilities and demonstrating the ease with which\nthey can be exploited in real-world computing environments, this paper\nhighlights an immediate need for robust, context-aware safeguards. WARNING:\nThis paper includes harmful or offensive model outputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed as computer-use\nagents, autonomously performing tasks within real desktop or web environments.\nWhile this evolution greatly expands practical use cases for humans, it also\ncreates serious security exposures. We present SUDO (Screen-based Universal\nDetox2Tox Offense), a novel attack framework that systematically bypasses\nrefusal trained safeguards in commercial computer-use agents, such as Claude\nComputer Use. The core mechanism, Detox2Tox, transforms harmful requests (that\nagents initially reject) into seemingly benign requests via detoxification,\nsecures detailed instructions from advanced vision language models (VLMs), and\nthen reintroduces malicious content via toxification just before execution.\nUnlike conventional jailbreaks, SUDO iteratively refines its attacks based on a\nbuilt-in refusal feedback, making it increasingly effective against robust\npolicy filters. In extensive tests spanning 50 real-world tasks and multiple\nstate-of-the-art VLMs, SUDO achieves a stark attack success rate of 24% (with\nno refinement), and up to 41% (by its iterative refinement) in Claude Computer\nUse. By revealing these vulnerabilities and demonstrating the ease with which\nthey can be exploited in real-world computing environments, this paper\nhighlights an immediate need for robust, context-aware safeguards. WARNING:\nThis paper includes harmful or offensive model outputs."
                },
                "authors": [
                    {
                        "name": "Sejin Lee"
                    },
                    {
                        "name": "Jian Kim"
                    },
                    {
                        "name": "Haon Park"
                    },
                    {
                        "name": "Ashkan Yousefpour"
                    },
                    {
                        "name": "Sangyoon Yu"
                    },
                    {
                        "name": "Min Song"
                    }
                ],
                "author_detail": {
                    "name": "Min Song"
                },
                "author": "Min Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20279v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20279v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08160v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08160v3",
                "updated": "2025-03-26T06:56:09Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    6,
                    56,
                    9,
                    2,
                    85,
                    0
                ],
                "published": "2024-08-15T13:49:14Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    13,
                    49,
                    14,
                    3,
                    228,
                    0
                ],
                "title": "General-purpose Clothes Manipulation with Semantic Keypoints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "General-purpose Clothes Manipulation with Semantic Keypoints"
                },
                "summary": "Clothes manipulation is a critical capability for household robots; yet,\nexisting methods are often confined to specific tasks, such as folding or\nflattening, due to the complex high-dimensional geometry of deformable fabric.\nThis paper presents CLothes mAnipulation with Semantic keyPoints (CLASP) for\ngeneral-purpose clothes manipulation, which enables the robot to perform\ndiverse manipulation tasks over different types of clothes. The key idea of\nCLASP is semantic keypoints -- e.g., \"right shoulder\", \"left sleeve\", etc. -- a\nsparse spatial-semantic representation that is salient for both perception and\naction. Semantic keypoints of clothes can be effectively extracted from depth\nimages and are sufficient to represent a broad range of clothes manipulation\npolicies. CLASP leverages semantic keypoints to bridge LLM-powered task\nplanning and low-level action execution in a two-level hierarchy. Extensive\nsimulation experiments show that CLASP outperforms baseline methods across\ndiverse clothes types in both seen and unseen tasks. Further, experiments with\na Kinova dual-arm system on four distinct tasks -- folding, flattening,\nhanging, and placing -- confirm CLASP's performance on a real robot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clothes manipulation is a critical capability for household robots; yet,\nexisting methods are often confined to specific tasks, such as folding or\nflattening, due to the complex high-dimensional geometry of deformable fabric.\nThis paper presents CLothes mAnipulation with Semantic keyPoints (CLASP) for\ngeneral-purpose clothes manipulation, which enables the robot to perform\ndiverse manipulation tasks over different types of clothes. The key idea of\nCLASP is semantic keypoints -- e.g., \"right shoulder\", \"left sleeve\", etc. -- a\nsparse spatial-semantic representation that is salient for both perception and\naction. Semantic keypoints of clothes can be effectively extracted from depth\nimages and are sufficient to represent a broad range of clothes manipulation\npolicies. CLASP leverages semantic keypoints to bridge LLM-powered task\nplanning and low-level action execution in a two-level hierarchy. Extensive\nsimulation experiments show that CLASP outperforms baseline methods across\ndiverse clothes types in both seen and unseen tasks. Further, experiments with\na Kinova dual-arm system on four distinct tasks -- folding, flattening,\nhanging, and placing -- confirm CLASP's performance on a real robot."
                },
                "authors": [
                    {
                        "name": "Yuhong Deng"
                    },
                    {
                        "name": "David Hsu"
                    }
                ],
                "author_detail": {
                    "name": "David Hsu"
                },
                "author": "David Hsu",
                "arxiv_comment": "accepted by IEEE International Conference on Robotics and Automation\n  (ICRA 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08160v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08160v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19739v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19739v2",
                "updated": "2025-03-26T06:54:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    6,
                    54,
                    19,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-25T15:04:53Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    15,
                    4,
                    53,
                    1,
                    84,
                    0
                ],
                "title": "FUSE: Label-Free Image-Event Joint Monocular Depth Estimation via\n  Frequency-Decoupled Alignment and Degradation-Robust Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FUSE: Label-Free Image-Event Joint Monocular Depth Estimation via\n  Frequency-Decoupled Alignment and Degradation-Robust Fusion"
                },
                "summary": "Image-event joint depth estimation methods leverage complementary modalities\nfor robust perception, yet face challenges in generalizability stemming from\ntwo factors: 1) limited annotated image-event-depth datasets causing\ninsufficient cross-modal supervision, and 2) inherent frequency mismatches\nbetween static images and dynamic event streams with distinct spatiotemporal\npatterns, leading to ineffective feature fusion. To address this dual\nchallenge, we propose Frequency-decoupled Unified Self-supervised Encoder\n(FUSE) with two synergistic components: The Parameter-efficient Self-supervised\nTransfer (PST) establishes cross-modal knowledge transfer through latent space\nalignment with image foundation models, effectively mitigating data scarcity by\nenabling joint encoding without depth ground truth. Complementing this, we\npropose the Frequency-Decoupled Fusion module (FreDFuse) to explicitly decouple\nhigh-frequency edge features from low-frequency structural components,\nresolving modality-specific frequency mismatches through physics-aware fusion.\nThis combined approach enables FUSE to construct a universal image-event\nencoder that only requires lightweight decoder adaptation for target datasets.\nExtensive experiments demonstrate state-of-the-art performance with 14% and\n24.9% improvements in Abs.Rel on MVSEC and DENSE datasets. The framework\nexhibits remarkable zero-shot adaptability to challenging scenarios including\nextreme lighting and motion blur, significantly advancing real-world deployment\ncapabilities. The source code for our method is publicly available at:\nhttps://github.com/sunpihai-up/FUSE",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Image-event joint depth estimation methods leverage complementary modalities\nfor robust perception, yet face challenges in generalizability stemming from\ntwo factors: 1) limited annotated image-event-depth datasets causing\ninsufficient cross-modal supervision, and 2) inherent frequency mismatches\nbetween static images and dynamic event streams with distinct spatiotemporal\npatterns, leading to ineffective feature fusion. To address this dual\nchallenge, we propose Frequency-decoupled Unified Self-supervised Encoder\n(FUSE) with two synergistic components: The Parameter-efficient Self-supervised\nTransfer (PST) establishes cross-modal knowledge transfer through latent space\nalignment with image foundation models, effectively mitigating data scarcity by\nenabling joint encoding without depth ground truth. Complementing this, we\npropose the Frequency-Decoupled Fusion module (FreDFuse) to explicitly decouple\nhigh-frequency edge features from low-frequency structural components,\nresolving modality-specific frequency mismatches through physics-aware fusion.\nThis combined approach enables FUSE to construct a universal image-event\nencoder that only requires lightweight decoder adaptation for target datasets.\nExtensive experiments demonstrate state-of-the-art performance with 14% and\n24.9% improvements in Abs.Rel on MVSEC and DENSE datasets. The framework\nexhibits remarkable zero-shot adaptability to challenging scenarios including\nextreme lighting and motion blur, significantly advancing real-world deployment\ncapabilities. The source code for our method is publicly available at:\nhttps://github.com/sunpihai-up/FUSE"
                },
                "authors": [
                    {
                        "name": "Pihai Sun"
                    },
                    {
                        "name": "Junjun Jiang"
                    },
                    {
                        "name": "Yuanqi Yao"
                    },
                    {
                        "name": "Youyu Chen"
                    },
                    {
                        "name": "Wenbo Zhao"
                    },
                    {
                        "name": "Kui Jiang"
                    },
                    {
                        "name": "Xianming Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xianming Liu"
                },
                "author": "Xianming Liu",
                "arxiv_comment": "8 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19739v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19739v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12138v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12138v2",
                "updated": "2025-03-26T06:48:11Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    6,
                    48,
                    11,
                    2,
                    85,
                    0
                ],
                "published": "2024-10-16T00:59:19Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    0,
                    59,
                    19,
                    2,
                    290,
                    0
                ],
                "title": "Preference Optimization with Multi-Sample Comparisons",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preference Optimization with Multi-Sample Comparisons"
                },
                "summary": "Recent advancements in generative models, particularly large language models\n(LLMs) and diffusion models, have been driven by extensive pretraining on large\ndatasets followed by post-training. However, current post-training methods such\nas reinforcement learning from human feedback (RLHF) and direct alignment from\npreference methods (DAP) primarily utilize single-sample comparisons. These\napproaches often fail to capture critical characteristics such as generative\ndiversity and bias, which are more accurately assessed through multiple\nsamples. To address these limitations, we introduce a novel approach that\nextends post-training to include multi-sample comparisons. To achieve this, we\npropose Multi-sample Direct Preference Optimization (mDPO) and Multi-sample\nIdentity Preference Optimization (mIPO). These methods improve traditional DAP\nmethods by focusing on group-wise characteristics. Empirically, we demonstrate\nthat multi-sample comparison is more effective in optimizing collective\ncharacteristics~(e.g., diversity and bias) for generative models than\nsingle-sample comparison. Additionally, our findings suggest that multi-sample\ncomparisons provide a more robust optimization framework, particularly for\ndataset with label noise.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in generative models, particularly large language models\n(LLMs) and diffusion models, have been driven by extensive pretraining on large\ndatasets followed by post-training. However, current post-training methods such\nas reinforcement learning from human feedback (RLHF) and direct alignment from\npreference methods (DAP) primarily utilize single-sample comparisons. These\napproaches often fail to capture critical characteristics such as generative\ndiversity and bias, which are more accurately assessed through multiple\nsamples. To address these limitations, we introduce a novel approach that\nextends post-training to include multi-sample comparisons. To achieve this, we\npropose Multi-sample Direct Preference Optimization (mDPO) and Multi-sample\nIdentity Preference Optimization (mIPO). These methods improve traditional DAP\nmethods by focusing on group-wise characteristics. Empirically, we demonstrate\nthat multi-sample comparison is more effective in optimizing collective\ncharacteristics~(e.g., diversity and bias) for generative models than\nsingle-sample comparison. Additionally, our findings suggest that multi-sample\ncomparisons provide a more robust optimization framework, particularly for\ndataset with label noise."
                },
                "authors": [
                    {
                        "name": "Chaoqi Wang"
                    },
                    {
                        "name": "Zhuokai Zhao"
                    },
                    {
                        "name": "Chen Zhu"
                    },
                    {
                        "name": "Karthik Abinav Sankararaman"
                    },
                    {
                        "name": "Michal Valko"
                    },
                    {
                        "name": "Xuefei Cao"
                    },
                    {
                        "name": "Zhaorun Chen"
                    },
                    {
                        "name": "Madian Khabsa"
                    },
                    {
                        "name": "Yuxin Chen"
                    },
                    {
                        "name": "Hao Ma"
                    },
                    {
                        "name": "Sinong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Sinong Wang"
                },
                "author": "Sinong Wang",
                "arxiv_comment": "Code is available at\n  https://github.com/alecwangcq/multi-sample-alignment",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12138v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12138v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19528v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19528v2",
                "updated": "2025-03-26T06:33:48Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    6,
                    33,
                    48,
                    2,
                    85,
                    0
                ],
                "published": "2024-09-29T03:07:34Z",
                "published_parsed": [
                    2024,
                    9,
                    29,
                    3,
                    7,
                    34,
                    6,
                    273,
                    0
                ],
                "title": "FoAM: Foresight-Augmented Multi-Task Imitation Policy for Robotic\n  Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FoAM: Foresight-Augmented Multi-Task Imitation Policy for Robotic\n  Manipulation"
                },
                "summary": "Multi-task imitation learning (MTIL) has shown significant potential in\nrobotic manipulation by enabling agents to perform various tasks using a single\npolicy. This simplifies the policy deployment and enhances the agent's\nadaptability across different scenarios. However, key challenges remain, such\nas maintaining action reliability (e.g., avoiding abnormal action sequences\nthat deviate from nominal task trajectories) and generalizing to unseen tasks\nwith a few expert demonstrations. To address these challenges, we introduce the\nForesight-Augmented Manipulation Policy (FoAM), a novel MTIL policy that\npioneers the use of multi-modal goal condition as input and introduces a\nforesight augmentation in addition to the general action reconstruction. FoAM\nenables the agent to reason about the visual consequences (states) of its\nactions and learn more expressive embedding that captures nuanced task\nvariations. Extensive experiments on over 100 tasks in simulation and\nreal-world settings demonstrate that FoAM significantly enhances MTIL policy\nperformance, outperforming state-of-the-art baselines by up to 41% in success\nrate. Meanwhile, we released our simulation suites, including a total of 10\nscenarios and over 80 challenging tasks designed for manipulation policy\ntraining and evaluation. See the project homepage projFoAM.github.io for\nproject details.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-task imitation learning (MTIL) has shown significant potential in\nrobotic manipulation by enabling agents to perform various tasks using a single\npolicy. This simplifies the policy deployment and enhances the agent's\nadaptability across different scenarios. However, key challenges remain, such\nas maintaining action reliability (e.g., avoiding abnormal action sequences\nthat deviate from nominal task trajectories) and generalizing to unseen tasks\nwith a few expert demonstrations. To address these challenges, we introduce the\nForesight-Augmented Manipulation Policy (FoAM), a novel MTIL policy that\npioneers the use of multi-modal goal condition as input and introduces a\nforesight augmentation in addition to the general action reconstruction. FoAM\nenables the agent to reason about the visual consequences (states) of its\nactions and learn more expressive embedding that captures nuanced task\nvariations. Extensive experiments on over 100 tasks in simulation and\nreal-world settings demonstrate that FoAM significantly enhances MTIL policy\nperformance, outperforming state-of-the-art baselines by up to 41% in success\nrate. Meanwhile, we released our simulation suites, including a total of 10\nscenarios and over 80 challenging tasks designed for manipulation policy\ntraining and evaluation. See the project homepage projFoAM.github.io for\nproject details."
                },
                "authors": [
                    {
                        "name": "Litao Liu"
                    },
                    {
                        "name": "Wentao Wang"
                    },
                    {
                        "name": "Yifan Han"
                    },
                    {
                        "name": "Zhuoli Xie"
                    },
                    {
                        "name": "Pengfei Yi"
                    },
                    {
                        "name": "Junyan Li"
                    },
                    {
                        "name": "Yi Qin"
                    },
                    {
                        "name": "Wenzhao Lian"
                    }
                ],
                "author_detail": {
                    "name": "Wenzhao Lian"
                },
                "author": "Wenzhao Lian",
                "arxiv_comment": "8 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19528v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19528v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20263v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20263v1",
                "updated": "2025-03-26T06:09:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    6,
                    9,
                    55,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T06:09:55Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    6,
                    9,
                    55,
                    2,
                    85,
                    0
                ],
                "title": "L4: Diagnosing Large-scale LLM Training Failures via Automated Log\n  Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "L4: Diagnosing Large-scale LLM Training Failures via Automated Log\n  Analysis"
                },
                "summary": "As Large Language Models (LLMs) show their capabilities across various\napplications, training customized LLMs has become essential for modern\nenterprises. However, due to the complexity of LLM training, which requires\nmassive computational resources and extensive training time, failures are\ninevitable during the training process. These failures result in considerable\nwaste of resource and time, highlighting the critical need for effective and\nefficient failure diagnosis to reduce the cost of LLM training.\n  In this paper, we present the first empirical study on the failure reports of\n428 LLM training failures in our production Platform-X between May 2023 and\nApril 2024. Our study reveals that hardware and user faults are the predominant\nroot causes, and current diagnosis processes rely heavily on training logs.\nUnfortunately, existing log-based diagnostic methods fall short in handling LLM\ntraining logs. Considering the unique features of LLM training, we identify\nthree distinct patterns of LLM training logs: cross-job, spatial, and temporal\npatterns. We then introduce our Log-based Large-scale LLM training failure\ndiagnosis framework, L4, which can automatically extract failure-indicating\ninformation (i.e., log events, nodes, stages, and iterations) from extensive\ntraining logs, thereby reducing manual effort and facilitating failure\nrecovery. Experimental results on real-world datasets show that L4 outperforms\nexisting approaches in identifying failure-indicating logs and localizing\nfaulty nodes. Furthermore, L4 has been applied in Platform-X and demonstrated\nits effectiveness in enabling accurate and efficient failure diagnosis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) show their capabilities across various\napplications, training customized LLMs has become essential for modern\nenterprises. However, due to the complexity of LLM training, which requires\nmassive computational resources and extensive training time, failures are\ninevitable during the training process. These failures result in considerable\nwaste of resource and time, highlighting the critical need for effective and\nefficient failure diagnosis to reduce the cost of LLM training.\n  In this paper, we present the first empirical study on the failure reports of\n428 LLM training failures in our production Platform-X between May 2023 and\nApril 2024. Our study reveals that hardware and user faults are the predominant\nroot causes, and current diagnosis processes rely heavily on training logs.\nUnfortunately, existing log-based diagnostic methods fall short in handling LLM\ntraining logs. Considering the unique features of LLM training, we identify\nthree distinct patterns of LLM training logs: cross-job, spatial, and temporal\npatterns. We then introduce our Log-based Large-scale LLM training failure\ndiagnosis framework, L4, which can automatically extract failure-indicating\ninformation (i.e., log events, nodes, stages, and iterations) from extensive\ntraining logs, thereby reducing manual effort and facilitating failure\nrecovery. Experimental results on real-world datasets show that L4 outperforms\nexisting approaches in identifying failure-indicating logs and localizing\nfaulty nodes. Furthermore, L4 has been applied in Platform-X and demonstrated\nits effectiveness in enabling accurate and efficient failure diagnosis."
                },
                "authors": [
                    {
                        "name": "Zhihan Jiang"
                    },
                    {
                        "name": "Junjie Huang"
                    },
                    {
                        "name": "Zhuangbin Chen"
                    },
                    {
                        "name": "Yichen Li"
                    },
                    {
                        "name": "Guangba Yu"
                    },
                    {
                        "name": "Cong Feng"
                    },
                    {
                        "name": "Yongqiang Yang"
                    },
                    {
                        "name": "Zengyin Yang"
                    },
                    {
                        "name": "Michael R. Lyu"
                    }
                ],
                "author_detail": {
                    "name": "Michael R. Lyu"
                },
                "author": "Michael R. Lyu",
                "arxiv_comment": "To appear in companion proceedings of the 33rd ACM International\n  Conference on the Foundations of Software Engineering (FSE'25). 13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20263v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20263v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20244v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20244v1",
                "updated": "2025-03-26T05:22:48Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    5,
                    22,
                    48,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T05:22:48Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    5,
                    22,
                    48,
                    2,
                    85,
                    0
                ],
                "title": "Software Vulnerability Analysis Across Programming Language and Program\n  Representation Landscapes: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software Vulnerability Analysis Across Programming Language and Program\n  Representation Landscapes: A Survey"
                },
                "summary": "Modern software systems are developed in diverse programming languages and\noften harbor critical vulnerabilities that attackers can exploit to compromise\nsecurity. These vulnerabilities have been actively targeted in real-world\nattacks, causing substantial harm to users and cyberinfrastructure. Since many\nof these flaws originate from the code itself, a variety of techniques have\nbeen proposed to detect and mitigate them prior to software deployment.\nHowever, a comprehensive comparative study that spans different programming\nlanguages, program representations, bug types, and analysis techniques is still\nlacking. As a result, the relationships among programming languages,\nabstraction levels, vulnerability types, and detection approaches remain\nfragmented, and the limitations and research gaps across the landscape are not\nclearly understood. This article aims to bridge that gap by systematically\nexamining widely used programming languages, levels of program representation,\ncategories of vulnerabilities, and mainstream detection techniques. The survey\nprovides a detailed understanding of current practices in vulnerability\ndiscovery, highlighting their strengths, limitations, and distinguishing\ncharacteristics. Furthermore, it identifies persistent challenges and outlines\npromising directions for future research in the field of software security.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern software systems are developed in diverse programming languages and\noften harbor critical vulnerabilities that attackers can exploit to compromise\nsecurity. These vulnerabilities have been actively targeted in real-world\nattacks, causing substantial harm to users and cyberinfrastructure. Since many\nof these flaws originate from the code itself, a variety of techniques have\nbeen proposed to detect and mitigate them prior to software deployment.\nHowever, a comprehensive comparative study that spans different programming\nlanguages, program representations, bug types, and analysis techniques is still\nlacking. As a result, the relationships among programming languages,\nabstraction levels, vulnerability types, and detection approaches remain\nfragmented, and the limitations and research gaps across the landscape are not\nclearly understood. This article aims to bridge that gap by systematically\nexamining widely used programming languages, levels of program representation,\ncategories of vulnerabilities, and mainstream detection techniques. The survey\nprovides a detailed understanding of current practices in vulnerability\ndiscovery, highlighting their strengths, limitations, and distinguishing\ncharacteristics. Furthermore, it identifies persistent challenges and outlines\npromising directions for future research in the field of software security."
                },
                "authors": [
                    {
                        "name": "Zhuoyun Qian"
                    },
                    {
                        "name": "Fangtian Zhong"
                    },
                    {
                        "name": "Qin Hu"
                    },
                    {
                        "name": "Yili Jiang"
                    },
                    {
                        "name": "Jiaqi Huang"
                    },
                    {
                        "name": "Mengfei Ren"
                    },
                    {
                        "name": "Jiguo Yu"
                    }
                ],
                "author_detail": {
                    "name": "Jiguo Yu"
                },
                "author": "Jiguo Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20244v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20244v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20241v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20241v1",
                "updated": "2025-03-26T05:15:26Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    5,
                    15,
                    26,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T05:15:26Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    5,
                    15,
                    26,
                    2,
                    85,
                    0
                ],
                "title": "LGR: LLM-Guided Ranking of Frontiers for Object Goal Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LGR: LLM-Guided Ranking of Frontiers for Object Goal Navigation"
                },
                "summary": "Object Goal Navigation (OGN) is a fundamental task for robots and AI, with\nkey applications such as mobile robot image databases (MRID). In particular,\nmapless OGN is essential in scenarios involving unknown or dynamic\nenvironments. This study aims to enhance recent modular mapless OGN systems by\nleveraging the commonsense reasoning capabilities of large language models\n(LLMs). Specifically, we address the challenge of determining the visiting\norder in frontier-based exploration by framing it as a frontier ranking\nproblem. Our approach is grounded in recent findings that, while LLMs cannot\ndetermine the absolute value of a frontier, they excel at evaluating the\nrelative value between multiple frontiers viewed within a single image using\nthe view image as context. We dynamically manage the frontier list by adding\nand removing elements, using an LLM as a ranking model. The ranking results are\nrepresented as reciprocal rank vectors, which are ideal for multi-view,\nmulti-query information fusion. We validate the effectiveness of our method\nthrough evaluations in Habitat-Sim.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Object Goal Navigation (OGN) is a fundamental task for robots and AI, with\nkey applications such as mobile robot image databases (MRID). In particular,\nmapless OGN is essential in scenarios involving unknown or dynamic\nenvironments. This study aims to enhance recent modular mapless OGN systems by\nleveraging the commonsense reasoning capabilities of large language models\n(LLMs). Specifically, we address the challenge of determining the visiting\norder in frontier-based exploration by framing it as a frontier ranking\nproblem. Our approach is grounded in recent findings that, while LLMs cannot\ndetermine the absolute value of a frontier, they excel at evaluating the\nrelative value between multiple frontiers viewed within a single image using\nthe view image as context. We dynamically manage the frontier list by adding\nand removing elements, using an LLM as a ranking model. The ranking results are\nrepresented as reciprocal rank vectors, which are ideal for multi-view,\nmulti-query information fusion. We validate the effectiveness of our method\nthrough evaluations in Habitat-Sim."
                },
                "authors": [
                    {
                        "name": "Mitsuaki Uno"
                    },
                    {
                        "name": "Kanji Tanaka"
                    },
                    {
                        "name": "Daiki Iwata"
                    },
                    {
                        "name": "Yudai Noda"
                    },
                    {
                        "name": "Shoya Miyazaki"
                    },
                    {
                        "name": "Kouki Terashima"
                    }
                ],
                "author_detail": {
                    "name": "Kouki Terashima"
                },
                "author": "Kouki Terashima",
                "arxiv_comment": "10 pages, 11 figures, technical report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20241v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20241v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20228v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20228v1",
                "updated": "2025-03-26T04:46:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    4,
                    46,
                    31,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T04:46:31Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    4,
                    46,
                    31,
                    2,
                    85,
                    0
                ],
                "title": "TeleLoRA: Teleporting Model-Specific Alignment Across LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TeleLoRA: Teleporting Model-Specific Alignment Across LLMs"
                },
                "summary": "Mitigating Trojans in Large Language Models (LLMs) is one of many tasks where\nalignment data is LLM specific, as different LLMs have different Trojan\ntriggers and trigger behaviors to be removed. In this paper, we introduce\nTeleLoRA (Teleporting Low-Rank Adaptation), a novel framework that synergizes\nmodel-specific alignment data across multiple LLMs to enable zero-shot Trojan\nmitigation on unseen LLMs without alignment data. TeleLoRA learns a unified\ngenerator of LoRA adapter weights by leveraging local activation information\nacross multiple LLMs. This generator is designed to be permutation symmetric to\ngeneralize across models with different architectures and sizes. We optimize\nthe model design for memory efficiency, making it feasible to learn with\nlarge-scale LLMs with minimal computational resources. Experiments on LLM\nTrojan mitigation benchmarks demonstrate that TeleLoRA effectively reduces\nattack success rates while preserving the benign performance of the models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Trojans in Large Language Models (LLMs) is one of many tasks where\nalignment data is LLM specific, as different LLMs have different Trojan\ntriggers and trigger behaviors to be removed. In this paper, we introduce\nTeleLoRA (Teleporting Low-Rank Adaptation), a novel framework that synergizes\nmodel-specific alignment data across multiple LLMs to enable zero-shot Trojan\nmitigation on unseen LLMs without alignment data. TeleLoRA learns a unified\ngenerator of LoRA adapter weights by leveraging local activation information\nacross multiple LLMs. This generator is designed to be permutation symmetric to\ngeneralize across models with different architectures and sizes. We optimize\nthe model design for memory efficiency, making it feasible to learn with\nlarge-scale LLMs with minimal computational resources. Experiments on LLM\nTrojan mitigation benchmarks demonstrate that TeleLoRA effectively reduces\nattack success rates while preserving the benign performance of the models."
                },
                "authors": [
                    {
                        "name": "Xiao Lin"
                    },
                    {
                        "name": "Manoj Acharya"
                    },
                    {
                        "name": "Anirban Roy"
                    },
                    {
                        "name": "Susmit Jha"
                    }
                ],
                "author_detail": {
                    "name": "Susmit Jha"
                },
                "author": "Susmit Jha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20228v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20228v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20226v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20226v1",
                "updated": "2025-03-26T04:42:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    4,
                    42,
                    15,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T04:42:15Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    4,
                    42,
                    15,
                    2,
                    85,
                    0
                ],
                "title": "Raising Awareness of Location Information Vulnerabilities in Social\n  Media Photos using LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Raising Awareness of Location Information Vulnerabilities in Social\n  Media Photos using LLMs"
                },
                "summary": "Location privacy leaks can lead to unauthorised tracking, identity theft, and\ntargeted attacks, compromising personal security and privacy. This study\nexplores LLM-powered location privacy leaks associated with photo sharing on\nsocial media, focusing on user awareness, attitudes, and opinions. We developed\nand introduced an LLM-powered location privacy intervention app to 19\nparticipants, who used it over a two-week period. The app prompted users to\nreflect on potential privacy leaks that a widely available LLM could easily\ndetect, such as visual landmarks & cues that could reveal their location, and\nprovided ways to conceal this information. Through in-depth interviews, we\nfound that our intervention effectively increased users' awareness of location\nprivacy and the risks posed by LLMs. It also encouraged users to consider the\nimportance of maintaining control over their privacy data and sparked\ndiscussions about the future of location privacy-preserving technologies. Based\non these insights, we offer design implications to support the development of\nfuture user-centred, location privacy-preserving technologies for social media\nphotos.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Location privacy leaks can lead to unauthorised tracking, identity theft, and\ntargeted attacks, compromising personal security and privacy. This study\nexplores LLM-powered location privacy leaks associated with photo sharing on\nsocial media, focusing on user awareness, attitudes, and opinions. We developed\nand introduced an LLM-powered location privacy intervention app to 19\nparticipants, who used it over a two-week period. The app prompted users to\nreflect on potential privacy leaks that a widely available LLM could easily\ndetect, such as visual landmarks & cues that could reveal their location, and\nprovided ways to conceal this information. Through in-depth interviews, we\nfound that our intervention effectively increased users' awareness of location\nprivacy and the risks posed by LLMs. It also encouraged users to consider the\nimportance of maintaining control over their privacy data and sparked\ndiscussions about the future of location privacy-preserving technologies. Based\non these insights, we offer design implications to support the development of\nfuture user-centred, location privacy-preserving technologies for social media\nphotos."
                },
                "authors": [
                    {
                        "name": "Ying Ma"
                    },
                    {
                        "name": "Shiquan Zhang"
                    },
                    {
                        "name": "Dongju Yang"
                    },
                    {
                        "name": "Zhanna Sarsenbayeva"
                    },
                    {
                        "name": "Jarrod Knibbe"
                    },
                    {
                        "name": "Jorge Goncalves"
                    }
                ],
                "author_detail": {
                    "name": "Jorge Goncalves"
                },
                "author": "Jorge Goncalves",
                "arxiv_doi": "10.1145/3706598.3714074",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3706598.3714074",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.20226v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20226v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published at ACM CHI 2025 Conference on Human Factors in Computing\n  Systems",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14985v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14985v2",
                "updated": "2025-03-26T04:06:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    4,
                    6,
                    37,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-19T08:31:39Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    8,
                    31,
                    39,
                    2,
                    78,
                    0
                ],
                "title": "ML-Triton, A Multi-Level Compilation and Language Extension to Triton\n  GPU Programming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ML-Triton, A Multi-Level Compilation and Language Extension to Triton\n  GPU Programming"
                },
                "summary": "In the era of LLMs, dense operations such as GEMM and MHA are critical\ncomponents. These operations are well-suited for parallel execution using a\ntilebased approach. While traditional GPU programming often relies on low level\ninterfaces like CUDA or SYCL, Triton has emerged as a DSL that offers a more\nuser-friendly and portable alternative by programming at a higher level. The\ncurrent Triton starts at the workgroup (aka threadblock) level, and directly\nlowers to per-thread level. And then attempt to coalesce and amend through a\nseries of passes, promoting information from low-level representation. We\nbelieve this is pre-mature lowering based on the below observations. 1. GPU has\na hierarchical structure both physically and logically. Modern GPUs often\nfeature SIMD units capable of directly operating on tiles on a warp or\nwarpgroup basis, such as blocked load and blocked MMA. 2. Multi-level gradual\nlowering can make compiler decoupled and clean by separating considerations\ninter and intra a logical layer. 3. Kernel developers often need fine control\nto get good performance on the latest hardware. FlashAttention2 advocates\nexplicit data partition between warps to make a performance boost. In this\ncontext, we propose ML-Triton which features multi-level compilation flow and\nprogramming interface. Our approach begins at the workgroup level and\nprogressively lowers to the warp and intrinsic level, implementing a multilevel\nlowering align with the hierarchical nature of GPU. Additionally, we extend\ntriton language to support user-set compiler hint and warp level programming,\nenabling researchers to get good out-of-the box performance without awaiting\ncompiler updates. Experimental results demonstrate that our approach achieves\nperformance above 95% of expert-written kernels on Intel GPU, as measured by\nthe geometric mean.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the era of LLMs, dense operations such as GEMM and MHA are critical\ncomponents. These operations are well-suited for parallel execution using a\ntilebased approach. While traditional GPU programming often relies on low level\ninterfaces like CUDA or SYCL, Triton has emerged as a DSL that offers a more\nuser-friendly and portable alternative by programming at a higher level. The\ncurrent Triton starts at the workgroup (aka threadblock) level, and directly\nlowers to per-thread level. And then attempt to coalesce and amend through a\nseries of passes, promoting information from low-level representation. We\nbelieve this is pre-mature lowering based on the below observations. 1. GPU has\na hierarchical structure both physically and logically. Modern GPUs often\nfeature SIMD units capable of directly operating on tiles on a warp or\nwarpgroup basis, such as blocked load and blocked MMA. 2. Multi-level gradual\nlowering can make compiler decoupled and clean by separating considerations\ninter and intra a logical layer. 3. Kernel developers often need fine control\nto get good performance on the latest hardware. FlashAttention2 advocates\nexplicit data partition between warps to make a performance boost. In this\ncontext, we propose ML-Triton which features multi-level compilation flow and\nprogramming interface. Our approach begins at the workgroup level and\nprogressively lowers to the warp and intrinsic level, implementing a multilevel\nlowering align with the hierarchical nature of GPU. Additionally, we extend\ntriton language to support user-set compiler hint and warp level programming,\nenabling researchers to get good out-of-the box performance without awaiting\ncompiler updates. Experimental results demonstrate that our approach achieves\nperformance above 95% of expert-written kernels on Intel GPU, as measured by\nthe geometric mean."
                },
                "authors": [
                    {
                        "name": "Dewei Wang"
                    },
                    {
                        "name": "Wei Zhu"
                    },
                    {
                        "name": "Liyang Ling"
                    },
                    {
                        "name": "Ettore Tiotto"
                    },
                    {
                        "name": "Quintin Wang"
                    },
                    {
                        "name": "Whitney Tsang"
                    },
                    {
                        "name": "Julian Opperman"
                    },
                    {
                        "name": "Jacky Deng"
                    }
                ],
                "author_detail": {
                    "name": "Jacky Deng"
                },
                "author": "Jacky Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14985v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14985v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20208v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20208v1",
                "updated": "2025-03-26T04:05:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    4,
                    5,
                    50,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T04:05:50Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    4,
                    5,
                    50,
                    2,
                    85,
                    0
                ],
                "title": "Learning Adaptive Dexterous Grasping from Single Demonstrations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Adaptive Dexterous Grasping from Single Demonstrations"
                },
                "summary": "How can robots learn dexterous grasping skills efficiently and apply them\nadaptively based on user instructions? This work tackles two key challenges:\nefficient skill acquisition from limited human demonstrations and\ncontext-driven skill selection. We introduce AdaDexGrasp, a framework that\nlearns a library of grasping skills from a single human demonstration per skill\nand selects the most suitable one using a vision-language model (VLM). To\nimprove sample efficiency, we propose a trajectory following reward that guides\nreinforcement learning (RL) toward states close to a human demonstration while\nallowing flexibility in exploration. To learn beyond the single demonstration,\nwe employ curriculum learning, progressively increasing object pose variations\nto enhance robustness. At deployment, a VLM retrieves the appropriate skill\nbased on user instructions, bridging low-level learned skills with high-level\nintent. We evaluate AdaDexGrasp in both simulation and real-world settings,\nshowing that our approach significantly improves RL efficiency and enables\nlearning human-like grasp strategies across varied object configurations.\nFinally, we demonstrate zero-shot transfer of our learned policies to a\nreal-world PSYONIC Ability Hand, with a 90% success rate across objects,\nsignificantly outperforming the baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How can robots learn dexterous grasping skills efficiently and apply them\nadaptively based on user instructions? This work tackles two key challenges:\nefficient skill acquisition from limited human demonstrations and\ncontext-driven skill selection. We introduce AdaDexGrasp, a framework that\nlearns a library of grasping skills from a single human demonstration per skill\nand selects the most suitable one using a vision-language model (VLM). To\nimprove sample efficiency, we propose a trajectory following reward that guides\nreinforcement learning (RL) toward states close to a human demonstration while\nallowing flexibility in exploration. To learn beyond the single demonstration,\nwe employ curriculum learning, progressively increasing object pose variations\nto enhance robustness. At deployment, a VLM retrieves the appropriate skill\nbased on user instructions, bridging low-level learned skills with high-level\nintent. We evaluate AdaDexGrasp in both simulation and real-world settings,\nshowing that our approach significantly improves RL efficiency and enables\nlearning human-like grasp strategies across varied object configurations.\nFinally, we demonstrate zero-shot transfer of our learned policies to a\nreal-world PSYONIC Ability Hand, with a 90% success rate across objects,\nsignificantly outperforming the baseline."
                },
                "authors": [
                    {
                        "name": "Liangzhi Shi"
                    },
                    {
                        "name": "Yulin Liu"
                    },
                    {
                        "name": "Lingqi Zeng"
                    },
                    {
                        "name": "Bo Ai"
                    },
                    {
                        "name": "Zhengdong Hong"
                    },
                    {
                        "name": "Hao Su"
                    }
                ],
                "author_detail": {
                    "name": "Hao Su"
                },
                "author": "Hao Su",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20208v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20208v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20202v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20202v1",
                "updated": "2025-03-26T03:55:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    3,
                    55,
                    41,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T03:55:41Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    3,
                    55,
                    41,
                    2,
                    85,
                    0
                ],
                "title": "SARGes: Semantically Aligned Reliable Gesture Generation via Intent\n  Chain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SARGes: Semantically Aligned Reliable Gesture Generation via Intent\n  Chain"
                },
                "summary": "Co-speech gesture generation enhances human-computer interaction realism\nthrough speech-synchronized gesture synthesis. However, generating semantically\nmeaningful gestures remains a challenging problem. We propose SARGes, a novel\nframework that leverages large language models (LLMs) to parse speech content\nand generate reliable semantic gesture labels, which subsequently guide the\nsynthesis of meaningful co-speech gestures.First, we constructed a\ncomprehensive co-speech gesture ethogram and developed an LLM-based intent\nchain reasoning mechanism that systematically parses and decomposes gesture\nsemantics into structured inference steps following ethogram criteria,\neffectively guiding LLMs to generate context-aware gesture labels.\nSubsequently, we constructed an intent chain-annotated text-to-gesture label\ndataset and trained a lightweight gesture label generation model, which then\nguides the generation of credible and semantically coherent co-speech gestures.\nExperimental results demonstrate that SARGes achieves highly\nsemantically-aligned gesture labeling (50.2% accuracy) with efficient\nsingle-pass inference (0.4 seconds). The proposed method provides an\ninterpretable intent reasoning pathway for semantic gesture synthesis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Co-speech gesture generation enhances human-computer interaction realism\nthrough speech-synchronized gesture synthesis. However, generating semantically\nmeaningful gestures remains a challenging problem. We propose SARGes, a novel\nframework that leverages large language models (LLMs) to parse speech content\nand generate reliable semantic gesture labels, which subsequently guide the\nsynthesis of meaningful co-speech gestures.First, we constructed a\ncomprehensive co-speech gesture ethogram and developed an LLM-based intent\nchain reasoning mechanism that systematically parses and decomposes gesture\nsemantics into structured inference steps following ethogram criteria,\neffectively guiding LLMs to generate context-aware gesture labels.\nSubsequently, we constructed an intent chain-annotated text-to-gesture label\ndataset and trained a lightweight gesture label generation model, which then\nguides the generation of credible and semantically coherent co-speech gestures.\nExperimental results demonstrate that SARGes achieves highly\nsemantically-aligned gesture labeling (50.2% accuracy) with efficient\nsingle-pass inference (0.4 seconds). The proposed method provides an\ninterpretable intent reasoning pathway for semantic gesture synthesis."
                },
                "authors": [
                    {
                        "name": "Nan Gao"
                    },
                    {
                        "name": "Yihua Bao"
                    },
                    {
                        "name": "Dongdong Weng"
                    },
                    {
                        "name": "Jiayi Zhao"
                    },
                    {
                        "name": "Jia Li"
                    },
                    {
                        "name": "Yan Zhou"
                    },
                    {
                        "name": "Pengfei Wan"
                    },
                    {
                        "name": "Di Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Di Zhang"
                },
                "author": "Di Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20202v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20202v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20201v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20201v1",
                "updated": "2025-03-26T03:51:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    3,
                    51,
                    32,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T03:51:32Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    3,
                    51,
                    32,
                    2,
                    85,
                    0
                ],
                "title": "Open Deep Search: Democratizing Search with Open-source Reasoning Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open Deep Search: Democratizing Search with Open-source Reasoning Agents"
                },
                "summary": "We introduce Open Deep Search (ODS) to close the increasing gap between the\nproprietary search AI solutions, such as Perplexity's Sonar Reasoning Pro and\nOpenAI's GPT-4o Search Preview, and their open-source counterparts. The main\ninnovation introduced in ODS is to augment the reasoning capabilities of the\nlatest open-source LLMs with reasoning agents that can judiciously use web\nsearch tools to answer queries. Concretely, ODS consists of two components that\nwork with a base LLM chosen by the user: Open Search Tool and Open Reasoning\nAgent. Open Reasoning Agent interprets the given task and completes it by\norchestrating a sequence of actions that includes calling tools, one of which\nis the Open Search Tool. Open Search Tool is a novel web search tool that\noutperforms proprietary counterparts. Together with powerful open-source\nreasoning LLMs, such as DeepSeek-R1, ODS nearly matches and sometimes surpasses\nthe existing state-of-the-art baselines on two benchmarks: SimpleQA and FRAMES.\nFor example, on the FRAMES evaluation benchmark, ODS improves the best existing\nbaseline of the recently released GPT-4o Search Preview by 9.7% in accuracy.\nODS is a general framework for seamlessly augmenting any LLMs -- for example,\nDeepSeek-R1 that achieves 82.4% on SimpleQA and 30.1% on FRAMES -- with search\nand reasoning capabilities to achieve state-of-the-art performance: 88.3% on\nSimpleQA and 75.3% on FRAMES.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Open Deep Search (ODS) to close the increasing gap between the\nproprietary search AI solutions, such as Perplexity's Sonar Reasoning Pro and\nOpenAI's GPT-4o Search Preview, and their open-source counterparts. The main\ninnovation introduced in ODS is to augment the reasoning capabilities of the\nlatest open-source LLMs with reasoning agents that can judiciously use web\nsearch tools to answer queries. Concretely, ODS consists of two components that\nwork with a base LLM chosen by the user: Open Search Tool and Open Reasoning\nAgent. Open Reasoning Agent interprets the given task and completes it by\norchestrating a sequence of actions that includes calling tools, one of which\nis the Open Search Tool. Open Search Tool is a novel web search tool that\noutperforms proprietary counterparts. Together with powerful open-source\nreasoning LLMs, such as DeepSeek-R1, ODS nearly matches and sometimes surpasses\nthe existing state-of-the-art baselines on two benchmarks: SimpleQA and FRAMES.\nFor example, on the FRAMES evaluation benchmark, ODS improves the best existing\nbaseline of the recently released GPT-4o Search Preview by 9.7% in accuracy.\nODS is a general framework for seamlessly augmenting any LLMs -- for example,\nDeepSeek-R1 that achieves 82.4% on SimpleQA and 30.1% on FRAMES -- with search\nand reasoning capabilities to achieve state-of-the-art performance: 88.3% on\nSimpleQA and 75.3% on FRAMES."
                },
                "authors": [
                    {
                        "name": "Salaheddin Alzubi"
                    },
                    {
                        "name": "Creston Brooks"
                    },
                    {
                        "name": "Purva Chiniya"
                    },
                    {
                        "name": "Edoardo Contente"
                    },
                    {
                        "name": "Chiara von Gerlach"
                    },
                    {
                        "name": "Lucas Irwin"
                    },
                    {
                        "name": "Yihan Jiang"
                    },
                    {
                        "name": "Arda Kaz"
                    },
                    {
                        "name": "Windsor Nguyen"
                    },
                    {
                        "name": "Sewoong Oh"
                    },
                    {
                        "name": "Himanshu Tyagi"
                    },
                    {
                        "name": "Pramod Viswanath"
                    }
                ],
                "author_detail": {
                    "name": "Pramod Viswanath"
                },
                "author": "Pramod Viswanath",
                "arxiv_comment": "27 pages, 8 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20201v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20201v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19404v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19404v2",
                "updated": "2025-03-26T03:46:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    3,
                    46,
                    53,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-25T07:24:27Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    7,
                    24,
                    27,
                    1,
                    84,
                    0
                ],
                "title": "LangBridge: Interpreting Image as a Combination of Language Embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LangBridge: Interpreting Image as a Combination of Language Embeddings"
                },
                "summary": "Recent years have witnessed remarkable advances in Large Vision-Language\nModels (LVLMs), which have achieved human-level performance across various\ncomplex vision-language tasks. Following LLaVA's paradigm, mainstream LVLMs\ntypically employ a shallow MLP for visual-language alignment through a\ntwo-stage training process: pretraining for cross-modal alignment followed by\ninstruction tuning. While this approach has proven effective, the underlying\nmechanisms of how MLPs bridge the modality gap remain poorly understood.\nAlthough some research has explored how LLMs process transformed visual tokens,\nfew studies have investigated the fundamental alignment mechanism. Furthermore,\nthe MLP adapter requires retraining whenever switching LLM backbones. To\naddress these limitations, we first investigate the working principles of MLP\nadapters and discover that they learn to project visual embeddings into\nsubspaces spanned by corresponding text embeddings progressively. Based on this\ninsight, we propose LangBridge, a novel adapter that explicitly maps visual\ntokens to linear combinations of LLM vocabulary embeddings. This innovative\ndesign enables pretraining-free adapter transfer across different LLMs while\nmaintaining performance. Our experimental results demonstrate that a LangBridge\nadapter pre-trained on Qwen2-0.5B can be directly applied to larger models such\nas LLaMA3-8B or Qwen2.5-14B while maintaining competitive performance. Overall,\nLangBridge enables interpretable vision-language alignment by grounding visual\nrepresentations in LLM vocab embedding, while its plug-and-play design ensures\nefficient reuse across multiple LLMs with nearly no performance degradation.\nSee our project page at https://jiaqiliao77.github.io/LangBridge.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent years have witnessed remarkable advances in Large Vision-Language\nModels (LVLMs), which have achieved human-level performance across various\ncomplex vision-language tasks. Following LLaVA's paradigm, mainstream LVLMs\ntypically employ a shallow MLP for visual-language alignment through a\ntwo-stage training process: pretraining for cross-modal alignment followed by\ninstruction tuning. While this approach has proven effective, the underlying\nmechanisms of how MLPs bridge the modality gap remain poorly understood.\nAlthough some research has explored how LLMs process transformed visual tokens,\nfew studies have investigated the fundamental alignment mechanism. Furthermore,\nthe MLP adapter requires retraining whenever switching LLM backbones. To\naddress these limitations, we first investigate the working principles of MLP\nadapters and discover that they learn to project visual embeddings into\nsubspaces spanned by corresponding text embeddings progressively. Based on this\ninsight, we propose LangBridge, a novel adapter that explicitly maps visual\ntokens to linear combinations of LLM vocabulary embeddings. This innovative\ndesign enables pretraining-free adapter transfer across different LLMs while\nmaintaining performance. Our experimental results demonstrate that a LangBridge\nadapter pre-trained on Qwen2-0.5B can be directly applied to larger models such\nas LLaMA3-8B or Qwen2.5-14B while maintaining competitive performance. Overall,\nLangBridge enables interpretable vision-language alignment by grounding visual\nrepresentations in LLM vocab embedding, while its plug-and-play design ensures\nefficient reuse across multiple LLMs with nearly no performance degradation.\nSee our project page at https://jiaqiliao77.github.io/LangBridge.github.io/"
                },
                "authors": [
                    {
                        "name": "Jiaqi Liao"
                    },
                    {
                        "name": "Yuwei Niu"
                    },
                    {
                        "name": "Fanqing Meng"
                    },
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Changyao Tian"
                    },
                    {
                        "name": "Yinuo Du"
                    },
                    {
                        "name": "Yuwen Xiong"
                    },
                    {
                        "name": "Dianqi Li"
                    },
                    {
                        "name": "Xizhou Zhu"
                    },
                    {
                        "name": "Li Yuan"
                    },
                    {
                        "name": "Jifeng Dai"
                    },
                    {
                        "name": "Yu Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Yu Cheng"
                },
                "author": "Yu Cheng",
                "arxiv_comment": "The code and weights will be open-sourced. Project page:\n  https://jiaqiliao77.github.io/LangBridge.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19404v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19404v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20197v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20197v1",
                "updated": "2025-03-26T03:44:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    3,
                    44,
                    3,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T03:44:03Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    3,
                    44,
                    3,
                    2,
                    85,
                    0
                ],
                "title": "Enhancing the Robustness of LLM-Generated Code: Empirical Study and\n  Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing the Robustness of LLM-Generated Code: Empirical Study and\n  Framework"
                },
                "summary": "Ensuring the robustness of code generated by large language models (LLMs) is\ncrucial for real-world reliability. However, existing evaluations predominantly\nfocus on correctness, often neglecting key robustness concerns such as missing\ninput validation and insufficient error handling. In this paper, we present the\nfirst empirical study on the robustness of LLM-generated code. We introduce\nnovel robustness metrics and analyze four state-of-the-art code LLMs, revealing\nthat, on average, 43.1% of their generated code is less robust than\nhuman-written counterparts. Notably, over 90% of robustness deficiencies stem\nfrom missing conditional checks, with 70% of these omissions occurring in the\nfirst line of code. Additionally, in 69% of cases where a conditional statement\nis necessary but absent, the \"if\" token still ranks third or higher in the\nmodel's predicted token probabilities, indicating an implicit recognition of\ncontrol structures. Building on these findings, we propose RobGen, a framework\ndesigned to enhance code robustness without requiring model retraining. RobGen\nleverages two model-agnostic techniques: RobGen-Adj, which dynamically adjusts\ntoken probabilities during decoding to encourage the inclusion of control\nstructures, and RobGen-Ins, which improves generated code by inserting missing\nconditionals after generation. Experimental results demonstrate that RobGen\nreduces the proportion of less robust model-generated code by 20.0%,\nsignificantly enhancing code reliability across diverse tasks. As a lightweight\nand adaptable solution, RobGen effectively mitigates robustness challenges in\nLLM-generated code. All code and data are available at\nhttps://github.com/SYSUSELab/RobGen.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring the robustness of code generated by large language models (LLMs) is\ncrucial for real-world reliability. However, existing evaluations predominantly\nfocus on correctness, often neglecting key robustness concerns such as missing\ninput validation and insufficient error handling. In this paper, we present the\nfirst empirical study on the robustness of LLM-generated code. We introduce\nnovel robustness metrics and analyze four state-of-the-art code LLMs, revealing\nthat, on average, 43.1% of their generated code is less robust than\nhuman-written counterparts. Notably, over 90% of robustness deficiencies stem\nfrom missing conditional checks, with 70% of these omissions occurring in the\nfirst line of code. Additionally, in 69% of cases where a conditional statement\nis necessary but absent, the \"if\" token still ranks third or higher in the\nmodel's predicted token probabilities, indicating an implicit recognition of\ncontrol structures. Building on these findings, we propose RobGen, a framework\ndesigned to enhance code robustness without requiring model retraining. RobGen\nleverages two model-agnostic techniques: RobGen-Adj, which dynamically adjusts\ntoken probabilities during decoding to encourage the inclusion of control\nstructures, and RobGen-Ins, which improves generated code by inserting missing\nconditionals after generation. Experimental results demonstrate that RobGen\nreduces the proportion of less robust model-generated code by 20.0%,\nsignificantly enhancing code reliability across diverse tasks. As a lightweight\nand adaptable solution, RobGen effectively mitigates robustness challenges in\nLLM-generated code. All code and data are available at\nhttps://github.com/SYSUSELab/RobGen."
                },
                "authors": [
                    {
                        "name": "ZiKe Li"
                    },
                    {
                        "name": "MingWei Liu"
                    },
                    {
                        "name": "Anji Li"
                    },
                    {
                        "name": "Kaifeng He"
                    },
                    {
                        "name": "Yanlin Wang"
                    },
                    {
                        "name": "Xin Peng"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20197v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20197v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20196v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20196v1",
                "updated": "2025-03-26T03:40:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    3,
                    40,
                    45,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T03:40:45Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    3,
                    40,
                    45,
                    2,
                    85,
                    0
                ],
                "title": "Underwater Willis lens for broadband low-frequency focusing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Underwater Willis lens for broadband low-frequency focusing"
                },
                "summary": "Broadband underwater sound focusing in the low-frequency range is essential\nfor various applications such as battery-free environmental monitoring and\nsensing. However, achieving low-frequency underwater focusing typically\nnecessitates bulky, heavy structures that hinder practical deployment. Here, we\nintroduce a three-dimensional underwater lens comprising cavity-based locally\nresonant asymmetric structures, enabling the efficient manipulation of\nlow-frequency waterborne sound through a densely packed lattice configuration.\nWe experimentally validated its broadband focusing performance over a range of\n20-35 kHz. In addition, we observed that our lens exhibits asymmetric\nbackscattering-a distinctive effect arising from its bianisotropic nature-which\nwe term the Willis lens. Unlike conventional underwater lenses that rely on\nfully filled structures, our design employs cavity-based scatterers, achieving\na lighter yet robust focusing performance. With its lightweight, efficient, and\nreliable design, the Willis lens provides a promising platform for underwater\nsensor networks and future advancements in on-demand waterborne sound focusing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Broadband underwater sound focusing in the low-frequency range is essential\nfor various applications such as battery-free environmental monitoring and\nsensing. However, achieving low-frequency underwater focusing typically\nnecessitates bulky, heavy structures that hinder practical deployment. Here, we\nintroduce a three-dimensional underwater lens comprising cavity-based locally\nresonant asymmetric structures, enabling the efficient manipulation of\nlow-frequency waterborne sound through a densely packed lattice configuration.\nWe experimentally validated its broadband focusing performance over a range of\n20-35 kHz. In addition, we observed that our lens exhibits asymmetric\nbackscattering-a distinctive effect arising from its bianisotropic nature-which\nwe term the Willis lens. Unlike conventional underwater lenses that rely on\nfully filled structures, our design employs cavity-based scatterers, achieving\na lighter yet robust focusing performance. With its lightweight, efficient, and\nreliable design, the Willis lens provides a promising platform for underwater\nsensor networks and future advancements in on-demand waterborne sound focusing."
                },
                "authors": [
                    {
                        "name": "Beomseok Oh"
                    },
                    {
                        "name": "Dongwoo Lee"
                    },
                    {
                        "name": "Yeon-Seong Choo"
                    },
                    {
                        "name": "Sung-Hoon Byun"
                    },
                    {
                        "name": "Jehyeon Shin"
                    },
                    {
                        "name": "Sea-Moon Kim"
                    },
                    {
                        "name": "Junsuk Rho"
                    }
                ],
                "author_detail": {
                    "name": "Junsuk Rho"
                },
                "author": "Junsuk Rho",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20196v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20196v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mes-hall",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20194v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20194v1",
                "updated": "2025-03-26T03:37:52Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    3,
                    37,
                    52,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T03:37:52Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    3,
                    37,
                    52,
                    2,
                    85,
                    0
                ],
                "title": "GAPO: Learning Preferential Prompt through Generative Adversarial Policy\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GAPO: Learning Preferential Prompt through Generative Adversarial Policy\n  Optimization"
                },
                "summary": "Recent advances in large language models have highlighted the critical need\nfor precise control over model outputs through predefined constraints. While\nexisting methods attempt to achieve this through either direct\ninstruction-response synthesis or preferential response optimization, they\noften struggle with constraint understanding and adaptation. This limitation\nbecomes particularly evident when handling fine-grained constraints, leading to\neither hallucination or brittle performance. We introduce Generative\nAdversarial Policy Optimization (GAPO), a novel framework that combines\nGAN-based training dynamics with an encoder-only reward model to progressively\nlearn and adapt to increasingly complex constraints. GAPO leverages adversarial\ntraining to automatically generate training samples of varying difficulty while\nutilizing the encoder-only architecture to better capture prompt-response\nrelationships. Extensive experiments demonstrate GAPO's superior performance\nacross multiple benchmarks, particularly in scenarios requiring fine-grained\nconstraint handling, where it significantly outperforms existing methods like\nPPO, DPO, and KTO. Our results suggest that GAPO's unique approach to\npreferential prompt learning offers a more robust and effective solution for\ncontrolling LLM outputs. Code is avaliable in\nhttps://github.com/MikeGu721/GAPO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models have highlighted the critical need\nfor precise control over model outputs through predefined constraints. While\nexisting methods attempt to achieve this through either direct\ninstruction-response synthesis or preferential response optimization, they\noften struggle with constraint understanding and adaptation. This limitation\nbecomes particularly evident when handling fine-grained constraints, leading to\neither hallucination or brittle performance. We introduce Generative\nAdversarial Policy Optimization (GAPO), a novel framework that combines\nGAN-based training dynamics with an encoder-only reward model to progressively\nlearn and adapt to increasingly complex constraints. GAPO leverages adversarial\ntraining to automatically generate training samples of varying difficulty while\nutilizing the encoder-only architecture to better capture prompt-response\nrelationships. Extensive experiments demonstrate GAPO's superior performance\nacross multiple benchmarks, particularly in scenarios requiring fine-grained\nconstraint handling, where it significantly outperforms existing methods like\nPPO, DPO, and KTO. Our results suggest that GAPO's unique approach to\npreferential prompt learning offers a more robust and effective solution for\ncontrolling LLM outputs. Code is avaliable in\nhttps://github.com/MikeGu721/GAPO."
                },
                "authors": [
                    {
                        "name": "Zhouhong Gu"
                    },
                    {
                        "name": "Xingzhou Chen"
                    },
                    {
                        "name": "Xiaoran Shi"
                    },
                    {
                        "name": "Tao Wang"
                    },
                    {
                        "name": "Suhang Zheng"
                    },
                    {
                        "name": "Tianyu Li"
                    },
                    {
                        "name": "Hongwei Feng"
                    },
                    {
                        "name": "Yanghua Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Yanghua Xiao"
                },
                "author": "Yanghua Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20194v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20194v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20191v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20191v1",
                "updated": "2025-03-26T03:33:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    3,
                    33,
                    1,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T03:33:01Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    3,
                    33,
                    1,
                    2,
                    85,
                    0
                ],
                "title": "Maya: Optimizing Deep Learning Training Workloads using Emulated Virtual\n  Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Maya: Optimizing Deep Learning Training Workloads using Emulated Virtual\n  Accelerators"
                },
                "summary": "Training large foundation models costs hundreds of millions of dollars,\nmaking deployment optimization critical. Current approaches require machine\nlearning engineers to manually craft training recipes through error-prone\ntrial-and-error on expensive compute clusters. To enable efficient exploration\nof training configurations, researchers have developed performance modeling\nsystems. However, these systems force users to translate their workloads into\ncustom specification languages, introducing a fundamental semantic gap between\nthe actual workload and its representation. This gap creates an inherent\ntradeoff: systems must either support a narrow set of workloads to maintain\nusability, require complex specifications that limit practical adoption, or\ncompromise prediction accuracy with simplified models.\n  We present Maya, a performance modeling system that eliminates these\ntradeoffs through transparent device emulation. By operating at the narrow\ninterface between training frameworks and accelerator devices, Maya can capture\ncomplete workload behavior without requiring code modifications or\ntranslations. Maya intercepts device API calls from unmodified training code to\ndirectly observe low-level operations, enabling accurate performance prediction\nwhile maintaining both ease of use and generality. Our evaluation shows Maya\nachieves less than 5% prediction error across diverse models and optimization\nstrategies, identifying configurations that reduce training costs by up to 56%\ncompared to existing approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training large foundation models costs hundreds of millions of dollars,\nmaking deployment optimization critical. Current approaches require machine\nlearning engineers to manually craft training recipes through error-prone\ntrial-and-error on expensive compute clusters. To enable efficient exploration\nof training configurations, researchers have developed performance modeling\nsystems. However, these systems force users to translate their workloads into\ncustom specification languages, introducing a fundamental semantic gap between\nthe actual workload and its representation. This gap creates an inherent\ntradeoff: systems must either support a narrow set of workloads to maintain\nusability, require complex specifications that limit practical adoption, or\ncompromise prediction accuracy with simplified models.\n  We present Maya, a performance modeling system that eliminates these\ntradeoffs through transparent device emulation. By operating at the narrow\ninterface between training frameworks and accelerator devices, Maya can capture\ncomplete workload behavior without requiring code modifications or\ntranslations. Maya intercepts device API calls from unmodified training code to\ndirectly observe low-level operations, enabling accurate performance prediction\nwhile maintaining both ease of use and generality. Our evaluation shows Maya\nachieves less than 5% prediction error across diverse models and optimization\nstrategies, identifying configurations that reduce training costs by up to 56%\ncompared to existing approaches."
                },
                "authors": [
                    {
                        "name": "Srihas Yarlagadda"
                    },
                    {
                        "name": "Amey Agrawal"
                    },
                    {
                        "name": "Elton Pinto"
                    },
                    {
                        "name": "Hakesh Darapaneni"
                    },
                    {
                        "name": "Mitali Meratwal"
                    },
                    {
                        "name": "Shivam Mittal"
                    },
                    {
                        "name": "Pranavi Bajjuri"
                    },
                    {
                        "name": "Srinivas Sridharan"
                    },
                    {
                        "name": "Alexey Tumanov"
                    }
                ],
                "author_detail": {
                    "name": "Alexey Tumanov"
                },
                "author": "Alexey Tumanov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20191v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20191v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20190v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20190v1",
                "updated": "2025-03-26T03:31:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    3,
                    31,
                    7,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T03:31:07Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    3,
                    31,
                    7,
                    2,
                    85,
                    0
                ],
                "title": "Cross-Modal Prototype Allocation: Unsupervised Slide Representation\n  Learning via Patch-Text Contrast in Computational Pathology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Modal Prototype Allocation: Unsupervised Slide Representation\n  Learning via Patch-Text Contrast in Computational Pathology"
                },
                "summary": "With the rapid advancement of pathology foundation models (FMs), the\nrepresentation learning of whole slide images (WSIs) attracts increasing\nattention. Existing studies develop high-quality patch feature extractors and\nemploy carefully designed aggregation schemes to derive slide-level\nrepresentations. However, mainstream weakly supervised slide representation\nlearning methods, primarily based on multiple instance learning (MIL), are\ntailored to specific downstream tasks, which limits their generalizability. To\naddress this issue, some studies explore unsupervised slide representation\nlearning. However, these approaches focus solely on the visual modality of\npatches, neglecting the rich semantic information embedded in textual data. In\nthis work, we propose ProAlign, a cross-modal unsupervised slide representation\nlearning framework. Specifically, we leverage a large language model (LLM) to\ngenerate descriptive text for the prototype types present in a WSI, introducing\npatch-text contrast to construct initial prototype embeddings. Furthermore, we\npropose a parameter-free attention aggregation strategy that utilizes the\nsimilarity between patches and these prototypes to form unsupervised slide\nembeddings applicable to a wide range of downstream tasks. Extensive\nexperiments on four public datasets show that ProAlign outperforms existing\nunsupervised frameworks and achieves performance comparable to some weakly\nsupervised models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid advancement of pathology foundation models (FMs), the\nrepresentation learning of whole slide images (WSIs) attracts increasing\nattention. Existing studies develop high-quality patch feature extractors and\nemploy carefully designed aggregation schemes to derive slide-level\nrepresentations. However, mainstream weakly supervised slide representation\nlearning methods, primarily based on multiple instance learning (MIL), are\ntailored to specific downstream tasks, which limits their generalizability. To\naddress this issue, some studies explore unsupervised slide representation\nlearning. However, these approaches focus solely on the visual modality of\npatches, neglecting the rich semantic information embedded in textual data. In\nthis work, we propose ProAlign, a cross-modal unsupervised slide representation\nlearning framework. Specifically, we leverage a large language model (LLM) to\ngenerate descriptive text for the prototype types present in a WSI, introducing\npatch-text contrast to construct initial prototype embeddings. Furthermore, we\npropose a parameter-free attention aggregation strategy that utilizes the\nsimilarity between patches and these prototypes to form unsupervised slide\nembeddings applicable to a wide range of downstream tasks. Extensive\nexperiments on four public datasets show that ProAlign outperforms existing\nunsupervised frameworks and achieves performance comparable to some weakly\nsupervised models."
                },
                "authors": [
                    {
                        "name": "Yuxuan Chen"
                    },
                    {
                        "name": "Jiawen Li"
                    },
                    {
                        "name": "Jiali Hu"
                    },
                    {
                        "name": "Xitong Ling"
                    },
                    {
                        "name": "Tian Guan"
                    },
                    {
                        "name": "Anjia Han"
                    },
                    {
                        "name": "Yonghong He"
                    }
                ],
                "author_detail": {
                    "name": "Yonghong He"
                },
                "author": "Yonghong He",
                "arxiv_comment": "11pages,3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20190v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20190v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20188v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20188v1",
                "updated": "2025-03-26T03:28:46Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    3,
                    28,
                    46,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T03:28:46Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    3,
                    28,
                    46,
                    2,
                    85,
                    0
                ],
                "title": "Rethinking Vision-Language Model in Face Forensics: Multi-Modal\n  Interpretable Forged Face Detector",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Vision-Language Model in Face Forensics: Multi-Modal\n  Interpretable Forged Face Detector"
                },
                "summary": "Deepfake detection is a long-established research topic vital for mitigating\nthe spread of malicious misinformation. Unlike prior methods that provide\neither binary classification results or textual explanations separately, we\nintroduce a novel method capable of generating both simultaneously. Our method\nharnesses the multi-modal learning capability of the pre-trained CLIP and the\nunprecedented interpretability of large language models (LLMs) to enhance both\nthe generalization and explainability of deepfake detection. Specifically, we\nintroduce a multi-modal face forgery detector (M2F2-Det) that employs tailored\nface forgery prompt learning, incorporating the pre-trained CLIP to improve\ngeneralization to unseen forgeries. Also, M2F2-Det incorporates an LLM to\nprovide detailed textual explanations of its detection decisions, enhancing\ninterpretability by bridging the gap between natural language and subtle cues\nof facial forgeries. Empirically, we evaluate M2F2-Det on both detection and\nexplanation generation tasks, where it achieves state-of-the-art performance,\ndemonstrating its effectiveness in identifying and explaining diverse\nforgeries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deepfake detection is a long-established research topic vital for mitigating\nthe spread of malicious misinformation. Unlike prior methods that provide\neither binary classification results or textual explanations separately, we\nintroduce a novel method capable of generating both simultaneously. Our method\nharnesses the multi-modal learning capability of the pre-trained CLIP and the\nunprecedented interpretability of large language models (LLMs) to enhance both\nthe generalization and explainability of deepfake detection. Specifically, we\nintroduce a multi-modal face forgery detector (M2F2-Det) that employs tailored\nface forgery prompt learning, incorporating the pre-trained CLIP to improve\ngeneralization to unseen forgeries. Also, M2F2-Det incorporates an LLM to\nprovide detailed textual explanations of its detection decisions, enhancing\ninterpretability by bridging the gap between natural language and subtle cues\nof facial forgeries. Empirically, we evaluate M2F2-Det on both detection and\nexplanation generation tasks, where it achieves state-of-the-art performance,\ndemonstrating its effectiveness in identifying and explaining diverse\nforgeries."
                },
                "authors": [
                    {
                        "name": "Xiao Guo"
                    },
                    {
                        "name": "Xiufeng Song"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Xiaohong Liu"
                    },
                    {
                        "name": "Xiaoming Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoming Liu"
                },
                "author": "Xiaoming Liu",
                "arxiv_comment": "8 figures; 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20188v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20188v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.01422v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.01422v2",
                "updated": "2025-03-26T03:26:09Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    3,
                    26,
                    9,
                    2,
                    85,
                    0
                ],
                "published": "2024-06-03T15:20:06Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    15,
                    20,
                    6,
                    0,
                    155,
                    0
                ],
                "title": "Alibaba LingmaAgent: Improving Automated Issue Resolution via\n  Comprehensive Repository Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alibaba LingmaAgent: Improving Automated Issue Resolution via\n  Comprehensive Repository Exploration"
                },
                "summary": "This paper presents Alibaba LingmaAgent, a novel Automated Software\nEngineering method designed to comprehensively understand and utilize whole\nsoftware repositories for issue resolution. Deployed in TONGYI Lingma, an\nIDE-based coding assistant developed by Alibaba Cloud, LingmaAgent addresses\nthe limitations of existing LLM-based agents that primarily focus on local code\ninformation. Our approach introduces a top-down method to condense critical\nrepository information into a knowledge graph, reducing complexity, and employs\na Monte Carlo tree search based strategy enabling agents to explore and\nunderstand entire repositories. We guide agents to summarize, analyze, and plan\nusing repository-level knowledge, allowing them to dynamically acquire\ninformation and generate patches for real-world GitHub issues. In extensive\nexperiments, LingmaAgent demonstrated significant improvements, achieving an\n18.5\\% relative improvement on the SWE-bench Lite benchmark compared to\nSWE-agent. In production deployment and evaluation at Alibaba Cloud,\nLingmaAgent automatically resolved 16.9\\% of in-house issues faced by\ndevelopment engineers, and solved 43.3\\% of problems after manual intervention.\nAdditionally, we have open-sourced a Python prototype of LingmaAgent for\nreference by other industrial developers\nhttps://github.com/RepoUnderstander/RepoUnderstander. In fact, LingmaAgent has\nbeen used as a developed reference by many subsequently agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents Alibaba LingmaAgent, a novel Automated Software\nEngineering method designed to comprehensively understand and utilize whole\nsoftware repositories for issue resolution. Deployed in TONGYI Lingma, an\nIDE-based coding assistant developed by Alibaba Cloud, LingmaAgent addresses\nthe limitations of existing LLM-based agents that primarily focus on local code\ninformation. Our approach introduces a top-down method to condense critical\nrepository information into a knowledge graph, reducing complexity, and employs\na Monte Carlo tree search based strategy enabling agents to explore and\nunderstand entire repositories. We guide agents to summarize, analyze, and plan\nusing repository-level knowledge, allowing them to dynamically acquire\ninformation and generate patches for real-world GitHub issues. In extensive\nexperiments, LingmaAgent demonstrated significant improvements, achieving an\n18.5\\% relative improvement on the SWE-bench Lite benchmark compared to\nSWE-agent. In production deployment and evaluation at Alibaba Cloud,\nLingmaAgent automatically resolved 16.9\\% of in-house issues faced by\ndevelopment engineers, and solved 43.3\\% of problems after manual intervention.\nAdditionally, we have open-sourced a Python prototype of LingmaAgent for\nreference by other industrial developers\nhttps://github.com/RepoUnderstander/RepoUnderstander. In fact, LingmaAgent has\nbeen used as a developed reference by many subsequently agents."
                },
                "authors": [
                    {
                        "name": "Yingwei Ma"
                    },
                    {
                        "name": "Qingping Yang"
                    },
                    {
                        "name": "Rongyu Cao"
                    },
                    {
                        "name": "Binhua Li"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Yongbin Li"
                    }
                ],
                "author_detail": {
                    "name": "Yongbin Li"
                },
                "author": "Yongbin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.01422v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.01422v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20182v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20182v1",
                "updated": "2025-03-26T03:14:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    3,
                    14,
                    31,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T03:14:31Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    3,
                    14,
                    31,
                    2,
                    85,
                    0
                ],
                "title": "Leveraging Implicit Sentiments: Enhancing Reliability and Validity in\n  Psychological Trait Evaluation of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Implicit Sentiments: Enhancing Reliability and Validity in\n  Psychological Trait Evaluation of LLMs"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have led to their\nincreasing integration into human life. With the transition from mere tools to\nhuman-like assistants, understanding their psychological aspects-such as\nemotional tendencies and personalities-becomes essential for ensuring their\ntrustworthiness. However, current psychological evaluations of LLMs, often\nbased on human psychological assessments like the BFI, face significant\nlimitations. The results from these approaches often lack reliability and have\nlimited validity when predicting LLM behavior in real-world scenarios. In this\nwork, we introduce a novel evaluation instrument specifically designed for\nLLMs, called Core Sentiment Inventory (CSI). CSI is a bilingual tool, covering\nboth English and Chinese, that implicitly evaluates models' sentiment\ntendencies, providing an insightful psychological portrait of LLM across three\ndimensions: optimism, pessimism, and neutrality. Through extensive experiments,\nwe demonstrate that: 1) CSI effectively captures nuanced emotional patterns,\nrevealing significant variation in LLMs across languages and contexts; 2)\nCompared to current approaches, CSI significantly improves reliability,\nyielding more consistent results; and 3) The correlation between CSI scores and\nthe sentiment of LLM's real-world outputs exceeds 0.85, demonstrating its\nstrong validity in predicting LLM behavior. We make CSI public available via:\nhttps://github.com/dependentsign/CSI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have led to their\nincreasing integration into human life. With the transition from mere tools to\nhuman-like assistants, understanding their psychological aspects-such as\nemotional tendencies and personalities-becomes essential for ensuring their\ntrustworthiness. However, current psychological evaluations of LLMs, often\nbased on human psychological assessments like the BFI, face significant\nlimitations. The results from these approaches often lack reliability and have\nlimited validity when predicting LLM behavior in real-world scenarios. In this\nwork, we introduce a novel evaluation instrument specifically designed for\nLLMs, called Core Sentiment Inventory (CSI). CSI is a bilingual tool, covering\nboth English and Chinese, that implicitly evaluates models' sentiment\ntendencies, providing an insightful psychological portrait of LLM across three\ndimensions: optimism, pessimism, and neutrality. Through extensive experiments,\nwe demonstrate that: 1) CSI effectively captures nuanced emotional patterns,\nrevealing significant variation in LLMs across languages and contexts; 2)\nCompared to current approaches, CSI significantly improves reliability,\nyielding more consistent results; and 3) The correlation between CSI scores and\nthe sentiment of LLM's real-world outputs exceeds 0.85, demonstrating its\nstrong validity in predicting LLM behavior. We make CSI public available via:\nhttps://github.com/dependentsign/CSI."
                },
                "authors": [
                    {
                        "name": "Huanhuan Ma"
                    },
                    {
                        "name": "Haisong Gong"
                    },
                    {
                        "name": "Xiaoyuan Yi"
                    },
                    {
                        "name": "Xing Xie"
                    },
                    {
                        "name": "Dongkuan Xu"
                    }
                ],
                "author_detail": {
                    "name": "Dongkuan Xu"
                },
                "author": "Dongkuan Xu",
                "arxiv_comment": "Code available via https://github.com/dependentsign/CSI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20182v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20182v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09078v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09078v4",
                "updated": "2025-03-26T02:56:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    2,
                    56,
                    45,
                    2,
                    85,
                    0
                ],
                "published": "2024-12-12T09:01:18Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    9,
                    1,
                    18,
                    3,
                    347,
                    0
                ],
                "title": "Forest-of-Thought: Scaling Test-Time Compute for Enhancing LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forest-of-Thought: Scaling Test-Time Compute for Enhancing LLM Reasoning"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable abilities across\nvarious language tasks, but solving complex reasoning problems remains a\nsignificant challenge. While existing methods, such as Chain-of-Thought (CoT)\nand Tree-of-Thought (ToT), enhance reasoning by decomposing problems or\nstructuring prompts, they typically perform a single pass of reasoning and may\nfail to revisit flawed paths, compromising accuracy. To address this\nlimitation, we propose a novel reasoning framework called Forest-of-Thought\n(FoT), which integrates multiple reasoning trees to leverage collective\ndecision-making for solving complex logical problems. FoT employs sparse\nactivation strategies to select the most relevant reasoning paths, improving\nboth efficiency and accuracy. Additionally, we introduce a dynamic\nself-correction strategy that enables real-time error correction, along with\nconsensus-guided decision-making strategies to optimize both correctness and\ncomputational resources. Experimental results demonstrate that the FoT\nframework, combined with these strategies, significantly enhances the reasoning\ncapabilities of LLMs, enabling them to solve complex tasks with greater\nprecision and efficiency. Code will be available at\nhttps://github.com/iamhankai/Forest-of-Thought.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable abilities across\nvarious language tasks, but solving complex reasoning problems remains a\nsignificant challenge. While existing methods, such as Chain-of-Thought (CoT)\nand Tree-of-Thought (ToT), enhance reasoning by decomposing problems or\nstructuring prompts, they typically perform a single pass of reasoning and may\nfail to revisit flawed paths, compromising accuracy. To address this\nlimitation, we propose a novel reasoning framework called Forest-of-Thought\n(FoT), which integrates multiple reasoning trees to leverage collective\ndecision-making for solving complex logical problems. FoT employs sparse\nactivation strategies to select the most relevant reasoning paths, improving\nboth efficiency and accuracy. Additionally, we introduce a dynamic\nself-correction strategy that enables real-time error correction, along with\nconsensus-guided decision-making strategies to optimize both correctness and\ncomputational resources. Experimental results demonstrate that the FoT\nframework, combined with these strategies, significantly enhances the reasoning\ncapabilities of LLMs, enabling them to solve complex tasks with greater\nprecision and efficiency. Code will be available at\nhttps://github.com/iamhankai/Forest-of-Thought."
                },
                "authors": [
                    {
                        "name": "Zhenni Bi"
                    },
                    {
                        "name": "Kai Han"
                    },
                    {
                        "name": "Chuanjian Liu"
                    },
                    {
                        "name": "Yehui Tang"
                    },
                    {
                        "name": "Yunhe Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yunhe Wang"
                },
                "author": "Yunhe Wang",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09078v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09078v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17264v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17264v2",
                "updated": "2025-03-26T01:58:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    1,
                    58,
                    40,
                    2,
                    85,
                    0
                ],
                "published": "2024-09-25T18:21:05Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    18,
                    21,
                    5,
                    2,
                    269,
                    0
                ],
                "title": "Medha: Efficiently Serving Multi-Million Context Length LLM Inference\n  Requests Without Approximations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medha: Efficiently Serving Multi-Million Context Length LLM Inference\n  Requests Without Approximations"
                },
                "summary": "As large language models (LLMs) handle increasingly longer contexts, serving\ninference requests for context lengths in the range of millions of tokens\npresents unique challenges. While existing techniques are effective for\ntraining, they fail to address the unique challenges of inference, such as\nvarying prefill and decode phases and their associated latency constraints --\nlike Time to First Token (TTFT) and Time per Output Token (TPOT). Furthermore,\nno long-context inference solutions address head-of-line blocking today.\n  We present Medha, a system for efficient long-context LLM inference that\nintroduces three key innovations: adaptive chunking with slack-aware scheduling\nto prevent head-ofline blocking, Sequence Pipeline Parallelism (SPP) to reduce\nTTFT, and KV Cache Parallelism (KVP) to minimize TPOT. By combining these into\na novel 3D parallelism serving engine, Medha achieves unprecedented scale --\nsupporting contexts up to 10M tokens with production-grade latency. Our\nevaluation shows Medha reduces median latency by up to 30x compared to\nstate-of-the-art systems when serving a mix of short and long requests, while\nimproving throughput by upwards of 5x. This enables, for the first time,\nefficient long-context LLM inference at scale without compromising on shorter\nrequest latencies or system efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) handle increasingly longer contexts, serving\ninference requests for context lengths in the range of millions of tokens\npresents unique challenges. While existing techniques are effective for\ntraining, they fail to address the unique challenges of inference, such as\nvarying prefill and decode phases and their associated latency constraints --\nlike Time to First Token (TTFT) and Time per Output Token (TPOT). Furthermore,\nno long-context inference solutions address head-of-line blocking today.\n  We present Medha, a system for efficient long-context LLM inference that\nintroduces three key innovations: adaptive chunking with slack-aware scheduling\nto prevent head-ofline blocking, Sequence Pipeline Parallelism (SPP) to reduce\nTTFT, and KV Cache Parallelism (KVP) to minimize TPOT. By combining these into\na novel 3D parallelism serving engine, Medha achieves unprecedented scale --\nsupporting contexts up to 10M tokens with production-grade latency. Our\nevaluation shows Medha reduces median latency by up to 30x compared to\nstate-of-the-art systems when serving a mix of short and long requests, while\nimproving throughput by upwards of 5x. This enables, for the first time,\nefficient long-context LLM inference at scale without compromising on shorter\nrequest latencies or system efficiency."
                },
                "authors": [
                    {
                        "name": "Amey Agrawal"
                    },
                    {
                        "name": "Haoran Qiu"
                    },
                    {
                        "name": "Junda Chen"
                    },
                    {
                        "name": "Íñigo Goiri"
                    },
                    {
                        "name": "Ramachandran Ramjee"
                    },
                    {
                        "name": "Chaojie Zhang"
                    },
                    {
                        "name": "Alexey Tumanov"
                    },
                    {
                        "name": "Esha Choukse"
                    }
                ],
                "author_detail": {
                    "name": "Esha Choukse"
                },
                "author": "Esha Choukse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17264v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17264v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12206v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12206v3",
                "updated": "2025-03-26T01:49:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    1,
                    49,
                    37,
                    2,
                    85,
                    0
                ],
                "published": "2025-01-21T15:22:31Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    15,
                    22,
                    31,
                    1,
                    21,
                    0
                ],
                "title": "PAINT: Paying Attention to INformed Tokens to Mitigate Hallucination in\n  Large Vision-Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PAINT: Paying Attention to INformed Tokens to Mitigate Hallucination in\n  Large Vision-Language Model"
                },
                "summary": "Large Vision Language Models (LVLMs) have demonstrated remarkable\ncapabilities in understanding and describing visual content, achieving\nstate-of-the-art performance across various vision-language tasks. However,\nthese models often generate descriptions containing objects or details that are\nabsent in the input image, a phenomenon commonly known as hallucination. Our\nwork investigates the key reasons behind this issue by analyzing the pattern of\nself-attention in transformer layers. We find that hallucinations often arise\nfrom the progressive weakening of attention weight to visual tokens in the\ndeeper layers of the LLM. Some previous works naively boost the attention of\nall visual tokens to mitigate this issue, resulting in suboptimal hallucination\nreduction. To address this, we identify two critical sets of visual tokens that\nfacilitate the transfer of visual information from the vision encoder to the\nLLM. Local tokens encode grounded information about objects present in an\nimage, while summary tokens capture the overall aggregated representation of\nthe image. Importantly, these two sets of tokens require different levels of\nweight enhancement. To this end, we propose \\textbf{PAINT} (\\textbf{P}aying\n\\textbf{A}ttention to \\textbf{IN}formed \\textbf{T}okens), a plug-and-play\nframework that intervenes in the self-attention mechanism of the LLM,\nselectively boosting the attention weights of local and summary tokens with\nexperimentally learned margins. Evaluation on the MSCOCO image captioning\ndataset demonstrate that our approach reduces hallucination rates by up to\n62.3\\% compared to baseline models while maintaining accuracy. Code is\navailable at\n\\href{https://github.com/hasanar1f/PAINT}{https://github.com/hasanar1f/PAINT}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Vision Language Models (LVLMs) have demonstrated remarkable\ncapabilities in understanding and describing visual content, achieving\nstate-of-the-art performance across various vision-language tasks. However,\nthese models often generate descriptions containing objects or details that are\nabsent in the input image, a phenomenon commonly known as hallucination. Our\nwork investigates the key reasons behind this issue by analyzing the pattern of\nself-attention in transformer layers. We find that hallucinations often arise\nfrom the progressive weakening of attention weight to visual tokens in the\ndeeper layers of the LLM. Some previous works naively boost the attention of\nall visual tokens to mitigate this issue, resulting in suboptimal hallucination\nreduction. To address this, we identify two critical sets of visual tokens that\nfacilitate the transfer of visual information from the vision encoder to the\nLLM. Local tokens encode grounded information about objects present in an\nimage, while summary tokens capture the overall aggregated representation of\nthe image. Importantly, these two sets of tokens require different levels of\nweight enhancement. To this end, we propose \\textbf{PAINT} (\\textbf{P}aying\n\\textbf{A}ttention to \\textbf{IN}formed \\textbf{T}okens), a plug-and-play\nframework that intervenes in the self-attention mechanism of the LLM,\nselectively boosting the attention weights of local and summary tokens with\nexperimentally learned margins. Evaluation on the MSCOCO image captioning\ndataset demonstrate that our approach reduces hallucination rates by up to\n62.3\\% compared to baseline models while maintaining accuracy. Code is\navailable at\n\\href{https://github.com/hasanar1f/PAINT}{https://github.com/hasanar1f/PAINT}"
                },
                "authors": [
                    {
                        "name": "Kazi Hasan Ibn Arif"
                    },
                    {
                        "name": "Sajib Acharjee Dip"
                    },
                    {
                        "name": "Khizar Hussain"
                    },
                    {
                        "name": "Lang Zhang"
                    },
                    {
                        "name": "Chris Thomas"
                    }
                ],
                "author_detail": {
                    "name": "Chris Thomas"
                },
                "author": "Chris Thomas",
                "arxiv_comment": "6 pages, 4 tables, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12206v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12206v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.04743v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.04743v2",
                "updated": "2025-03-26T00:53:00Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    0,
                    53,
                    0,
                    2,
                    85,
                    0
                ],
                "published": "2023-12-07T22:55:48Z",
                "published_parsed": [
                    2023,
                    12,
                    7,
                    22,
                    55,
                    48,
                    3,
                    341,
                    0
                ],
                "title": "Hiding Functions within Functions: Steganography by Implicit Neural\n  Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hiding Functions within Functions: Steganography by Implicit Neural\n  Representations"
                },
                "summary": "Deep steganography utilizes the powerful capabilities of deep neural networks\nto embed and extract messages, but its reliance on an additional message\nextractor limits its practical use due to the added suspicion it can raise from\nsteganalyzers. To address this problem, we propose StegaINR, which utilizes\nImplicit Neural Representation (INR) to implement steganography. StegaINR\nembeds a secret function into a stego function, which serves as both the\nmessage extractor and the stego media for secure transmission on a public\nchannel. Recipients need only use a shared key to recover the secret function\nfrom the stego function, allowing them to obtain the secret message. Our\napproach makes use of continuous functions, enabling it to handle various types\nof messages. To our knowledge, this is the first work to introduce INR into\nsteganography. We performed evaluations on image and climate data to test our\nmethod in different deployment contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep steganography utilizes the powerful capabilities of deep neural networks\nto embed and extract messages, but its reliance on an additional message\nextractor limits its practical use due to the added suspicion it can raise from\nsteganalyzers. To address this problem, we propose StegaINR, which utilizes\nImplicit Neural Representation (INR) to implement steganography. StegaINR\nembeds a secret function into a stego function, which serves as both the\nmessage extractor and the stego media for secure transmission on a public\nchannel. Recipients need only use a shared key to recover the secret function\nfrom the stego function, allowing them to obtain the secret message. Our\napproach makes use of continuous functions, enabling it to handle various types\nof messages. To our knowledge, this is the first work to introduce INR into\nsteganography. We performed evaluations on image and climate data to test our\nmethod in different deployment contexts."
                },
                "authors": [
                    {
                        "name": "Jia Liu"
                    },
                    {
                        "name": "Peng Luo"
                    },
                    {
                        "name": "Yan Ke"
                    },
                    {
                        "name": "Dang Qian"
                    },
                    {
                        "name": "Zhang Minqing"
                    },
                    {
                        "name": "Mu Dejun"
                    }
                ],
                "author_detail": {
                    "name": "Mu Dejun"
                },
                "author": "Mu Dejun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.04743v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.04743v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]