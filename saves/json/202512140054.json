[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2512.10576v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10576v1",
                "title": "ESS: An Offload-Centric Latent-Cache Management Architecture for DeepSeek-V3.2-Exp",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ESS: An Offload-Centric Latent-Cache Management Architecture for DeepSeek-V3.2-Exp"
                },
                "updated": "2025-12-11T12:06:00Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    12,
                    6,
                    0,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10576v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10576v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "DeepSeek-V3.2-Exp introduces a sparse attention mechanism that significantly reduces inference latency in long-context scenarios. Although the overall throughput has improved greatly, the Decode-stage of PD disaggregation remains to be a major bottleneck. This bottleneck primarily stems from the conflict between linear growth of Latent-Cache with sequence length and the limited GPU memory capacity, which constrains the feasible batch-size and thereby suppresses Decode-stage throughput.\n  To address this challenge, we propose ESS (Extended Sparse Server), an offload-centric system design tailored for DeepSeek-V3.2-Exp. ESS selectively offloads Latent-Cache to CPU memory while preserving latency-critical components on GPU. By freeing up GPU memory, ESS effectively decoupling batch-size scaling from GPU memory constraints. This design significantly improves Decode-stage throughput, thereby reducing deployment costs in real-world settings.\n  Our high-fidelity simulations show that ESS delivers 69.4\\% throughput improvement at 32K context length and up to 123\\% throughput improvement at 128K, demonstrating its effectiveness for large-context inference workloads. These results highlight ESS as a practical and scalable solution for long-context LLM serving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepSeek-V3.2-Exp introduces a sparse attention mechanism that significantly reduces inference latency in long-context scenarios. Although the overall throughput has improved greatly, the Decode-stage of PD disaggregation remains to be a major bottleneck. This bottleneck primarily stems from the conflict between linear growth of Latent-Cache with sequence length and the limited GPU memory capacity, which constrains the feasible batch-size and thereby suppresses Decode-stage throughput.\n  To address this challenge, we propose ESS (Extended Sparse Server), an offload-centric system design tailored for DeepSeek-V3.2-Exp. ESS selectively offloads Latent-Cache to CPU memory while preserving latency-critical components on GPU. By freeing up GPU memory, ESS effectively decoupling batch-size scaling from GPU memory constraints. This design significantly improves Decode-stage throughput, thereby reducing deployment costs in real-world settings.\n  Our high-fidelity simulations show that ESS delivers 69.4\\% throughput improvement at 32K context length and up to 123\\% throughput improvement at 128K, demonstrating its effectiveness for large-context inference workloads. These results highlight ESS as a practical and scalable solution for long-context LLM serving."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T12:06:00Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    12,
                    6,
                    0,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Xinhang Chen"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Jiahuan He"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Jianming Zhang"
                    },
                    {
                        "name": "Wenlong Zhou"
                    },
                    {
                        "name": "Xiao Li"
                    },
                    {
                        "name": "Pai Zeng"
                    },
                    {
                        "name": "Shiyong Li"
                    },
                    {
                        "name": "Yuanpan Qian"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Zhaogeng Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhaogeng Li"
                },
                "author": "Zhaogeng Li"
            },
            {
                "id": "http://arxiv.org/abs/2512.10547v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10547v1",
                "title": "Unlocking the Address Book: Dissecting the Sparse Semantic Structure of LLM Key-Value Caches via Sparse Autoencoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking the Address Book: Dissecting the Sparse Semantic Structure of LLM Key-Value Caches via Sparse Autoencoders"
                },
                "updated": "2025-12-11T11:23:50Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    11,
                    23,
                    50,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10547v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10547v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The Key-Value (KV) cache is the primary memory bottleneck in long-context Large Language Models, yet it is typically treated as an opaque numerical tensor. In this work, we propose \\textbf{STA-Attention}, a framework that utilizes Top-K Sparse Autoencoders (SAEs) to decompose the KV cache into interpretable ``semantic atoms.'' Unlike standard $L_1$-regularized SAEs, our Top-K approach eliminates shrinkage bias, preserving the precise dot-product geometry required for attention. Our analysis uncovers a fundamental \\textbf{Key-Value Asymmetry}: while Key vectors serve as highly sparse routers dominated by a ``Semantic Elbow,'' deep Value vectors carry dense content payloads requiring a larger budget. Based on this structure, we introduce a Dual-Budget Strategy that selectively preserves the most informative semantic components while filtering representational noise. Experiments on Yi-6B, Mistral-7B, Qwen2.5-32B, and others show that our semantic reconstructions maintain perplexity and zero-shot performance comparable to the original models, effectively bridging the gap between mechanistic interpretability and faithful attention modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Key-Value (KV) cache is the primary memory bottleneck in long-context Large Language Models, yet it is typically treated as an opaque numerical tensor. In this work, we propose \\textbf{STA-Attention}, a framework that utilizes Top-K Sparse Autoencoders (SAEs) to decompose the KV cache into interpretable ``semantic atoms.'' Unlike standard $L_1$-regularized SAEs, our Top-K approach eliminates shrinkage bias, preserving the precise dot-product geometry required for attention. Our analysis uncovers a fundamental \\textbf{Key-Value Asymmetry}: while Key vectors serve as highly sparse routers dominated by a ``Semantic Elbow,'' deep Value vectors carry dense content payloads requiring a larger budget. Based on this structure, we introduce a Dual-Budget Strategy that selectively preserves the most informative semantic components while filtering representational noise. Experiments on Yi-6B, Mistral-7B, Qwen2.5-32B, and others show that our semantic reconstructions maintain perplexity and zero-shot performance comparable to the original models, effectively bridging the gap between mechanistic interpretability and faithful attention modeling."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T11:23:50Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    11,
                    23,
                    50,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Qingsen Ma"
                    },
                    {
                        "name": "Dianyun Wang"
                    },
                    {
                        "name": "Jiaming Lyu"
                    },
                    {
                        "name": "Yaoye Wang"
                    },
                    {
                        "name": "Lechen Ning"
                    },
                    {
                        "name": "Sujie Zhu"
                    },
                    {
                        "name": "Zhenbo Xu"
                    },
                    {
                        "name": "Liuyu Xiang"
                    },
                    {
                        "name": "Huining Li"
                    },
                    {
                        "name": "Huijia Wu"
                    },
                    {
                        "name": "Zhaofeng He"
                    }
                ],
                "author_detail": {
                    "name": "Zhaofeng He"
                },
                "author": "Zhaofeng He"
            },
            {
                "id": "http://arxiv.org/abs/2512.10405v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10405v1",
                "title": "Electric-Field-Controlled Altermagnetic Transition for Neuromorphic Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electric-Field-Controlled Altermagnetic Transition for Neuromorphic Computing"
                },
                "updated": "2025-12-11T08:14:00Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    8,
                    14,
                    0,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10405v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10405v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1021/jacs.5c15276",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Altermagnets represent a novel magnetic phase with transformative potential for ultrafast spintronics, yet efficient control of their magnetic states remains challenging. We demonstrate an ultra-low-power electric-field control of altermagnetism in MnTe through strain-mediated coupling in MnTe/PMN-PT heterostructures with negligible Joule heating. Application of +6 kV/cm electric fields induces piezoelectric strain in PMN-PT, modulating the Néel temperature from 310 to 328 K. As a result, around the magnetic phase transition, the altermagnetic spin splitting of MnTe is reversibly switched \"on\" and \"off\" by the electric fields. Meanwhile, the piezoelectric strain generates lattice distortions and magnetic structure changes in MnTe, enabling up to 9.7% resistance modulation around the magnetic phase transition temperature. Leveraging this effect, we implement programmable resistance states in a Hopfield neuromorphic network, achieving 100% pattern recognition accuracy at <=40% noise levels. This approach establishes the electric-field control as a low-power strategy for altermagnetic manipulation while demonstrating the viability of altermagnetic materials for energy-efficient neuromorphic computing beyond conventional charge-based architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Altermagnets represent a novel magnetic phase with transformative potential for ultrafast spintronics, yet efficient control of their magnetic states remains challenging. We demonstrate an ultra-low-power electric-field control of altermagnetism in MnTe through strain-mediated coupling in MnTe/PMN-PT heterostructures with negligible Joule heating. Application of +6 kV/cm electric fields induces piezoelectric strain in PMN-PT, modulating the Néel temperature from 310 to 328 K. As a result, around the magnetic phase transition, the altermagnetic spin splitting of MnTe is reversibly switched \"on\" and \"off\" by the electric fields. Meanwhile, the piezoelectric strain generates lattice distortions and magnetic structure changes in MnTe, enabling up to 9.7% resistance modulation around the magnetic phase transition temperature. Leveraging this effect, we implement programmable resistance states in a Hopfield neuromorphic network, achieving 100% pattern recognition accuracy at <=40% noise levels. This approach establishes the electric-field control as a low-power strategy for altermagnetic manipulation while demonstrating the viability of altermagnetic materials for energy-efficient neuromorphic computing beyond conventional charge-based architectures."
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T08:14:00Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    8,
                    14,
                    0,
                    3,
                    345,
                    0
                ],
                "arxiv_comment": "42 pages, 13 figures, published online at Journal of the American Chemical Society",
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci"
                },
                "authors": [
                    {
                        "name": "Zhiyuan Duan"
                    },
                    {
                        "name": "Peixin Qin"
                    },
                    {
                        "name": "Chengyan Zhong"
                    },
                    {
                        "name": "Shaoxuan Zhang"
                    },
                    {
                        "name": "Li Liu"
                    },
                    {
                        "name": "Guojian Zhao"
                    },
                    {
                        "name": "Xiaoning Wang"
                    },
                    {
                        "name": "Hongyu Chen"
                    },
                    {
                        "name": "Ziang Meng"
                    },
                    {
                        "name": "Jingyu Li"
                    },
                    {
                        "name": "Sixu Jiang"
                    },
                    {
                        "name": "Xiaoyang Tan"
                    },
                    {
                        "name": "Qiong Wu"
                    },
                    {
                        "name": "Yu Liu"
                    },
                    {
                        "name": "Zhiqi Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqi Liu"
                },
                "author": "Zhiqi Liu",
                "arxiv_doi": "10.1021/jacs.5c15276"
            },
            {
                "id": "http://arxiv.org/abs/2511.00090v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.00090v3",
                "title": "LeMiCa: Lexicographic Minimax Path Caching for Efficient Diffusion-Based Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LeMiCa: Lexicographic Minimax Path Caching for Efficient Diffusion-Based Video Generation"
                },
                "updated": "2025-12-11T08:10:13Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    8,
                    10,
                    13,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.00090v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.00090v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We present LeMiCa, a training-free and efficient acceleration framework for diffusion-based video generation. While existing caching strategies primarily focus on reducing local heuristic errors, they often overlook the accumulation of global errors, leading to noticeable content degradation between accelerated and original videos. To address this issue, we formulate cache scheduling as a directed graph with error-weighted edges and introduce a Lexicographic Minimax Path Optimization strategy that explicitly bounds the worst-case path error. This approach substantially improves the consistency of global content and style across generated frames. Extensive experiments on multiple text-to-video benchmarks demonstrate that LeMiCa delivers dual improvements in both inference speed and generation quality. Notably, our method achieves a 2.9x speedup on the Latte model and reaches an LPIPS score of 0.05 on Open-Sora, outperforming prior caching techniques. Importantly, these gains come with minimal perceptual quality degradation, making LeMiCa a robust and generalizable paradigm for accelerating diffusion-based video generation. We believe this approach can serve as a strong foundation for future research on efficient and reliable video synthesis. Our code is available at :https://github.com/UnicomAI/LeMiCa",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present LeMiCa, a training-free and efficient acceleration framework for diffusion-based video generation. While existing caching strategies primarily focus on reducing local heuristic errors, they often overlook the accumulation of global errors, leading to noticeable content degradation between accelerated and original videos. To address this issue, we formulate cache scheduling as a directed graph with error-weighted edges and introduce a Lexicographic Minimax Path Optimization strategy that explicitly bounds the worst-case path error. This approach substantially improves the consistency of global content and style across generated frames. Extensive experiments on multiple text-to-video benchmarks demonstrate that LeMiCa delivers dual improvements in both inference speed and generation quality. Notably, our method achieves a 2.9x speedup on the Latte model and reaches an LPIPS score of 0.05 on Open-Sora, outperforming prior caching techniques. Importantly, these gains come with minimal perceptual quality degradation, making LeMiCa a robust and generalizable paradigm for accelerating diffusion-based video generation. We believe this approach can serve as a strong foundation for future research on efficient and reliable video synthesis. Our code is available at :https://github.com/UnicomAI/LeMiCa"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-30T04:57:26Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    4,
                    57,
                    26,
                    3,
                    303,
                    0
                ],
                "arxiv_comment": "NeurIPS 2025 Spotlight",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Huanlin Gao"
                    },
                    {
                        "name": "Ping Chen"
                    },
                    {
                        "name": "Fuyuan Shi"
                    },
                    {
                        "name": "Chao Tan"
                    },
                    {
                        "name": "Zhaoxiang Liu"
                    },
                    {
                        "name": "Fang Zhao"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Shiguo Lian"
                    }
                ],
                "author_detail": {
                    "name": "Shiguo Lian"
                },
                "author": "Shiguo Lian"
            },
            {
                "id": "http://arxiv.org/abs/2512.10310v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10310v1",
                "title": "Efficient-VLN: A Training-Efficient Vision-Language Navigation Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient-VLN: A Training-Efficient Vision-Language Navigation Model"
                },
                "updated": "2025-12-11T05:57:48Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    5,
                    57,
                    48,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10310v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10310v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Multimodal large language models (MLLMs) have shown promising potential in Vision-Language Navigation (VLN). However, their practical development is severely hindered by the substantial training overhead. We recognize two key issues that contribute to the overhead: (1) the quadratic computational burden from processing long-horizon historical observations as massive sequences of tokens, and (2) the exploration-efficiency trade-off in DAgger, i.e., a data aggregation process of collecting agent-explored trajectories. While more exploration yields effective error-recovery trajectories for handling test-time distribution shifts, it comes at the cost of longer trajectory lengths for both training and inference. To address these challenges, we propose Efficient-VLN, a training-efficient VLN model. Specifically, to mitigate the token processing burden, we design two efficient memory mechanisms: a progressive memory that dynamically allocates more tokens to recent observations, and a learnable recursive memory that utilizes the key-value cache of learnable tokens as the memory state. Moreover, we introduce a dynamic mixed policy to balance the exploration-efficiency trade-off. Extensive experiments show that Efficient-VLN achieves state-of-the-art performance on R2R-CE (64.2% SR) and RxR-CE (67.0% SR). Critically, our model consumes merely 282 H800 GPU hours, demonstrating a dramatic reduction in training overhead compared to state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) have shown promising potential in Vision-Language Navigation (VLN). However, their practical development is severely hindered by the substantial training overhead. We recognize two key issues that contribute to the overhead: (1) the quadratic computational burden from processing long-horizon historical observations as massive sequences of tokens, and (2) the exploration-efficiency trade-off in DAgger, i.e., a data aggregation process of collecting agent-explored trajectories. While more exploration yields effective error-recovery trajectories for handling test-time distribution shifts, it comes at the cost of longer trajectory lengths for both training and inference. To address these challenges, we propose Efficient-VLN, a training-efficient VLN model. Specifically, to mitigate the token processing burden, we design two efficient memory mechanisms: a progressive memory that dynamically allocates more tokens to recent observations, and a learnable recursive memory that utilizes the key-value cache of learnable tokens as the memory state. Moreover, we introduce a dynamic mixed policy to balance the exploration-efficiency trade-off. Extensive experiments show that Efficient-VLN achieves state-of-the-art performance on R2R-CE (64.2% SR) and RxR-CE (67.0% SR). Critically, our model consumes merely 282 H800 GPU hours, demonstrating a dramatic reduction in training overhead compared to state-of-the-art methods."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T05:57:48Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    5,
                    57,
                    48,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Duo Zheng"
                    },
                    {
                        "name": "Shijia Huang"
                    },
                    {
                        "name": "Yanyang Li"
                    },
                    {
                        "name": "Liwei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Liwei Wang"
                },
                "author": "Liwei Wang"
            },
            {
                "id": "http://arxiv.org/abs/2512.10293v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10293v1",
                "title": "Physically Aware 360$^\\circ$ View Generation from a Single Image using Disentangled Scene Embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physically Aware 360$^\\circ$ View Generation from a Single Image using Disentangled Scene Embeddings"
                },
                "updated": "2025-12-11T05:20:24Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    5,
                    20,
                    24,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10293v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10293v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We introduce Disentangled360, an innovative 3D-aware technology that integrates the advantages of direction disentangled volume rendering with single-image 360° unique view synthesis for applications in medical imaging and natural scene reconstruction. In contrast to current techniques that either oversimplify anisotropic light behavior or lack generalizability across various contexts, our framework distinctly differentiates between isotropic and anisotropic contributions inside a Gaussian Splatting backbone. We implement a dual-branch conditioning framework, one optimized for CT intensity driven scattering in volumetric data and the other for real-world RGB scenes through normalized camera embeddings. To address scale ambiguity and maintain structural realism, we present a hybrid pose agnostic anchoring method that adaptively samples scene depth and material transitions, functioning as stable pivots during scene distillation. Our design integrates preoperative radiography simulation and consumer-grade 360° rendering into a singular inference pipeline, facilitating rapid, photorealistic view synthesis with inherent directionality. Evaluations on the Mip-NeRF 360, RealEstate10K, and DeepDRR datasets indicate superior SSIM and LPIPS performance, while runtime assessments confirm its viability for interactive applications. Disentangled360 facilitates mixed-reality medical supervision, robotic perception, and immersive content creation, eliminating the necessity for scene-specific finetuning or expensive photon simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Disentangled360, an innovative 3D-aware technology that integrates the advantages of direction disentangled volume rendering with single-image 360° unique view synthesis for applications in medical imaging and natural scene reconstruction. In contrast to current techniques that either oversimplify anisotropic light behavior or lack generalizability across various contexts, our framework distinctly differentiates between isotropic and anisotropic contributions inside a Gaussian Splatting backbone. We implement a dual-branch conditioning framework, one optimized for CT intensity driven scattering in volumetric data and the other for real-world RGB scenes through normalized camera embeddings. To address scale ambiguity and maintain structural realism, we present a hybrid pose agnostic anchoring method that adaptively samples scene depth and material transitions, functioning as stable pivots during scene distillation. Our design integrates preoperative radiography simulation and consumer-grade 360° rendering into a singular inference pipeline, facilitating rapid, photorealistic view synthesis with inherent directionality. Evaluations on the Mip-NeRF 360, RealEstate10K, and DeepDRR datasets indicate superior SSIM and LPIPS performance, while runtime assessments confirm its viability for interactive applications. Disentangled360 facilitates mixed-reality medical supervision, robotic perception, and immersive content creation, eliminating the necessity for scene-specific finetuning or expensive photon simulations."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T05:20:24Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    5,
                    20,
                    24,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Karthikeya KV"
                    },
                    {
                        "name": "Narendra Bandaru"
                    }
                ],
                "author_detail": {
                    "name": "Narendra Bandaru"
                },
                "author": "Narendra Bandaru"
            },
            {
                "id": "http://arxiv.org/abs/2512.09723v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.09723v1",
                "title": "Mixture of Lookup Key-Value Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Lookup Key-Value Experts"
                },
                "updated": "2025-12-10T15:05:55Z",
                "updated_parsed": [
                    2025,
                    12,
                    10,
                    15,
                    5,
                    55,
                    2,
                    344,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.09723v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.09723v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent research has developed several LLM architectures suitable for inference on end-user devices, such as the Mixture of Lookup Experts (MoLE)~\\parencite{jie_mixture_2025}. A key feature of MoLE is that each token id is associated with a dedicated group of experts. For a given input, only the experts corresponding to the input token id will be activated. Since the communication overhead of loading this small number of activated experts into RAM during inference is negligible, expert parameters can be offloaded to storage, making MoLE suitable for resource-constrained devices. However, MoLE's context-independent expert selection mechanism, based solely on input ids, may limit model performance. To address this, we propose the \\textbf{M}ixture \\textbf{o}f \\textbf{L}ookup \\textbf{K}ey-\\textbf{V}alue Experts (\\textbf{MoLKV}) model. In MoLKV, each expert is structured as a key-value pair. For a given input, the input-derived query interacts with the cached key-value experts from the current sequence, generating a context-aware expert output. This context-aware mechanism alleviates the limitation of MoLE, and experimental results demonstrate that MoLKV achieves significantly lower validation loss in small-scale evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research has developed several LLM architectures suitable for inference on end-user devices, such as the Mixture of Lookup Experts (MoLE)~\\parencite{jie_mixture_2025}. A key feature of MoLE is that each token id is associated with a dedicated group of experts. For a given input, only the experts corresponding to the input token id will be activated. Since the communication overhead of loading this small number of activated experts into RAM during inference is negligible, expert parameters can be offloaded to storage, making MoLE suitable for resource-constrained devices. However, MoLE's context-independent expert selection mechanism, based solely on input ids, may limit model performance. To address this, we propose the \\textbf{M}ixture \\textbf{o}f \\textbf{L}ookup \\textbf{K}ey-\\textbf{V}alue Experts (\\textbf{MoLKV}) model. In MoLKV, each expert is structured as a key-value pair. For a given input, the input-derived query interacts with the cached key-value experts from the current sequence, generating a context-aware expert output. This context-aware mechanism alleviates the limitation of MoLE, and experimental results demonstrate that MoLKV achieves significantly lower validation loss in small-scale evaluations."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-10T15:05:55Z",
                "published_parsed": [
                    2025,
                    12,
                    10,
                    15,
                    5,
                    55,
                    2,
                    344,
                    0
                ],
                "arxiv_comment": "Preliminary Version; Work in Progress",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Zongcheng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zongcheng Wang"
                },
                "author": "Zongcheng Wang"
            },
            {
                "id": "http://arxiv.org/abs/2505.16056v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.16056v3",
                "title": "Not All Models Suit Expert Offloading: On Local Routing Consistency of Mixture-of-Expert Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Models Suit Expert Offloading: On Local Routing Consistency of Mixture-of-Expert Models"
                },
                "updated": "2025-12-10T14:34:07Z",
                "updated_parsed": [
                    2025,
                    12,
                    10,
                    14,
                    34,
                    7,
                    2,
                    344,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.16056v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.16056v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Mixture-of-Experts (MoE) enables efficient scaling of large language models (LLMs) with sparsely activated experts during inference. To effectively deploy large MoE models on memory-constrained devices, many systems introduce *expert offloading* that caches a subset of experts in fast memory, leaving others on slow memory to run on CPU or load on demand. While some research has exploited the locality of expert activations, where consecutive tokens activate similar experts, the degree of this **local routing consistency** varies across models and remains understudied. In this paper, we propose two metrics to measure local routing consistency of MoE models: (1) **Segment Routing Best Performance (SRP)**, which evaluates how well a fixed group of experts can cover the needs of a segment of tokens, and (2) **Segment Cache Best Hit Rate (SCH)**, which measures the hit rate of an expert cache utilizing a length of future information under a cache limit. We analyze 20 MoE LLMs with diverse sizes and architectures and use toy models to verify key factors related to local routing consistency. We find a strong trade-off between local routing consistency and *local* load balance, while showing that *global* load balance can coexist with local routing consistency. Meanwhile, settings like shared experts that decrease expert combination space can lead to low local routing consistency. We further reveal that domain-specialized experts contribute more to routing consistency than vocabulary-specialized ones, and that most models balance between cache effectiveness and efficiency with cache sizes approximately twice the active experts. These findings pave the way for memory-efficient MoE design and deployment without compromising inference speed. We publish the code for replicating experiments at https://github.com/ljcleo/moe-lrc .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) enables efficient scaling of large language models (LLMs) with sparsely activated experts during inference. To effectively deploy large MoE models on memory-constrained devices, many systems introduce *expert offloading* that caches a subset of experts in fast memory, leaving others on slow memory to run on CPU or load on demand. While some research has exploited the locality of expert activations, where consecutive tokens activate similar experts, the degree of this **local routing consistency** varies across models and remains understudied. In this paper, we propose two metrics to measure local routing consistency of MoE models: (1) **Segment Routing Best Performance (SRP)**, which evaluates how well a fixed group of experts can cover the needs of a segment of tokens, and (2) **Segment Cache Best Hit Rate (SCH)**, which measures the hit rate of an expert cache utilizing a length of future information under a cache limit. We analyze 20 MoE LLMs with diverse sizes and architectures and use toy models to verify key factors related to local routing consistency. We find a strong trade-off between local routing consistency and *local* load balance, while showing that *global* load balance can coexist with local routing consistency. Meanwhile, settings like shared experts that decrease expert combination space can lead to low local routing consistency. We further reveal that domain-specialized experts contribute more to routing consistency than vocabulary-specialized ones, and that most models balance between cache effectiveness and efficiency with cache sizes approximately twice the active experts. These findings pave the way for memory-efficient MoE design and deployment without compromising inference speed. We publish the code for replicating experiments at https://github.com/ljcleo/moe-lrc ."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-21T22:13:09Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    22,
                    13,
                    9,
                    2,
                    141,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Jingcong Liang"
                    },
                    {
                        "name": "Siyuan Wang"
                    },
                    {
                        "name": "Miren Tian"
                    },
                    {
                        "name": "Yitong Li"
                    },
                    {
                        "name": "Duyu Tang"
                    },
                    {
                        "name": "Zhongyu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Zhongyu Wei"
                },
                "author": "Zhongyu Wei"
            },
            {
                "id": "http://arxiv.org/abs/2512.08571v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08571v2",
                "title": "Matrix-free algorithms for fast ab initio calculations on distributed CPU architectures using finite-element discretization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matrix-free algorithms for fast ab initio calculations on distributed CPU architectures using finite-element discretization"
                },
                "updated": "2025-12-10T13:19:21Z",
                "updated_parsed": [
                    2025,
                    12,
                    10,
                    13,
                    19,
                    21,
                    2,
                    344,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08571v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08571v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Finite-element (FE) discretisations have emerged as a powerful real-space alternative to large-scale Kohn-Sham density functional theory (DFT) calculations, offering systematic convergence, excellent parallel scalability, while accommodating generic boundary conditions. However, the dominant computational bottleneck in FE-based DFT arises from the repeated application of the discretised sparse Hamiltonian to large blocks of trial vectors during iterations in an iterative eigensolver. Traditional sparse matrix-vector multiplications and FE cell-matrix approaches encounter memory limitations and high data-movement overheads, particularly at higher polynomial orders, typically used in DFT calculations. To overcome these challenges, this work develops matrix-free algorithms for FE-discretised DFT that substantially accelerate these products by doing on-the-fly operations that utilize structured tensor contractions over 1D basis functions and quadrature data. A unified multilevel batched data layout that handles both real and complex-valued operators is introduced to maximise cache reuse and SIMD utilisation on Frontier (AVX2), Param Pravega (AVX512) and Fugaku (SVE). We also combine terms for optimal cache reuse, even-odd decomposition to reduce FLOP, and mixed-precision intrinsics. Extensive benchmarks show that for large multivector pseudopotential DFT calculations, the matrix-free kernels deliver 1.5-4x speedups over the state-of-the-art cell-matrix approach baselines. For all-electron DFT calculations, the matrix-free operator achieves gains of up to 5.8x due to its efficient implementation and superior arithmetic intensity. When integrated with an error-tolerant Chebyshev-filtered subspace iteration eigensolver, the matrix-free formalism yields substantial reductions in end-to-end time-to-solution using FE meshes that deliver desired accuracies in ground-state properties.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finite-element (FE) discretisations have emerged as a powerful real-space alternative to large-scale Kohn-Sham density functional theory (DFT) calculations, offering systematic convergence, excellent parallel scalability, while accommodating generic boundary conditions. However, the dominant computational bottleneck in FE-based DFT arises from the repeated application of the discretised sparse Hamiltonian to large blocks of trial vectors during iterations in an iterative eigensolver. Traditional sparse matrix-vector multiplications and FE cell-matrix approaches encounter memory limitations and high data-movement overheads, particularly at higher polynomial orders, typically used in DFT calculations. To overcome these challenges, this work develops matrix-free algorithms for FE-discretised DFT that substantially accelerate these products by doing on-the-fly operations that utilize structured tensor contractions over 1D basis functions and quadrature data. A unified multilevel batched data layout that handles both real and complex-valued operators is introduced to maximise cache reuse and SIMD utilisation on Frontier (AVX2), Param Pravega (AVX512) and Fugaku (SVE). We also combine terms for optimal cache reuse, even-odd decomposition to reduce FLOP, and mixed-precision intrinsics. Extensive benchmarks show that for large multivector pseudopotential DFT calculations, the matrix-free kernels deliver 1.5-4x speedups over the state-of-the-art cell-matrix approach baselines. For all-electron DFT calculations, the matrix-free operator achieves gains of up to 5.8x due to its efficient implementation and superior arithmetic intensity. When integrated with an error-tolerant Chebyshev-filtered subspace iteration eigensolver, the matrix-free formalism yields substantial reductions in end-to-end time-to-solution using FE meshes that deliver desired accuracies in ground-state properties."
                },
                "tags": [
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T13:09:22Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    13,
                    9,
                    22,
                    1,
                    343,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "physics.comp-ph"
                },
                "authors": [
                    {
                        "name": "Gourab Panigrahi"
                    },
                    {
                        "name": "Phani Motamarri"
                    }
                ],
                "author_detail": {
                    "name": "Phani Motamarri"
                },
                "author": "Phani Motamarri"
            },
            {
                "id": "http://arxiv.org/abs/2512.09548v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.09548v1",
                "title": "Supporting Dynamic Agentic Workloads: How Data and Agents Interact",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supporting Dynamic Agentic Workloads: How Data and Agents Interact"
                },
                "updated": "2025-12-10T11:38:59Z",
                "updated_parsed": [
                    2025,
                    12,
                    10,
                    11,
                    38,
                    59,
                    2,
                    344,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.09548v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.09548v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The rise of multi-agent systems powered by large language models (LLMs) and specialized reasoning agents exposes fundamental limitations in today's data management architectures. Traditional databases and data fabrics were designed for static, well-defined workloads, whereas agentic systems exhibit dynamic, context-driven, and collaborative behaviors. Agents continuously decompose tasks, shift attention across modalities, and share intermediate results with peers - producing non-deterministic, multi-modal workloads that strain conventional query optimizers and caching mechanisms. We propose an Agent-Centric Data Fabric, a unified architecture that rethinks how data systems serve, optimize, coordinate, and learn from agentic workloads. To achieve this we exploit the concepts of attention-guided data retrieval, semantic micro-caching for context-driven agent federations, predictive data prefetching and quorum-based data serving. Together, these mechanisms enable agents to access representative data faster and more efficiently, while reducing redundant queries, data movement, and inference load across systems. By framing data systems as adaptive collaborators, instead of static executors, we outline new research directions toward behaviorally responsive data infrastructures, where caching, probing, and orchestration jointly enable efficient, context-rich data exchange among dynamic, reasoning-driven agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of multi-agent systems powered by large language models (LLMs) and specialized reasoning agents exposes fundamental limitations in today's data management architectures. Traditional databases and data fabrics were designed for static, well-defined workloads, whereas agentic systems exhibit dynamic, context-driven, and collaborative behaviors. Agents continuously decompose tasks, shift attention across modalities, and share intermediate results with peers - producing non-deterministic, multi-modal workloads that strain conventional query optimizers and caching mechanisms. We propose an Agent-Centric Data Fabric, a unified architecture that rethinks how data systems serve, optimize, coordinate, and learn from agentic workloads. To achieve this we exploit the concepts of attention-guided data retrieval, semantic micro-caching for context-driven agent federations, predictive data prefetching and quorum-based data serving. Together, these mechanisms enable agents to access representative data faster and more efficiently, while reducing redundant queries, data movement, and inference load across systems. By framing data systems as adaptive collaborators, instead of static executors, we outline new research directions toward behaviorally responsive data infrastructures, where caching, probing, and orchestration jointly enable efficient, context-rich data exchange among dynamic, reasoning-driven agents."
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-10T11:38:59Z",
                "published_parsed": [
                    2025,
                    12,
                    10,
                    11,
                    38,
                    59,
                    2,
                    344,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA"
                },
                "authors": [
                    {
                        "name": "Ioana Giurgiu"
                    },
                    {
                        "name": "Michael E. Nidd"
                    }
                ],
                "author_detail": {
                    "name": "Michael E. Nidd"
                },
                "author": "Michael E. Nidd"
            },
            {
                "id": "http://arxiv.org/abs/2512.01802v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.01802v3",
                "title": "JFR: An Efficient Jump Frontier Relaxation Strategy for Bellman-Ford",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JFR: An Efficient Jump Frontier Relaxation Strategy for Bellman-Ford"
                },
                "updated": "2025-12-10T08:35:45Z",
                "updated_parsed": [
                    2025,
                    12,
                    10,
                    8,
                    35,
                    45,
                    2,
                    344,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.01802v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.01802v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We propose JFR, a Bellman-Ford-based optimization framework leveraging frontier contraction and abstract multi-hop jump propagation to accelerate shortest-path computation while strictly preserving correctness. JFR achieves substantial reductions in relaxation operations, ranging from 25 to 99 percent, across sparse, dense, and negative-edge graphs, ensuring robust performance even under adversarial or highly connected topologies. On ultra-large graphs with up to N=20,000 nodes and 295 million edges, JFR maintains strong operational reductions and comparable or improved runtime relative to SPFA-SLF, demonstrating consistent robustness across graph size and density. Lower relaxation counts imply reduced memory-access overheads and computational effort; this normalized work reduction highlights JFR's suitability for scenarios requiring high throughput or energy-conscious operation. Future work focuses on integrating high-performance queue structures, adaptive frontier strategies, and cache-aware techniques to further reduce constant-factor overheads and fully realize JFR's practical runtime potential.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose JFR, a Bellman-Ford-based optimization framework leveraging frontier contraction and abstract multi-hop jump propagation to accelerate shortest-path computation while strictly preserving correctness. JFR achieves substantial reductions in relaxation operations, ranging from 25 to 99 percent, across sparse, dense, and negative-edge graphs, ensuring robust performance even under adversarial or highly connected topologies. On ultra-large graphs with up to N=20,000 nodes and 295 million edges, JFR maintains strong operational reductions and comparable or improved runtime relative to SPFA-SLF, demonstrating consistent robustness across graph size and density. Lower relaxation counts imply reduced memory-access overheads and computational effort; this normalized work reduction highlights JFR's suitability for scenarios requiring high throughput or energy-conscious operation. Future work focuses on integrating high-performance queue structures, adaptive frontier strategies, and cache-aware techniques to further reduce constant-factor overheads and fully realize JFR's practical runtime potential."
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-01T15:35:53Z",
                "published_parsed": [
                    2025,
                    12,
                    1,
                    15,
                    35,
                    53,
                    0,
                    335,
                    0
                ],
                "arxiv_comment": "with editor,24 pages",
                "arxiv_primary_category": {
                    "term": "cs.DS"
                },
                "authors": [
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Xi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xi Chen"
                },
                "author": "Xi Chen"
            },
            {
                "id": "http://arxiv.org/abs/2512.09378v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.09378v1",
                "title": "Federated Distillation Assisted Vehicle Edge Caching Scheme Based on Lightweight DDPM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Distillation Assisted Vehicle Edge Caching Scheme Based on Lightweight DDPM"
                },
                "updated": "2025-12-10T07:19:32Z",
                "updated_parsed": [
                    2025,
                    12,
                    10,
                    7,
                    19,
                    32,
                    2,
                    344,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.09378v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.09378v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Vehicle edge caching is a promising technology that can significantly reduce the latency for vehicle users (VUs) to access content by pre-caching user-interested content at edge nodes. It is crucial to accurately predict the content that VUs are interested in without exposing their privacy. Traditional federated learning (FL) can protect user privacy by sharing models rather than raw data. However, the training of FL requires frequent model transmission, which can result in significant communication overhead. Additionally, vehicles may leave the road side unit (RSU) coverage area before training is completed, leading to training failures. To address these issues, in this letter, we propose a federated distillation-assisted vehicle edge caching scheme based on lightweight denoising diffusion probabilistic model (LDPM). The simulation results demonstrate that the proposed vehicle edge caching scheme has good robustness to variations in vehicle speed, significantly reducing communication overhead and improving cache hit percentage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vehicle edge caching is a promising technology that can significantly reduce the latency for vehicle users (VUs) to access content by pre-caching user-interested content at edge nodes. It is crucial to accurately predict the content that VUs are interested in without exposing their privacy. Traditional federated learning (FL) can protect user privacy by sharing models rather than raw data. However, the training of FL requires frequent model transmission, which can result in significant communication overhead. Additionally, vehicles may leave the road side unit (RSU) coverage area before training is completed, leading to training failures. To address these issues, in this letter, we propose a federated distillation-assisted vehicle edge caching scheme based on lightweight denoising diffusion probabilistic model (LDPM). The simulation results demonstrate that the proposed vehicle edge caching scheme has good robustness to variations in vehicle speed, significantly reducing communication overhead and improving cache hit percentage."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-10T07:19:32Z",
                "published_parsed": [
                    2025,
                    12,
                    10,
                    7,
                    19,
                    32,
                    2,
                    344,
                    0
                ],
                "arxiv_comment": "This paper has been submitted to IEEE letters. The source code has been released at: https://github.com/qiongwu86/Federated-Distillation-Assisted-Vehicle-Edge-Caching-Scheme-Based-on-Lightweight-DDPM",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Xun Li"
                    },
                    {
                        "name": "Qiong Wu"
                    },
                    {
                        "name": "Pingyi Fan"
                    },
                    {
                        "name": "Kezhi Wang"
                    },
                    {
                        "name": "Wen Chen"
                    },
                    {
                        "name": "Khaled B. Letaief"
                    }
                ],
                "author_detail": {
                    "name": "Khaled B. Letaief"
                },
                "author": "Khaled B. Letaief"
            },
            {
                "id": "http://arxiv.org/abs/2512.09961v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.09961v1",
                "title": "TDC-Cache: A Trustworthy Decentralized Cooperative Caching Framework for Web3.0",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TDC-Cache: A Trustworthy Decentralized Cooperative Caching Framework for Web3.0"
                },
                "updated": "2025-12-10T02:52:41Z",
                "updated_parsed": [
                    2025,
                    12,
                    10,
                    2,
                    52,
                    41,
                    2,
                    344,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.09961v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.09961v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The rapid growth of Web3.0 is transforming the Internet from a centralized structure to decentralized, which empowers users with unprecedented self-sovereignty over their own data. However, in the context of decentralized data access within Web3.0, it is imperative to cope with efficiency concerns caused by the replication of redundant data, as well as security vulnerabilities caused by data inconsistency. To address these challenges, we develop a Trustworthy Decentralized Cooperative Caching (TDC-Cache) framework for Web3.0 to ensure efficient caching and enhance system resilience against adversarial threats. This framework features a two-layer architecture, wherein the Decentralized Oracle Network (DON) layer serves as a trusted intermediary platform for decentralized caching, bridging the contents from decentralized storage and the content requests from users. In light of the complexity of Web3.0 network topologies and data flows, we propose a Deep Reinforcement Learning-Based Decentralized Caching (DRL-DC) for TDC-Cache to dynamically optimize caching strategies of distributed oracles. Furthermore, we develop a Proof of Cooperative Learning (PoCL) consensus to maintain the consistency of decentralized caching decisions within DON. Experimental results show that, compared with existing approaches, the proposed framework reduces average access latency by 20%, increases the cache hit rate by at most 18%, and improves the average success consensus rate by 10%. Overall, this paper serves as a first foray into the investigation of decentralized caching framework and strategy for Web3.0.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of Web3.0 is transforming the Internet from a centralized structure to decentralized, which empowers users with unprecedented self-sovereignty over their own data. However, in the context of decentralized data access within Web3.0, it is imperative to cope with efficiency concerns caused by the replication of redundant data, as well as security vulnerabilities caused by data inconsistency. To address these challenges, we develop a Trustworthy Decentralized Cooperative Caching (TDC-Cache) framework for Web3.0 to ensure efficient caching and enhance system resilience against adversarial threats. This framework features a two-layer architecture, wherein the Decentralized Oracle Network (DON) layer serves as a trusted intermediary platform for decentralized caching, bridging the contents from decentralized storage and the content requests from users. In light of the complexity of Web3.0 network topologies and data flows, we propose a Deep Reinforcement Learning-Based Decentralized Caching (DRL-DC) for TDC-Cache to dynamically optimize caching strategies of distributed oracles. Furthermore, we develop a Proof of Cooperative Learning (PoCL) consensus to maintain the consistency of decentralized caching decisions within DON. Experimental results show that, compared with existing approaches, the proposed framework reduces average access latency by 20%, increases the cache hit rate by at most 18%, and improves the average success consensus rate by 10%. Overall, this paper serves as a first foray into the investigation of decentralized caching framework and strategy for Web3.0."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-10T02:52:41Z",
                "published_parsed": [
                    2025,
                    12,
                    10,
                    2,
                    52,
                    41,
                    2,
                    344,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Jinyu Chen"
                    },
                    {
                        "name": "Long Shi"
                    },
                    {
                        "name": "Taotao Wang"
                    },
                    {
                        "name": "Jiaheng Wang"
                    },
                    {
                        "name": "Wei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Zhang"
                },
                "author": "Wei Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2512.09238v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.09238v1",
                "title": "Training-free Context-adaptive Attention for Efficient Long Context Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-free Context-adaptive Attention for Efficient Long Context Modeling"
                },
                "updated": "2025-12-10T01:54:57Z",
                "updated_parsed": [
                    2025,
                    12,
                    10,
                    1,
                    54,
                    57,
                    2,
                    344,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.09238v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.09238v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across a wide range of natural language processing tasks. These capabilities stem primarily from the self-attention mechanism, which enables modeling of long-range dependencies. However, the quadratic complexity of self-attention with respect to sequence length poses significant computational and memory challenges, especially as sequence length extends to extremes. While various sparse attention and KV cache compression methods have been proposed to improve efficiency, they often suffer from limitations such as reliance on fixed patterns, inability to handle both prefilling and decoding stages, or the requirement for additional training. In this paper, we propose Training-free Context-adaptive Attention (TCA-Attention), a training-free sparse attention mechanism that selectively attends to only the informative tokens for efficient long-context inference. Our method consists of two lightweight phases: i) an offline calibration phase that determines head-specific sparsity budgets via a single forward pass, and ii) an online token selection phase that adaptively retains core context tokens using a lightweight redundancy metric. TCA-Attention provides a unified solution that accelerates both prefilling and decoding while reducing KV cache memory footprint, without requiring parameter updates or architectural changes. Theoretical analysis shows that our approach maintains bounded approximation error. Extensive experiments demonstrate that TCA-Attention achieves a 2.8$\\times$ speedup and reduces KV cache by 61% at 128K context length while maintaining performance comparable to full attention across various benchmarks, offering a practical plug-and-play solution for efficient long-context inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities across a wide range of natural language processing tasks. These capabilities stem primarily from the self-attention mechanism, which enables modeling of long-range dependencies. However, the quadratic complexity of self-attention with respect to sequence length poses significant computational and memory challenges, especially as sequence length extends to extremes. While various sparse attention and KV cache compression methods have been proposed to improve efficiency, they often suffer from limitations such as reliance on fixed patterns, inability to handle both prefilling and decoding stages, or the requirement for additional training. In this paper, we propose Training-free Context-adaptive Attention (TCA-Attention), a training-free sparse attention mechanism that selectively attends to only the informative tokens for efficient long-context inference. Our method consists of two lightweight phases: i) an offline calibration phase that determines head-specific sparsity budgets via a single forward pass, and ii) an online token selection phase that adaptively retains core context tokens using a lightweight redundancy metric. TCA-Attention provides a unified solution that accelerates both prefilling and decoding while reducing KV cache memory footprint, without requiring parameter updates or architectural changes. Theoretical analysis shows that our approach maintains bounded approximation error. Extensive experiments demonstrate that TCA-Attention achieves a 2.8$\\times$ speedup and reduces KV cache by 61% at 128K context length while maintaining performance comparable to full attention across various benchmarks, offering a practical plug-and-play solution for efficient long-context inference."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-10T01:54:57Z",
                "published_parsed": [
                    2025,
                    12,
                    10,
                    1,
                    54,
                    57,
                    2,
                    344,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Zeng You"
                    },
                    {
                        "name": "Yaofo Chen"
                    },
                    {
                        "name": "Shuhai Zhang"
                    },
                    {
                        "name": "Zhijie Qiu"
                    },
                    {
                        "name": "Tingyu Wu"
                    },
                    {
                        "name": "Yingjian Li"
                    },
                    {
                        "name": "Yaowei Wang"
                    },
                    {
                        "name": "Mingkui Tan"
                    }
                ],
                "author_detail": {
                    "name": "Mingkui Tan"
                },
                "author": "Mingkui Tan"
            },
            {
                "id": "http://arxiv.org/abs/2511.03092v5",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.03092v5",
                "title": "SnapStream: Efficient Long Sequence Decoding on Dataflow Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SnapStream: Efficient Long Sequence Decoding on Dataflow Accelerators"
                },
                "updated": "2025-12-10T00:29:21Z",
                "updated_parsed": [
                    2025,
                    12,
                    10,
                    0,
                    29,
                    21,
                    2,
                    344,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.03092v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.03092v5",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The proliferation of 100B+ parameter Large Language Models (LLMs) with 100k+ context length support have resulted in increasing demands for on-chip memory to support large KV caches. Techniques such as StreamingLLM and SnapKV demonstrate how to control KV cache size while maintaining model accuracy. Yet, these techniques are not commonly used within industrial deployments using frameworks like vLLM or SGLang. The reason is twofold: on one hand, the static graphs and continuous batching methodology employed by these frameworks make it difficult to admit modifications to the standard multi-head attention algorithm, while on the other hand, the accuracy implications of such techniques on modern instruction-following and reasoning models are not well understood, obfuscating the need for implementing these techniques. In this paper, we explore these accuracy implications on Llama-3.1-8B-Instruct and DeepSeek-R1, and develop SnapStream, a KV cache compression method that can be deployed at scale. We demonstrate the efficacy of SnapStream in a 16-way tensor-parallel deployment of DeepSeek-671B on SambaNova SN40L accelerators running at 128k context length and up to 1832 tokens per second in a real production setting. SnapStream enables $4\\times$ improved on-chip memory usage and introduces minimal accuracy degradation on LongBench-v2, AIME24 and LiveCodeBench. To the best of our knowledge, this is the first implementation of sparse KV attention techniques deployed in a production inference system with static graphs and continuous batching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of 100B+ parameter Large Language Models (LLMs) with 100k+ context length support have resulted in increasing demands for on-chip memory to support large KV caches. Techniques such as StreamingLLM and SnapKV demonstrate how to control KV cache size while maintaining model accuracy. Yet, these techniques are not commonly used within industrial deployments using frameworks like vLLM or SGLang. The reason is twofold: on one hand, the static graphs and continuous batching methodology employed by these frameworks make it difficult to admit modifications to the standard multi-head attention algorithm, while on the other hand, the accuracy implications of such techniques on modern instruction-following and reasoning models are not well understood, obfuscating the need for implementing these techniques. In this paper, we explore these accuracy implications on Llama-3.1-8B-Instruct and DeepSeek-R1, and develop SnapStream, a KV cache compression method that can be deployed at scale. We demonstrate the efficacy of SnapStream in a 16-way tensor-parallel deployment of DeepSeek-671B on SambaNova SN40L accelerators running at 128k context length and up to 1832 tokens per second in a real production setting. SnapStream enables $4\\times$ improved on-chip memory usage and introduces minimal accuracy degradation on LongBench-v2, AIME24 and LiveCodeBench. To the best of our knowledge, this is the first implementation of sparse KV attention techniques deployed in a production inference system with static graphs and continuous batching."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-05T00:38:31Z",
                "published_parsed": [
                    2025,
                    11,
                    5,
                    0,
                    38,
                    31,
                    2,
                    309,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Jonathan Li"
                    },
                    {
                        "name": "Nasim Farahini"
                    },
                    {
                        "name": "Evgenii Iuliugin"
                    },
                    {
                        "name": "Magnus Vesterlund"
                    },
                    {
                        "name": "Christian Häggström"
                    },
                    {
                        "name": "Guangtao Wang"
                    },
                    {
                        "name": "Shubhangi Upasani"
                    },
                    {
                        "name": "Ayush Sachdeva"
                    },
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Faline Fu"
                    },
                    {
                        "name": "Chen Wu"
                    },
                    {
                        "name": "Ayesha Siddiqua"
                    },
                    {
                        "name": "John Long"
                    },
                    {
                        "name": "Tuowen Zhao"
                    },
                    {
                        "name": "Matheen Musaddiq"
                    },
                    {
                        "name": "Håkan Zeffer"
                    },
                    {
                        "name": "Yun Du"
                    },
                    {
                        "name": "Mingran Wang"
                    },
                    {
                        "name": "Qinghua Li"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Urmish Thakker"
                    },
                    {
                        "name": "Raghu Prabhakar"
                    }
                ],
                "author_detail": {
                    "name": "Raghu Prabhakar"
                },
                "author": "Raghu Prabhakar"
            },
            {
                "id": "http://arxiv.org/abs/2510.17238v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.17238v2",
                "title": "StreamingThinker: Large Language Models Can Think While Reading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamingThinker: Large Language Models Can Think While Reading"
                },
                "updated": "2025-12-09T17:34:02Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    17,
                    34,
                    2,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.17238v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.17238v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in chain of thought (CoT) reasoning. However, the current LLM reasoning paradigm initiates thinking only after the entire input is available, which introduces unnecessary latency and weakens attention to earlier information in dynamic scenarios. Inspired by human cognition of thinking while reading, we first design a \\textit{\\textbf{streaming thinking}} paradigm for LLMs, where reasoning unfolds in the order of input and further adjusts its depth once reading is complete. We instantiate this paradigm with \\textit{StreamingThinker}, a framework that enables LLMs to think while reading through the integration of streaming CoT generation, streaming-constraint training, and streaming parallel inference. Specifically, StreamingThinker employs streaming reasoning units with quality control for CoT generation, enforces order-preserving reasoning through streaming attention masks and position encoding, and leverages parallel KV caches that decouple input encoding from reasoning generation, thereby ensuring alignment and enabling true concurrency. We evaluate StreamingThinker on the Qwen3 model family across math reasoning, logical reasoning, and context-based QA reasoning tasks. Experimental results show that the StreamingThinker preserves performance comparable to batch thinking, while yielding an 80\\% reduction in token waiting before the onset of reasoning and a more than 60\\% reduction in time-level latency for producing the final answer, demonstrating the effectiveness of the streaming paradigm for LLM reasoning. Code will be released at https://github.com/EIT-NLP/StreamingLLM/tree/main/StreamingThinker.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities in chain of thought (CoT) reasoning. However, the current LLM reasoning paradigm initiates thinking only after the entire input is available, which introduces unnecessary latency and weakens attention to earlier information in dynamic scenarios. Inspired by human cognition of thinking while reading, we first design a \\textit{\\textbf{streaming thinking}} paradigm for LLMs, where reasoning unfolds in the order of input and further adjusts its depth once reading is complete. We instantiate this paradigm with \\textit{StreamingThinker}, a framework that enables LLMs to think while reading through the integration of streaming CoT generation, streaming-constraint training, and streaming parallel inference. Specifically, StreamingThinker employs streaming reasoning units with quality control for CoT generation, enforces order-preserving reasoning through streaming attention masks and position encoding, and leverages parallel KV caches that decouple input encoding from reasoning generation, thereby ensuring alignment and enabling true concurrency. We evaluate StreamingThinker on the Qwen3 model family across math reasoning, logical reasoning, and context-based QA reasoning tasks. Experimental results show that the StreamingThinker preserves performance comparable to batch thinking, while yielding an 80\\% reduction in token waiting before the onset of reasoning and a more than 60\\% reduction in time-level latency for producing the final answer, demonstrating the effectiveness of the streaming paradigm for LLM reasoning. Code will be released at https://github.com/EIT-NLP/StreamingLLM/tree/main/StreamingThinker."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-20T07:27:37Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    7,
                    27,
                    37,
                    0,
                    293,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Junlong Tong"
                    },
                    {
                        "name": "Yingqi Fan"
                    },
                    {
                        "name": "Anhao Zhao"
                    },
                    {
                        "name": "Yunpu Ma"
                    },
                    {
                        "name": "Xiaoyu Shen"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoyu Shen"
                },
                "author": "Xiaoyu Shen"
            },
            {
                "id": "http://arxiv.org/abs/2512.08829v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08829v1",
                "title": "InfiniteVL: Synergizing Linear and Sparse Attention for Highly-Efficient, Unlimited-Input Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiniteVL: Synergizing Linear and Sparse Attention for Highly-Efficient, Unlimited-Input Vision-Language Models"
                },
                "updated": "2025-12-09T17:18:32Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    17,
                    18,
                    32,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08829v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08829v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Window attention and linear attention represent two principal strategies for mitigating the quadratic complexity and ever-growing KV cache in Vision-Language Models (VLMs). However, we observe that window-based VLMs suffer performance degradation when sequence length exceeds the window size, while linear attention underperforms on information-intensive tasks such as OCR and document understanding. To overcome these limitations, we propose InfiniteVL, a linear-complexity VLM architecture that synergizes sliding window attention (SWA) with Gated DeltaNet. For achieving competitive multimodal performance under constrained resources, we design a three-stage training strategy comprising distillation pretraining, instruction tuning, and long-sequence SFT. Remarkably, using less than 2\\% of the training data required by leading VLMs, InfiniteVL not only substantially outperforms previous linear-complexity VLMs but also matches the performance of leading Transformer-based VLMs, while demonstrating effective long-term memory retention. Compared to similar-sized Transformer-based VLMs accelerated by FlashAttention-2, InfiniteVL achieves over 3.6\\times inference speedup while maintaining constant latency and memory footprint. In streaming video understanding scenarios, it sustains a stable 24 FPS real-time prefill speed while preserving long-term memory cache. Code and models are available at https://github.com/hustvl/InfiniteVL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Window attention and linear attention represent two principal strategies for mitigating the quadratic complexity and ever-growing KV cache in Vision-Language Models (VLMs). However, we observe that window-based VLMs suffer performance degradation when sequence length exceeds the window size, while linear attention underperforms on information-intensive tasks such as OCR and document understanding. To overcome these limitations, we propose InfiniteVL, a linear-complexity VLM architecture that synergizes sliding window attention (SWA) with Gated DeltaNet. For achieving competitive multimodal performance under constrained resources, we design a three-stage training strategy comprising distillation pretraining, instruction tuning, and long-sequence SFT. Remarkably, using less than 2\\% of the training data required by leading VLMs, InfiniteVL not only substantially outperforms previous linear-complexity VLMs but also matches the performance of leading Transformer-based VLMs, while demonstrating effective long-term memory retention. Compared to similar-sized Transformer-based VLMs accelerated by FlashAttention-2, InfiniteVL achieves over 3.6\\times inference speedup while maintaining constant latency and memory footprint. In streaming video understanding scenarios, it sustains a stable 24 FPS real-time prefill speed while preserving long-term memory cache. Code and models are available at https://github.com/hustvl/InfiniteVL."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T17:18:32Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    17,
                    18,
                    32,
                    1,
                    343,
                    0
                ],
                "arxiv_comment": "16 pages, 8 figures, conference or other essential info",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Hongyuan Tao"
                    },
                    {
                        "name": "Bencheng Liao"
                    },
                    {
                        "name": "Shaoyu Chen"
                    },
                    {
                        "name": "Haoran Yin"
                    },
                    {
                        "name": "Qian Zhang"
                    },
                    {
                        "name": "Wenyu Liu"
                    },
                    {
                        "name": "Xinggang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinggang Wang"
                },
                "author": "Xinggang Wang"
            },
            {
                "id": "http://arxiv.org/abs/2512.01646v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.01646v2",
                "title": "StarDist: A Code Generator for Distributed Graph Algorithms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StarDist: A Code Generator for Distributed Graph Algorithms"
                },
                "updated": "2025-12-09T15:15:19Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    15,
                    15,
                    19,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.01646v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.01646v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Relational data, occurring in the real world, are often structured as graphs, which provide the logical abstraction required to make analytical derivations simpler. As graphs get larger, the irregular access patterns exhibited in most graph algorithms, hamper performance. This, along with NUMA and physical memory limits, results in scaling complexities with sequential/shared memory frameworks. StarPlat's MPI backend abstracts away the programmatic complexity involved in designing optimal distributed graph algorithms. It provides an instrument for coding graph algorithms that scale over distributed memory. In this work, we provide an analysis-transformation framework that leverages general semantics associated with iterations involving nodes and their neighbors, within StarPlat, to aggregate communication. The framework scans for patterns that warrant re-ordering in neighborhood access patterns, aggregate communication, and avoid communication altogether with opportunistic caching in reduction constructs. We also architect an optimized bulk-reduction substrate using Open MPI's passive Remote Memory Access (RMA) constructs. We applied our optimization logic to StarPlat's distributed backend and outperformed d-Galois by 2.05 and DRONE by 1.44 times in Single Source Shortest Paths across several big data graphs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Relational data, occurring in the real world, are often structured as graphs, which provide the logical abstraction required to make analytical derivations simpler. As graphs get larger, the irregular access patterns exhibited in most graph algorithms, hamper performance. This, along with NUMA and physical memory limits, results in scaling complexities with sequential/shared memory frameworks. StarPlat's MPI backend abstracts away the programmatic complexity involved in designing optimal distributed graph algorithms. It provides an instrument for coding graph algorithms that scale over distributed memory. In this work, we provide an analysis-transformation framework that leverages general semantics associated with iterations involving nodes and their neighbors, within StarPlat, to aggregate communication. The framework scans for patterns that warrant re-ordering in neighborhood access patterns, aggregate communication, and avoid communication altogether with opportunistic caching in reduction constructs. We also architect an optimized bulk-reduction substrate using Open MPI's passive Remote Memory Access (RMA) constructs. We applied our optimization logic to StarPlat's distributed backend and outperformed d-Galois by 2.05 and DRONE by 1.44 times in Single Source Shortest Paths across several big data graphs."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-01T13:18:32Z",
                "published_parsed": [
                    2025,
                    12,
                    1,
                    13,
                    18,
                    32,
                    0,
                    335,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Barenya Kumar Nandy"
                    },
                    {
                        "name": "Rupesh Nasre"
                    }
                ],
                "author_detail": {
                    "name": "Rupesh Nasre"
                },
                "author": "Rupesh Nasre"
            },
            {
                "id": "http://arxiv.org/abs/2512.08626v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08626v1",
                "title": "Inferring Causal Relationships to Improve Caching for Clients with Correlated Requests: Applications to VR",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring Causal Relationships to Improve Caching for Clients with Correlated Requests: Applications to VR"
                },
                "updated": "2025-12-09T14:10:41Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    14,
                    10,
                    41,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08626v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08626v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Efficient edge caching reduces latency and alleviates backhaul congestion in modern networks. Traditional caching policies, such as Least Recently Used (LRU) and Least Frequently Used (LFU), perform well under specific request patterns. LRU excels in workloads with strong temporal locality, while LFU is effective when content popularity remains static. However, real-world client requests often exhibit correlations due to shared contexts and coordinated activities. This is particularly evident in Virtual Reality (VR) environments, where groups of clients navigate shared virtual spaces, leading to correlated content requests.\n  In this paper, we introduce the \\textit{grouped client request model}, a generalization of the Independent Reference Model that explicitly captures different types of request correlations. Our theoretical analysis of LRU under this model reveals that the optimal causal caching policy depends on cache size: LFU is optimal for small to moderate caches, while LRU outperforms it for larger caches. To address the limitations of existing policies, we propose Least Following and Recently Used (LFRU), a novel online caching policy that dynamically infers and adapts to causal relationships in client requests to optimize evictions. LFRU prioritizes objects likely to be requested based on inferred dependencies, achieving near-optimal performance compared to the offline optimal Belady policy in structured correlation settings.\n  We develop VR based datasets to evaluate caching policies under realistic correlated requests. Our results show that LFRU consistently performs at least as well as LRU and LFU, outperforming LRU by up to 2.9x and LFU by up to1.9x in certain settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient edge caching reduces latency and alleviates backhaul congestion in modern networks. Traditional caching policies, such as Least Recently Used (LRU) and Least Frequently Used (LFU), perform well under specific request patterns. LRU excels in workloads with strong temporal locality, while LFU is effective when content popularity remains static. However, real-world client requests often exhibit correlations due to shared contexts and coordinated activities. This is particularly evident in Virtual Reality (VR) environments, where groups of clients navigate shared virtual spaces, leading to correlated content requests.\n  In this paper, we introduce the \\textit{grouped client request model}, a generalization of the Independent Reference Model that explicitly captures different types of request correlations. Our theoretical analysis of LRU under this model reveals that the optimal causal caching policy depends on cache size: LFU is optimal for small to moderate caches, while LRU outperforms it for larger caches. To address the limitations of existing policies, we propose Least Following and Recently Used (LFRU), a novel online caching policy that dynamically infers and adapts to causal relationships in client requests to optimize evictions. LFRU prioritizes objects likely to be requested based on inferred dependencies, achieving near-optimal performance compared to the offline optimal Belady policy in structured correlation settings.\n  We develop VR based datasets to evaluate caching policies under realistic correlated requests. Our results show that LFRU consistently performs at least as well as LRU and LFU, outperforming LRU by up to 2.9x and LFU by up to1.9x in certain settings."
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T14:10:41Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    14,
                    10,
                    41,
                    1,
                    343,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI"
                },
                "authors": [
                    {
                        "name": "Agrim Bari"
                    },
                    {
                        "name": "Gustavo de Veciana"
                    },
                    {
                        "name": "Yuqi Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yuqi Zhou"
                },
                "author": "Yuqi Zhou"
            },
            {
                "id": "http://arxiv.org/abs/2512.07155v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.07155v2",
                "title": "CHIMERA: Adaptive Cache Injection and Semantic Anchor Prompting for Zero-shot Image Morphing with Morphing-oriented Metrics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CHIMERA: Adaptive Cache Injection and Semantic Anchor Prompting for Zero-shot Image Morphing with Morphing-oriented Metrics"
                },
                "updated": "2025-12-09T07:35:43Z",
                "updated_parsed": [
                    2025,
                    12,
                    9,
                    7,
                    35,
                    43,
                    1,
                    343,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.07155v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.07155v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion models exhibit remarkable generative ability, yet achieving smooth and semantically consistent image morphing remains a challenge. Existing approaches often yield abrupt transitions or over-saturated appearances due to the lack of adaptive structural and semantic alignments. We propose CHIMERA, a zero-shot diffusion-based framework that formulates morphing as a cached inversion-guided denoising process. To handle large semantic and appearance disparities, we propose Adaptive Cache Injection and Semantic Anchor Prompting. Adaptive Cache Injection (ACI) caches down, mid, and up blocks features from both inputs during DDIM inversion and re-injects them adaptively during denoising, enabling spatial and semantic alignment in depth- and time-adaptive manners and enabling natural feature fusion and smooth transitions. Semantic Anchor Prompting (SAP) leverages a vision-language model to generate a shared anchor prompt that serves as a semantic anchor, bridging dissimilar inputs and guiding the denoising process toward coherent results. Finally, we introduce the Global-Local Consistency Score (GLCS), a morphing-oriented metric that simultaneously evaluates the global harmonization of the two inputs and the smoothness of the local morphing transition. Extensive experiments and user studies show that CHIMERA achieves smoother and more semantically aligned transitions than existing methods, establishing a new state of the art in image morphing. The code and project page will be publicly released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models exhibit remarkable generative ability, yet achieving smooth and semantically consistent image morphing remains a challenge. Existing approaches often yield abrupt transitions or over-saturated appearances due to the lack of adaptive structural and semantic alignments. We propose CHIMERA, a zero-shot diffusion-based framework that formulates morphing as a cached inversion-guided denoising process. To handle large semantic and appearance disparities, we propose Adaptive Cache Injection and Semantic Anchor Prompting. Adaptive Cache Injection (ACI) caches down, mid, and up blocks features from both inputs during DDIM inversion and re-injects them adaptively during denoising, enabling spatial and semantic alignment in depth- and time-adaptive manners and enabling natural feature fusion and smooth transitions. Semantic Anchor Prompting (SAP) leverages a vision-language model to generate a shared anchor prompt that serves as a semantic anchor, bridging dissimilar inputs and guiding the denoising process toward coherent results. Finally, we introduce the Global-Local Consistency Score (GLCS), a morphing-oriented metric that simultaneously evaluates the global harmonization of the two inputs and the smoothness of the local morphing transition. Extensive experiments and user studies show that CHIMERA achieves smoother and more semantically aligned transitions than existing methods, establishing a new state of the art in image morphing. The code and project page will be publicly released."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-08T04:39:12Z",
                "published_parsed": [
                    2025,
                    12,
                    8,
                    4,
                    39,
                    12,
                    0,
                    342,
                    0
                ],
                "arxiv_comment": "Please visit our project page at https://cmlab-korea.github.io/CHIMERA/",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Dahyeon Kye"
                    },
                    {
                        "name": "Jeahun Sung"
                    },
                    {
                        "name": "Mingyu Jeon"
                    },
                    {
                        "name": "Jihyong Oh"
                    }
                ],
                "author_detail": {
                    "name": "Jihyong Oh"
                },
                "author": "Jihyong Oh"
            },
            {
                "id": "http://arxiv.org/abs/2512.07993v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.07993v1",
                "title": "SkipKV: Selective Skipping of KV Generation and Storage for Efficient Inference with Large Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SkipKV: Selective Skipping of KV Generation and Storage for Efficient Inference with Large Reasoning Models"
                },
                "updated": "2025-12-08T19:32:06Z",
                "updated_parsed": [
                    2025,
                    12,
                    8,
                    19,
                    32,
                    6,
                    0,
                    342,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.07993v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.07993v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large reasoning models (LRMs) often cost significant key-value (KV) cache overhead, due to their linear growth with the verbose chain-of-thought (CoT) reasoning process. This costs both memory and throughput bottleneck limiting their efficient deployment. Towards reducing KV cache size during inference, we first investigate the effectiveness of existing KV cache eviction methods for CoT reasoning. Interestingly, we find that due to unstable token-wise scoring and the reduced effective KV budget caused by padding tokens, state-of-the-art (SoTA) eviction methods fail to maintain accuracy in the multi-batch setting. Additionally, these methods often generate longer sequences than the original model, as semantic-unaware token-wise eviction leads to repeated revalidation during reasoning. To address these issues, we present \\textbf{SkipKV}, a \\textbf{\\textit{training-free}} KV compression method for selective \\textit{eviction} and \\textit{generation} operating at a coarse-grained sentence-level sequence removal for efficient CoT reasoning. In specific, it introduces a \\textit{sentence-scoring metric} to identify and remove highly similar sentences while maintaining semantic coherence. To suppress redundant generation, SkipKV dynamically adjusts a steering vector to update the hidden activation states during inference enforcing the LRM to generate concise response. Extensive evaluations on multiple reasoning benchmarks demonstrate the effectiveness of SkipKV in maintaining up to $\\mathbf{26.7}\\%$ improved accuracy compared to the alternatives, at a similar compression budget. Additionally, compared to SoTA, SkipKV yields up to $\\mathbf{1.6}\\times$ fewer generation length while improving throughput up to $\\mathbf{1.7}\\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large reasoning models (LRMs) often cost significant key-value (KV) cache overhead, due to their linear growth with the verbose chain-of-thought (CoT) reasoning process. This costs both memory and throughput bottleneck limiting their efficient deployment. Towards reducing KV cache size during inference, we first investigate the effectiveness of existing KV cache eviction methods for CoT reasoning. Interestingly, we find that due to unstable token-wise scoring and the reduced effective KV budget caused by padding tokens, state-of-the-art (SoTA) eviction methods fail to maintain accuracy in the multi-batch setting. Additionally, these methods often generate longer sequences than the original model, as semantic-unaware token-wise eviction leads to repeated revalidation during reasoning. To address these issues, we present \\textbf{SkipKV}, a \\textbf{\\textit{training-free}} KV compression method for selective \\textit{eviction} and \\textit{generation} operating at a coarse-grained sentence-level sequence removal for efficient CoT reasoning. In specific, it introduces a \\textit{sentence-scoring metric} to identify and remove highly similar sentences while maintaining semantic coherence. To suppress redundant generation, SkipKV dynamically adjusts a steering vector to update the hidden activation states during inference enforcing the LRM to generate concise response. Extensive evaluations on multiple reasoning benchmarks demonstrate the effectiveness of SkipKV in maintaining up to $\\mathbf{26.7}\\%$ improved accuracy compared to the alternatives, at a similar compression budget. Additionally, compared to SoTA, SkipKV yields up to $\\mathbf{1.6}\\times$ fewer generation length while improving throughput up to $\\mathbf{1.7}\\times$."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-08T19:32:06Z",
                "published_parsed": [
                    2025,
                    12,
                    8,
                    19,
                    32,
                    6,
                    0,
                    342,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Jiayi Tian"
                    },
                    {
                        "name": "Seyedarmin Azizi"
                    },
                    {
                        "name": "Yequan Zhao"
                    },
                    {
                        "name": "Erfan Baghaei Potraghloo"
                    },
                    {
                        "name": "Sean McPherson"
                    },
                    {
                        "name": "Sharath Nittur Sridhar"
                    },
                    {
                        "name": "Zhengyang Wang"
                    },
                    {
                        "name": "Zheng Zhang"
                    },
                    {
                        "name": "Massoud Pedram"
                    },
                    {
                        "name": "Souvik Kundu"
                    }
                ],
                "author_detail": {
                    "name": "Souvik Kundu"
                },
                "author": "Souvik Kundu"
            },
            {
                "id": "http://arxiv.org/abs/2507.08143v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2507.08143v2",
                "title": "Compactor: Calibrated Query-Agnostic KV Cache Compression with Approximate Leverage Scores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compactor: Calibrated Query-Agnostic KV Cache Compression with Approximate Leverage Scores"
                },
                "updated": "2025-12-08T19:02:12Z",
                "updated_parsed": [
                    2025,
                    12,
                    8,
                    19,
                    2,
                    12,
                    0,
                    342,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2507.08143v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2507.08143v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Modern Large Language Models (LLMs) are increasingly trained to support very large context windows. We present Compactor, a training-free, query-agnostic KV compression strategy that uses approximate leverage scores to determine token importance. We show that Compactor can achieve the same performance as competing methods while retaining 20% fewer tokens in both synthetic and real-world context tasks, while being more task-robust. We further introduce a procedure for context-calibrated compression: inferring the maximum compression a given context supports before significant performance loss. Using context-calibrated compression, we show that Compactor achieves full KV performance on Longbench while reducing the KV memory burden by 68%, on average. To demonstrate the efficacy and generalizability of our approach, we apply Compactor to 27 synthetic and real-world tasks from RULER and Longbench, with models from both the Qwen 2.5 and Llama 3.1 families. Finally, we release compactor-vllm, an inference engine and suite of optimized Triton kernels designed to efficiently support the sparse, non-contiguous memory access patterns inherent to compressed KV caches. This work demonstrates that Compactor offers a practical, high-performance solution for alleviating the memory bottleneck in modern LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Large Language Models (LLMs) are increasingly trained to support very large context windows. We present Compactor, a training-free, query-agnostic KV compression strategy that uses approximate leverage scores to determine token importance. We show that Compactor can achieve the same performance as competing methods while retaining 20% fewer tokens in both synthetic and real-world context tasks, while being more task-robust. We further introduce a procedure for context-calibrated compression: inferring the maximum compression a given context supports before significant performance loss. Using context-calibrated compression, we show that Compactor achieves full KV performance on Longbench while reducing the KV memory burden by 68%, on average. To demonstrate the efficacy and generalizability of our approach, we apply Compactor to 27 synthetic and real-world tasks from RULER and Longbench, with models from both the Qwen 2.5 and Llama 3.1 families. Finally, we release compactor-vllm, an inference engine and suite of optimized Triton kernels designed to efficiently support the sparse, non-contiguous memory access patterns inherent to compressed KV caches. This work demonstrates that Compactor offers a practical, high-performance solution for alleviating the memory bottleneck in modern LLM deployment."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-07-10T20:03:35Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    20,
                    3,
                    35,
                    3,
                    191,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Vivek Chari"
                    },
                    {
                        "name": "Benjamin Van Durme"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Van Durme"
                },
                "author": "Benjamin Van Durme"
            },
            {
                "id": "http://arxiv.org/abs/2508.16653v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.16653v2",
                "title": "H2EAL: Hybrid-Bonding Architecture with Hybrid Sparse Attention for Efficient Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "H2EAL: Hybrid-Bonding Architecture with Hybrid Sparse Attention for Efficient Long-Context LLM Inference"
                },
                "updated": "2025-12-08T13:48:55Z",
                "updated_parsed": [
                    2025,
                    12,
                    8,
                    13,
                    48,
                    55,
                    0,
                    342,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.16653v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.16653v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) have demonstrated remarkable proficiency in a wide range of natural language processing applications. However, the high energy and latency overhead induced by the KV cache limits the edge deployment, especially for long contexts. Emerging hybrid bonding (HB) technology has been proposed as a promising alternative to conventional near-memory processing (NMP) architectures, offering improved bandwidth efficiency and lower power consumption while exhibiting characteristics of distributed memory. In this paper, we propose H2EAL, a hybrid bonding-based accelerator with sparse attention algorithm-hardware co-design for efficient LLM inference at the edge. At the algorithm level, we propose a hybrid sparse attention scheme with static and dynamic sparsity for different heads to fully leverage the sparsity with high accuracy. At the hardware level, we co-design the hardware to support hybrid sparse attention and propose memory-compute co-placement to address the distributed memory bottleneck. Since different attention heads exhibit different sparse patterns and the attention structure often mismatches the HB architecture, we further develop a load-balancing scheduler with parallel tiled attention to address workload imbalance and optimize the mapping strategy. Extensive experiments demonstrate H2EAL achieves 5.20~48.21x speedup and 6.22~73.48x energy efficiency improvement over baseline HB implementation, with a negligible average accuracy drop of 0.87% on multiple benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable proficiency in a wide range of natural language processing applications. However, the high energy and latency overhead induced by the KV cache limits the edge deployment, especially for long contexts. Emerging hybrid bonding (HB) technology has been proposed as a promising alternative to conventional near-memory processing (NMP) architectures, offering improved bandwidth efficiency and lower power consumption while exhibiting characteristics of distributed memory. In this paper, we propose H2EAL, a hybrid bonding-based accelerator with sparse attention algorithm-hardware co-design for efficient LLM inference at the edge. At the algorithm level, we propose a hybrid sparse attention scheme with static and dynamic sparsity for different heads to fully leverage the sparsity with high accuracy. At the hardware level, we co-design the hardware to support hybrid sparse attention and propose memory-compute co-placement to address the distributed memory bottleneck. Since different attention heads exhibit different sparse patterns and the attention structure often mismatches the HB architecture, we further develop a load-balancing scheduler with parallel tiled attention to address workload imbalance and optimize the mapping strategy. Extensive experiments demonstrate H2EAL achieves 5.20~48.21x speedup and 6.22~73.48x energy efficiency improvement over baseline HB implementation, with a negligible average accuracy drop of 0.87% on multiple benchmarks."
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-20T03:42:37Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    3,
                    42,
                    37,
                    2,
                    232,
                    0
                ],
                "arxiv_comment": "International Conference on Computer-Aided Design (ICCAD) 2025",
                "arxiv_primary_category": {
                    "term": "cs.PF"
                },
                "authors": [
                    {
                        "name": "Zizhuo Fu"
                    },
                    {
                        "name": "Xiaotian Guo"
                    },
                    {
                        "name": "Wenxuan Zeng"
                    },
                    {
                        "name": "Shuzhang Zhong"
                    },
                    {
                        "name": "Yadong Zhang"
                    },
                    {
                        "name": "Peiyu Chen"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Le Ye"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li"
            },
            {
                "id": "http://arxiv.org/abs/2512.07312v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.07312v1",
                "title": "DCO: Dynamic Cache Orchestration for LLM Accelerators through Predictive Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DCO: Dynamic Cache Orchestration for LLM Accelerators through Predictive Management"
                },
                "updated": "2025-12-08T08:56:10Z",
                "updated_parsed": [
                    2025,
                    12,
                    8,
                    8,
                    56,
                    10,
                    0,
                    342,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.07312v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.07312v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The rapid adoption of large language models (LLMs) is pushing AI accelerators toward increasingly powerful and specialized designs. Instead of further complicating software development with deeply hierarchical scratchpad memories (SPMs) and their asynchronous management, we investigate the opposite point of the design spectrum: a multi-core AI accelerator equipped with a shared system-level cache and application-aware management policies, which keeps the programming effort modest. Our approach exploits dataflow information available in the software stack to guide cache replacement (including dead-block prediction), in concert with bypass decisions and mechanisms that alleviate cache thrashing.\n  We assess the proposal using a cycle-accurate simulator and observe substantial performance gains (up to 1.80x speedup) compared with conventional cache architectures. In addition, we build and validate an analytical model that takes into account the actual overlapping behaviors to extend the measurement results of our policies to real-world larger-scale workloads. Experiment results show that when functioning together, our bypassing and thrashing mitigation strategies can handle scenarios both with and without inter-core data sharing and achieve remarkable speedups.\n  Finally, we implement the design in RTL and the area of our design is $\\mathbf{0.064mm^2}$ with 15nm process, which can run at 2 GHz clock frequency. Our findings explore the potential of the shared cache design to assist the development of future AI accelerator systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid adoption of large language models (LLMs) is pushing AI accelerators toward increasingly powerful and specialized designs. Instead of further complicating software development with deeply hierarchical scratchpad memories (SPMs) and their asynchronous management, we investigate the opposite point of the design spectrum: a multi-core AI accelerator equipped with a shared system-level cache and application-aware management policies, which keeps the programming effort modest. Our approach exploits dataflow information available in the software stack to guide cache replacement (including dead-block prediction), in concert with bypass decisions and mechanisms that alleviate cache thrashing.\n  We assess the proposal using a cycle-accurate simulator and observe substantial performance gains (up to 1.80x speedup) compared with conventional cache architectures. In addition, we build and validate an analytical model that takes into account the actual overlapping behaviors to extend the measurement results of our policies to real-world larger-scale workloads. Experiment results show that when functioning together, our bypassing and thrashing mitigation strategies can handle scenarios both with and without inter-core data sharing and achieve remarkable speedups.\n  Finally, we implement the design in RTL and the area of our design is $\\mathbf{0.064mm^2}$ with 15nm process, which can run at 2 GHz clock frequency. Our findings explore the potential of the shared cache design to assist the development of future AI accelerator systems."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-08T08:56:10Z",
                "published_parsed": [
                    2025,
                    12,
                    8,
                    8,
                    56,
                    10,
                    0,
                    342,
                    0
                ],
                "arxiv_comment": "\\c{opyright} 2025 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works",
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Zhongchun Zhou"
                    },
                    {
                        "name": "Chengtao Lai"
                    },
                    {
                        "name": "Yuhang Gu"
                    },
                    {
                        "name": "Wei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Zhang"
                },
                "author": "Wei Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2512.07173v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.07173v1",
                "title": "Improving the Throughput of Diffusion-based Large Language Models via a Training-Free Confidence-Aware Calibration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving the Throughput of Diffusion-based Large Language Models via a Training-Free Confidence-Aware Calibration"
                },
                "updated": "2025-12-08T05:15:41Z",
                "updated_parsed": [
                    2025,
                    12,
                    8,
                    5,
                    15,
                    41,
                    0,
                    342,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.07173v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.07173v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We present CadLLM, a training-free method to accelerate the inference throughput of diffusion-based LLMs (dLLMs). We first investigate the dynamic nature of token unmasking confidence across blocks and steps. Based on this observation, we present a lightweight adaptive approach that controls the generation block size, step size, and threshold based on the average confidence of unmasked tokens. We further reduce softmax overhead by dynamically leveraging a subset of the vocabulary to regulate sampling breadth. CadLLM is a plug-and-play, model-agnostic method compatible with KV-cache-based dLLMs. Extensive experiments on four popular tasks demonstrate that CadLLM yields up to 2.28x throughput improvement over the state-of-the-art baseline with competitive accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present CadLLM, a training-free method to accelerate the inference throughput of diffusion-based LLMs (dLLMs). We first investigate the dynamic nature of token unmasking confidence across blocks and steps. Based on this observation, we present a lightweight adaptive approach that controls the generation block size, step size, and threshold based on the average confidence of unmasked tokens. We further reduce softmax overhead by dynamically leveraging a subset of the vocabulary to regulate sampling breadth. CadLLM is a plug-and-play, model-agnostic method compatible with KV-cache-based dLLMs. Extensive experiments on four popular tasks demonstrate that CadLLM yields up to 2.28x throughput improvement over the state-of-the-art baseline with competitive accuracy."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-08T05:15:41Z",
                "published_parsed": [
                    2025,
                    12,
                    8,
                    5,
                    15,
                    41,
                    0,
                    342,
                    0
                ],
                "arxiv_comment": "8 pages, 3 figures. Preprint under review",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Jucheng Shen"
                    },
                    {
                        "name": "Gaurav Sarkar"
                    },
                    {
                        "name": "Yeonju Ro"
                    },
                    {
                        "name": "Sharath Nittur Sridhar"
                    },
                    {
                        "name": "Zhangyang Wang"
                    },
                    {
                        "name": "Aditya Akella"
                    },
                    {
                        "name": "Souvik Kundu"
                    }
                ],
                "author_detail": {
                    "name": "Souvik Kundu"
                },
                "author": "Souvik Kundu"
            },
            {
                "id": "http://arxiv.org/abs/2508.09442v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.09442v3",
                "title": "Shadow in the Cache: Unveiling and Mitigating Privacy Risks of KV-cache in LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shadow in the Cache: Unveiling and Mitigating Privacy Risks of KV-cache in LLM Inference"
                },
                "updated": "2025-12-08T02:23:36Z",
                "updated_parsed": [
                    2025,
                    12,
                    8,
                    2,
                    23,
                    36,
                    0,
                    342,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.09442v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.09442v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The Key-Value (KV) cache, which stores intermediate attention computations (Key and Value pairs) to avoid redundant calculations, is a fundamental mechanism for accelerating Large Language Model (LLM) inference. However, this efficiency optimization introduces significant yet underexplored privacy risks. This paper provides the first comprehensive analysis of these vulnerabilities, demonstrating that an attacker can reconstruct sensitive user inputs directly from the KV-cache. We design and implement three distinct attack vectors: a direct Inversion Attack, a more broadly applicable and potent Collision Attack, and a semantic-based Injection Attack. These methods demonstrate the practicality and severity of KV-cache privacy leakage issues. To mitigate this, we propose KV-Cloak, a novel, lightweight, and efficient defense mechanism. KV-Cloak uses a reversible matrix-based obfuscation scheme, combined with operator fusion, to secure the KV-cache. Our extensive experiments show that KV-Cloak effectively thwarts all proposed attacks, reducing reconstruction quality to random noise. Crucially, it achieves this robust security with virtually no degradation in model accuracy and minimal performance overhead, offering a practical solution for trustworthy LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Key-Value (KV) cache, which stores intermediate attention computations (Key and Value pairs) to avoid redundant calculations, is a fundamental mechanism for accelerating Large Language Model (LLM) inference. However, this efficiency optimization introduces significant yet underexplored privacy risks. This paper provides the first comprehensive analysis of these vulnerabilities, demonstrating that an attacker can reconstruct sensitive user inputs directly from the KV-cache. We design and implement three distinct attack vectors: a direct Inversion Attack, a more broadly applicable and potent Collision Attack, and a semantic-based Injection Attack. These methods demonstrate the practicality and severity of KV-cache privacy leakage issues. To mitigate this, we propose KV-Cloak, a novel, lightweight, and efficient defense mechanism. KV-Cloak uses a reversible matrix-based obfuscation scheme, combined with operator fusion, to secure the KV-cache. Our extensive experiments show that KV-Cloak effectively thwarts all proposed attacks, reducing reconstruction quality to random noise. Crucially, it achieves this robust security with virtually no degradation in model accuracy and minimal performance overhead, offering a practical solution for trustworthy LLM deployment."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-13T02:48:25Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    2,
                    48,
                    25,
                    2,
                    225,
                    0
                ],
                "arxiv_comment": "This paper is accepted by Network and Distributed System Security Symposium (NDSS) 2026",
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Zhifan Luo"
                    },
                    {
                        "name": "Shuo Shao"
                    },
                    {
                        "name": "Su Zhang"
                    },
                    {
                        "name": "Lijing Zhou"
                    },
                    {
                        "name": "Yuke Hu"
                    },
                    {
                        "name": "Chenxu Zhao"
                    },
                    {
                        "name": "Zhihao Liu"
                    },
                    {
                        "name": "Zhan Qin"
                    }
                ],
                "author_detail": {
                    "name": "Zhan Qin"
                },
                "author": "Zhan Qin"
            },
            {
                "id": "http://arxiv.org/abs/2512.07090v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.07090v1",
                "title": "Leveraging KV Similarity for Online Structured Pruning in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging KV Similarity for Online Structured Pruning in LLMs"
                },
                "updated": "2025-12-08T01:56:27Z",
                "updated_parsed": [
                    2025,
                    12,
                    8,
                    1,
                    56,
                    27,
                    0,
                    342,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.07090v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.07090v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Pruning has emerged as a promising direction for accelerating large language model (LLM) inference, yet existing approaches often suffer from instability because they rely on offline calibration data that may not generalize across inputs. In this work, we introduce Token Filtering, a lightweight online structured pruning technique that makes pruning decisions directly during inference without any calibration data. The key idea is to measure token redundancy via joint key-value similarity and skip redundant attention computations, thereby reducing inference cost while preserving critical information. To further enhance stability, we design a variance-aware fusion strategy that adaptively weights key and value similarity across heads, ensuring that informative tokens are retained even under high pruning ratios. This design introduces no additional memory overhead and provides a more reliable criterion for token importance. Extensive experiments on LLaMA-2 (7B/13B), LLaMA-3 (8B), and Mistral (7B) demonstrate that Token Filtering consistently outperforms prior structured pruning methods, preserving accuracy on commonsense reasoning benchmarks and maintaining strong performance on challenging tasks such as MMLU, even with 50% pruning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pruning has emerged as a promising direction for accelerating large language model (LLM) inference, yet existing approaches often suffer from instability because they rely on offline calibration data that may not generalize across inputs. In this work, we introduce Token Filtering, a lightweight online structured pruning technique that makes pruning decisions directly during inference without any calibration data. The key idea is to measure token redundancy via joint key-value similarity and skip redundant attention computations, thereby reducing inference cost while preserving critical information. To further enhance stability, we design a variance-aware fusion strategy that adaptively weights key and value similarity across heads, ensuring that informative tokens are retained even under high pruning ratios. This design introduces no additional memory overhead and provides a more reliable criterion for token importance. Extensive experiments on LLaMA-2 (7B/13B), LLaMA-3 (8B), and Mistral (7B) demonstrate that Token Filtering consistently outperforms prior structured pruning methods, preserving accuracy on commonsense reasoning benchmarks and maintaining strong performance on challenging tasks such as MMLU, even with 50% pruning."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-08T01:56:27Z",
                "published_parsed": [
                    2025,
                    12,
                    8,
                    1,
                    56,
                    27,
                    0,
                    342,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Jungmin Lee"
                    },
                    {
                        "name": "Gwangeun Byeon"
                    },
                    {
                        "name": "Yulhwa Kim"
                    },
                    {
                        "name": "Seokin Hong"
                    }
                ],
                "author_detail": {
                    "name": "Seokin Hong"
                },
                "author": "Seokin Hong"
            },
            {
                "id": "http://arxiv.org/abs/2512.02924v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.02924v2",
                "title": "AutoNeural: Co-Designing Vision-Language Models for NPU Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoNeural: Co-Designing Vision-Language Models for NPU Inference"
                },
                "updated": "2025-12-08T00:15:33Z",
                "updated_parsed": [
                    2025,
                    12,
                    8,
                    0,
                    15,
                    33,
                    0,
                    342,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.02924v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.02924v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "While Neural Processing Units (NPUs) offer high theoretical efficiency for edge AI, state-of-the-art Vision--Language Models (VLMs) tailored for GPUs often falter on these substrates. We attribute this hardware-model mismatch to two primary factors: the quantization brittleness of Vision Transformers (ViTs) and the I/O-bound nature of autoregressive attention mechanisms, which fail to utilize the high arithmetic throughput of NPUs. To bridge this gap, we propose AutoNeural, an NPU-native VLM architecture co-designed for integer-only inference. We replace the standard ViT encoder with a MobileNetV5-style backbone utilizing depthwise separable convolutions, which ensures bounded activation distributions for stable INT4/8/16 quantization. Complementing this, our language backbone integrates State-Space Model (SSM) principles with Transformer layers, employing efficient gated convolutions to achieve linear-time complexity. This hybrid design eliminates the heavy memory I/O overhead of Key-Value caching during generation. Our approach delivers substantial efficiency gains, reducing quantization error of vision encoder by up to 7x and end-to-end latency by 14x compared to conventional baselines. The AutoNeural also delivers 3x decoding speed and 4x longer context window than the baseline. We validate these improvements via a real-world automotive case study on the Qualcomm SA8295P SoC, demonstrating real-time performance for cockpit applications. Our results highlight that rethinking model topology specifically for NPU constraints is a prerequisite for robust multi-modal edge intelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Neural Processing Units (NPUs) offer high theoretical efficiency for edge AI, state-of-the-art Vision--Language Models (VLMs) tailored for GPUs often falter on these substrates. We attribute this hardware-model mismatch to two primary factors: the quantization brittleness of Vision Transformers (ViTs) and the I/O-bound nature of autoregressive attention mechanisms, which fail to utilize the high arithmetic throughput of NPUs. To bridge this gap, we propose AutoNeural, an NPU-native VLM architecture co-designed for integer-only inference. We replace the standard ViT encoder with a MobileNetV5-style backbone utilizing depthwise separable convolutions, which ensures bounded activation distributions for stable INT4/8/16 quantization. Complementing this, our language backbone integrates State-Space Model (SSM) principles with Transformer layers, employing efficient gated convolutions to achieve linear-time complexity. This hybrid design eliminates the heavy memory I/O overhead of Key-Value caching during generation. Our approach delivers substantial efficiency gains, reducing quantization error of vision encoder by up to 7x and end-to-end latency by 14x compared to conventional baselines. The AutoNeural also delivers 3x decoding speed and 4x longer context window than the baseline. We validate these improvements via a real-world automotive case study on the Qualcomm SA8295P SoC, demonstrating real-time performance for cockpit applications. Our results highlight that rethinking model topology specifically for NPU constraints is a prerequisite for robust multi-modal edge intelligence."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-02T16:45:25Z",
                "published_parsed": [
                    2025,
                    12,
                    2,
                    16,
                    45,
                    25,
                    1,
                    336,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Liangmin Wu"
                    },
                    {
                        "name": "Yunhai Hu"
                    },
                    {
                        "name": "Zhiyuan Li"
                    },
                    {
                        "name": "Zhiyuan Cheng"
                    },
                    {
                        "name": "Yicheng Qian"
                    },
                    {
                        "name": "Lingyue Zhu"
                    },
                    {
                        "name": "Zhipeng Hu"
                    },
                    {
                        "name": "Luoyi Liang"
                    },
                    {
                        "name": "Qiang Tang"
                    },
                    {
                        "name": "Zhen Liu"
                    },
                    {
                        "name": "Han Yang"
                    }
                ],
                "author_detail": {
                    "name": "Han Yang"
                },
                "author": "Han Yang"
            },
            {
                "id": "http://arxiv.org/abs/2511.01266v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.01266v2",
                "title": "MotionStream: Real-Time Video Generation with Interactive Motion Controls",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MotionStream: Real-Time Video Generation with Interactive Motion Controls"
                },
                "updated": "2025-12-08T00:05:23Z",
                "updated_parsed": [
                    2025,
                    12,
                    8,
                    0,
                    5,
                    23,
                    0,
                    342,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.01266v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.01266v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Current motion-conditioned video generation methods suffer from prohibitive latency (minutes per video) and non-causal processing that prevents real-time interaction. We present MotionStream, enabling sub-second latency with up to 29 FPS streaming generation on a single GPU. Our approach begins by augmenting a text-to-video model with motion control, which generates high-quality videos that adhere to the global text prompt and local motion guidance, but does not perform inference on the fly. As such, we distill this bidirectional teacher into a causal student through Self Forcing with Distribution Matching Distillation, enabling real-time streaming inference. Several key challenges arise when generating videos of long, potentially infinite time-horizons -- (1) bridging the domain gap from training on finite length and extrapolating to infinite horizons, (2) sustaining high quality by preventing error accumulation, and (3) maintaining fast inference, without incurring growth in computational cost due to increasing context windows. A key to our approach is introducing carefully designed sliding-window causal attention, combined with attention sinks. By incorporating self-rollout with attention sinks and KV cache rolling during training, we properly simulate inference-time extrapolations with a fixed context window, enabling constant-speed generation of arbitrarily long videos. Our models achieve state-of-the-art results in motion following and video quality while being two orders of magnitude faster, uniquely enabling infinite-length streaming. With MotionStream, users can paint trajectories, control cameras, or transfer motion, and see results unfold in real-time, delivering a truly interactive experience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current motion-conditioned video generation methods suffer from prohibitive latency (minutes per video) and non-causal processing that prevents real-time interaction. We present MotionStream, enabling sub-second latency with up to 29 FPS streaming generation on a single GPU. Our approach begins by augmenting a text-to-video model with motion control, which generates high-quality videos that adhere to the global text prompt and local motion guidance, but does not perform inference on the fly. As such, we distill this bidirectional teacher into a causal student through Self Forcing with Distribution Matching Distillation, enabling real-time streaming inference. Several key challenges arise when generating videos of long, potentially infinite time-horizons -- (1) bridging the domain gap from training on finite length and extrapolating to infinite horizons, (2) sustaining high quality by preventing error accumulation, and (3) maintaining fast inference, without incurring growth in computational cost due to increasing context windows. A key to our approach is introducing carefully designed sliding-window causal attention, combined with attention sinks. By incorporating self-rollout with attention sinks and KV cache rolling during training, we properly simulate inference-time extrapolations with a fixed context window, enabling constant-speed generation of arbitrarily long videos. Our models achieve state-of-the-art results in motion following and video quality while being two orders of magnitude faster, uniquely enabling infinite-length streaming. With MotionStream, users can paint trajectories, control cameras, or transfer motion, and see results unfold in real-time, delivering a truly interactive experience."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-03T06:37:53Z",
                "published_parsed": [
                    2025,
                    11,
                    3,
                    6,
                    37,
                    53,
                    0,
                    307,
                    0
                ],
                "arxiv_comment": "Project webpage: https://joonghyuk.com/motionstream-web/",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Joonghyuk Shin"
                    },
                    {
                        "name": "Zhengqi Li"
                    },
                    {
                        "name": "Richard Zhang"
                    },
                    {
                        "name": "Jun-Yan Zhu"
                    },
                    {
                        "name": "Jaesik Park"
                    },
                    {
                        "name": "Eli Shechtman"
                    },
                    {
                        "name": "Xun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xun Huang"
                },
                "author": "Xun Huang"
            },
            {
                "id": "http://arxiv.org/abs/2512.09946v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.09946v1",
                "title": "ELANA: A Simple Energy and Latency Analyzer for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ELANA: A Simple Energy and Latency Analyzer for LLMs"
                },
                "updated": "2025-12-07T18:43:47Z",
                "updated_parsed": [
                    2025,
                    12,
                    7,
                    18,
                    43,
                    47,
                    6,
                    341,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.09946v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.09946v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The latency and power consumption of large language models (LLMs) are major constraints when serving them across a wide spectrum of hardware platforms, from mobile edge devices to cloud GPU clusters. Benchmarking is crucial for optimizing efficiency in both model deployment and next-generation model development. To address this need, we open-source a simple profiling tool, \\textbf{ELANA}, for evaluating LLMs. ELANA is designed as a lightweight, academic-friendly profiler for analyzing model size, key-value (KV) cache size, prefilling latency (Time-to-first-token, TTFT), generation latency (Time-per-output-token, TPOT), and end-to-end latency (Time-to-last-token, TTLT) of LLMs on both multi-GPU and edge GPU platforms. It supports all publicly available models on Hugging Face and offers a simple command-line interface, along with optional energy consumption logging. Moreover, ELANA is fully compatible with popular Hugging Face APIs and can be easily customized or adapted to compressed or low bit-width models, making it ideal for research on efficient LLMs or for small-scale proof-of-concept studies. We release the ELANA profiling tool at: https://github.com/enyac-group/Elana.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The latency and power consumption of large language models (LLMs) are major constraints when serving them across a wide spectrum of hardware platforms, from mobile edge devices to cloud GPU clusters. Benchmarking is crucial for optimizing efficiency in both model deployment and next-generation model development. To address this need, we open-source a simple profiling tool, \\textbf{ELANA}, for evaluating LLMs. ELANA is designed as a lightweight, academic-friendly profiler for analyzing model size, key-value (KV) cache size, prefilling latency (Time-to-first-token, TTFT), generation latency (Time-per-output-token, TPOT), and end-to-end latency (Time-to-last-token, TTLT) of LLMs on both multi-GPU and edge GPU platforms. It supports all publicly available models on Hugging Face and offers a simple command-line interface, along with optional energy consumption logging. Moreover, ELANA is fully compatible with popular Hugging Face APIs and can be easily customized or adapted to compressed or low bit-width models, making it ideal for research on efficient LLMs or for small-scale proof-of-concept studies. We release the ELANA profiling tool at: https://github.com/enyac-group/Elana."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-07T18:43:47Z",
                "published_parsed": [
                    2025,
                    12,
                    7,
                    18,
                    43,
                    47,
                    6,
                    341,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Hung-Yueh Chiang"
                    },
                    {
                        "name": "Bokun Wang"
                    },
                    {
                        "name": "Diana Marculescu"
                    }
                ],
                "author_detail": {
                    "name": "Diana Marculescu"
                },
                "author": "Diana Marculescu"
            },
            {
                "id": "http://arxiv.org/abs/2512.06865v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.06865v1",
                "title": "Spatial Retrieval Augmented Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatial Retrieval Augmented Autonomous Driving"
                },
                "updated": "2025-12-07T14:40:49Z",
                "updated_parsed": [
                    2025,
                    12,
                    7,
                    14,
                    40,
                    49,
                    6,
                    341,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.06865v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.06865v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Existing autonomous driving systems rely on onboard sensors (cameras, LiDAR, IMU, etc) for environmental perception. However, this paradigm is limited by the drive-time perception horizon and often fails under limited view scope, occlusion or extreme conditions such as darkness and rain. In contrast, human drivers are able to recall road structure even under poor visibility. To endow models with this ``recall\" ability, we propose the spatial retrieval paradigm, introducing offline retrieved geographic images as an additional input. These images are easy to obtain from offline caches (e.g, Google Maps or stored autonomous driving datasets) without requiring additional sensors, making it a plug-and-play extension for existing AD tasks.\n  For experiments, we first extend the nuScenes dataset with geographic images retrieved via Google Maps APIs and align the new data with ego-vehicle trajectories. We establish baselines across five core autonomous driving tasks: object detection, online mapping, occupancy prediction, end-to-end planning, and generative world modeling. Extensive experiments show that the extended modality could enhance the performance of certain tasks. We will open-source dataset curation code, data, and benchmarks for further study of this new autonomous driving paradigm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing autonomous driving systems rely on onboard sensors (cameras, LiDAR, IMU, etc) for environmental perception. However, this paradigm is limited by the drive-time perception horizon and often fails under limited view scope, occlusion or extreme conditions such as darkness and rain. In contrast, human drivers are able to recall road structure even under poor visibility. To endow models with this ``recall\" ability, we propose the spatial retrieval paradigm, introducing offline retrieved geographic images as an additional input. These images are easy to obtain from offline caches (e.g, Google Maps or stored autonomous driving datasets) without requiring additional sensors, making it a plug-and-play extension for existing AD tasks.\n  For experiments, we first extend the nuScenes dataset with geographic images retrieved via Google Maps APIs and align the new data with ego-vehicle trajectories. We establish baselines across five core autonomous driving tasks: object detection, online mapping, occupancy prediction, end-to-end planning, and generative world modeling. Extensive experiments show that the extended modality could enhance the performance of certain tasks. We will open-source dataset curation code, data, and benchmarks for further study of this new autonomous driving paradigm."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-07T14:40:49Z",
                "published_parsed": [
                    2025,
                    12,
                    7,
                    14,
                    40,
                    49,
                    6,
                    341,
                    0
                ],
                "arxiv_comment": "Demo Page: https://spatialretrievalad.github.io/ with open sourced code, dataset, and checkpoints",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Xiaosong Jia"
                    },
                    {
                        "name": "Chenhe Zhang"
                    },
                    {
                        "name": "Yule Jiang"
                    },
                    {
                        "name": "Songbur Wong"
                    },
                    {
                        "name": "Zhiyuan Zhang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Shaofeng Zhang"
                    },
                    {
                        "name": "Xuanhe Zhou"
                    },
                    {
                        "name": "Xue Yang"
                    },
                    {
                        "name": "Junchi Yan"
                    },
                    {
                        "name": "Yu-Gang Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Gang Jiang"
                },
                "author": "Yu-Gang Jiang"
            },
            {
                "id": "http://arxiv.org/abs/2508.10963v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.10963v2",
                "title": "EVCtrl: Efficient Control Adapter for Visual Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EVCtrl: Efficient Control Adapter for Visual Generation"
                },
                "updated": "2025-12-07T14:21:14Z",
                "updated_parsed": [
                    2025,
                    12,
                    7,
                    14,
                    21,
                    14,
                    6,
                    341,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.10963v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.10963v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Visual generation includes both image and video generation, training probabilistic models to create coherent, diverse, and semantically faithful content from scratch. While early research focused on unconditional sampling, practitioners now demand controllable generation that allows precise specification of layout, pose, motion, or style. While ControlNet grants precise spatial-temporal control, its auxiliary branch markedly increases latency and introduces redundant computation in both uncontrolled regions and denoising steps, especially for video. To address this problem, we introduce EVCtrl, a lightweight, plug-and-play control adapter that slashes overhead without retraining the model. Specifically, we propose a spatio-temporal dual caching strategy for sparse control information. For spatial redundancy, we first profile how each layer of DiT-ControlNet responds to fine-grained control, then partition the network into global and local functional zones. A locality-aware cache focuses computation on the local zones that truly need the control signal, skipping the bulk of redundant computation in global regions. For temporal redundancy, we selectively omit unnecessary denoising steps to improve efficiency. Extensive experiments on CogVideo-Controlnet, Wan2.1-Controlnet, and Flux demonstrate that our method is effective in image and video control generation without the need for training. For example, it achieves 2.16 and 2.05 times speedups on CogVideo-Controlnet and Wan2.1-Controlnet, respectively, with almost no degradation in generation quality.Codes are available in the supplementary materials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual generation includes both image and video generation, training probabilistic models to create coherent, diverse, and semantically faithful content from scratch. While early research focused on unconditional sampling, practitioners now demand controllable generation that allows precise specification of layout, pose, motion, or style. While ControlNet grants precise spatial-temporal control, its auxiliary branch markedly increases latency and introduces redundant computation in both uncontrolled regions and denoising steps, especially for video. To address this problem, we introduce EVCtrl, a lightweight, plug-and-play control adapter that slashes overhead without retraining the model. Specifically, we propose a spatio-temporal dual caching strategy for sparse control information. For spatial redundancy, we first profile how each layer of DiT-ControlNet responds to fine-grained control, then partition the network into global and local functional zones. A locality-aware cache focuses computation on the local zones that truly need the control signal, skipping the bulk of redundant computation in global regions. For temporal redundancy, we selectively omit unnecessary denoising steps to improve efficiency. Extensive experiments on CogVideo-Controlnet, Wan2.1-Controlnet, and Flux demonstrate that our method is effective in image and video control generation without the need for training. For example, it achieves 2.16 and 2.05 times speedups on CogVideo-Controlnet and Wan2.1-Controlnet, respectively, with almost no degradation in generation quality.Codes are available in the supplementary materials."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-14T14:11:48Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    14,
                    11,
                    48,
                    3,
                    226,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Zixiang Yang"
                    },
                    {
                        "name": "Yue Ma"
                    },
                    {
                        "name": "Yinhan Zhang"
                    },
                    {
                        "name": "Shanhui Mo"
                    },
                    {
                        "name": "Dongrui Liu"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2504.09261v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2504.09261v2",
                "title": "Head-Aware KV Cache Compression for Efficient Visual Autoregressive Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Head-Aware KV Cache Compression for Efficient Visual Autoregressive Modeling"
                },
                "updated": "2025-12-07T12:44:02Z",
                "updated_parsed": [
                    2025,
                    12,
                    7,
                    12,
                    44,
                    2,
                    6,
                    341,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2504.09261v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2504.09261v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Visual Autoregressive (VAR) models adopt a next-scale prediction paradigm, offering high-quality content generation with substantially fewer decoding steps. However, existing VAR models suffer from significant attention complexity and severe memory overhead due to the accumulation of key-value (KV) caches across scales. In this paper, we tackle this challenge by introducing KV cache compression into the next-scale generation paradigm. We begin with a crucial observation: attention heads in VAR models can be divided into two functionally distinct categories: Contextual Heads focus on maintaining semantic consistency, while Structural Heads are responsible for preserving spatial coherence. This structural divergence causes existing one-size-fits-all compression methods to perform poorly on VAR models. To address this, we propose HACK, a training-free Head-Aware KV cache Compression frameworK. HACK utilizes an offline classification scheme to separate head types, enabling it to apply pattern-specific compression strategies with asymmetric cache budgets for each category. By doing so, HACK effectively constrains the average KV cache length within a fixed budget $B$, reducing the theoretical attention complexity from $\\mathcal{O}(n^4)$ to $\\mathcal{O}(Bn^2)$. Extensive experiments on multiple VAR models across text-to-image and class-conditional tasks validate the effectiveness and generalizability of HACK. It achieves up to 70% KV cache compression without degrading output quality, resulting in memory savings and faster inference. For example, HACK provides a $1.75\\times$ memory reduction and a $1.57\\times$ speedup on Infinity-8B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Autoregressive (VAR) models adopt a next-scale prediction paradigm, offering high-quality content generation with substantially fewer decoding steps. However, existing VAR models suffer from significant attention complexity and severe memory overhead due to the accumulation of key-value (KV) caches across scales. In this paper, we tackle this challenge by introducing KV cache compression into the next-scale generation paradigm. We begin with a crucial observation: attention heads in VAR models can be divided into two functionally distinct categories: Contextual Heads focus on maintaining semantic consistency, while Structural Heads are responsible for preserving spatial coherence. This structural divergence causes existing one-size-fits-all compression methods to perform poorly on VAR models. To address this, we propose HACK, a training-free Head-Aware KV cache Compression frameworK. HACK utilizes an offline classification scheme to separate head types, enabling it to apply pattern-specific compression strategies with asymmetric cache budgets for each category. By doing so, HACK effectively constrains the average KV cache length within a fixed budget $B$, reducing the theoretical attention complexity from $\\mathcal{O}(n^4)$ to $\\mathcal{O}(Bn^2)$. Extensive experiments on multiple VAR models across text-to-image and class-conditional tasks validate the effectiveness and generalizability of HACK. It achieves up to 70% KV cache compression without degrading output quality, resulting in memory savings and faster inference. For example, HACK provides a $1.75\\times$ memory reduction and a $1.57\\times$ speedup on Infinity-8B."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-04-12T15:42:17Z",
                "published_parsed": [
                    2025,
                    4,
                    12,
                    15,
                    42,
                    17,
                    5,
                    102,
                    0
                ],
                "arxiv_comment": "Accepted by AAAI 2026",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Ziran Qin"
                    },
                    {
                        "name": "Youru Lv"
                    },
                    {
                        "name": "Mingbao Lin"
                    },
                    {
                        "name": "Hang Guo"
                    },
                    {
                        "name": "Zeren Zhang"
                    },
                    {
                        "name": "Danping Zou"
                    },
                    {
                        "name": "Weiyao Lin"
                    }
                ],
                "author_detail": {
                    "name": "Weiyao Lin"
                },
                "author": "Weiyao Lin"
            },
            {
                "id": "http://arxiv.org/abs/2512.06727v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.06727v1",
                "title": "KV-CAR: KV Cache Compression using Autoencoders and KV Reuse in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-CAR: KV Cache Compression using Autoencoders and KV Reuse in Large Language Models"
                },
                "updated": "2025-12-07T08:40:52Z",
                "updated_parsed": [
                    2025,
                    12,
                    7,
                    8,
                    40,
                    52,
                    6,
                    341,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.06727v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.06727v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As Large Language Models (LLMs) scale in size and context length, the memory requirements of the key value (KV) cache have emerged as a major bottleneck during autoregressive decoding. The KV cache grows with sequence length and embedding dimension, often exceeding the memory footprint of the model itself and limiting achievable batch sizes and context windows. To address this challenge, we present KV CAR, a unified and architecture agnostic framework that significantly reduces KV cache storage while maintaining model fidelity. KV CAR combines two complementary techniques. First, a lightweight autoencoder learns compact representations of key and value tensors along the embedding dimension, compressing them before they are stored in the KV cache and restoring them upon retrieval. Second, a similarity driven reuse mechanism identifies opportunities to reuse KV tensors of specific attention heads across adjacent layers. Together, these methods reduce the dimensional and structural redundancy in KV tensors without requiring changes to the transformer architecture. Evaluations on GPT 2 and TinyLLaMA models across Wikitext, C4, PIQA, and Winogrande datasets demonstrate that KV CAR achieves up to 47.85 percent KV cache memory reduction with minimal impact on perplexity and zero shot accuracy. System level measurements on an NVIDIA A40 GPU show that the reduced KV footprint directly translates into longer sequence lengths and larger batch sizes during inference. These results highlight the effectiveness of KV CAR in enabling memory efficient LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) scale in size and context length, the memory requirements of the key value (KV) cache have emerged as a major bottleneck during autoregressive decoding. The KV cache grows with sequence length and embedding dimension, often exceeding the memory footprint of the model itself and limiting achievable batch sizes and context windows. To address this challenge, we present KV CAR, a unified and architecture agnostic framework that significantly reduces KV cache storage while maintaining model fidelity. KV CAR combines two complementary techniques. First, a lightweight autoencoder learns compact representations of key and value tensors along the embedding dimension, compressing them before they are stored in the KV cache and restoring them upon retrieval. Second, a similarity driven reuse mechanism identifies opportunities to reuse KV tensors of specific attention heads across adjacent layers. Together, these methods reduce the dimensional and structural redundancy in KV tensors without requiring changes to the transformer architecture. Evaluations on GPT 2 and TinyLLaMA models across Wikitext, C4, PIQA, and Winogrande datasets demonstrate that KV CAR achieves up to 47.85 percent KV cache memory reduction with minimal impact on perplexity and zero shot accuracy. System level measurements on an NVIDIA A40 GPU show that the reduced KV footprint directly translates into longer sequence lengths and larger batch sizes during inference. These results highlight the effectiveness of KV CAR in enabling memory efficient LLM inference."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-07T08:40:52Z",
                "published_parsed": [
                    2025,
                    12,
                    7,
                    8,
                    40,
                    52,
                    6,
                    341,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Sourjya Roy"
                    },
                    {
                        "name": "Shrihari Sridharan"
                    },
                    {
                        "name": "Surya Selvam"
                    },
                    {
                        "name": "Anand Raghunathan"
                    }
                ],
                "author_detail": {
                    "name": "Anand Raghunathan"
                },
                "author": "Anand Raghunathan"
            },
            {
                "id": "http://arxiv.org/abs/2512.06664v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.06664v1",
                "title": "Statistic-Augmented, Decoupled MoE Routing and Aggregating in Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistic-Augmented, Decoupled MoE Routing and Aggregating in Autonomous Driving"
                },
                "updated": "2025-12-07T05:28:40Z",
                "updated_parsed": [
                    2025,
                    12,
                    7,
                    5,
                    28,
                    40,
                    6,
                    341,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.06664v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.06664v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Autonomous driving (AD) scenarios are inherently complex and diverse, posing significant challenges for a single deep learning model to effectively cover all possible conditions, such as varying weather, traffic densities, and road types. Large Model (LM)-Driven Mixture of Experts (MoE) paradigm offers a promising solution, where LM serves as the backbone to extract latent features while MoE serves as the downstream head to dynamically select and aggregate specialized experts to adapt to different scenarios. However, routing and aggregating in MoE face intrinsic challenges, including imprecise expert selection due to flawed routing strategy and inefficient expert aggregation leading to suboptimal prediction. To address these issues, we propose a statistic-augmented, decoupled MoE }outing and Aggregating Mechanism (MoE-RAM) driven by LM. Specifically, on the one hand, MoE-RAM enhances expert routing by incorporating statistical retrieval mechanism to match LM-extracted latent features with cached prototypical features of the most relevant experts; on the other hand, MoE-RAM adaptively reweights experts' outputs in fusion by measuring statistical distances of experts' instant features against LM-extracted latent features. Benefiting from the synergy of the statistic-augmented MoE's routing and aggregating, MoE-RAM ultimately improves the prediction performance. We take the AD semantic segmentation task as an example to assess the proposed MoE-RAM. Extensive experiments on AD datasets demonstrate the superiority of MoE-RAM compared to other MoE baselines and conventional single-model approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous driving (AD) scenarios are inherently complex and diverse, posing significant challenges for a single deep learning model to effectively cover all possible conditions, such as varying weather, traffic densities, and road types. Large Model (LM)-Driven Mixture of Experts (MoE) paradigm offers a promising solution, where LM serves as the backbone to extract latent features while MoE serves as the downstream head to dynamically select and aggregate specialized experts to adapt to different scenarios. However, routing and aggregating in MoE face intrinsic challenges, including imprecise expert selection due to flawed routing strategy and inefficient expert aggregation leading to suboptimal prediction. To address these issues, we propose a statistic-augmented, decoupled MoE }outing and Aggregating Mechanism (MoE-RAM) driven by LM. Specifically, on the one hand, MoE-RAM enhances expert routing by incorporating statistical retrieval mechanism to match LM-extracted latent features with cached prototypical features of the most relevant experts; on the other hand, MoE-RAM adaptively reweights experts' outputs in fusion by measuring statistical distances of experts' instant features against LM-extracted latent features. Benefiting from the synergy of the statistic-augmented MoE's routing and aggregating, MoE-RAM ultimately improves the prediction performance. We take the AD semantic segmentation task as an example to assess the proposed MoE-RAM. Extensive experiments on AD datasets demonstrate the superiority of MoE-RAM compared to other MoE baselines and conventional single-model approaches."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-07T05:28:40Z",
                "published_parsed": [
                    2025,
                    12,
                    7,
                    5,
                    28,
                    40,
                    6,
                    341,
                    0
                ],
                "arxiv_comment": "9 pages",
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Wei-Bin Kou"
                    },
                    {
                        "name": "Guangxu Zhu"
                    },
                    {
                        "name": "Jingreng Lei"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Yik-Chung Wu"
                    },
                    {
                        "name": "Jianping Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jianping Wang"
                },
                "author": "Jianping Wang"
            },
            {
                "id": "http://arxiv.org/abs/2512.06523v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.06523v1",
                "title": "Solving larger Travelling Salesman Problem networks with a penalty-free Variational Quantum Algorithm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Solving larger Travelling Salesman Problem networks with a penalty-free Variational Quantum Algorithm"
                },
                "updated": "2025-12-06T18:21:21Z",
                "updated_parsed": [
                    2025,
                    12,
                    6,
                    18,
                    21,
                    21,
                    5,
                    340,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.06523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.06523v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The Travelling Salesman Problem (TSP) is a well-known NP-Hard combinatorial optimisation problem, with industrial use cases such as last-mile delivery. Although TSP has been studied extensively on quantum computers, it is rare to find quantum solutions of TSP network with more than a dozen locations. In this paper, we present high quality solutions in noise-free Qiskit simulations of networks with up to twelve locations using a hybrid penalty-free, circuit-model, Variational Quantum Algorithm (VQA). Noisy qubits are also simulated. To our knowledge, this is the first successful VQA simulation of a twelve-location TSP on circuit-model devices. Multiple encoding strategies, including factorial, non-factorial, and Gray encoding are evaluated. Our formulation scales as $\\mathcal{O}(nlog_2(n))$ qubits, requiring only 29 qubits for twelve locations, compared with over 100 qubits for conventional approaches scaling as $\\mathcal{O}(n^2)$. Computational time is further reduced by almost two orders of magnitude through the use of Simultaneous Perturbation Stochastic Approximation (SPSA) gradient estimation and cost-function caching. We also introduce a novel machine-learning model, and benchmark both quantum and classical approaches against a Monte Carlo baseline. The VQA outperforms the classical machine-learning approach, and performs similarly to Monte Carlo for the small networks simulated. Additionally, the results indicate a trend toward improved performance with problem size, outlining a pathway to solving larger TSP instances on quantum devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Travelling Salesman Problem (TSP) is a well-known NP-Hard combinatorial optimisation problem, with industrial use cases such as last-mile delivery. Although TSP has been studied extensively on quantum computers, it is rare to find quantum solutions of TSP network with more than a dozen locations. In this paper, we present high quality solutions in noise-free Qiskit simulations of networks with up to twelve locations using a hybrid penalty-free, circuit-model, Variational Quantum Algorithm (VQA). Noisy qubits are also simulated. To our knowledge, this is the first successful VQA simulation of a twelve-location TSP on circuit-model devices. Multiple encoding strategies, including factorial, non-factorial, and Gray encoding are evaluated. Our formulation scales as $\\mathcal{O}(nlog_2(n))$ qubits, requiring only 29 qubits for twelve locations, compared with over 100 qubits for conventional approaches scaling as $\\mathcal{O}(n^2)$. Computational time is further reduced by almost two orders of magnitude through the use of Simultaneous Perturbation Stochastic Approximation (SPSA) gradient estimation and cost-function caching. We also introduce a novel machine-learning model, and benchmark both quantum and classical approaches against a Monte Carlo baseline. The VQA outperforms the classical machine-learning approach, and performs similarly to Monte Carlo for the small networks simulated. Additionally, the results indicate a trend toward improved performance with problem size, outlining a pathway to solving larger TSP instances on quantum devices."
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-06T18:21:21Z",
                "published_parsed": [
                    2025,
                    12,
                    6,
                    18,
                    21,
                    21,
                    5,
                    340,
                    0
                ],
                "arxiv_comment": "59 pages and 11 figures",
                "arxiv_primary_category": {
                    "term": "quant-ph"
                },
                "authors": [
                    {
                        "name": "Daniel Goldsmith"
                    },
                    {
                        "name": "Xing Liang"
                    },
                    {
                        "name": "Dimitrios Makris"
                    },
                    {
                        "name": "Hongwei Wu"
                    }
                ],
                "author_detail": {
                    "name": "Hongwei Wu"
                },
                "author": "Hongwei Wu"
            },
            {
                "id": "http://arxiv.org/abs/2512.06468v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.06468v1",
                "title": "Convolution operators preserving the set of totally positive sequences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Convolution operators preserving the set of totally positive sequences"
                },
                "updated": "2025-12-06T15:13:02Z",
                "updated_parsed": [
                    2025,
                    12,
                    6,
                    15,
                    13,
                    2,
                    5,
                    340,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.06468v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.06468v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "A real sequence $(a_k)_{k=0}^\\infty$ is called {\\it totally positive} if all minors of the infinite Toeplitz matrix $ \\left\\| a_{j-i} \\right\\|_{i, j =0}^\\infty$ are nonnegative (here $a_k=0$ for $k<0$). In this paper, which continues our earlier work \\cite{kv}, we investigate the set of real sequences $(b_k)_{k=0}^\\infty$ with the property that for every totally positive sequence $(a_k)_{k=0}^\\infty,$ the sequense of termwise products $(a_k b_k)_{k=0}^\\infty$ is also totally positive. In particular, we show that for every totally positive sequence $(a_k)_{k=0}^\\infty$ the sequence $\\left(a_k a^{-k (k-1)}\\right)_{k=0}^\\infty$ is totally positive whenever $a^2\\geq 3{.}503.$ We also propose several open problems concerning convolution operators that preserve total positivity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A real sequence $(a_k)_{k=0}^\\infty$ is called {\\it totally positive} if all minors of the infinite Toeplitz matrix $ \\left\\| a_{j-i} \\right\\|_{i, j =0}^\\infty$ are nonnegative (here $a_k=0$ for $k<0$). In this paper, which continues our earlier work \\cite{kv}, we investigate the set of real sequences $(b_k)_{k=0}^\\infty$ with the property that for every totally positive sequence $(a_k)_{k=0}^\\infty,$ the sequense of termwise products $(a_k b_k)_{k=0}^\\infty$ is also totally positive. In particular, we show that for every totally positive sequence $(a_k)_{k=0}^\\infty$ the sequence $\\left(a_k a^{-k (k-1)}\\right)_{k=0}^\\infty$ is totally positive whenever $a^2\\geq 3{.}503.$ We also propose several open problems concerning convolution operators that preserve total positivity."
                },
                "tags": [
                    {
                        "term": "math.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.FA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-06T15:13:02Z",
                "published_parsed": [
                    2025,
                    12,
                    6,
                    15,
                    13,
                    2,
                    5,
                    340,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "math.CV"
                },
                "authors": [
                    {
                        "name": "Olga Katkova"
                    },
                    {
                        "name": "Anna Vishnyakova"
                    }
                ],
                "author_detail": {
                    "name": "Anna Vishnyakova"
                },
                "author": "Anna Vishnyakova"
            },
            {
                "id": "http://arxiv.org/abs/2512.06443v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.06443v1",
                "title": "Vec-LUT: Vector Table Lookup for Parallel Ultra-Low-Bit LLM Inference on Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vec-LUT: Vector Table Lookup for Parallel Ultra-Low-Bit LLM Inference on Edge Devices"
                },
                "updated": "2025-12-06T14:14:01Z",
                "updated_parsed": [
                    2025,
                    12,
                    6,
                    14,
                    14,
                    1,
                    5,
                    340,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.06443v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.06443v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) are increasingly deployed on edge devices. To meet strict resource constraints, real-world deployment has pushed LLM quantization from 8-bit to 4-bit, 2-bit, and now 1.58-bit. Combined with lookup table (LUT)-based inference, CPUs run these ultra-low-bit LLMs even faster than NPUs, opening new opportunities for ubiquitous on-device intelligence.\n  However, this paper identifies that LUT-based inference underutilizes memory bandwidth during parallel inference, which is required for prefilling, test-time scaling, and other multi-token scenarios. The root cause is the scalar LUT paradigm, which performs repetitive and non-contiguous memory accesses for each token.\n  To solve the issue, we propose vector LUT, a new lookup paradigm that constructs a unified LUT across parallel tokens, and performs a single $1 \\rightarrow N$ lookup per index. To realize it efficiently, we further introduce (1) Vector LUT-Centric Tensor Layout, and (2) Cache-Aware Streamed Lookup techniques. Evaluations on 5 edge devices across 3 LLMs show that Vec-LUT outperforms state-of-the-art baselines by up to $4.2\\times$. Our implementation is integrated into llama.cpp. The code is available at https://github.com/Cipherxzc/vlut.cpp.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly deployed on edge devices. To meet strict resource constraints, real-world deployment has pushed LLM quantization from 8-bit to 4-bit, 2-bit, and now 1.58-bit. Combined with lookup table (LUT)-based inference, CPUs run these ultra-low-bit LLMs even faster than NPUs, opening new opportunities for ubiquitous on-device intelligence.\n  However, this paper identifies that LUT-based inference underutilizes memory bandwidth during parallel inference, which is required for prefilling, test-time scaling, and other multi-token scenarios. The root cause is the scalar LUT paradigm, which performs repetitive and non-contiguous memory accesses for each token.\n  To solve the issue, we propose vector LUT, a new lookup paradigm that constructs a unified LUT across parallel tokens, and performs a single $1 \\rightarrow N$ lookup per index. To realize it efficiently, we further introduce (1) Vector LUT-Centric Tensor Layout, and (2) Cache-Aware Streamed Lookup techniques. Evaluations on 5 edge devices across 3 LLMs show that Vec-LUT outperforms state-of-the-art baselines by up to $4.2\\times$. Our implementation is integrated into llama.cpp. The code is available at https://github.com/Cipherxzc/vlut.cpp."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-06T14:14:01Z",
                "published_parsed": [
                    2025,
                    12,
                    6,
                    14,
                    14,
                    1,
                    5,
                    340,
                    0
                ],
                "arxiv_comment": "Preprint",
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Xiangyu Li"
                    },
                    {
                        "name": "Chengyu Yin"
                    },
                    {
                        "name": "Weijun Wang"
                    },
                    {
                        "name": "Jianyu Wei"
                    },
                    {
                        "name": "Ting Cao"
                    },
                    {
                        "name": "Yunxin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yunxin Liu"
                },
                "author": "Yunxin Liu"
            },
            {
                "id": "http://arxiv.org/abs/2502.02493v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2502.02493v2",
                "title": "EasySpec: Layer-Parallel Speculative Decoding for Efficient Multi-GPU Utilization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EasySpec: Layer-Parallel Speculative Decoding for Efficient Multi-GPU Utilization"
                },
                "updated": "2025-12-06T08:55:13Z",
                "updated_parsed": [
                    2025,
                    12,
                    6,
                    8,
                    55,
                    13,
                    5,
                    340,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2502.02493v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2502.02493v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Speculative decoding is an effective and lossless method for Large Language Model (LLM) inference acceleration. It employs a smaller model to generate a draft token sequence, which is then verified by the original base model. In multi-GPU systems, inference latency can be further reduced through tensor parallelism (TP), while the optimal TP size of the draft model is typically smaller than that of the base model, leading to GPU idling during the drafting stage. We observe that such inefficiency stems from the sequential execution of layers, which is seemingly natural but actually unnecessary. Therefore, we propose EasySpec, a layer-parallel speculation strategy that optimizes the efficiency of multi-GPU utilization. EasySpec breaks the inter-layer data dependencies in the draft model, enabling multiple layers to run simultaneously across multiple devices as 'fuzzy' speculation. After each drafting-and-verification iteration, the draft model's key-value cache is calibrated in a single forward pass, preventing long-term fuzzy-error accumulation at minimal additional latency. EasySpec is a training-free and plug-in method. We evaluated EasySpec on several mainstream open-source LLMs, using smaller versions of models from the same series as drafters. The results demonstrate that EasySpec can achieve a peak speedup of 4.17x compared to vanilla decoding, while preserving the original distributions of the base LLMs. Specifically, the drafting stage can be accelerated by up to 1.62x with a maximum speculation accuracy drop of only 7%. The code is available at https://github.com/Yize-Wu/EasySpec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is an effective and lossless method for Large Language Model (LLM) inference acceleration. It employs a smaller model to generate a draft token sequence, which is then verified by the original base model. In multi-GPU systems, inference latency can be further reduced through tensor parallelism (TP), while the optimal TP size of the draft model is typically smaller than that of the base model, leading to GPU idling during the drafting stage. We observe that such inefficiency stems from the sequential execution of layers, which is seemingly natural but actually unnecessary. Therefore, we propose EasySpec, a layer-parallel speculation strategy that optimizes the efficiency of multi-GPU utilization. EasySpec breaks the inter-layer data dependencies in the draft model, enabling multiple layers to run simultaneously across multiple devices as 'fuzzy' speculation. After each drafting-and-verification iteration, the draft model's key-value cache is calibrated in a single forward pass, preventing long-term fuzzy-error accumulation at minimal additional latency. EasySpec is a training-free and plug-in method. We evaluated EasySpec on several mainstream open-source LLMs, using smaller versions of models from the same series as drafters. The results demonstrate that EasySpec can achieve a peak speedup of 4.17x compared to vanilla decoding, while preserving the original distributions of the base LLMs. Specifically, the drafting stage can be accelerated by up to 1.62x with a maximum speculation accuracy drop of only 7%. The code is available at https://github.com/Yize-Wu/EasySpec."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-02-04T17:09:21Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    17,
                    9,
                    21,
                    1,
                    35,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Yize Wu"
                    },
                    {
                        "name": "Ke Gao"
                    },
                    {
                        "name": "Ling Li"
                    },
                    {
                        "name": "Yanjun Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yanjun Wu"
                },
                "author": "Yanjun Wu"
            },
            {
                "id": "http://arxiv.org/abs/2512.05916v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05916v1",
                "title": "KQ-SVD: Compressing the KV Cache with Provable Guarantees on Attention Fidelity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KQ-SVD: Compressing the KV Cache with Provable Guarantees on Attention Fidelity"
                },
                "updated": "2025-12-05T17:51:10Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    17,
                    51,
                    10,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05916v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05916v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The Key-Value (KV) cache is central to the efficiency of transformer-based large language models (LLMs), storing previously computed vectors to accelerate inference. Yet, as sequence length and batch size grow, the cache becomes a major memory bottleneck. Prior compression methods typically apply low-rank decomposition to keys alone or attempt to jointly embed queries and keys, but both approaches neglect that attention fundamentally depends on their inner products. In this work, we prove that such strategies are suboptimal for approximating the attention matrix. We introduce KQ-SVD, a simple and computationally efficient method that directly performs an optimal low-rank decomposition of the attention matrix via a closed-form solution. By targeting the true source of redundancy, KQ-SVD preserves attention outputs with higher fidelity under compression. Extensive evaluations on LLaMA and Mistral models demonstrate that our approach consistently delivers superior projection quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Key-Value (KV) cache is central to the efficiency of transformer-based large language models (LLMs), storing previously computed vectors to accelerate inference. Yet, as sequence length and batch size grow, the cache becomes a major memory bottleneck. Prior compression methods typically apply low-rank decomposition to keys alone or attempt to jointly embed queries and keys, but both approaches neglect that attention fundamentally depends on their inner products. In this work, we prove that such strategies are suboptimal for approximating the attention matrix. We introduce KQ-SVD, a simple and computationally efficient method that directly performs an optimal low-rank decomposition of the attention matrix via a closed-form solution. By targeting the true source of redundancy, KQ-SVD preserves attention outputs with higher fidelity under compression. Extensive evaluations on LLaMA and Mistral models demonstrate that our approach consistently delivers superior projection quality."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T17:51:10Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    17,
                    51,
                    10,
                    4,
                    339,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Damien Lesens"
                    },
                    {
                        "name": "Beheshteh T. Rakhshan"
                    },
                    {
                        "name": "Guillaume Rabusseau"
                    }
                ],
                "author_detail": {
                    "name": "Guillaume Rabusseau"
                },
                "author": "Guillaume Rabusseau"
            },
            {
                "id": "http://arxiv.org/abs/2512.01678v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.01678v3",
                "title": "Morphling: Fast, Fused, and Flexible GNN Training at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Morphling: Fast, Fused, and Flexible GNN Training at Scale"
                },
                "updated": "2025-12-05T16:07:38Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    16,
                    7,
                    38,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.01678v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.01678v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Graph Neural Networks (GNNs) present a fundamental hardware challenge by fusing irregular, memory-bound graph traversals with regular, compute-intensive dense matrix operations. While frameworks such as PyTorch Geometric (PyG) and Deep Graph Library (DGL) prioritize high-level usability, they fail to address these divergent execution characteristics. As a result, they rely on generic kernels that suffer from poor cache locality, excessive memory movement, and substantial intermediate allocations. To address these limitations, we present Morphling, a domain-specific code synthesizer designed to bridge this gap. Morphling compiles high-level GNN specifications into portable, backend-specialized implementations targeting OpenMP, CUDA, and MPI. It achieves this by instantiating a library of optimized, architecture-aware primitives tailored to each execution environment. Morphling also incorporates a runtime sparsity-aware execution engine that dynamically selects dense or sparse execution paths using input feature statistics, reducing unnecessary computation on zero-valued entries. We evaluate Morphling on eleven real-world datasets spanning diverse graph structures, feature dimensionalities, and sparsity regimes. Morphling improves per-epoch training throughput by an average of 20X on CPUs, 19X on GPUs, and 6X in distributed settings over PyG and DGL, with peak speedups reaching 66X. Morphling's memory-efficient layouts further reduce peak memory consumption by up to 15X, enabling large-scale GNN training on commodity hardware. These findings demonstrate that specialized, architecture-aware code synthesis provides an effective and scalable path toward high-performance GNN execution across diverse parallel and distributed platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) present a fundamental hardware challenge by fusing irregular, memory-bound graph traversals with regular, compute-intensive dense matrix operations. While frameworks such as PyTorch Geometric (PyG) and Deep Graph Library (DGL) prioritize high-level usability, they fail to address these divergent execution characteristics. As a result, they rely on generic kernels that suffer from poor cache locality, excessive memory movement, and substantial intermediate allocations. To address these limitations, we present Morphling, a domain-specific code synthesizer designed to bridge this gap. Morphling compiles high-level GNN specifications into portable, backend-specialized implementations targeting OpenMP, CUDA, and MPI. It achieves this by instantiating a library of optimized, architecture-aware primitives tailored to each execution environment. Morphling also incorporates a runtime sparsity-aware execution engine that dynamically selects dense or sparse execution paths using input feature statistics, reducing unnecessary computation on zero-valued entries. We evaluate Morphling on eleven real-world datasets spanning diverse graph structures, feature dimensionalities, and sparsity regimes. Morphling improves per-epoch training throughput by an average of 20X on CPUs, 19X on GPUs, and 6X in distributed settings over PyG and DGL, with peak speedups reaching 66X. Morphling's memory-efficient layouts further reduce peak memory consumption by up to 15X, enabling large-scale GNN training on commodity hardware. These findings demonstrate that specialized, architecture-aware code synthesis provides an effective and scalable path toward high-performance GNN execution across diverse parallel and distributed platforms."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-01T13:45:03Z",
                "published_parsed": [
                    2025,
                    12,
                    1,
                    13,
                    45,
                    3,
                    0,
                    335,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Anubhab"
                    },
                    {
                        "name": "Rupesh Nasre"
                    }
                ],
                "author_detail": {
                    "name": "Rupesh Nasre"
                },
                "author": "Rupesh Nasre"
            },
            {
                "id": "http://arxiv.org/abs/2512.04677v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.04677v2",
                "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length"
                },
                "updated": "2025-12-05T06:32:30Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    6,
                    32,
                    30,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.04677v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.04677v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis. We present Live Avatar, an algorithm-system co-designed framework that enables efficient, high-fidelity, and infinite-length avatar generation using a 14-billion-parameter diffusion model. Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), a distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming. To further enhance temporal consistency and mitigate identity drift and color artifacts, we propose the Rolling Sink Frame Mechanism (RSFM), which maintains sequence fidelity by dynamically recalibrating appearance using a cached reference image. Additionally, we leverage Self-Forcing Distribution Matching Distillation to facilitate causal, streamable adaptation of large-scale models without sacrificing visual quality. Live Avatar demonstrates state-of-the-art performance, reaching 20 FPS end-to-end generation on 5 H800 GPUs, and, to the best of our knowledge, is the first to achieve practical, real-time, high-fidelity avatar generation at this scale. Our work establishes a new paradigm for deploying advanced diffusion models in industrial long-form video synthesis applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis. We present Live Avatar, an algorithm-system co-designed framework that enables efficient, high-fidelity, and infinite-length avatar generation using a 14-billion-parameter diffusion model. Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), a distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming. To further enhance temporal consistency and mitigate identity drift and color artifacts, we propose the Rolling Sink Frame Mechanism (RSFM), which maintains sequence fidelity by dynamically recalibrating appearance using a cached reference image. Additionally, we leverage Self-Forcing Distribution Matching Distillation to facilitate causal, streamable adaptation of large-scale models without sacrificing visual quality. Live Avatar demonstrates state-of-the-art performance, reaching 20 FPS end-to-end generation on 5 H800 GPUs, and, to the best of our knowledge, is the first to achieve practical, real-time, high-fidelity avatar generation at this scale. Our work establishes a new paradigm for deploying advanced diffusion models in industrial long-form video synthesis applications."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-04T11:11:24Z",
                "published_parsed": [
                    2025,
                    12,
                    4,
                    11,
                    11,
                    24,
                    3,
                    338,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Yubo Huang"
                    },
                    {
                        "name": "Hailong Guo"
                    },
                    {
                        "name": "Fangtai Wu"
                    },
                    {
                        "name": "Shifeng Zhang"
                    },
                    {
                        "name": "Shijie Huang"
                    },
                    {
                        "name": "Qijun Gan"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Sirui Zhao"
                    },
                    {
                        "name": "Enhong Chen"
                    },
                    {
                        "name": "Jiaming Liu"
                    },
                    {
                        "name": "Steven Hoi"
                    }
                ],
                "author_detail": {
                    "name": "Steven Hoi"
                },
                "author": "Steven Hoi"
            },
            {
                "id": "http://arxiv.org/abs/2510.09665v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.09665v2",
                "title": "LMCache: An Efficient KV Cache Layer for Enterprise-Scale LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LMCache: An Efficient KV Cache Layer for Enterprise-Scale LLM Inference"
                },
                "updated": "2025-12-05T04:52:54Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    4,
                    52,
                    54,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.09665v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.09665v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "KV cache has traditionally been stored in GPU memory to accelerate the decoding phase of large language model (LLM) inference. However, it is increasingly necessary to move KV caches outside GPU devices, to enable cache reuse across different queries and inference engines. Our real-world usage statistics confirm this trend: over time, the total KV cache stored by users has grown rapidly, far exceeding the capacity of GPU memory. Despite this need, there lacks an efficient solution for offloading and transferring KV caches. We present LMCACHE, the first and so far the most efficient open-source KV caching solution, which extracts and stores KV caches generated by modern LLM engines (vLLM and SGLang) out of the GPU memory and shares them across engines and queries. LMCACHE supports both cache offloading (prefix reuse across queries) and prefill-decode (PD) disaggregation (cross-engine/GPU cache transfer). LMCACHE's high performance and wide adoption stem from the following contributions: (1) highly optimized KV cache data movement powered by batched data movement operations, compute and I/O pipelining; (2) a modular KV cache connector component, decoupling LMCACHE from the rapid evolution of inference engines; (3) a first-class control API for flexible cache orchestration across GPU, CPU, storage, and network layers. Our evaluation shows that combining LMCACHE with vLLM achieves up to 15x improvement in throughput across workloads such as multi-round question answering and document analysis. Large-scale adoption of LMCACHE in enterprise settings provides us valuable insights, for example, fetching KV cache from remote storage has unsurprisingly benefits to prefill delay, and that context truncation, which is a widely applied technique in industry, can greatly reduce prefix cache hit ratio by half. The source code of LMCACHE is at: https://github.com/LMCache/LMCache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache has traditionally been stored in GPU memory to accelerate the decoding phase of large language model (LLM) inference. However, it is increasingly necessary to move KV caches outside GPU devices, to enable cache reuse across different queries and inference engines. Our real-world usage statistics confirm this trend: over time, the total KV cache stored by users has grown rapidly, far exceeding the capacity of GPU memory. Despite this need, there lacks an efficient solution for offloading and transferring KV caches. We present LMCACHE, the first and so far the most efficient open-source KV caching solution, which extracts and stores KV caches generated by modern LLM engines (vLLM and SGLang) out of the GPU memory and shares them across engines and queries. LMCACHE supports both cache offloading (prefix reuse across queries) and prefill-decode (PD) disaggregation (cross-engine/GPU cache transfer). LMCACHE's high performance and wide adoption stem from the following contributions: (1) highly optimized KV cache data movement powered by batched data movement operations, compute and I/O pipelining; (2) a modular KV cache connector component, decoupling LMCACHE from the rapid evolution of inference engines; (3) a first-class control API for flexible cache orchestration across GPU, CPU, storage, and network layers. Our evaluation shows that combining LMCACHE with vLLM achieves up to 15x improvement in throughput across workloads such as multi-round question answering and document analysis. Large-scale adoption of LMCACHE in enterprise settings provides us valuable insights, for example, fetching KV cache from remote storage has unsurprisingly benefits to prefill delay, and that context truncation, which is a widely applied technique in industry, can greatly reduce prefix cache hit ratio by half. The source code of LMCACHE is at: https://github.com/LMCache/LMCache."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-08T00:15:04Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    0,
                    15,
                    4,
                    2,
                    281,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Yuwei An"
                    },
                    {
                        "name": "Xiaokun Chen"
                    },
                    {
                        "name": "Shaoting Feng"
                    },
                    {
                        "name": "Yuyang Huang"
                    },
                    {
                        "name": "Samuel Shen"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang"
            },
            {
                "id": "http://arxiv.org/abs/2511.21958v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.21958v2",
                "title": "Clock2Q+: A Simple and Efficient Replacement Algorithm for Metadata Cache in VMware vSAN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clock2Q+: A Simple and Efficient Replacement Algorithm for Metadata Cache in VMware vSAN"
                },
                "updated": "2025-12-05T02:13:10Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    2,
                    13,
                    10,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.21958v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.21958v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Cache replacement algorithms are critical building blocks of storage systems. This paper examines the characteristics of metadata caches and argues that they inherently exhibit correlated references, even when the corresponding data accesses do not contain correlated references. The presence of correlated references reduces the effectiveness of cache replacement algorithms because these references are often mistakenly categorized as hot blocks. Clock2Q+ is specifically designed for metadata caches and has been implemented in vSAN and VDFS, two flagship storage products of VMware by Broadcom. Similar to S3-FIFO, Clock2Q+ uses three queues; however, Clock2Q+ introduces a correlation window in the Small FIFO queue, where blocks in this window do not set the reference bit. This simple enhancement allows Clock2Q+ to outperform state-of-the-art replacement algorithms. Compared to S3-FIFO, the second-best performing algorithm, Clock2Q+ achieves up to a 28.5% lower miss ratio on metadata traces. Clock2Q+ possesses the essential properties required for large-scale storage systems: it has low CPU overhead on cache hits, low memory overhead, scales efficiently to multiple CPUs, and is both easy to tune and implement. Additionally, Clock2Q+ outperforms state-of-the-art cache replacement algorithms on data traces as well.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache replacement algorithms are critical building blocks of storage systems. This paper examines the characteristics of metadata caches and argues that they inherently exhibit correlated references, even when the corresponding data accesses do not contain correlated references. The presence of correlated references reduces the effectiveness of cache replacement algorithms because these references are often mistakenly categorized as hot blocks. Clock2Q+ is specifically designed for metadata caches and has been implemented in vSAN and VDFS, two flagship storage products of VMware by Broadcom. Similar to S3-FIFO, Clock2Q+ uses three queues; however, Clock2Q+ introduces a correlation window in the Small FIFO queue, where blocks in this window do not set the reference bit. This simple enhancement allows Clock2Q+ to outperform state-of-the-art replacement algorithms. Compared to S3-FIFO, the second-best performing algorithm, Clock2Q+ achieves up to a 28.5% lower miss ratio on metadata traces. Clock2Q+ possesses the essential properties required for large-scale storage systems: it has low CPU overhead on cache hits, low memory overhead, scales efficiently to multiple CPUs, and is both easy to tune and implement. Additionally, Clock2Q+ outperforms state-of-the-art cache replacement algorithms on data traces as well."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-26T22:34:26Z",
                "published_parsed": [
                    2025,
                    11,
                    26,
                    22,
                    34,
                    26,
                    2,
                    330,
                    0
                ],
                "arxiv_comment": "12 pages, 14 figures",
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Yiyan Zhai"
                    },
                    {
                        "name": "Bintang Dwi Marthen"
                    },
                    {
                        "name": "Sarath Balivada"
                    },
                    {
                        "name": "Vamsi Sudhakar Bojji"
                    },
                    {
                        "name": "Eric Knauft"
                    },
                    {
                        "name": "Jitender Rohilla"
                    },
                    {
                        "name": "Jiaqi Zuo"
                    },
                    {
                        "name": "Quanxing Liu"
                    },
                    {
                        "name": "Maxime Austruy"
                    },
                    {
                        "name": "Wenguang Wang"
                    },
                    {
                        "name": "Juncheng Yang"
                    }
                ],
                "author_detail": {
                    "name": "Juncheng Yang"
                },
                "author": "Juncheng Yang"
            },
            {
                "id": "http://arxiv.org/abs/2511.14748v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.14748v2",
                "title": "Cloud-Native Vector Search: A Comprehensive Performance Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud-Native Vector Search: A Comprehensive Performance Analysis"
                },
                "updated": "2025-12-05T00:04:45Z",
                "updated_parsed": [
                    2025,
                    12,
                    5,
                    0,
                    4,
                    45,
                    4,
                    339,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.14748v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.14748v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Vector search has been widely employed in recommender system and retrieval-augmented-generation pipelines, commonly performed with vector indexes to efficiently find similar items in large datasets. Recent growths in both data and task complexity have motivated placing vector indexes onto remote storage -- cloud-native vector search, which cloud providers have recently introduced services for. Yet, despite varying workload characteristics and various available vector index forms, providers default to using cluster-based indexes, which on paper do adapt well to differences between disk and cloud-based environment: their fetch granularities and lack of notable intra-query dependencies aligns with the large optimal fetch sizes and minimizes costly round-trips (i.e., as opposed to graph-based indexes) to remote storage, respectively.\n  This paper systematically studies cloud-native vector search: What and how should indexes be built and used for on-cloud vector search? We analyze bottlenecks of two common index classes, cluster and graph indexes, on remote storage, and show that despite current standardized adoption of cluster indexes on the cloud, graph indexes are favored in workloads requiring high concurrency and recall, or operating on high-dimensional data or large datatypes. We further find that on-cloud search demands significantly different indexing and search parameterizations versus on-disk search for optimal performance. Finally, we incorporate existing cloud-based caching setups into vector search and find that certain index optimizations work against caching, and study how this can be mitigated to maximize gains under various available cache sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vector search has been widely employed in recommender system and retrieval-augmented-generation pipelines, commonly performed with vector indexes to efficiently find similar items in large datasets. Recent growths in both data and task complexity have motivated placing vector indexes onto remote storage -- cloud-native vector search, which cloud providers have recently introduced services for. Yet, despite varying workload characteristics and various available vector index forms, providers default to using cluster-based indexes, which on paper do adapt well to differences between disk and cloud-based environment: their fetch granularities and lack of notable intra-query dependencies aligns with the large optimal fetch sizes and minimizes costly round-trips (i.e., as opposed to graph-based indexes) to remote storage, respectively.\n  This paper systematically studies cloud-native vector search: What and how should indexes be built and used for on-cloud vector search? We analyze bottlenecks of two common index classes, cluster and graph indexes, on remote storage, and show that despite current standardized adoption of cluster indexes on the cloud, graph indexes are favored in workloads requiring high concurrency and recall, or operating on high-dimensional data or large datatypes. We further find that on-cloud search demands significantly different indexing and search parameterizations versus on-disk search for optimal performance. Finally, we incorporate existing cloud-based caching setups into vector search and find that certain index optimizations work against caching, and study how this can be mitigated to maximize gains under various available cache sizes."
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-18T18:50:15Z",
                "published_parsed": [
                    2025,
                    11,
                    18,
                    18,
                    50,
                    15,
                    1,
                    322,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB"
                },
                "authors": [
                    {
                        "name": "Zhaoheng Li"
                    },
                    {
                        "name": "Wei Ding"
                    },
                    {
                        "name": "Silu Huang"
                    },
                    {
                        "name": "Zikang Wang"
                    },
                    {
                        "name": "Yuanjin Lin"
                    },
                    {
                        "name": "Ke Wu"
                    },
                    {
                        "name": "Yongjoo Park"
                    },
                    {
                        "name": "Jianjun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jianjun Chen"
                },
                "author": "Jianjun Chen"
            },
            {
                "id": "http://arxiv.org/abs/2512.05081v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05081v1",
                "title": "Deep Forcing: Training-Free Long Video Generation with Deep Sink and Participative Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Forcing: Training-Free Long Video Generation with Deep Sink and Participative Compression"
                },
                "updated": "2025-12-04T18:46:44Z",
                "updated_parsed": [
                    2025,
                    12,
                    4,
                    18,
                    46,
                    44,
                    3,
                    338,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05081v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05081v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent advances in autoregressive video diffusion have enabled real-time frame streaming, yet existing solutions still suffer from temporal repetition, drift, and motion deceleration. We find that naively applying StreamingLLM-style attention sinks to video diffusion leads to fidelity degradation and motion stagnation. To overcome this, we introduce Deep Forcing, which consists of two training-free mechanisms that address this without any fine-tuning. Specifically, 1) Deep Sink dedicates half of the sliding window to persistent sink tokens and re-aligns their temporal RoPE phase to the current timeline, stabilizing global context during long rollouts. 2) Participative Compression performs importance-aware KV cache pruning that preserves only tokens actively participating in recent attention while safely discarding redundant and degraded history, minimizing error accumulation under out-of-distribution length generation. Together, these components enable over 12x extrapolation (e.g. 5s-trained to 60s+ generation) with better imaging quality than LongLive, better aesthetic quality than RollingForcing, almost maintaining overall consistency, and substantial gains in dynamic degree, all while maintaining real-time generation. Our results demonstrate that training-free KV-cache management can match or exceed training-based approaches for autoregressively streaming long-video generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in autoregressive video diffusion have enabled real-time frame streaming, yet existing solutions still suffer from temporal repetition, drift, and motion deceleration. We find that naively applying StreamingLLM-style attention sinks to video diffusion leads to fidelity degradation and motion stagnation. To overcome this, we introduce Deep Forcing, which consists of two training-free mechanisms that address this without any fine-tuning. Specifically, 1) Deep Sink dedicates half of the sliding window to persistent sink tokens and re-aligns their temporal RoPE phase to the current timeline, stabilizing global context during long rollouts. 2) Participative Compression performs importance-aware KV cache pruning that preserves only tokens actively participating in recent attention while safely discarding redundant and degraded history, minimizing error accumulation under out-of-distribution length generation. Together, these components enable over 12x extrapolation (e.g. 5s-trained to 60s+ generation) with better imaging quality than LongLive, better aesthetic quality than RollingForcing, almost maintaining overall consistency, and substantial gains in dynamic degree, all while maintaining real-time generation. Our results demonstrate that training-free KV-cache management can match or exceed training-based approaches for autoregressively streaming long-video generation."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-04T18:46:44Z",
                "published_parsed": [
                    2025,
                    12,
                    4,
                    18,
                    46,
                    44,
                    3,
                    338,
                    0
                ],
                "arxiv_comment": "Project Page: https://cvlab-kaist.github.io/DeepForcing/",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Jung Yi"
                    },
                    {
                        "name": "Wooseok Jang"
                    },
                    {
                        "name": "Paul Hyunbin Cho"
                    },
                    {
                        "name": "Jisu Nam"
                    },
                    {
                        "name": "Heeji Yoon"
                    },
                    {
                        "name": "Seungryong Kim"
                    }
                ],
                "author_detail": {
                    "name": "Seungryong Kim"
                },
                "author": "Seungryong Kim"
            },
            {
                "id": "http://arxiv.org/abs/2508.10875v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.10875v2",
                "title": "A Survey on Diffusion Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Diffusion Language Models"
                },
                "updated": "2025-12-04T17:57:10Z",
                "updated_parsed": [
                    2025,
                    12,
                    4,
                    17,
                    57,
                    10,
                    3,
                    338,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.10875v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.10875v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion Language Models (DLMs) are rapidly emerging as a powerful and promising alternative to the dominant autoregressive (AR) paradigm. By generating tokens in parallel through an iterative denoising process, DLMs possess inherent advantages in reducing inference latency and capturing bidirectional context, thereby enabling fine-grained control over the generation process. While achieving a several-fold speed-up, recent advancements have allowed DLMs to show performance comparable to their autoregressive counterparts, making them a compelling choice for various natural language processing tasks. In this survey, we provide a holistic overview of the current DLM landscape. We trace its evolution and relationship with other paradigms, such as autoregressive and masked language models, and cover both foundational principles and state-of-the-art models. Our work offers an up-to-date, comprehensive taxonomy and an in-depth analysis of current techniques, from pre-training strategies to advanced post-training methods. Another contribution of this survey is a thorough review of DLM inference strategies and optimizations, including improvements in decoding parallelism, caching mechanisms, and generation quality. We also highlight the latest approaches to multimodal extensions of DLMs and delineate their applications across various practical scenarios. Furthermore, our discussion addresses the limitations and challenges of DLMs, including efficiency, long-sequence handling, and infrastructure requirements, while outlining future research directions to sustain progress in this rapidly evolving field. Project GitHub is available at https://github.com/VILA-Lab/Awesome-DLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Language Models (DLMs) are rapidly emerging as a powerful and promising alternative to the dominant autoregressive (AR) paradigm. By generating tokens in parallel through an iterative denoising process, DLMs possess inherent advantages in reducing inference latency and capturing bidirectional context, thereby enabling fine-grained control over the generation process. While achieving a several-fold speed-up, recent advancements have allowed DLMs to show performance comparable to their autoregressive counterparts, making them a compelling choice for various natural language processing tasks. In this survey, we provide a holistic overview of the current DLM landscape. We trace its evolution and relationship with other paradigms, such as autoregressive and masked language models, and cover both foundational principles and state-of-the-art models. Our work offers an up-to-date, comprehensive taxonomy and an in-depth analysis of current techniques, from pre-training strategies to advanced post-training methods. Another contribution of this survey is a thorough review of DLM inference strategies and optimizations, including improvements in decoding parallelism, caching mechanisms, and generation quality. We also highlight the latest approaches to multimodal extensions of DLMs and delineate their applications across various practical scenarios. Furthermore, our discussion addresses the limitations and challenges of DLMs, including efficiency, long-sequence handling, and infrastructure requirements, while outlining future research directions to sustain progress in this rapidly evolving field. Project GitHub is available at https://github.com/VILA-Lab/Awesome-DLMs."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-14T17:47:22Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    17,
                    47,
                    22,
                    3,
                    226,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Tianyi Li"
                    },
                    {
                        "name": "Mingda Chen"
                    },
                    {
                        "name": "Bowei Guo"
                    },
                    {
                        "name": "Zhiqiang Shen"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Shen"
                },
                "author": "Zhiqiang Shen"
            },
            {
                "id": "http://arxiv.org/abs/2512.04939v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.04939v1",
                "title": "LiteVGGT: Boosting Vanilla VGGT via Geometry-aware Cached Token Merging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiteVGGT: Boosting Vanilla VGGT via Geometry-aware Cached Token Merging"
                },
                "updated": "2025-12-04T16:07:02Z",
                "updated_parsed": [
                    2025,
                    12,
                    4,
                    16,
                    7,
                    2,
                    3,
                    338,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.04939v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.04939v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "3D vision foundation models like Visual Geometry Grounded Transformer (VGGT) have advanced greatly in geometric perception. However, it is time-consuming and memory-intensive for long sequences, limiting application to large-scale scenes beyond hundreds of images. To address this, we propose LiteVGGT, achieving up to 10x speedup and substantial memory reduction, enabling efficient processing of 1000-image scenes. We derive two key insights for 3D reconstruction: (1) tokens from local image regions have inherent geometric correlations, leading to high similarity and computational redundancy; (2) token similarity across adjacent network layers remains stable, allowing for reusable merge decisions. Guided by these, we design a simple yet efficient strategy, dubbed geometry-aware cached token merging. We analyze each token's geometric importance, optimizing anchor token selection to better preserve key information for reconstruction. We also cache and reuse merge indices across layers, substantially reducing latency with minimal accuracy impact. This strategy retains VGGT's core performance, enabling efficient fine-tuning and FP8 quantization for further gains. Extensive experiments validate LiteVGGT's effectiveness, scalability, and robustness. Project page: https://garlicba.github.io/LiteVGGT/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D vision foundation models like Visual Geometry Grounded Transformer (VGGT) have advanced greatly in geometric perception. However, it is time-consuming and memory-intensive for long sequences, limiting application to large-scale scenes beyond hundreds of images. To address this, we propose LiteVGGT, achieving up to 10x speedup and substantial memory reduction, enabling efficient processing of 1000-image scenes. We derive two key insights for 3D reconstruction: (1) tokens from local image regions have inherent geometric correlations, leading to high similarity and computational redundancy; (2) token similarity across adjacent network layers remains stable, allowing for reusable merge decisions. Guided by these, we design a simple yet efficient strategy, dubbed geometry-aware cached token merging. We analyze each token's geometric importance, optimizing anchor token selection to better preserve key information for reconstruction. We also cache and reuse merge indices across layers, substantially reducing latency with minimal accuracy impact. This strategy retains VGGT's core performance, enabling efficient fine-tuning and FP8 quantization for further gains. Extensive experiments validate LiteVGGT's effectiveness, scalability, and robustness. Project page: https://garlicba.github.io/LiteVGGT/"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-04T16:07:02Z",
                "published_parsed": [
                    2025,
                    12,
                    4,
                    16,
                    7,
                    2,
                    3,
                    338,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Zhijian Shu"
                    },
                    {
                        "name": "Cheng Lin"
                    },
                    {
                        "name": "Tao Xie"
                    },
                    {
                        "name": "Wei Yin"
                    },
                    {
                        "name": "Ben Li"
                    },
                    {
                        "name": "Zhiyuan Pu"
                    },
                    {
                        "name": "Weize Li"
                    },
                    {
                        "name": "Yao Yao"
                    },
                    {
                        "name": "Xun Cao"
                    },
                    {
                        "name": "Xiaoyang Guo"
                    },
                    {
                        "name": "Xiao-Xiao Long"
                    }
                ],
                "author_detail": {
                    "name": "Xiao-Xiao Long"
                },
                "author": "Xiao-Xiao Long"
            },
            {
                "id": "http://arxiv.org/abs/2512.04857v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.04857v1",
                "title": "Autoregressive Image Generation Needs Only a Few Lines of Cached Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Image Generation Needs Only a Few Lines of Cached Tokens"
                },
                "updated": "2025-12-04T14:41:21Z",
                "updated_parsed": [
                    2025,
                    12,
                    4,
                    14,
                    41,
                    21,
                    3,
                    338,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.04857v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.04857v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Autoregressive (AR) visual generation has emerged as a powerful paradigm for image and multimodal synthesis, owing to its scalability and generality. However, existing AR image generation suffers from severe memory bottlenecks due to the need to cache all previously generated visual tokens during decoding, leading to both high storage requirements and low throughput. In this paper, we introduce \\textbf{LineAR}, a novel, training-free progressive key-value (KV) cache compression pipeline for autoregressive image generation. By fully exploiting the intrinsic characteristics of visual attention, LineAR manages the cache at the line level using a 2D view, preserving the visual dependency regions while progressively evicting less-informative tokens that are harmless for subsequent line generation, guided by inter-line attention. LineAR enables efficient autoregressive (AR) image generation by utilizing only a few lines of cache, achieving both memory savings and throughput speedup, while maintaining or even improving generation quality. Extensive experiments across six autoregressive image generation models, including class-conditional and text-to-image generation, validate its effectiveness and generality. LineAR improves ImageNet FID from 2.77 to 2.68 and COCO FID from 23.85 to 22.86 on LlamaGen-XL and Janus-Pro-1B, while retaining only 1/6 KV cache. It also improves DPG on Lumina-mGPT-768 with just 1/8 KV cache. Additionally, LineAR achieves significant memory and throughput gains, including up to 67.61% memory reduction and 7.57x speedup on LlamaGen-XL, and 39.66% memory reduction and 5.62x speedup on Janus-Pro-7B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive (AR) visual generation has emerged as a powerful paradigm for image and multimodal synthesis, owing to its scalability and generality. However, existing AR image generation suffers from severe memory bottlenecks due to the need to cache all previously generated visual tokens during decoding, leading to both high storage requirements and low throughput. In this paper, we introduce \\textbf{LineAR}, a novel, training-free progressive key-value (KV) cache compression pipeline for autoregressive image generation. By fully exploiting the intrinsic characteristics of visual attention, LineAR manages the cache at the line level using a 2D view, preserving the visual dependency regions while progressively evicting less-informative tokens that are harmless for subsequent line generation, guided by inter-line attention. LineAR enables efficient autoregressive (AR) image generation by utilizing only a few lines of cache, achieving both memory savings and throughput speedup, while maintaining or even improving generation quality. Extensive experiments across six autoregressive image generation models, including class-conditional and text-to-image generation, validate its effectiveness and generality. LineAR improves ImageNet FID from 2.77 to 2.68 and COCO FID from 23.85 to 22.86 on LlamaGen-XL and Janus-Pro-1B, while retaining only 1/6 KV cache. It also improves DPG on Lumina-mGPT-768 with just 1/8 KV cache. Additionally, LineAR achieves significant memory and throughput gains, including up to 67.61% memory reduction and 7.57x speedup on LlamaGen-XL, and 39.66% memory reduction and 5.62x speedup on Janus-Pro-7B."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-04T14:41:21Z",
                "published_parsed": [
                    2025,
                    12,
                    4,
                    14,
                    41,
                    21,
                    3,
                    338,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Ziran Qin"
                    },
                    {
                        "name": "Youru Lv"
                    },
                    {
                        "name": "Mingbao Lin"
                    },
                    {
                        "name": "Zeren Zhang"
                    },
                    {
                        "name": "Chanfan Gan"
                    },
                    {
                        "name": "Tieyuan Chen"
                    },
                    {
                        "name": "Weiyao Lin"
                    }
                ],
                "author_detail": {
                    "name": "Weiyao Lin"
                },
                "author": "Weiyao Lin"
            },
            {
                "id": "http://arxiv.org/abs/2512.03397v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.03397v2",
                "title": "Surfel-LIO: Fast LiDAR-Inertial Odometry with Pre-computed Surfels and Hierarchical Z-order Voxel Hashing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surfel-LIO: Fast LiDAR-Inertial Odometry with Pre-computed Surfels and Hierarchical Z-order Voxel Hashing"
                },
                "updated": "2025-12-04T12:53:34Z",
                "updated_parsed": [
                    2025,
                    12,
                    4,
                    12,
                    53,
                    34,
                    3,
                    338,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.03397v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.03397v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "LiDAR-inertial odometry (LIO) is an active research area, as it enables accurate real-time state estimation in GPS-denied environments. Recent advances in map data structures and spatial indexing have significantly improved the efficiency of LIO systems. Nevertheless, we observe that two aspects may still leave room for improvement: (1) nearest neighbor search often requires examining multiple spatial units to gather sufficient points for plane fitting, and (2) plane parameters are typically recomputed at every iteration despite unchanged map geometry. Motivated by these observations, we propose Surfel-LIO, which employs a hierarchical voxel structure (hVox) with pre-computed surfel representation. This design enables O(1) correspondence retrieval without runtime neighbor enumeration or plane fitting, combined with Z-order curve encoding for cache-friendly spatial indexing. Experimental results on the M3DGR dataset demonstrate that our method achieves significantly faster processing speed compared to recent state-of-the-art methods while maintaining comparable state estimation accuracy. Our implementation is publicly available at https://github.com/93won/lidar_inertial_odometry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiDAR-inertial odometry (LIO) is an active research area, as it enables accurate real-time state estimation in GPS-denied environments. Recent advances in map data structures and spatial indexing have significantly improved the efficiency of LIO systems. Nevertheless, we observe that two aspects may still leave room for improvement: (1) nearest neighbor search often requires examining multiple spatial units to gather sufficient points for plane fitting, and (2) plane parameters are typically recomputed at every iteration despite unchanged map geometry. Motivated by these observations, we propose Surfel-LIO, which employs a hierarchical voxel structure (hVox) with pre-computed surfel representation. This design enables O(1) correspondence retrieval without runtime neighbor enumeration or plane fitting, combined with Z-order curve encoding for cache-friendly spatial indexing. Experimental results on the M3DGR dataset demonstrate that our method achieves significantly faster processing speed compared to recent state-of-the-art methods while maintaining comparable state estimation accuracy. Our implementation is publicly available at https://github.com/93won/lidar_inertial_odometry."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-03T03:07:08Z",
                "published_parsed": [
                    2025,
                    12,
                    3,
                    3,
                    7,
                    8,
                    2,
                    337,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Seungwon Choi"
                    },
                    {
                        "name": "Dong-Gyu Park"
                    },
                    {
                        "name": "Seo-Yeon Hwang"
                    },
                    {
                        "name": "Tae-Wan Kim"
                    }
                ],
                "author_detail": {
                    "name": "Tae-Wan Kim"
                },
                "author": "Tae-Wan Kim"
            },
            {
                "id": "http://arxiv.org/abs/2511.22421v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.22421v2",
                "title": "Semantic-Aware Caching for Efficient Image Generation in Edge Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic-Aware Caching for Efficient Image Generation in Edge Computing"
                },
                "updated": "2025-12-04T07:11:48Z",
                "updated_parsed": [
                    2025,
                    12,
                    4,
                    7,
                    11,
                    48,
                    3,
                    338,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.22421v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.22421v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Text-to-image generation employing diffusion models has attained significant popularity due to its capability to produce high-quality images that adhere to textual prompts. However, the integration of diffusion models faces critical challenges into resource-constrained mobile and edge environments because it requires multiple denoising steps from the original random noise. A practical way to speed up denoising is to initialize the process with a noised reference image that is similar to the target, since both images share similar layouts, structures, and details, allowing for fewer denoising steps. Based on this idea, we present CacheGenius, a hybrid image generation system in edge computing that accelerates generation by combining text-toimage and image-to-image workflows. It generates images from user text prompts using cached reference images. CacheGenius introduces a semantic-aware classified storage scheme and a request-scheduling algorithm that ensures semantic alignment between references and targets. To ensure sustained performance, it employs a cache maintenance policy that proactively evicts obsolete entries via correlation analysis. Evaluated in a distributed edge computing system, CacheGenius reduces generation latency by 41% and computational costs by 48% relative to baselines, while maintaining competitive evaluation metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image generation employing diffusion models has attained significant popularity due to its capability to produce high-quality images that adhere to textual prompts. However, the integration of diffusion models faces critical challenges into resource-constrained mobile and edge environments because it requires multiple denoising steps from the original random noise. A practical way to speed up denoising is to initialize the process with a noised reference image that is similar to the target, since both images share similar layouts, structures, and details, allowing for fewer denoising steps. Based on this idea, we present CacheGenius, a hybrid image generation system in edge computing that accelerates generation by combining text-toimage and image-to-image workflows. It generates images from user text prompts using cached reference images. CacheGenius introduces a semantic-aware classified storage scheme and a request-scheduling algorithm that ensures semantic alignment between references and targets. To ensure sustained performance, it employs a cache maintenance policy that proactively evicts obsolete entries via correlation analysis. Evaluated in a distributed edge computing system, CacheGenius reduces generation latency by 41% and computational costs by 48% relative to baselines, while maintaining competitive evaluation metrics."
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-27T12:58:25Z",
                "published_parsed": [
                    2025,
                    11,
                    27,
                    12,
                    58,
                    25,
                    3,
                    331,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI"
                },
                "authors": [
                    {
                        "name": "Hanshuai Cui"
                    },
                    {
                        "name": "Zhiqing Tang"
                    },
                    {
                        "name": "Zhi Yao"
                    },
                    {
                        "name": "Weijia Jia"
                    },
                    {
                        "name": "Wei Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Wei Zhao"
                },
                "author": "Wei Zhao"
            },
            {
                "id": "http://arxiv.org/abs/2512.04515v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.04515v1",
                "title": "EgoLCD: Egocentric Video Generation with Long Context Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EgoLCD: Egocentric Video Generation with Long Context Diffusion"
                },
                "updated": "2025-12-04T06:53:01Z",
                "updated_parsed": [
                    2025,
                    12,
                    4,
                    6,
                    53,
                    1,
                    3,
                    338,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.04515v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.04515v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Generating long, coherent egocentric videos is difficult, as hand-object interactions and procedural tasks require reliable long-term memory. Existing autoregressive models suffer from content drift, where object identity and scene semantics degrade over time. To address this challenge, we introduce EgoLCD, an end-to-end framework for egocentric long-context video generation that treats long video synthesis as a problem of efficient and stable memory management. EgoLCD combines a Long-Term Sparse KV Cache for stable global context with an attention-based short-term memory, extended by LoRA for local adaptation. A Memory Regulation Loss enforces consistent memory usage, and Structured Narrative Prompting provides explicit temporal guidance. Extensive experiments on the EgoVid-5M benchmark demonstrate that EgoLCD achieves state-of-the-art performance in both perceptual quality and temporal consistency, effectively mitigating generative forgetting and representing a significant step toward building scalable world models for embodied AI. Code: https://github.com/AIGeeksGroup/EgoLCD. Website: https://aigeeksgroup.github.io/EgoLCD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating long, coherent egocentric videos is difficult, as hand-object interactions and procedural tasks require reliable long-term memory. Existing autoregressive models suffer from content drift, where object identity and scene semantics degrade over time. To address this challenge, we introduce EgoLCD, an end-to-end framework for egocentric long-context video generation that treats long video synthesis as a problem of efficient and stable memory management. EgoLCD combines a Long-Term Sparse KV Cache for stable global context with an attention-based short-term memory, extended by LoRA for local adaptation. A Memory Regulation Loss enforces consistent memory usage, and Structured Narrative Prompting provides explicit temporal guidance. Extensive experiments on the EgoVid-5M benchmark demonstrate that EgoLCD achieves state-of-the-art performance in both perceptual quality and temporal consistency, effectively mitigating generative forgetting and representing a significant step toward building scalable world models for embodied AI. Code: https://github.com/AIGeeksGroup/EgoLCD. Website: https://aigeeksgroup.github.io/EgoLCD."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-04T06:53:01Z",
                "published_parsed": [
                    2025,
                    12,
                    4,
                    6,
                    53,
                    1,
                    3,
                    338,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Liuzhou Zhang"
                    },
                    {
                        "name": "Jiarui Ye"
                    },
                    {
                        "name": "Yuanlei Wang"
                    },
                    {
                        "name": "Ming Zhong"
                    },
                    {
                        "name": "Mingju Cao"
                    },
                    {
                        "name": "Wanke Xia"
                    },
                    {
                        "name": "Bowen Zeng"
                    },
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Hao Tang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Tang"
                },
                "author": "Hao Tang"
            },
            {
                "id": "http://arxiv.org/abs/2511.08003v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.08003v2",
                "title": "Sharp Eyes and Memory for VideoLLMs: Information-Aware Visual Token Pruning for Efficient and Reliable VideoLLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sharp Eyes and Memory for VideoLLMs: Information-Aware Visual Token Pruning for Efficient and Reliable VideoLLM Reasoning"
                },
                "updated": "2025-12-04T06:19:39Z",
                "updated_parsed": [
                    2025,
                    12,
                    4,
                    6,
                    19,
                    39,
                    3,
                    338,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.08003v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.08003v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Current Video Large Language Models (VideoLLMs) suffer from quadratic computational complexity and key-value cache scaling, due to their reliance on processing excessive redundant visual tokens. To address this problem, we propose SharpV, a minimalist and efficient method for adaptive pruning of visual tokens and KV cache. Different from most uniform compression approaches, SharpV dynamically adjusts pruning ratios based on spatial-temporal information. Remarkably, this adaptive mechanism occasionally achieves performance gains over dense models, offering a novel paradigm for adaptive pruning. During the KV cache pruning stage, based on observations of visual information degradation, SharpV prunes degraded visual features via a self-calibration manner, guided by similarity to original visual features. In this way, SharpV achieves hierarchical cache pruning from the perspective of information bottleneck, offering a new insight into VideoLLMs' information flow. Experiments on multiple public benchmarks demonstrate the superiority of SharpV. Moreover, to the best of our knowledge, SharpV is notably the first two-stage pruning framework that operates without requiring access to exposed attention scores, ensuring full compatibility with hardware acceleration techniques like Flash Attention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current Video Large Language Models (VideoLLMs) suffer from quadratic computational complexity and key-value cache scaling, due to their reliance on processing excessive redundant visual tokens. To address this problem, we propose SharpV, a minimalist and efficient method for adaptive pruning of visual tokens and KV cache. Different from most uniform compression approaches, SharpV dynamically adjusts pruning ratios based on spatial-temporal information. Remarkably, this adaptive mechanism occasionally achieves performance gains over dense models, offering a novel paradigm for adaptive pruning. During the KV cache pruning stage, based on observations of visual information degradation, SharpV prunes degraded visual features via a self-calibration manner, guided by similarity to original visual features. In this way, SharpV achieves hierarchical cache pruning from the perspective of information bottleneck, offering a new insight into VideoLLMs' information flow. Experiments on multiple public benchmarks demonstrate the superiority of SharpV. Moreover, to the best of our knowledge, SharpV is notably the first two-stage pruning framework that operates without requiring access to exposed attention scores, ensuring full compatibility with hardware acceleration techniques like Flash Attention."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-11T09:07:40Z",
                "published_parsed": [
                    2025,
                    11,
                    11,
                    9,
                    7,
                    40,
                    1,
                    315,
                    0
                ],
                "arxiv_comment": "The 40th Annual AAAI Conference on Artificial Intelligence (AAAI-26) Poster",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Jialong Qin"
                    },
                    {
                        "name": "Xin Zou"
                    },
                    {
                        "name": "Di Lu"
                    },
                    {
                        "name": "Yibo Yan"
                    },
                    {
                        "name": "Xuming Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xuming Hu"
                },
                "author": "Xuming Hu"
            },
            {
                "id": "http://arxiv.org/abs/2512.04226v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.04226v1",
                "title": "tritonBLAS: Triton-based Analytical Approach for GEMM Kernel Parameter Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "tritonBLAS: Triton-based Analytical Approach for GEMM Kernel Parameter Selection"
                },
                "updated": "2025-12-03T19:46:11Z",
                "updated_parsed": [
                    2025,
                    12,
                    3,
                    19,
                    46,
                    11,
                    2,
                    337,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.04226v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.04226v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We present tritonBLAS, a fast and deterministic analytical model that uses architectural parameters like the cache hierarchy, and relative code and data placement to generate performant GPU GEMM kernels. tritonBLAS explicitly models the relationship between architectural topology, matrix shapes, and algorithmic blocking behavior to predict near-optimal configurations without runtime autotuning. Based on this model, we developed and implemented a lightweight GEMM framework entirely within Triton. We evaluate the performance of tritonBLAS across a diverse set of GEMM problem sizes on modern GPUs. tritonBLAS achieves over 95% of the performance of autotuning solutions, while reducing autotuning time to zero. This makes tritonBLAS a practical drop-in replacement for empirical tuning in production HPC and ML workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present tritonBLAS, a fast and deterministic analytical model that uses architectural parameters like the cache hierarchy, and relative code and data placement to generate performant GPU GEMM kernels. tritonBLAS explicitly models the relationship between architectural topology, matrix shapes, and algorithmic blocking behavior to predict near-optimal configurations without runtime autotuning. Based on this model, we developed and implemented a lightweight GEMM framework entirely within Triton. We evaluate the performance of tritonBLAS across a diverse set of GEMM problem sizes on modern GPUs. tritonBLAS achieves over 95% of the performance of autotuning solutions, while reducing autotuning time to zero. This makes tritonBLAS a practical drop-in replacement for empirical tuning in production HPC and ML workloads."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-03T19:46:11Z",
                "published_parsed": [
                    2025,
                    12,
                    3,
                    19,
                    46,
                    11,
                    2,
                    337,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Ryan Swann"
                    },
                    {
                        "name": "Muhammad Osama"
                    },
                    {
                        "name": "Xiaohu Guo"
                    },
                    {
                        "name": "Bryant Nelson"
                    },
                    {
                        "name": "Lixun Zhang"
                    },
                    {
                        "name": "Alex Brown"
                    },
                    {
                        "name": "Yen Ong"
                    },
                    {
                        "name": "Ali Yazdani"
                    },
                    {
                        "name": "Sean Siddens"
                    },
                    {
                        "name": "Ganesh Dasika"
                    },
                    {
                        "name": "Alex Underwood"
                    }
                ],
                "author_detail": {
                    "name": "Alex Underwood"
                },
                "author": "Alex Underwood"
            },
            {
                "id": "http://arxiv.org/abs/2512.04040v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.04040v1",
                "title": "RELIC: Interactive Video World Model with Long-Horizon Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RELIC: Interactive Video World Model with Long-Horizon Memory"
                },
                "updated": "2025-12-03T18:29:20Z",
                "updated_parsed": [
                    2025,
                    12,
                    3,
                    18,
                    29,
                    20,
                    2,
                    337,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.04040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.04040v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "A truly interactive world model requires three key ingredients: real-time long-horizon streaming, consistent spatial memory, and precise user control. However, most existing approaches address only one of these aspects in isolation, as achieving all three simultaneously is highly challenging-for example, long-term memory mechanisms often degrade real-time performance. In this work, we present RELIC, a unified framework that tackles these three challenges altogether. Given a single image and a text description, RELIC enables memory-aware, long-duration exploration of arbitrary scenes in real time. Built upon recent autoregressive video-diffusion distillation techniques, our model represents long-horizon memory using highly compressed historical latent tokens encoded with both relative actions and absolute camera poses within the KV cache. This compact, camera-aware memory structure supports implicit 3D-consistent content retrieval and enforces long-term coherence with minimal computational overhead. In parallel, we fine-tune a bidirectional teacher video model to generate sequences beyond its original 5-second training horizon, and transform it into a causal student generator using a new memory-efficient self-forcing paradigm that enables full-context distillation over long-duration teacher as well as long student self-rollouts. Implemented as a 14B-parameter model and trained on a curated Unreal Engine-rendered dataset, RELIC achieves real-time generation at 16 FPS while demonstrating more accurate action following, more stable long-horizon streaming, and more robust spatial-memory retrieval compared with prior work. These capabilities establish RELIC as a strong foundation for the next generation of interactive world modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A truly interactive world model requires three key ingredients: real-time long-horizon streaming, consistent spatial memory, and precise user control. However, most existing approaches address only one of these aspects in isolation, as achieving all three simultaneously is highly challenging-for example, long-term memory mechanisms often degrade real-time performance. In this work, we present RELIC, a unified framework that tackles these three challenges altogether. Given a single image and a text description, RELIC enables memory-aware, long-duration exploration of arbitrary scenes in real time. Built upon recent autoregressive video-diffusion distillation techniques, our model represents long-horizon memory using highly compressed historical latent tokens encoded with both relative actions and absolute camera poses within the KV cache. This compact, camera-aware memory structure supports implicit 3D-consistent content retrieval and enforces long-term coherence with minimal computational overhead. In parallel, we fine-tune a bidirectional teacher video model to generate sequences beyond its original 5-second training horizon, and transform it into a causal student generator using a new memory-efficient self-forcing paradigm that enables full-context distillation over long-duration teacher as well as long student self-rollouts. Implemented as a 14B-parameter model and trained on a curated Unreal Engine-rendered dataset, RELIC achieves real-time generation at 16 FPS while demonstrating more accurate action following, more stable long-horizon streaming, and more robust spatial-memory retrieval compared with prior work. These capabilities establish RELIC as a strong foundation for the next generation of interactive world modeling."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-03T18:29:20Z",
                "published_parsed": [
                    2025,
                    12,
                    3,
                    18,
                    29,
                    20,
                    2,
                    337,
                    0
                ],
                "arxiv_comment": "22 pages",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Yicong Hong"
                    },
                    {
                        "name": "Yiqun Mei"
                    },
                    {
                        "name": "Chongjian Ge"
                    },
                    {
                        "name": "Yiran Xu"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Sai Bi"
                    },
                    {
                        "name": "Yannick Hold-Geoffroy"
                    },
                    {
                        "name": "Mike Roberts"
                    },
                    {
                        "name": "Matthew Fisher"
                    },
                    {
                        "name": "Eli Shechtman"
                    },
                    {
                        "name": "Kalyan Sunkavalli"
                    },
                    {
                        "name": "Feng Liu"
                    },
                    {
                        "name": "Zhengqi Li"
                    },
                    {
                        "name": "Hao Tan"
                    }
                ],
                "author_detail": {
                    "name": "Hao Tan"
                },
                "author": "Hao Tan"
            },
            {
                "id": "http://arxiv.org/abs/2512.04033v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.04033v1",
                "title": "Demonstration of KV-Class \\b{eta}-Ga2O3 Trench Junction Barrier Schottky Diodes with SpaceModulated Junction Termination Extension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demonstration of KV-Class \\b{eta}-Ga2O3 Trench Junction Barrier Schottky Diodes with SpaceModulated Junction Termination Extension"
                },
                "updated": "2025-12-03T18:17:12Z",
                "updated_parsed": [
                    2025,
                    12,
                    3,
                    18,
                    17,
                    12,
                    2,
                    337,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.04033v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.04033v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In this work, we report on the design and fabrication of p-NiO/Ga2O3 trench junction barrier schottky diodes (JBSD) integrated with space-modulated junction termination extension (SM-JTE) and compare the performance with planar Ni/Ga2O3 schottky diodes (SBDs) and p-NiO/Ga2O3 heterojunction diodes (HJDs). The JBSDs achieved breakdown voltages exceeding 1.8 kV along with low leakage currents (<10-2 A/cm2), while displaying low turn on voltage (VON) of ~1V, which is similar to that of planar Ni/Ga2O3 SBDs. The fabricated devices showed excellent forward characteristics with low differential on-resistance (Ron,sp) ranging from 4-10.5 mΩ-cm2, for fin width between 0.6- 1.25 microns. Best performing device with fin width of 0.85μm showed a unipolar figure of merit (FOM) of ~0.7GW/cm2. This work showcases the benefits of trench JBS design along with SM-JTE edge-termination for efficient high-performance kilovolt-class \\b{eta}- Ga2O3 diodes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we report on the design and fabrication of p-NiO/Ga2O3 trench junction barrier schottky diodes (JBSD) integrated with space-modulated junction termination extension (SM-JTE) and compare the performance with planar Ni/Ga2O3 schottky diodes (SBDs) and p-NiO/Ga2O3 heterojunction diodes (HJDs). The JBSDs achieved breakdown voltages exceeding 1.8 kV along with low leakage currents (<10-2 A/cm2), while displaying low turn on voltage (VON) of ~1V, which is similar to that of planar Ni/Ga2O3 SBDs. The fabricated devices showed excellent forward characteristics with low differential on-resistance (Ron,sp) ranging from 4-10.5 mΩ-cm2, for fin width between 0.6- 1.25 microns. Best performing device with fin width of 0.85μm showed a unipolar figure of merit (FOM) of ~0.7GW/cm2. This work showcases the benefits of trench JBS design along with SM-JTE edge-termination for efficient high-performance kilovolt-class \\b{eta}- Ga2O3 diodes."
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-03T18:17:12Z",
                "published_parsed": [
                    2025,
                    12,
                    3,
                    18,
                    17,
                    12,
                    2,
                    337,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph"
                },
                "authors": [
                    {
                        "name": "Advait Gilankar"
                    },
                    {
                        "name": "Julian Gervassi-Saga"
                    },
                    {
                        "name": "Martha R. McCartney"
                    },
                    {
                        "name": "Nabasindhu Das"
                    },
                    {
                        "name": "David Malcolm McComas"
                    },
                    {
                        "name": "David J. Smith"
                    },
                    {
                        "name": "Nidhin Kurian Kalarickal"
                    }
                ],
                "author_detail": {
                    "name": "Nidhin Kurian Kalarickal"
                },
                "author": "Nidhin Kurian Kalarickal"
            },
            {
                "id": "http://arxiv.org/abs/2512.04025v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.04025v1",
                "title": "PSA: Pyramid Sparse Attention for Efficient Video Understanding and Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PSA: Pyramid Sparse Attention for Efficient Video Understanding and Generation"
                },
                "updated": "2025-12-03T18:02:11Z",
                "updated_parsed": [
                    2025,
                    12,
                    3,
                    18,
                    2,
                    11,
                    2,
                    337,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.04025v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.04025v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Attention mechanisms are the core of foundation models, but their quadratic complexity remains a critical bottleneck for scaling. This challenge has driven the development of efficient attention mechanisms, with sparsity emerging as the dominant paradigm. Current methods typically retain or discard entire key-value blocks with binary masks, resulting in substantial information loss under high sparsity. To mitigate this gap, we present Pyramid Sparse Attention (PSA), a versatile module applicable to both video understanding and generation tasks. Instead of binary masking, PSA introduces multi-level pooled KV representations, enabling finer mask granularity. Specifically, each query block dynamically allocates lower pooling levels to critical KV blocks and higher levels to less important ones, creating an informative interpolation between full retention and complete pruning. This design, analogous to fixed-point quantization and classical feature pyramid networks in computer vision, effectively mitigates information loss while preserving computational efficiency under a low compute budget. It works with a native, hardware-friendly kernel that leverages decoupled block-tile design to ensure efficient execution. Across video understanding and generation benchmarks, PSA preserves contextual information and visual fidelity, consistently outperforming or achieving comparable performance over existing sparse attention baselines with superior efficiency-quality trade-offs. Our code and model weights are publicly available at: http://ziplab.co/PSA",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention mechanisms are the core of foundation models, but their quadratic complexity remains a critical bottleneck for scaling. This challenge has driven the development of efficient attention mechanisms, with sparsity emerging as the dominant paradigm. Current methods typically retain or discard entire key-value blocks with binary masks, resulting in substantial information loss under high sparsity. To mitigate this gap, we present Pyramid Sparse Attention (PSA), a versatile module applicable to both video understanding and generation tasks. Instead of binary masking, PSA introduces multi-level pooled KV representations, enabling finer mask granularity. Specifically, each query block dynamically allocates lower pooling levels to critical KV blocks and higher levels to less important ones, creating an informative interpolation between full retention and complete pruning. This design, analogous to fixed-point quantization and classical feature pyramid networks in computer vision, effectively mitigates information loss while preserving computational efficiency under a low compute budget. It works with a native, hardware-friendly kernel that leverages decoupled block-tile design to ensure efficient execution. Across video understanding and generation benchmarks, PSA preserves contextual information and visual fidelity, consistently outperforming or achieving comparable performance over existing sparse attention baselines with superior efficiency-quality trade-offs. Our code and model weights are publicly available at: http://ziplab.co/PSA"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-03T18:02:11Z",
                "published_parsed": [
                    2025,
                    12,
                    3,
                    18,
                    2,
                    11,
                    2,
                    337,
                    0
                ],
                "arxiv_comment": "Tech report",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Xiaolong Li"
                    },
                    {
                        "name": "Youping Gu"
                    },
                    {
                        "name": "Xi Lin"
                    },
                    {
                        "name": "Weijie Wang"
                    },
                    {
                        "name": "Bohan Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Bohan Zhuang"
                },
                "author": "Bohan Zhuang"
            },
            {
                "id": "http://arxiv.org/abs/2512.03972v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.03972v1",
                "title": "OOPredictor: Predicting Object-Oriented Accesses using Static Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OOPredictor: Predicting Object-Oriented Accesses using Static Analysis"
                },
                "updated": "2025-12-03T17:05:58Z",
                "updated_parsed": [
                    2025,
                    12,
                    3,
                    17,
                    5,
                    58,
                    2,
                    337,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.03972v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.03972v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Object-oriented Programming has become one of the most dominant design paradigms as the separation of concerns and adaptability of design reduce development and maintenance costs. However, the convenience is not without cost. The added indirection inherent in such designs causes excessive pointer chasing, negatively affecting locality, which in turn degrades the performance of cache structures. Furthermore, modern hardware prefetchers are mostly stride prefetchers that are ill-equipped to handle the unpredictability of access patterns generated by pointer chasing. Most software approaches that seek to address this problem resort to profiling the program as it runs, which comes with a significant run-time overhead or requires data from previous runs. In this paper, we propose the use of compile-time static analysis to predict the most common access patterns displayed by a program during run time. Since Java is one of the most popular object-oriented languages, we implement our prototype within the OpenJ9 JVM, inside the OMR optimizer infrastructure. The outputs of our proposed predictor are Markov chains that model the expected behavior of the program. The effectiveness of the proposed predictor is evaluated by comparing the model with the actual run-time behavior of the program measured using an instrumented interpreter. Our experiments show that the proposed predictor exhibits good accuracy and can be used to inform minimally intrusive load stall mitigation strategies, e.g. informing copying GCs on more locality-friendly copying orders",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Object-oriented Programming has become one of the most dominant design paradigms as the separation of concerns and adaptability of design reduce development and maintenance costs. However, the convenience is not without cost. The added indirection inherent in such designs causes excessive pointer chasing, negatively affecting locality, which in turn degrades the performance of cache structures. Furthermore, modern hardware prefetchers are mostly stride prefetchers that are ill-equipped to handle the unpredictability of access patterns generated by pointer chasing. Most software approaches that seek to address this problem resort to profiling the program as it runs, which comes with a significant run-time overhead or requires data from previous runs. In this paper, we propose the use of compile-time static analysis to predict the most common access patterns displayed by a program during run time. Since Java is one of the most popular object-oriented languages, we implement our prototype within the OpenJ9 JVM, inside the OMR optimizer infrastructure. The outputs of our proposed predictor are Markov chains that model the expected behavior of the program. The effectiveness of the proposed predictor is evaluated by comparing the model with the actual run-time behavior of the program measured using an instrumented interpreter. Our experiments show that the proposed predictor exhibits good accuracy and can be used to inform minimally intrusive load stall mitigation strategies, e.g. informing copying GCs on more locality-friendly copying orders"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-03T17:05:58Z",
                "published_parsed": [
                    2025,
                    12,
                    3,
                    17,
                    5,
                    58,
                    2,
                    337,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL"
                },
                "authors": [
                    {
                        "name": "Hassan Arafat"
                    },
                    {
                        "name": "David Bremner"
                    },
                    {
                        "name": "Kenneth B. Kent"
                    },
                    {
                        "name": "Julian Wang"
                    }
                ],
                "author_detail": {
                    "name": "Julian Wang"
                },
                "author": "Julian Wang"
            },
            {
                "id": "http://arxiv.org/abs/2512.03927v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.03927v1",
                "title": "OD-MoE: On-Demand Expert Loading for Cacheless Edge-Distributed MoE Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OD-MoE: On-Demand Expert Loading for Cacheless Edge-Distributed MoE Inference"
                },
                "updated": "2025-12-03T16:27:16Z",
                "updated_parsed": [
                    2025,
                    12,
                    3,
                    16,
                    27,
                    16,
                    2,
                    337,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.03927v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.03927v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Mixture-of-Experts (MoE), while offering significant advantages as a Large Language Model (LLM) architecture, faces substantial challenges when deployed on low-cost edge devices with tight memory constraints. Expert offloading mitigates this issue by storing expert parameters in CPU memory and caching a subset of popular experts in GPU memory. Although this approach improves GPU memory utilization by caching only the likely-used experts, the GPU memory reserved for expert caching is underutilized compared with dense LLMs. This paper presents OD-MoE, a distributed MoE inference framework that obviates the need for expert caches via fully on-demand expert loading. OD-MoE is built upon two key mechanisms: 1) parallelizing expert loading and expert computation across distributed edge nodes, and 2) an ultra-accurate emulative predictor that forecasts expert activations multiple layers ahead while expert computation is ongoing. With these innovations, OD-MoE dynamically loads each target expert to one of the distributed nodes just-in-time before its activation and promptly evicts it afterward, freeing GPU memory for subsequent experts. We comprehensively benchmark OD-MoE against state-of-the-art MoE offloading systems on a ten-node testbed. Experimental results show that: 1) OD-MoE achieves 99.94% expert activation prediction accuracy, substantially surpassing all existing methods; and 2) OD-MoE delivers approximately 75% of the decoding speed of a fully GPU-cached MoE deployment while using only 1/3 of the GPU memory. More importantly, by eliminating the need for expert caches, OD-MoE enables MoE inference on edge nodes with less-than-1GB GPU memory, paving the way for practical MoE deployment of low-cost IoT devices at the edge in the LLM era.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE), while offering significant advantages as a Large Language Model (LLM) architecture, faces substantial challenges when deployed on low-cost edge devices with tight memory constraints. Expert offloading mitigates this issue by storing expert parameters in CPU memory and caching a subset of popular experts in GPU memory. Although this approach improves GPU memory utilization by caching only the likely-used experts, the GPU memory reserved for expert caching is underutilized compared with dense LLMs. This paper presents OD-MoE, a distributed MoE inference framework that obviates the need for expert caches via fully on-demand expert loading. OD-MoE is built upon two key mechanisms: 1) parallelizing expert loading and expert computation across distributed edge nodes, and 2) an ultra-accurate emulative predictor that forecasts expert activations multiple layers ahead while expert computation is ongoing. With these innovations, OD-MoE dynamically loads each target expert to one of the distributed nodes just-in-time before its activation and promptly evicts it afterward, freeing GPU memory for subsequent experts. We comprehensively benchmark OD-MoE against state-of-the-art MoE offloading systems on a ten-node testbed. Experimental results show that: 1) OD-MoE achieves 99.94% expert activation prediction accuracy, substantially surpassing all existing methods; and 2) OD-MoE delivers approximately 75% of the decoding speed of a fully GPU-cached MoE deployment while using only 1/3 of the GPU memory. More importantly, by eliminating the need for expert caches, OD-MoE enables MoE inference on edge nodes with less-than-1GB GPU memory, paving the way for practical MoE deployment of low-cost IoT devices at the edge in the LLM era."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-03T16:27:16Z",
                "published_parsed": [
                    2025,
                    12,
                    3,
                    16,
                    27,
                    16,
                    2,
                    337,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Liujianfu Wang"
                    },
                    {
                        "name": "Yuyang Du"
                    },
                    {
                        "name": "Yuchen Pan"
                    },
                    {
                        "name": "Soung Chang Liew"
                    },
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Kexin Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kexin Chen"
                },
                "author": "Kexin Chen"
            },
            {
                "id": "http://arxiv.org/abs/2408.05235v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2408.05235v2",
                "title": "SLO-aware GPU Frequency Scaling for Energy Efficient LLM Inference Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLO-aware GPU Frequency Scaling for Energy Efficient LLM Inference Serving"
                },
                "updated": "2025-12-03T16:21:24Z",
                "updated_parsed": [
                    2025,
                    12,
                    3,
                    16,
                    21,
                    24,
                    2,
                    337,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2408.05235v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2408.05235v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As Large Language Models (LLMs) gain traction, their reliance on power-hungry GPUs places ever-increasing energy demands, raising environmental and monetary concerns. Inference dominates LLM workloads, presenting a critical challenge for providers: minimizing energy costs under Service-Level Objectives (SLOs) that ensure optimal user experience. In this paper, we present \\textit{throttLL'eM}, a framework that reduces energy consumption while meeting SLOs through the use of instance and GPU frequency scaling. \\textit{throttLL'eM} features mechanisms that project future KV cache usage and batch size. Leveraging a Machine-Learning (ML) model that receives these projections as inputs, \\textit{throttLL'eM} manages performance at the iteration level to satisfy SLOs with reduced frequencies and instance sizes. We show that the proposed ML model achieves $R^2$ scores greater than 0.97 and miss-predicts performance by less than 1 iteration per second on average. Experimental results on LLM inference traces show that \\textit{throttLL'eM} achieves up to 43.8\\% lower energy consumption and an energy efficiency improvement of at least $1.71\\times$ under SLOs, when compared to NVIDIA's Triton server.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) gain traction, their reliance on power-hungry GPUs places ever-increasing energy demands, raising environmental and monetary concerns. Inference dominates LLM workloads, presenting a critical challenge for providers: minimizing energy costs under Service-Level Objectives (SLOs) that ensure optimal user experience. In this paper, we present \\textit{throttLL'eM}, a framework that reduces energy consumption while meeting SLOs through the use of instance and GPU frequency scaling. \\textit{throttLL'eM} features mechanisms that project future KV cache usage and batch size. Leveraging a Machine-Learning (ML) model that receives these projections as inputs, \\textit{throttLL'eM} manages performance at the iteration level to satisfy SLOs with reduced frequencies and instance sizes. We show that the proposed ML model achieves $R^2$ scores greater than 0.97 and miss-predicts performance by less than 1 iteration per second on average. Experimental results on LLM inference traces show that \\textit{throttLL'eM} achieves up to 43.8\\% lower energy consumption and an energy efficiency improvement of at least $1.71\\times$ under SLOs, when compared to NVIDIA's Triton server."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-08-05T09:07:06Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    9,
                    7,
                    6,
                    0,
                    218,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Andreas Kosmas Kakolyris"
                    },
                    {
                        "name": "Dimosthenis Masouros"
                    },
                    {
                        "name": "Petros Vavaroutsos"
                    },
                    {
                        "name": "Sotirios Xydis"
                    },
                    {
                        "name": "Dimitrios Soudris"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios Soudris"
                },
                "author": "Dimitrios Soudris"
            },
            {
                "id": "http://arxiv.org/abs/2512.03870v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.03870v1",
                "title": "Reconstructing KV Caches with Cross-layer Fusion For Enhanced Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconstructing KV Caches with Cross-layer Fusion For Enhanced Transformers"
                },
                "updated": "2025-12-03T15:22:00Z",
                "updated_parsed": [
                    2025,
                    12,
                    3,
                    15,
                    22,
                    0,
                    2,
                    337,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.03870v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.03870v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Transformer decoders have achieved strong results across tasks, but the memory required for the KV cache becomes prohibitive at long sequence lengths. Although Cross-layer KV Cache sharing (e.g., YOCO, CLA) offers a path to mitigate KV Cache bottleneck, it typically underperforms within-layer methods like GQA. To understand the root cause, we investigate the information flow of keys and values of the top-layers. Our preliminary reveals a clear distribution: values are predominantly derived from the bottom layer, while keys draw more information from both bottom and middle layers. Building upon this, we propose FusedKV, whose top-layer KV caches are a learnable fusion of the most informative ones from the bottom and middle layers. This fusion operates directly on post-RoPE keys, preserving relative positional information without the computational cost of re-applying rotary embeddings. To further improve efficiency, we propose FusedKV-Lite, an cross-layer sharing approach, where top-layer KV caches are directly derived from the bottom-layer values and the middle-layer keys. Compared to FusedKV, FusedKV-Lite reduces I/O overhead at the cost of a slight increase in perplexity. In experiments on LLMs ranging from 332M to 4B parameters, our proposed method reduce 50\\% cache memory while achieving lower validation perplexity than the standard Transformer decoder, establishing it as a memory-efficient, high-performance architectural alternative.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer decoders have achieved strong results across tasks, but the memory required for the KV cache becomes prohibitive at long sequence lengths. Although Cross-layer KV Cache sharing (e.g., YOCO, CLA) offers a path to mitigate KV Cache bottleneck, it typically underperforms within-layer methods like GQA. To understand the root cause, we investigate the information flow of keys and values of the top-layers. Our preliminary reveals a clear distribution: values are predominantly derived from the bottom layer, while keys draw more information from both bottom and middle layers. Building upon this, we propose FusedKV, whose top-layer KV caches are a learnable fusion of the most informative ones from the bottom and middle layers. This fusion operates directly on post-RoPE keys, preserving relative positional information without the computational cost of re-applying rotary embeddings. To further improve efficiency, we propose FusedKV-Lite, an cross-layer sharing approach, where top-layer KV caches are directly derived from the bottom-layer values and the middle-layer keys. Compared to FusedKV, FusedKV-Lite reduces I/O overhead at the cost of a slight increase in perplexity. In experiments on LLMs ranging from 332M to 4B parameters, our proposed method reduce 50\\% cache memory while achieving lower validation perplexity than the standard Transformer decoder, establishing it as a memory-efficient, high-performance architectural alternative."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-03T15:22:00Z",
                "published_parsed": [
                    2025,
                    12,
                    3,
                    15,
                    22,
                    0,
                    2,
                    337,
                    0
                ],
                "arxiv_comment": "under review",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Hongzhan Lin"
                    },
                    {
                        "name": "Zhiqi Bai"
                    },
                    {
                        "name": "Xinmiao Zhang"
                    },
                    {
                        "name": "Sen Yang"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Siran Yang"
                    },
                    {
                        "name": "Yunlong Xu"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Yongchi Zhao"
                    },
                    {
                        "name": "Jiamang Wang"
                    },
                    {
                        "name": "Yuchi Xu"
                    },
                    {
                        "name": "Wenbo Su"
                    },
                    {
                        "name": "Bo Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zheng"
                },
                "author": "Bo Zheng"
            },
            {
                "id": "http://arxiv.org/abs/2512.03608v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.03608v1",
                "title": "KVNAND: Efficient On-Device Large Language Model Inference Using DRAM-Free In-Flash Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVNAND: Efficient On-Device Large Language Model Inference Using DRAM-Free In-Flash Computing"
                },
                "updated": "2025-12-03T09:41:03Z",
                "updated_parsed": [
                    2025,
                    12,
                    3,
                    9,
                    41,
                    3,
                    2,
                    337,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.03608v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.03608v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Deploying large language models (LLMs) on edge devices enables personalized agents with strong privacy and low cost. However, with tens to hundreds of billions of parameters, single-batch autoregressive inference suffers from extremely low arithmetic intensity, creating severe weight-loading and bandwidth pressures on resource-constrained platforms. Recent in-flash computing (IFC) solutions alleviate this bottleneck by co-locating weight-related linear computations in the decode phase with flash, yet still rely on DRAM for the key-value (KV) cache. As context length grows, the KV cache can exceed model weights in size, imposing prohibitive DRAM cost and capacity requirements. Attempts to offload KV cache to flash suffer from severe performance penalties.\n  We propose KVNAND, the first DRAM-free, IFC-based architecture that stores both model weights and KV cache entirely in compute-enabled 3D NAND flash. KVNAND addresses the fundamental performance challenges of flash under intensive KV cache access by leveraging IFC for all memory-bound operations to reduce data transfer overhead, introducing head-group parallelism to boost throughput, and employing page-level KV cache mapping to align token access patterns with flash organization. In addition, we propose a design space exploration framework that evaluates discrete and compact KVNAND variants to balance weight and KV placement, automatically identifying the optimal design trade-off. These techniques mitigate latency, energy, and reliability concerns, turning flash into a practical medium for long-context KV storage. Evaluations on MHA 7B and GQA 70B LLMs show that KVNAND achieves 1.98\\(\\times\\)/1.94\\(\\times\\)/2.05\\(\\times\\) geomean speedup at 128/1K/10K-token contexts compared to DRAM-equipped IFC designs and addresses out-of-memory failures at 100K context length.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying large language models (LLMs) on edge devices enables personalized agents with strong privacy and low cost. However, with tens to hundreds of billions of parameters, single-batch autoregressive inference suffers from extremely low arithmetic intensity, creating severe weight-loading and bandwidth pressures on resource-constrained platforms. Recent in-flash computing (IFC) solutions alleviate this bottleneck by co-locating weight-related linear computations in the decode phase with flash, yet still rely on DRAM for the key-value (KV) cache. As context length grows, the KV cache can exceed model weights in size, imposing prohibitive DRAM cost and capacity requirements. Attempts to offload KV cache to flash suffer from severe performance penalties.\n  We propose KVNAND, the first DRAM-free, IFC-based architecture that stores both model weights and KV cache entirely in compute-enabled 3D NAND flash. KVNAND addresses the fundamental performance challenges of flash under intensive KV cache access by leveraging IFC for all memory-bound operations to reduce data transfer overhead, introducing head-group parallelism to boost throughput, and employing page-level KV cache mapping to align token access patterns with flash organization. In addition, we propose a design space exploration framework that evaluates discrete and compact KVNAND variants to balance weight and KV placement, automatically identifying the optimal design trade-off. These techniques mitigate latency, energy, and reliability concerns, turning flash into a practical medium for long-context KV storage. Evaluations on MHA 7B and GQA 70B LLMs show that KVNAND achieves 1.98\\(\\times\\)/1.94\\(\\times\\)/2.05\\(\\times\\) geomean speedup at 128/1K/10K-token contexts compared to DRAM-equipped IFC designs and addresses out-of-memory failures at 100K context length."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-03T09:41:03Z",
                "published_parsed": [
                    2025,
                    12,
                    3,
                    9,
                    41,
                    3,
                    2,
                    337,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Lishuo Deng"
                    },
                    {
                        "name": "Shaojie Xu"
                    },
                    {
                        "name": "Jinwu Chen"
                    },
                    {
                        "name": "Changwei Yan"
                    },
                    {
                        "name": "Jiajie Wang"
                    },
                    {
                        "name": "Zhe Jiang"
                    },
                    {
                        "name": "Weiwei Shan"
                    }
                ],
                "author_detail": {
                    "name": "Weiwei Shan"
                },
                "author": "Weiwei Shan"
            },
            {
                "id": "http://arxiv.org/abs/2510.08351v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.08351v2",
                "title": "Fletch: File-System Metadata Caching in Programmable Switches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fletch: File-System Metadata Caching in Programmable Switches"
                },
                "updated": "2025-12-03T09:23:58Z",
                "updated_parsed": [
                    2025,
                    12,
                    3,
                    9,
                    23,
                    58,
                    2,
                    337,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.08351v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.08351v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Fast and scalable metadata management across multiple metadata servers is crucial for distributed file systems to handle numerous files and directories. Client-side caching of frequently accessed metadata can mitigate server loads, but incurs significant overhead and complexity in maintaining cache consistency when the number of clients increases. We explore caching in programmable switches by serving file-system metadata requests from multiple clients on the switch data plane. Despite prior efforts on in-switch key-value caching, they fail to address the path dependencies specific to file-system semantics. We propose Fletch, an in-switch file-system metadata caching framework that leverages programmable switches to serve file-system metadata requests from multiple clients directly in the switch data plane. Unlike prior in-switch key-value caching approaches, Fletch addresses file-system-specific path dependencies under stringent switch resource constraints. We implement Fletch atop Hadoop HDFS and evaluate it on a Tofino-switch testbed using real-world file-system metadata workloads. Fletch achieves up to 181.6% higher throughput than vanilla HDFS and complements client-side caching with additional throughput gains of up to 139.6%. It also incurs low latencies and limited switch resource usage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast and scalable metadata management across multiple metadata servers is crucial for distributed file systems to handle numerous files and directories. Client-side caching of frequently accessed metadata can mitigate server loads, but incurs significant overhead and complexity in maintaining cache consistency when the number of clients increases. We explore caching in programmable switches by serving file-system metadata requests from multiple clients on the switch data plane. Despite prior efforts on in-switch key-value caching, they fail to address the path dependencies specific to file-system semantics. We propose Fletch, an in-switch file-system metadata caching framework that leverages programmable switches to serve file-system metadata requests from multiple clients directly in the switch data plane. Unlike prior in-switch key-value caching approaches, Fletch addresses file-system-specific path dependencies under stringent switch resource constraints. We implement Fletch atop Hadoop HDFS and evaluate it on a Tofino-switch testbed using real-world file-system metadata workloads. Fletch achieves up to 181.6% higher throughput than vanilla HDFS and complements client-side caching with additional throughput gains of up to 139.6%. It also incurs low latencies and limited switch resource usage."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-09T15:38:13Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    15,
                    38,
                    13,
                    3,
                    282,
                    0
                ],
                "arxiv_comment": "13 pages",
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Qingxiu Liu"
                    },
                    {
                        "name": "Jiazhen Cai"
                    },
                    {
                        "name": "Siyuan Sheng"
                    },
                    {
                        "name": "Yuhui Chen"
                    },
                    {
                        "name": "Lu Tang"
                    },
                    {
                        "name": "Zhirong Shen"
                    },
                    {
                        "name": "Patrick P. C. Lee"
                    }
                ],
                "author_detail": {
                    "name": "Patrick P. C. Lee"
                },
                "author": "Patrick P. C. Lee"
            },
            {
                "id": "http://arxiv.org/abs/2512.02513v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.02513v2",
                "title": "Decentralized Fairness Aware Multi Task Federated Learning for VR Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralized Fairness Aware Multi Task Federated Learning for VR Network"
                },
                "updated": "2025-12-03T08:13:00Z",
                "updated_parsed": [
                    2025,
                    12,
                    3,
                    8,
                    13,
                    0,
                    2,
                    337,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.02513v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.02513v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Wireless connectivity promises to unshackle virtual reality (VR) experiences, allowing users to engage from anywhere, anytime. However, delivering seamless, high-quality, real-time VR video wirelessly is challenging due to the stringent quality of experience requirements, low latency constraints, and limited VR device capabilities. This paper addresses these challenges by introducing a novel decentralized multi task fair federated learning (DMTFL) based caching that caches and prefetches each VR user's field of view (FOV) at base stations (BSs) based on the caching strategies tailored to each BS. In federated learning (FL) in its naive form, often biases toward certain users, and a single global model fails to capture the statistical heterogeneity across users and BSs. In contrast, the proposed DMTFL algorithm personalizes content delivery by learning individual caching models at each BS. These models are further optimized to perform well under any target distribution, while providing theoretical guarantees via Rademacher complexity and a probably approximately correct (PAC) bound on the loss. Using a realistic VR head-tracking dataset, our simulations demonstrate the superiority of our proposed DMTFL algorithm compared to baseline algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wireless connectivity promises to unshackle virtual reality (VR) experiences, allowing users to engage from anywhere, anytime. However, delivering seamless, high-quality, real-time VR video wirelessly is challenging due to the stringent quality of experience requirements, low latency constraints, and limited VR device capabilities. This paper addresses these challenges by introducing a novel decentralized multi task fair federated learning (DMTFL) based caching that caches and prefetches each VR user's field of view (FOV) at base stations (BSs) based on the caching strategies tailored to each BS. In federated learning (FL) in its naive form, often biases toward certain users, and a single global model fails to capture the statistical heterogeneity across users and BSs. In contrast, the proposed DMTFL algorithm personalizes content delivery by learning individual caching models at each BS. These models are further optimized to perform well under any target distribution, while providing theoretical guarantees via Rademacher complexity and a probably approximately correct (PAC) bound on the loss. Using a realistic VR head-tracking dataset, our simulations demonstrate the superiority of our proposed DMTFL algorithm compared to baseline algorithms."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-02T08:13:38Z",
                "published_parsed": [
                    2025,
                    12,
                    2,
                    8,
                    13,
                    38,
                    1,
                    336,
                    0
                ],
                "arxiv_comment": "accepted at IEEE Globecom Workshop 2025",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Krishnendu S. Tharakan"
                    },
                    {
                        "name": "Carlo Fischione"
                    }
                ],
                "author_detail": {
                    "name": "Carlo Fischione"
                },
                "author": "Carlo Fischione"
            },
            {
                "id": "http://arxiv.org/abs/2512.03324v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.03324v1",
                "title": "Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs"
                },
                "updated": "2025-12-03T00:20:35Z",
                "updated_parsed": [
                    2025,
                    12,
                    3,
                    0,
                    20,
                    35,
                    2,
                    337,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.03324v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.03324v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Memory and computation remain core bottlenecks in long-horizon LLM inference due to the quadratic cost of self-attention and the ever-growing key-value (KV) cache. Existing strategies for memory-bounded inference, such as quantization, offloading, or heuristic KV eviction, either incur high orchestration costs or rely on unreliable attention-based proxies of importance. We propose TRIM-KV, a novel approach that learns each token's intrinsic importance at creation time via a lightweight retention gate. Each gate predicts a scalar retention score that decays over time, reflecting the long-term utility of the token for a specific layer and head. Tokens with low scores are evicted when the memory budget is exceeded, ensuring that the cache always contains the most critical tokens. TRIM-KV is trained efficiently through distillation from a frozen LLM combined with a capacity loss, requiring only gate fine-tuning and adding negligible inference overhead. Across mathematical reasoning (GSM8K, MATH-500, AIME24), procedural generation (LongProc), conversational long-memory benchmarks (LongMemEval), and long-context understanding (LongBench and SCBench), TRIM-KV consistently outperforms strong eviction and learnable retrieval baselines, especially in low-memory regimes. Remarkably, it even surpasses full-cache models in some settings, showing that selective retention can serve as a form of regularization, suppressing noise from uninformative tokens. Qualitative analyses further reveal that learned retention scores align with human intuition, naturally recovering heuristics such as sink tokens, sliding windows, and gist compression without explicit design. Beyond efficiency, retention scores provide insights into layer- and head-specific roles, suggesting a new path toward LLM interpretability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory and computation remain core bottlenecks in long-horizon LLM inference due to the quadratic cost of self-attention and the ever-growing key-value (KV) cache. Existing strategies for memory-bounded inference, such as quantization, offloading, or heuristic KV eviction, either incur high orchestration costs or rely on unreliable attention-based proxies of importance. We propose TRIM-KV, a novel approach that learns each token's intrinsic importance at creation time via a lightweight retention gate. Each gate predicts a scalar retention score that decays over time, reflecting the long-term utility of the token for a specific layer and head. Tokens with low scores are evicted when the memory budget is exceeded, ensuring that the cache always contains the most critical tokens. TRIM-KV is trained efficiently through distillation from a frozen LLM combined with a capacity loss, requiring only gate fine-tuning and adding negligible inference overhead. Across mathematical reasoning (GSM8K, MATH-500, AIME24), procedural generation (LongProc), conversational long-memory benchmarks (LongMemEval), and long-context understanding (LongBench and SCBench), TRIM-KV consistently outperforms strong eviction and learnable retrieval baselines, especially in low-memory regimes. Remarkably, it even surpasses full-cache models in some settings, showing that selective retention can serve as a form of regularization, suppressing noise from uninformative tokens. Qualitative analyses further reveal that learned retention scores align with human intuition, naturally recovering heuristics such as sink tokens, sliding windows, and gist compression without explicit design. Beyond efficiency, retention scores provide insights into layer- and head-specific roles, suggesting a new path toward LLM interpretability."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-03T00:20:35Z",
                "published_parsed": [
                    2025,
                    12,
                    3,
                    0,
                    20,
                    35,
                    2,
                    337,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Ngoc Bui"
                    },
                    {
                        "name": "Shubham Sharma"
                    },
                    {
                        "name": "Simran Lamba"
                    },
                    {
                        "name": "Saumitra Mishra"
                    },
                    {
                        "name": "Rex Ying"
                    }
                ],
                "author_detail": {
                    "name": "Rex Ying"
                },
                "author": "Rex Ying"
            },
            {
                "id": "http://arxiv.org/abs/2512.03007v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.03007v1",
                "title": "Generation of strong ultralow-phase-noise microwave fields with tunable ellipticity for ultracold polar molecules",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generation of strong ultralow-phase-noise microwave fields with tunable ellipticity for ultracold polar molecules"
                },
                "updated": "2025-12-02T18:32:11Z",
                "updated_parsed": [
                    2025,
                    12,
                    2,
                    18,
                    32,
                    11,
                    1,
                    336,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.03007v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.03007v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Microwave(MW) fields with strong field strength, ultralow phase-noise and tunable polarization are crucial for stabilizing and manipulating ultracold polar molecules, which have emerged as a promising platform for quantum sciences. In this letter, we present the design, characterization, and performance of a robust MW setup tailored for precise control of molecular states. This setup achieves a high electric field intensity of 6.9 kV/m in the near-field from a dual-feed waveguide antenna, enabling a Rabi frequency as high as 71 MHz for the rotational transition of sodium-potassium molecules. In addition, the low noise signal source and controlled electronics provide ultralow phase-noise and dynamically tunable polarization. Narrow-band filters within the MW circuitry further reduce phase-noise by more than 20 dB at 20 MHz offset frequency, ensuring prolonged one-body molecular lifetimes up to 10 seconds. We also show practical methods to measure the MW field strength and polarization using a simple homemade dipole probe, and to characterize phase-noise down to -170 dBc/Hz with a commercial spectrum analyser and a notch filter. Those capabilities allowed us to evaporatively cool our molecular sample to deep quantum degeneracy. Furthermore, the polarization tunability enabled the observation of field-linked resonances and facilitated the creation of field-linked tetramers.These techniques advance the study of ultracold polar molecules and broaden the potential applications of MW tools in other platforms of quantum sciences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Microwave(MW) fields with strong field strength, ultralow phase-noise and tunable polarization are crucial for stabilizing and manipulating ultracold polar molecules, which have emerged as a promising platform for quantum sciences. In this letter, we present the design, characterization, and performance of a robust MW setup tailored for precise control of molecular states. This setup achieves a high electric field intensity of 6.9 kV/m in the near-field from a dual-feed waveguide antenna, enabling a Rabi frequency as high as 71 MHz for the rotational transition of sodium-potassium molecules. In addition, the low noise signal source and controlled electronics provide ultralow phase-noise and dynamically tunable polarization. Narrow-band filters within the MW circuitry further reduce phase-noise by more than 20 dB at 20 MHz offset frequency, ensuring prolonged one-body molecular lifetimes up to 10 seconds. We also show practical methods to measure the MW field strength and polarization using a simple homemade dipole probe, and to characterize phase-noise down to -170 dBc/Hz with a commercial spectrum analyser and a notch filter. Those capabilities allowed us to evaporatively cool our molecular sample to deep quantum degeneracy. Furthermore, the polarization tunability enabled the observation of field-linked resonances and facilitated the creation of field-linked tetramers.These techniques advance the study of ultracold polar molecules and broaden the potential applications of MW tools in other platforms of quantum sciences."
                },
                "tags": [
                    {
                        "term": "physics.atom-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.quant-gas",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-02T18:32:11Z",
                "published_parsed": [
                    2025,
                    12,
                    2,
                    18,
                    32,
                    11,
                    1,
                    336,
                    0
                ],
                "arxiv_comment": "11 pages, 7 figures",
                "arxiv_primary_category": {
                    "term": "physics.atom-ph"
                },
                "authors": [
                    {
                        "name": "Shrestha Biswas"
                    },
                    {
                        "name": "Sebastian Eppelt"
                    },
                    {
                        "name": "Christian Buchberger"
                    },
                    {
                        "name": "Xing-Yan Chen"
                    },
                    {
                        "name": "Andreas Schindewolf"
                    },
                    {
                        "name": "Michael Hani"
                    },
                    {
                        "name": "Erwin Biebl"
                    },
                    {
                        "name": "Immanuel Bloch"
                    },
                    {
                        "name": "Xin-Yu Luo"
                    }
                ],
                "author_detail": {
                    "name": "Xin-Yu Luo"
                },
                "author": "Xin-Yu Luo"
            },
            {
                "id": "http://arxiv.org/abs/2505.04216v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.04216v2",
                "title": "Computational Model for Photoionization in Pure SF6 Streamer at 1-15 atm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational Model for Photoionization in Pure SF6 Streamer at 1-15 atm"
                },
                "updated": "2025-12-02T07:05:10Z",
                "updated_parsed": [
                    2025,
                    12,
                    2,
                    7,
                    5,
                    10,
                    1,
                    336,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.04216v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.04216v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1088/1361-6595/ae259e",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Photoionization plays a crucial role in achieving accurate quantitative predictions in SF6 streamer simulations, but accurate models for SF6 photoionization remains limited, motivating this paper. First, we develop a computational model for SF6 photoionization and provide the detailed theoretical modeling process, as well as comparison between experiment and simulation. A concise summary of model parameters within the comprehensive pressure range of 1 - 15 atm is provided for direct reference. Then, we perform comparative studies against simplified approaches. The results demonstrate that the proposed model effectively captures the non-local effects of SF6 photoionization, enhancing both the spatial numerical convergence and the accuracy of the streamer structure. Finally, we perform comparative studies by artificially increasing the photoionization intensity through multiplying the photoionization source term Sph by a factor of 50 (50*Sph) relative to the baseline intensity. Regarding breakdown voltage prediction, 50*Sph leads to a significant underestimation of the breakdown voltage for positive streamers, introducing errors greater than 0.5 kV, while exerting a small impact on negative streamers. Regarding streamer propagation dynamics, the radius of the positive streamer head exhibits pronounced shrinking, and 50*Sph reduces this shrinking and significantly lowers the head field by more than 700 Td. In contrast, 50*Sph has little impact on the morphology of the negative streamers and slightly enhances the head field by less than 30 Td.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Photoionization plays a crucial role in achieving accurate quantitative predictions in SF6 streamer simulations, but accurate models for SF6 photoionization remains limited, motivating this paper. First, we develop a computational model for SF6 photoionization and provide the detailed theoretical modeling process, as well as comparison between experiment and simulation. A concise summary of model parameters within the comprehensive pressure range of 1 - 15 atm is provided for direct reference. Then, we perform comparative studies against simplified approaches. The results demonstrate that the proposed model effectively captures the non-local effects of SF6 photoionization, enhancing both the spatial numerical convergence and the accuracy of the streamer structure. Finally, we perform comparative studies by artificially increasing the photoionization intensity through multiplying the photoionization source term Sph by a factor of 50 (50*Sph) relative to the baseline intensity. Regarding breakdown voltage prediction, 50*Sph leads to a significant underestimation of the breakdown voltage for positive streamers, introducing errors greater than 0.5 kV, while exerting a small impact on negative streamers. Regarding streamer propagation dynamics, the radius of the positive streamer head exhibits pronounced shrinking, and 50*Sph reduces this shrinking and significantly lowers the head field by more than 700 Td. In contrast, 50*Sph has little impact on the morphology of the negative streamers and slightly enhances the head field by less than 30 Td."
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-07T08:10:39Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    8,
                    10,
                    39,
                    2,
                    127,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph"
                },
                "arxiv_journal_ref": "Plasma Sources Sci. Technol. (2025)",
                "authors": [
                    {
                        "name": "Zihao Feng"
                    },
                    {
                        "name": "Liyang Zhang"
                    },
                    {
                        "name": "Xiaobing Zou"
                    },
                    {
                        "name": "Haiyun Luo"
                    }
                ],
                "author_detail": {
                    "name": "Haiyun Luo"
                },
                "author": "Haiyun Luo",
                "arxiv_doi": "10.1088/1361-6595/ae259e"
            },
            {
                "id": "http://arxiv.org/abs/2512.02444v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.02444v1",
                "title": "QJoin: Transformation-aware Joinable Data Discovery Using Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QJoin: Transformation-aware Joinable Data Discovery Using Reinforcement Learning"
                },
                "updated": "2025-12-02T06:05:48Z",
                "updated_parsed": [
                    2025,
                    12,
                    2,
                    6,
                    5,
                    48,
                    1,
                    336,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.02444v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.02444v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Discovering which tables in large, heterogeneous repositories can be joined and by what transformations is a central challenge in data integration and data discovery. Traditional join discovery methods are largely designed for equi-joins, which assume that join keys match exactly or nearly so. These techniques, while efficient in clean, well-normalized databases, fail in open or federated settings where identifiers are inconsistently formatted, embedded, or split across multiple columns. Approximate or fuzzy joins alleviate minor string variations but cannot capture systematic transformations. We introduce QJoin, a reinforcement-learning framework that learns and reuses transformation strategies across join tasks. QJoin trains an agent under a uniqueness-aware reward that balances similarity with key distinctiveness, enabling it to explore concise, high-value transformation chains. To accelerate new joins, we introduce two reuse mechanisms: (i) agent transfer, which initializes new policies from pretrained agents, and (ii) transformation reuse, which caches successful operator sequences for similar column clusters. On the AutoJoin Web benchmark (31 table pairs), QJoin achieves an average F1-score of 91.0%. For 19,990 join tasks in NYC+Chicago open datasets, Qjoin reduces runtime by up to 7.4% (13,747 s) by using reusing. These results demonstrate that transformation learning and reuse can make join discovery both more accurate and more efficient.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discovering which tables in large, heterogeneous repositories can be joined and by what transformations is a central challenge in data integration and data discovery. Traditional join discovery methods are largely designed for equi-joins, which assume that join keys match exactly or nearly so. These techniques, while efficient in clean, well-normalized databases, fail in open or federated settings where identifiers are inconsistently formatted, embedded, or split across multiple columns. Approximate or fuzzy joins alleviate minor string variations but cannot capture systematic transformations. We introduce QJoin, a reinforcement-learning framework that learns and reuses transformation strategies across join tasks. QJoin trains an agent under a uniqueness-aware reward that balances similarity with key distinctiveness, enabling it to explore concise, high-value transformation chains. To accelerate new joins, we introduce two reuse mechanisms: (i) agent transfer, which initializes new policies from pretrained agents, and (ii) transformation reuse, which caches successful operator sequences for similar column clusters. On the AutoJoin Web benchmark (31 table pairs), QJoin achieves an average F1-score of 91.0%. For 19,990 join tasks in NYC+Chicago open datasets, Qjoin reduces runtime by up to 7.4% (13,747 s) by using reusing. These results demonstrate that transformation learning and reuse can make join discovery both more accurate and more efficient."
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-02T06:05:48Z",
                "published_parsed": [
                    2025,
                    12,
                    2,
                    6,
                    5,
                    48,
                    1,
                    336,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB"
                },
                "authors": [
                    {
                        "name": "Ning Wang"
                    },
                    {
                        "name": "Sainyam Galhotra"
                    }
                ],
                "author_detail": {
                    "name": "Sainyam Galhotra"
                },
                "author": "Sainyam Galhotra"
            },
            {
                "id": "http://arxiv.org/abs/2512.02337v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.02337v1",
                "title": "SpecPV: Improving Self-Speculative Decoding for Long-Context Generation via Partial Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecPV: Improving Self-Speculative Decoding for Long-Context Generation via Partial Verification"
                },
                "updated": "2025-12-02T02:15:33Z",
                "updated_parsed": [
                    2025,
                    12,
                    2,
                    2,
                    15,
                    33,
                    1,
                    336,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.02337v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.02337v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Growing demands from tasks like code generation, deep reasoning, and long-document understanding have made long-context generation a crucial capability for large language models (LLMs). Speculative decoding is one of the most direct and effective approaches for accelerating generation. It follows a draft-verify paradigm, where a lightweight draft model proposes several candidate tokens and the target model verifies them. However, we find that as the context length grows, verification becomes the dominant bottleneck. To further accelerate speculative decoding in long-context generation, we introduce SpecPV, a self-speculative decoding approach that performs fast verification using partial key-value states (KV) and periodically applies full verification to eliminate accumulated errors. We validate SpecPV across multiple long-context benchmarks and models, including LLaMA-3.1-8B-Instruct and Qwen3-series. Experimental results show that SpecPV achieves up to 6x decoding speedup over standard autoregressive decoding with minor degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Growing demands from tasks like code generation, deep reasoning, and long-document understanding have made long-context generation a crucial capability for large language models (LLMs). Speculative decoding is one of the most direct and effective approaches for accelerating generation. It follows a draft-verify paradigm, where a lightweight draft model proposes several candidate tokens and the target model verifies them. However, we find that as the context length grows, verification becomes the dominant bottleneck. To further accelerate speculative decoding in long-context generation, we introduce SpecPV, a self-speculative decoding approach that performs fast verification using partial key-value states (KV) and periodically applies full verification to eliminate accumulated errors. We validate SpecPV across multiple long-context benchmarks and models, including LLaMA-3.1-8B-Instruct and Qwen3-series. Experimental results show that SpecPV achieves up to 6x decoding speedup over standard autoregressive decoding with minor degradation."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-02T02:15:33Z",
                "published_parsed": [
                    2025,
                    12,
                    2,
                    2,
                    15,
                    33,
                    1,
                    336,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Zhendong Tan"
                    },
                    {
                        "name": "Xingjun Zhang"
                    },
                    {
                        "name": "Chaoyi Hu"
                    },
                    {
                        "name": "Junjie Peng"
                    },
                    {
                        "name": "Kun Xia"
                    }
                ],
                "author_detail": {
                    "name": "Kun Xia"
                },
                "author": "Kun Xia"
            },
            {
                "id": "http://arxiv.org/abs/2511.09956v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.09956v2",
                "title": "Optimizing CPU Cache Utilization in Cloud VMs with Accurate Cache Abstraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing CPU Cache Utilization in Cloud VMs with Accurate Cache Abstraction"
                },
                "updated": "2025-12-02T01:24:46Z",
                "updated_parsed": [
                    2025,
                    12,
                    2,
                    1,
                    24,
                    46,
                    1,
                    336,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.09956v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.09956v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This paper shows that cache-based optimizations are often ineffective in cloud virtual machines (VMs) due to limited visibility into and control over provisioned caches. In public clouds, CPU caches can be partitioned or shared among VMs, but a VM is unaware of cache provisioning details. Moreover, a VM cannot influence cache usage via page placement policies, as memory-to-cache mappings are hidden. The paper proposes a novel solution, CacheX, which probes accurate and fine-grained cache abstraction within VMs using eviction sets without requiring hardware or hypervisor support, and showcases the utility of the probed information with two new techniques: LLC contention-aware task scheduling and virtual color-aware page cache management. Our evaluation of CacheX's implementation in x86 Linux kernel demonstrates that it can effectively improve cache utilization for various workloads in public cloud VMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper shows that cache-based optimizations are often ineffective in cloud virtual machines (VMs) due to limited visibility into and control over provisioned caches. In public clouds, CPU caches can be partitioned or shared among VMs, but a VM is unaware of cache provisioning details. Moreover, a VM cannot influence cache usage via page placement policies, as memory-to-cache mappings are hidden. The paper proposes a novel solution, CacheX, which probes accurate and fine-grained cache abstraction within VMs using eviction sets without requiring hardware or hypervisor support, and showcases the utility of the probed information with two new techniques: LLC contention-aware task scheduling and virtual color-aware page cache management. Our evaluation of CacheX's implementation in x86 Linux kernel demonstrates that it can effectively improve cache utilization for various workloads in public cloud VMs."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T04:37:52Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    4,
                    37,
                    52,
                    3,
                    317,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Mani Tofigh"
                    },
                    {
                        "name": "Edward Guo"
                    },
                    {
                        "name": "Weiwei Jia"
                    },
                    {
                        "name": "Xiaoning Ding"
                    },
                    {
                        "name": "Zirui Neil Zhao"
                    },
                    {
                        "name": "Jianchen Shan"
                    }
                ],
                "author_detail": {
                    "name": "Jianchen Shan"
                },
                "author": "Jianchen Shan"
            },
            {
                "id": "http://arxiv.org/abs/2504.19602v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2504.19602v3",
                "title": "Soft-Label Caching and Sharpening for Communication-Efficient Federated Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Soft-Label Caching and Sharpening for Communication-Efficient Federated Distillation"
                },
                "updated": "2025-12-02T00:43:12Z",
                "updated_parsed": [
                    2025,
                    12,
                    2,
                    0,
                    43,
                    12,
                    1,
                    336,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2504.19602v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2504.19602v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Federated Learning (FL) enables collaborative model training across decentralized clients, enhancing privacy by keeping data local. Yet conventional FL, relying on frequent parameter-sharing, suffers from high communication overhead and limited model heterogeneity. Distillation-based FL approaches address these issues by sharing predictions (soft-labels, i.e., normalized probability distributions) instead, but they often involve redundant transmissions across communication rounds, reducing efficiency. We propose SCARLET, a novel framework integrating synchronized soft-label caching and an enhanced Entropy Reduction Aggregation (Enhanced ERA) mechanism. SCARLET minimizes redundant communication by reusing cached soft-labels, achieving up to 50% reduction in communication costs compared to existing methods while maintaining competitive accuracy. Enhanced ERA resolves the fundamental instability of conventional temperature-based aggregation, ensuring robust control and high performance in diverse client scenarios. Experimental evaluations demonstrate that SCARLET consistently outperforms state-of-the-art distillation-based FL methods in terms of accuracy and communication efficiency. The implementation of SCARLET is publicly available at https://github.com/kitsuyaazuma/SCARLET.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) enables collaborative model training across decentralized clients, enhancing privacy by keeping data local. Yet conventional FL, relying on frequent parameter-sharing, suffers from high communication overhead and limited model heterogeneity. Distillation-based FL approaches address these issues by sharing predictions (soft-labels, i.e., normalized probability distributions) instead, but they often involve redundant transmissions across communication rounds, reducing efficiency. We propose SCARLET, a novel framework integrating synchronized soft-label caching and an enhanced Entropy Reduction Aggregation (Enhanced ERA) mechanism. SCARLET minimizes redundant communication by reusing cached soft-labels, achieving up to 50% reduction in communication costs compared to existing methods while maintaining competitive accuracy. Enhanced ERA resolves the fundamental instability of conventional temperature-based aggregation, ensuring robust control and high performance in diverse client scenarios. Experimental evaluations demonstrate that SCARLET consistently outperforms state-of-the-art distillation-based FL methods in terms of accuracy and communication efficiency. The implementation of SCARLET is publicly available at https://github.com/kitsuyaazuma/SCARLET."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-04-28T09:04:30Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    9,
                    4,
                    30,
                    0,
                    118,
                    0
                ],
                "arxiv_comment": "23 pages, 18 figures",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Kitsuya Azuma"
                    },
                    {
                        "name": "Takayuki Nishio"
                    },
                    {
                        "name": "Yuichi Kitagawa"
                    },
                    {
                        "name": "Wakako Nakano"
                    },
                    {
                        "name": "Takahito Tanimura"
                    }
                ],
                "author_detail": {
                    "name": "Takahito Tanimura"
                },
                "author": "Takahito Tanimura"
            },
            {
                "id": "http://arxiv.org/abs/2512.02281v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.02281v1",
                "title": "Trinity: Disaggregating Vector Search from Prefill-Decode Disaggregation in LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trinity: Disaggregating Vector Search from Prefill-Decode Disaggregation in LLM Serving"
                },
                "updated": "2025-12-01T23:53:42Z",
                "updated_parsed": [
                    2025,
                    12,
                    1,
                    23,
                    53,
                    42,
                    0,
                    335,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.02281v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.02281v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Prefill and decode (PD) disaggregation separates prompt prefill and token-by-token decode stages into distinct GPU pools and has become the dominant architecture for large-scale LLM serving in industry. Also, retrieval tasks via vector search remains entangled with the model inference process, like heterogeneous RAG requests and prompt answer caches, inflating tail latency. We are motivated to investigate how vector search should be orchestrated along with PD disaggregation with a dedicated deployment architecture without violating SLOs in various retrieval workloads. We present Trinity, a practical framework that consolidates all retrieval into a single, shared vector-search GPU pool and make it work with PD disaggregated LLM serving in match. Trinity introduces (1) a novel architecture for deploying GPU-based vector search service in PD disaggregation. (2) Continuous batching for vector search that make full used of GPUs under heterogeneous queries; (3) Stage-aware scheduling that preempts vector search requests between both decode and prefill tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prefill and decode (PD) disaggregation separates prompt prefill and token-by-token decode stages into distinct GPU pools and has become the dominant architecture for large-scale LLM serving in industry. Also, retrieval tasks via vector search remains entangled with the model inference process, like heterogeneous RAG requests and prompt answer caches, inflating tail latency. We are motivated to investigate how vector search should be orchestrated along with PD disaggregation with a dedicated deployment architecture without violating SLOs in various retrieval workloads. We present Trinity, a practical framework that consolidates all retrieval into a single, shared vector-search GPU pool and make it work with PD disaggregated LLM serving in match. Trinity introduces (1) a novel architecture for deploying GPU-based vector search service in PD disaggregation. (2) Continuous batching for vector search that make full used of GPUs under heterogeneous queries; (3) Stage-aware scheduling that preempts vector search requests between both decode and prefill tasks."
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-01T23:53:42Z",
                "published_parsed": [
                    2025,
                    12,
                    1,
                    23,
                    53,
                    42,
                    0,
                    335,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB"
                },
                "authors": [
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Chen Qian"
                    }
                ],
                "author_detail": {
                    "name": "Chen Qian"
                },
                "author": "Chen Qian"
            },
            {
                "id": "http://arxiv.org/abs/2506.05332v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.05332v2",
                "title": "Unleashing Hour-Scale Video Training for Long Video-Language Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing Hour-Scale Video Training for Long Video-Language Understanding"
                },
                "updated": "2025-12-01T22:47:17Z",
                "updated_parsed": [
                    2025,
                    12,
                    1,
                    22,
                    47,
                    17,
                    0,
                    335,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.05332v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.05332v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent long-form video-language understanding benchmarks have driven progress in video large multimodal models (Video-LMMs). However, the scarcity of well-annotated long videos has left the training of hour-long Video-LMMs underexplored. To close this gap, we present VideoMarathon, a large-scale hour-long video instruction-following dataset. This dataset includes around 9,700 hours of long videos sourced from diverse domains, ranging from 3 to 60 minutes per video. Specifically, it contains 3.3M high-quality QA pairs, spanning six fundamental topics: temporality, spatiality, object, action, scene, and event. Compared to existing video instruction datasets, VideoMarathon significantly extends training video durations up to 1 hour, and supports 22 diverse tasks requiring both short- and long-term video comprehension. Building on VideoMarathon, we propose Hour-LLaVA, a powerful and efficient Video-LMM for hour-scale video-language modeling. It enables hour-long video training and inference at 1-FPS sampling by leveraging a memory augmentation module, which adaptively integrates question-relevant and spatiotemporally informative semantics from the cached full video context. In our experiments, Hour-LLaVA achieves the best performance on multiple representative long video-language benchmarks, demonstrating the high quality of the VideoMarathon dataset and the superiority of the Hour-LLaVA model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent long-form video-language understanding benchmarks have driven progress in video large multimodal models (Video-LMMs). However, the scarcity of well-annotated long videos has left the training of hour-long Video-LMMs underexplored. To close this gap, we present VideoMarathon, a large-scale hour-long video instruction-following dataset. This dataset includes around 9,700 hours of long videos sourced from diverse domains, ranging from 3 to 60 minutes per video. Specifically, it contains 3.3M high-quality QA pairs, spanning six fundamental topics: temporality, spatiality, object, action, scene, and event. Compared to existing video instruction datasets, VideoMarathon significantly extends training video durations up to 1 hour, and supports 22 diverse tasks requiring both short- and long-term video comprehension. Building on VideoMarathon, we propose Hour-LLaVA, a powerful and efficient Video-LMM for hour-scale video-language modeling. It enables hour-long video training and inference at 1-FPS sampling by leveraging a memory augmentation module, which adaptively integrates question-relevant and spatiotemporally informative semantics from the cached full video context. In our experiments, Hour-LLaVA achieves the best performance on multiple representative long video-language benchmarks, demonstrating the high quality of the VideoMarathon dataset and the superiority of the Hour-LLaVA model."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-05T17:59:04Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    59,
                    4,
                    3,
                    156,
                    0
                ],
                "arxiv_comment": "NeurIPS 2025, Project page: https://videomarathon.github.io/",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Jingyang Lin"
                    },
                    {
                        "name": "Jialian Wu"
                    },
                    {
                        "name": "Ximeng Sun"
                    },
                    {
                        "name": "Ze Wang"
                    },
                    {
                        "name": "Jiang Liu"
                    },
                    {
                        "name": "Yusheng Su"
                    },
                    {
                        "name": "Xiaodong Yu"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Jiebo Luo"
                    },
                    {
                        "name": "Zicheng Liu"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum"
            },
            {
                "id": "http://arxiv.org/abs/2506.19686v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.19686v3",
                "title": "From Memories to Maps: Mechanisms of In-Context Reinforcement Learning in Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Memories to Maps: Mechanisms of In-Context Reinforcement Learning in Transformers"
                },
                "updated": "2025-12-01T21:56:32Z",
                "updated_parsed": [
                    2025,
                    12,
                    1,
                    21,
                    56,
                    32,
                    0,
                    335,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.19686v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.19686v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Humans and animals show remarkable learning efficiency, adapting to new environments with minimal experience. This capability is not well captured by standard reinforcement learning algorithms that rely on incremental value updates. Rapid adaptation likely depends on episodic memory -- the ability to retrieve specific past experiences to guide decisions in novel contexts. Transformers provide a useful setting for studying these questions because of their ability to learn rapidly in-context and because their key-value architecture resembles episodic memory systems in the brain. We train a transformer to in-context reinforcement learn in a distribution of planning tasks inspired by rodent behavior. We then characterize the learning algorithms that emerge in the model. We first find that representation learning is supported by in-context structure learning and cross-context alignment, where representations are aligned across environments with different sensory stimuli. We next demonstrate that the reinforcement learning strategies developed by the model are not interpretable as standard model-free or model-based planning. Instead, we show that in-context reinforcement learning is supported by caching intermediate computations within the model's memory tokens, which are then accessed at decision time. Overall, we find that memory may serve as a computational resource, storing both raw experience and cached computations to support flexible behavior. Furthermore, the representations developed in the model resemble computations associated with the hippocampal-entorhinal system in the brain, suggesting that our findings may be relevant for natural cognition. Taken together, our work offers a mechanistic hypothesis for the rapid adaptation that underlies in-context learning in artificial and natural settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humans and animals show remarkable learning efficiency, adapting to new environments with minimal experience. This capability is not well captured by standard reinforcement learning algorithms that rely on incremental value updates. Rapid adaptation likely depends on episodic memory -- the ability to retrieve specific past experiences to guide decisions in novel contexts. Transformers provide a useful setting for studying these questions because of their ability to learn rapidly in-context and because their key-value architecture resembles episodic memory systems in the brain. We train a transformer to in-context reinforcement learn in a distribution of planning tasks inspired by rodent behavior. We then characterize the learning algorithms that emerge in the model. We first find that representation learning is supported by in-context structure learning and cross-context alignment, where representations are aligned across environments with different sensory stimuli. We next demonstrate that the reinforcement learning strategies developed by the model are not interpretable as standard model-free or model-based planning. Instead, we show that in-context reinforcement learning is supported by caching intermediate computations within the model's memory tokens, which are then accessed at decision time. Overall, we find that memory may serve as a computational resource, storing both raw experience and cached computations to support flexible behavior. Furthermore, the representations developed in the model resemble computations associated with the hippocampal-entorhinal system in the brain, suggesting that our findings may be relevant for natural cognition. Taken together, our work offers a mechanistic hypothesis for the rapid adaptation that underlies in-context learning in artificial and natural settings."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-24T14:55:43Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    14,
                    55,
                    43,
                    1,
                    175,
                    0
                ],
                "arxiv_comment": "Revised to around 9 pages",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Ching Fang"
                    },
                    {
                        "name": "Kanaka Rajan"
                    }
                ],
                "author_detail": {
                    "name": "Kanaka Rajan"
                },
                "author": "Kanaka Rajan"
            },
            {
                "id": "http://arxiv.org/abs/2512.02189v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.02189v1",
                "title": "Microbenchmarking NVIDIA's Blackwell Architecture: An in-depth Architectural Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Microbenchmarking NVIDIA's Blackwell Architecture: An in-depth Architectural Analysis"
                },
                "updated": "2025-12-01T20:31:10Z",
                "updated_parsed": [
                    2025,
                    12,
                    1,
                    20,
                    31,
                    10,
                    0,
                    335,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.02189v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.02189v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As GPU architectures rapidly evolve to meet the overcoming demands of exascale computing and machine learning, the performance implications of architectural innovations remain poorly understood across diverse workloads. NVIDIA's Blackwell (B200) generation introduce significant architectural advances including the 5th generation tensor cores, tensor memory (TMEM), decompression engine (DE), and dual chips; however systematic methodologies for quantifying these improvements lag behind hardware development cycles. We contribute an open-source microbenchmark suite that offers practical insights into optimizing workloads to fully utilize the rich feature sets of the modern GPU architecture. This work aims to enable application developers make informed architectural decisions and guide future GPU design directions.\n  Our work studies Blackwell GPUs, compares them to H200 generation with regards to the memory subsystem, tensor core pipeline and floating-point precisions (FP32, FP16, FP8, FP6, FP4). Our systematic evaluation of dense/sparse GEMM, transformer inference, and training workloads demonstrate that B200's tensor core enhancements achieves 1.56x higher mixed-precision throughput and 42% better energy efficiency than H200. Our memory analysis reveals 58% reduction in memory access latency in cache-misses, fundamentally changing optimal algorithm design strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As GPU architectures rapidly evolve to meet the overcoming demands of exascale computing and machine learning, the performance implications of architectural innovations remain poorly understood across diverse workloads. NVIDIA's Blackwell (B200) generation introduce significant architectural advances including the 5th generation tensor cores, tensor memory (TMEM), decompression engine (DE), and dual chips; however systematic methodologies for quantifying these improvements lag behind hardware development cycles. We contribute an open-source microbenchmark suite that offers practical insights into optimizing workloads to fully utilize the rich feature sets of the modern GPU architecture. This work aims to enable application developers make informed architectural decisions and guide future GPU design directions.\n  Our work studies Blackwell GPUs, compares them to H200 generation with regards to the memory subsystem, tensor core pipeline and floating-point precisions (FP32, FP16, FP8, FP6, FP4). Our systematic evaluation of dense/sparse GEMM, transformer inference, and training workloads demonstrate that B200's tensor core enhancements achieves 1.56x higher mixed-precision throughput and 42% better energy efficiency than H200. Our memory analysis reveals 58% reduction in memory access latency in cache-misses, fundamentally changing optimal algorithm design strategies."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-01T20:31:10Z",
                "published_parsed": [
                    2025,
                    12,
                    1,
                    20,
                    31,
                    10,
                    0,
                    335,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Aaron Jarmusch"
                    },
                    {
                        "name": "Sunita Chandrasekaran"
                    }
                ],
                "author_detail": {
                    "name": "Sunita Chandrasekaran"
                },
                "author": "Sunita Chandrasekaran"
            },
            {
                "id": "http://arxiv.org/abs/2512.01953v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.01953v1",
                "title": "KV Pareto: Systems-Level Optimization of KV Cache and Model Compression for Long Context Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Pareto: Systems-Level Optimization of KV Cache and Model Compression for Long Context Inference"
                },
                "updated": "2025-12-01T18:03:47Z",
                "updated_parsed": [
                    2025,
                    12,
                    1,
                    18,
                    3,
                    47,
                    0,
                    335,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.01953v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.01953v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Long-context Large Language Models (LLMs) face significant memory bottlenecks during inference due to the linear growth of key-value (KV) cache with sequence length. While individual optimization techniques like KV cache quantization, chunked prefill, and model weight quantization have shown promise, their joint effects and optimal configurations for edge deployment remain underexplored. We introduce KV Pareto, a systems-level framework that systematically maps the trade-off frontier between total memory consumption and task accuracy across these three complementary optimization techniques. Our framework evaluates multiple LLM architectures (Qwen, Llama, Mistral) with varying KV quantization schemes (int2/4/8, mixed-precision), granularities (per-token, per-tensor, per-block), and 4-bit weight quantization via AWQ. Our framework identifies model-specific Pareto-optimal configurations that achieve 68-78% total memory reduction with minimal (1-3%) accuracy degradation on long-context tasks. We additionally verify the selected frontiers on additional benchmarks of Needle-in-a-Haystack, GSM8k and MMLU as well as extended context lengths of up to 128k to demonstrate the practical need of joint optimization for efficient LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context Large Language Models (LLMs) face significant memory bottlenecks during inference due to the linear growth of key-value (KV) cache with sequence length. While individual optimization techniques like KV cache quantization, chunked prefill, and model weight quantization have shown promise, their joint effects and optimal configurations for edge deployment remain underexplored. We introduce KV Pareto, a systems-level framework that systematically maps the trade-off frontier between total memory consumption and task accuracy across these three complementary optimization techniques. Our framework evaluates multiple LLM architectures (Qwen, Llama, Mistral) with varying KV quantization schemes (int2/4/8, mixed-precision), granularities (per-token, per-tensor, per-block), and 4-bit weight quantization via AWQ. Our framework identifies model-specific Pareto-optimal configurations that achieve 68-78% total memory reduction with minimal (1-3%) accuracy degradation on long-context tasks. We additionally verify the selected frontiers on additional benchmarks of Needle-in-a-Haystack, GSM8k and MMLU as well as extended context lengths of up to 128k to demonstrate the practical need of joint optimization for efficient LLM inference."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-01T18:03:47Z",
                "published_parsed": [
                    2025,
                    12,
                    1,
                    18,
                    3,
                    47,
                    0,
                    335,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Sai Gokhale"
                    },
                    {
                        "name": "Devleena Das"
                    },
                    {
                        "name": "Rajeev Patwari"
                    },
                    {
                        "name": "Ashish Sirasao"
                    },
                    {
                        "name": "Elliott Delaye"
                    }
                ],
                "author_detail": {
                    "name": "Elliott Delaye"
                },
                "author": "Elliott Delaye"
            },
            {
                "id": "http://arxiv.org/abs/2506.04844v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.04844v2",
                "title": "Characterization of the Hamamatsu R12699-406-M4 Photomultiplier Tube in Cold Xenon Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterization of the Hamamatsu R12699-406-M4 Photomultiplier Tube in Cold Xenon Environments"
                },
                "updated": "2025-12-01T17:42:16Z",
                "updated_parsed": [
                    2025,
                    12,
                    1,
                    17,
                    42,
                    16,
                    0,
                    335,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.04844v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.04844v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The Hamamatsu R12699-406-M2 is a $2\\times2$ multi-anode 2-inch photomultiplier tube that offers a compact form factor, low intrinsic radioactivity, and high photocathode coverage. These characteristics make it a promising candidate for next-generation xenon-based direct detection dark matter experiments, such as XLZD and PandaX-xT. We present a detailed characterization of this photosensor operated in cold xenon environments, focusing on its single photoelectron response, dark count rate, light emission, and afterpulsing behavior. The device demonstrated a gain exceeding $2\\cdot 10^6$ at the nominal voltage of -1.0 kV, along with a low dark count rate of $(0.4\\pm0.2)\\;\\text{Hz/cm}^2$. Due to the compact design, afterpulses exhibited short delay times, resulting in some cases in an overlap with the light-induced signal. To evaluate its applicability in a realistic detector environment, two R12699-406-M2 units were deployed in a small-scale dual-phase xenon time projection chamber. The segmented $2\\times2$ anode structure enabled lateral position reconstruction using a single photomultiplier tube, highlighting the potential of the sensor for effective event localization in future detectors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Hamamatsu R12699-406-M2 is a $2\\times2$ multi-anode 2-inch photomultiplier tube that offers a compact form factor, low intrinsic radioactivity, and high photocathode coverage. These characteristics make it a promising candidate for next-generation xenon-based direct detection dark matter experiments, such as XLZD and PandaX-xT. We present a detailed characterization of this photosensor operated in cold xenon environments, focusing on its single photoelectron response, dark count rate, light emission, and afterpulsing behavior. The device demonstrated a gain exceeding $2\\cdot 10^6$ at the nominal voltage of -1.0 kV, along with a low dark count rate of $(0.4\\pm0.2)\\;\\text{Hz/cm}^2$. Due to the compact design, afterpulses exhibited short delay times, resulting in some cases in an overlap with the light-induced signal. To evaluate its applicability in a realistic detector environment, two R12699-406-M2 units were deployed in a small-scale dual-phase xenon time projection chamber. The segmented $2\\times2$ anode structure enabled lateral position reconstruction using a single photomultiplier tube, highlighting the potential of the sensor for effective event localization in future detectors."
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-05T10:11:04Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    10,
                    11,
                    4,
                    3,
                    156,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det"
                },
                "authors": [
                    {
                        "name": "M. Adrover"
                    },
                    {
                        "name": "L. Baudis"
                    },
                    {
                        "name": "A. Bismark"
                    },
                    {
                        "name": "A. P. Colijn"
                    },
                    {
                        "name": "J. J. Cuenca-García"
                    },
                    {
                        "name": "M. P. Decowski"
                    },
                    {
                        "name": "M. Flierman"
                    },
                    {
                        "name": "T. den Hollander"
                    }
                ],
                "author_detail": {
                    "name": "T. den Hollander"
                },
                "author": "T. den Hollander"
            },
            {
                "id": "http://arxiv.org/abs/2512.01915v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.01915v1",
                "title": "A Low-Cost Reliable Racetrack Cache Based on Data Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Low-Cost Reliable Racetrack Cache Based on Data Compression"
                },
                "updated": "2025-12-01T17:32:25Z",
                "updated_parsed": [
                    2025,
                    12,
                    1,
                    17,
                    32,
                    25,
                    0,
                    335,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.01915v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.01915v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "SRAM-based cache memory faces several scalability limitations in deep nanoscale technologies, e.g., high leakage current, low cell stability, and low density. Emerging Non-Volatile Memory (NVM) technologies have received lots of attention in recent years, where Racetrack Memory (RTM) is among the most promising ones. RTM has the highest density among all NVMs and its access performance is comparable to SRAM technology. Therefore, RTM is a suitable alternative for SRAM in the Last-Level Caches (LLCs). Despite all its benefits, RTM confronts different reliability challenges due to the stochastic behavior of its storage element and highly error-prone data shifting, leading to a high probability of multiple-bit errors. Conventional Error-Correcting Codes (ECCs) are either incapable of tolerating multiple-bit errors or require a large amount of extra storage for check bits. This paper proposes taking advantage of value locality for compressing data blocks and freeing up a large fraction of cache blocks for storing data redundancy of strong ECCs. Utilizing the proposed scheme, a large majority of cache blocks are protected by strong ECCs to tolerate multiple-bit errors without any storage overhead. The evaluation using gem5 full-system simulator demonstrates that the proposed scheme enhances the mean-time-to-failure of the cache by an average of 11.3x with less than 1% hardware and performance overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SRAM-based cache memory faces several scalability limitations in deep nanoscale technologies, e.g., high leakage current, low cell stability, and low density. Emerging Non-Volatile Memory (NVM) technologies have received lots of attention in recent years, where Racetrack Memory (RTM) is among the most promising ones. RTM has the highest density among all NVMs and its access performance is comparable to SRAM technology. Therefore, RTM is a suitable alternative for SRAM in the Last-Level Caches (LLCs). Despite all its benefits, RTM confronts different reliability challenges due to the stochastic behavior of its storage element and highly error-prone data shifting, leading to a high probability of multiple-bit errors. Conventional Error-Correcting Codes (ECCs) are either incapable of tolerating multiple-bit errors or require a large amount of extra storage for check bits. This paper proposes taking advantage of value locality for compressing data blocks and freeing up a large fraction of cache blocks for storing data redundancy of strong ECCs. Utilizing the proposed scheme, a large majority of cache blocks are protected by strong ECCs to tolerate multiple-bit errors without any storage overhead. The evaluation using gem5 full-system simulator demonstrates that the proposed scheme enhances the mean-time-to-failure of the cache by an average of 11.3x with less than 1% hardware and performance overhead."
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-01T17:32:25Z",
                "published_parsed": [
                    2025,
                    12,
                    1,
                    17,
                    32,
                    25,
                    0,
                    335,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET"
                },
                "authors": [
                    {
                        "name": "Elham Cheshmikhani"
                    },
                    {
                        "name": "Fateme Shokouhinia"
                    },
                    {
                        "name": "Hamed Farbeh"
                    }
                ],
                "author_detail": {
                    "name": "Hamed Farbeh"
                },
                "author": "Hamed Farbeh"
            },
            {
                "id": "http://arxiv.org/abs/2510.23649v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.23649v2",
                "title": "Efficient Low Rank Attention for Long-Context Inference in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Low Rank Attention for Long-Context Inference in Large Language Models"
                },
                "updated": "2025-12-01T12:51:25Z",
                "updated_parsed": [
                    2025,
                    12,
                    1,
                    12,
                    51,
                    25,
                    0,
                    335,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.23649v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.23649v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As the length of input text grows, the key-value (KV) cache in LLMs imposes prohibitive GPU memory costs and limits long-context inference on resource constrained devices. Existing approaches, such as KV quantization and pruning, reduce memory usage but suffer from numerical precision loss or suboptimal retention of key-value pairs. We introduce Low Rank Query and Key attention (LRQK), a two-stage framework that jointly decomposes the full-precision query and key matrices into compact rank-\\(r\\) factors during the prefill stage, and then uses these low-dimensional projections to compute proxy attention scores in \\(\\mathcal{O}(lr)\\) time at each decode step. By selecting only the top-\\(k\\) tokens and a small fixed set of recent tokens, LRQK employs a mixed GPU-CPU cache with a hit-and-miss mechanism that transfers only missing full-precision KV pairs, thereby preserving exact attention outputs while reducing CPU-GPU data movement. Extensive experiments on the RULER and LongBench benchmarks with LLaMA-3-8B and Qwen2.5-7B demonstrate that LRQK matches or surpasses leading sparse-attention methods in long context settings, while delivering significant memory savings with minimal loss in accuracy. Our code is available at https://github.com/tenghuilee/LRQK.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the length of input text grows, the key-value (KV) cache in LLMs imposes prohibitive GPU memory costs and limits long-context inference on resource constrained devices. Existing approaches, such as KV quantization and pruning, reduce memory usage but suffer from numerical precision loss or suboptimal retention of key-value pairs. We introduce Low Rank Query and Key attention (LRQK), a two-stage framework that jointly decomposes the full-precision query and key matrices into compact rank-\\(r\\) factors during the prefill stage, and then uses these low-dimensional projections to compute proxy attention scores in \\(\\mathcal{O}(lr)\\) time at each decode step. By selecting only the top-\\(k\\) tokens and a small fixed set of recent tokens, LRQK employs a mixed GPU-CPU cache with a hit-and-miss mechanism that transfers only missing full-precision KV pairs, thereby preserving exact attention outputs while reducing CPU-GPU data movement. Extensive experiments on the RULER and LongBench benchmarks with LLaMA-3-8B and Qwen2.5-7B demonstrate that LRQK matches or surpasses leading sparse-attention methods in long context settings, while delivering significant memory savings with minimal loss in accuracy. Our code is available at https://github.com/tenghuilee/LRQK."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-25T11:43:27Z",
                "published_parsed": [
                    2025,
                    10,
                    25,
                    11,
                    43,
                    27,
                    5,
                    298,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Tenghui Li"
                    },
                    {
                        "name": "Guoxu Zhou"
                    },
                    {
                        "name": "Xuyang Zhao"
                    },
                    {
                        "name": "Yuning Qiu"
                    },
                    {
                        "name": "Qibin Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Qibin Zhao"
                },
                "author": "Qibin Zhao"
            },
            {
                "id": "http://arxiv.org/abs/2512.01541v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.01541v1",
                "title": "RoMe: Row Granularity Access Memory System for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoMe: Row Granularity Access Memory System for Large Language Models"
                },
                "updated": "2025-12-01T11:14:31Z",
                "updated_parsed": [
                    2025,
                    12,
                    1,
                    11,
                    14,
                    31,
                    0,
                    335,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.01541v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.01541v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Modern HBM-based memory systems have evolved over generations while retaining cache line granularity accesses. Preserving this fine granularity necessitated the introduction of bank groups and pseudo channels. These structures expand timing parameters and control overhead, significantly increasing memory controller scheduling complexity. Large language models (LLMs) now dominate deep learning workloads, streaming contiguous data blocks ranging from several kilobytes to megabytes per operation. In a conventional HBM-based memory system, these transfers are fragmented into hundreds of 32B cache line transactions. This forces the memory controller to employ unnecessarily intricate scheduling, leading to growing inefficiency.\n  To address this problem, we propose RoMe. RoMe accesses DRAM at row granularity and removes columns, bank groups, and pseudo channels from the memory interface. This design simplifies memory scheduling, thereby requiring fewer pins per channel. The freed pins are aggregated to form additional channels, increasing overall bandwidth by 12.5% with minimal extra pins. RoMe demonstrates how memory scheduling logic can be significantly simplified for representative LLM workloads, and presents an alternative approach for next-generation HBM-based memory systems achieving increased bandwidth with minimal hardware overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern HBM-based memory systems have evolved over generations while retaining cache line granularity accesses. Preserving this fine granularity necessitated the introduction of bank groups and pseudo channels. These structures expand timing parameters and control overhead, significantly increasing memory controller scheduling complexity. Large language models (LLMs) now dominate deep learning workloads, streaming contiguous data blocks ranging from several kilobytes to megabytes per operation. In a conventional HBM-based memory system, these transfers are fragmented into hundreds of 32B cache line transactions. This forces the memory controller to employ unnecessarily intricate scheduling, leading to growing inefficiency.\n  To address this problem, we propose RoMe. RoMe accesses DRAM at row granularity and removes columns, bank groups, and pseudo channels from the memory interface. This design simplifies memory scheduling, thereby requiring fewer pins per channel. The freed pins are aggregated to form additional channels, increasing overall bandwidth by 12.5% with minimal extra pins. RoMe demonstrates how memory scheduling logic can be significantly simplified for representative LLM workloads, and presents an alternative approach for next-generation HBM-based memory systems achieving increased bandwidth with minimal hardware overhead."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-01T11:14:31Z",
                "published_parsed": [
                    2025,
                    12,
                    1,
                    11,
                    14,
                    31,
                    0,
                    335,
                    0
                ],
                "arxiv_comment": "15 pages, 14 figures, accepted at HPCA 2026",
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Hwayong Nam"
                    },
                    {
                        "name": "Seungmin Baek"
                    },
                    {
                        "name": "Jumin Kim"
                    },
                    {
                        "name": "Michael Jaemin Kim"
                    },
                    {
                        "name": "Jung Ho Ahn"
                    }
                ],
                "author_detail": {
                    "name": "Jung Ho Ahn"
                },
                "author": "Jung Ho Ahn"
            },
            {
                "id": "http://arxiv.org/abs/2512.01540v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.01540v1",
                "title": "FlashVGGT: Efficient and Scalable Visual Geometry Transformers with Compressed Descriptor Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashVGGT: Efficient and Scalable Visual Geometry Transformers with Compressed Descriptor Attention"
                },
                "updated": "2025-12-01T11:12:37Z",
                "updated_parsed": [
                    2025,
                    12,
                    1,
                    11,
                    12,
                    37,
                    0,
                    335,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.01540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.01540v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "3D reconstruction from multi-view images is a core challenge in computer vision. Recently, feed-forward methods have emerged as efficient and robust alternatives to traditional per-scene optimization techniques. Among them, state-of-the-art models like the Visual Geometry Grounding Transformer (VGGT) leverage full self-attention over all image tokens to capture global relationships. However, this approach suffers from poor scalability due to the quadratic complexity of self-attention and the large number of tokens generated in long image sequences. In this work, we introduce FlashVGGT, an efficient alternative that addresses this bottleneck through a descriptor-based attention mechanism. Instead of applying dense global attention across all tokens, FlashVGGT compresses spatial information from each frame into a compact set of descriptor tokens. Global attention is then computed as cross-attention between the full set of image tokens and this smaller descriptor set, significantly reducing computational overhead. Moreover, the compactness of the descriptors enables online inference over long sequences via a chunk-recursive mechanism that reuses cached descriptors from previous chunks. Experimental results show that FlashVGGT achieves reconstruction accuracy competitive with VGGT while reducing inference time to just 9.3% of VGGT for 1,000 images, and scaling efficiently to sequences exceeding 3,000 images. Our project page is available at https://wzpscott.github.io/flashvggt_page/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D reconstruction from multi-view images is a core challenge in computer vision. Recently, feed-forward methods have emerged as efficient and robust alternatives to traditional per-scene optimization techniques. Among them, state-of-the-art models like the Visual Geometry Grounding Transformer (VGGT) leverage full self-attention over all image tokens to capture global relationships. However, this approach suffers from poor scalability due to the quadratic complexity of self-attention and the large number of tokens generated in long image sequences. In this work, we introduce FlashVGGT, an efficient alternative that addresses this bottleneck through a descriptor-based attention mechanism. Instead of applying dense global attention across all tokens, FlashVGGT compresses spatial information from each frame into a compact set of descriptor tokens. Global attention is then computed as cross-attention between the full set of image tokens and this smaller descriptor set, significantly reducing computational overhead. Moreover, the compactness of the descriptors enables online inference over long sequences via a chunk-recursive mechanism that reuses cached descriptors from previous chunks. Experimental results show that FlashVGGT achieves reconstruction accuracy competitive with VGGT while reducing inference time to just 9.3% of VGGT for 1,000 images, and scaling efficiently to sequences exceeding 3,000 images. Our project page is available at https://wzpscott.github.io/flashvggt_page/."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-01T11:12:37Z",
                "published_parsed": [
                    2025,
                    12,
                    1,
                    11,
                    12,
                    37,
                    0,
                    335,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Zipeng Wang"
                    },
                    {
                        "name": "Dan Xu"
                    }
                ],
                "author_detail": {
                    "name": "Dan Xu"
                },
                "author": "Dan Xu"
            },
            {
                "id": "http://arxiv.org/abs/2512.01357v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.01357v1",
                "title": "Tangram: Accelerating Serverless LLM Loading through GPU Memory Reuse and Affinity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tangram: Accelerating Serverless LLM Loading through GPU Memory Reuse and Affinity"
                },
                "updated": "2025-12-01T07:10:34Z",
                "updated_parsed": [
                    2025,
                    12,
                    1,
                    7,
                    10,
                    34,
                    0,
                    335,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.01357v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.01357v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Serverless Large Language Models (LLMs) have emerged as a cost-effective solution for deploying AI services by enabling a 'pay-as-you-go' pricing model through GPU resource sharing. However, cold-start latency, especially the model loading phase, has become a critical performance bottleneck, as it scales linearly with model size and severely limits the practical deployment of large-scale LLM services. This paper presents Tangram, a novel system that accelerates Serverless LLM loading through efficient GPU memory reuse. By leveraging the unused GPU memory to retain model parameters, Tangram significantly reduces model transfer time and cold-start latency. Its design includes three key components: unified GPU memory pool for tensor-level parameter sharing across models, on-demand KV cache allocation for dynamic memory management, and GPU-affinity-aware scheduling for maximizing resource utilization. These techniques collectively address the critical challenges of inefficient memory usage and the cold-start problem in Serverless LLM platforms. We have implemented a fully functional prototype, and experiments show that Tangram achieves up to 6.2 times faster loading and reduces Time-To-First-Token (TTFT) during cold-start by 23--55% over state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless Large Language Models (LLMs) have emerged as a cost-effective solution for deploying AI services by enabling a 'pay-as-you-go' pricing model through GPU resource sharing. However, cold-start latency, especially the model loading phase, has become a critical performance bottleneck, as it scales linearly with model size and severely limits the practical deployment of large-scale LLM services. This paper presents Tangram, a novel system that accelerates Serverless LLM loading through efficient GPU memory reuse. By leveraging the unused GPU memory to retain model parameters, Tangram significantly reduces model transfer time and cold-start latency. Its design includes three key components: unified GPU memory pool for tensor-level parameter sharing across models, on-demand KV cache allocation for dynamic memory management, and GPU-affinity-aware scheduling for maximizing resource utilization. These techniques collectively address the critical challenges of inefficient memory usage and the cold-start problem in Serverless LLM platforms. We have implemented a fully functional prototype, and experiments show that Tangram achieves up to 6.2 times faster loading and reduces Time-To-First-Token (TTFT) during cold-start by 23--55% over state-of-the-art methods."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-01T07:10:34Z",
                "published_parsed": [
                    2025,
                    12,
                    1,
                    7,
                    10,
                    34,
                    0,
                    335,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Wenbin Zhu"
                    },
                    {
                        "name": "Zhaoyan Shen"
                    },
                    {
                        "name": "Zili Shao"
                    },
                    {
                        "name": "Hongjun Dai"
                    },
                    {
                        "name": "Feng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Feng Chen"
                },
                "arxiv_affiliation": "Indiana University Bloomington",
                "author": "Feng Chen"
            },
            {
                "id": "http://arxiv.org/abs/2511.14712v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.14712v2",
                "title": "FreeSwim: Revisiting Sliding-Window Attention Mechanisms for Training-Free Ultra-High-Resolution Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FreeSwim: Revisiting Sliding-Window Attention Mechanisms for Training-Free Ultra-High-Resolution Video Generation"
                },
                "updated": "2025-12-01T06:11:56Z",
                "updated_parsed": [
                    2025,
                    12,
                    1,
                    6,
                    11,
                    56,
                    0,
                    335,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.14712v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.14712v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The quadratic time and memory complexity of the attention mechanism in modern Transformer based video generators makes end-to-end training for ultra high resolution videos prohibitively expensive. Motivated by this limitation, we introduce a training-free approach that leverages video Diffusion Transformers pretrained at their native scale to synthesize higher resolution videos without any additional training or adaptation. At the core of our method lies an inward sliding window attention mechanism, which originates from a key observation: maintaining each query token's training scale receptive field is crucial for preserving visual fidelity and detail. However, naive local window attention, unfortunately, often leads to repetitive content and exhibits a lack of global coherence in the generated results. To overcome this challenge, we devise a dual-path pipeline that backs up window attention with a novel cross-attention override strategy, enabling the semantic content produced by local attention to be guided by another branch with a full receptive field and, therefore, ensuring holistic consistency. Furthermore, to improve efficiency, we incorporate a cross-attention caching strategy for this branch to avoid the frequent computation of full 3D attention. Extensive experiments demonstrate that our method delivers ultra-high-resolution videos with fine-grained visual details and high efficiency in a training-free paradigm. Meanwhile, it achieves superior performance on VBench, even compared to training-based alternatives, with competitive or improved efficiency. Codes are available at: https://github.com/WillWu111/FreeSwim",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The quadratic time and memory complexity of the attention mechanism in modern Transformer based video generators makes end-to-end training for ultra high resolution videos prohibitively expensive. Motivated by this limitation, we introduce a training-free approach that leverages video Diffusion Transformers pretrained at their native scale to synthesize higher resolution videos without any additional training or adaptation. At the core of our method lies an inward sliding window attention mechanism, which originates from a key observation: maintaining each query token's training scale receptive field is crucial for preserving visual fidelity and detail. However, naive local window attention, unfortunately, often leads to repetitive content and exhibits a lack of global coherence in the generated results. To overcome this challenge, we devise a dual-path pipeline that backs up window attention with a novel cross-attention override strategy, enabling the semantic content produced by local attention to be guided by another branch with a full receptive field and, therefore, ensuring holistic consistency. Furthermore, to improve efficiency, we incorporate a cross-attention caching strategy for this branch to avoid the frequent computation of full 3D attention. Extensive experiments demonstrate that our method delivers ultra-high-resolution videos with fine-grained visual details and high efficiency in a training-free paradigm. Meanwhile, it achieves superior performance on VBench, even compared to training-based alternatives, with competitive or improved efficiency. Codes are available at: https://github.com/WillWu111/FreeSwim"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-18T17:56:04Z",
                "published_parsed": [
                    2025,
                    11,
                    18,
                    17,
                    56,
                    4,
                    1,
                    322,
                    0
                ],
                "arxiv_comment": "23 pages, 14 figures",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Yunfeng Wu"
                    },
                    {
                        "name": "Jiayi Song"
                    },
                    {
                        "name": "Zhenxiong Tan"
                    },
                    {
                        "name": "Zihao He"
                    },
                    {
                        "name": "Songhua Liu"
                    }
                ],
                "author_detail": {
                    "name": "Songhua Liu"
                },
                "author": "Songhua Liu"
            },
            {
                "id": "http://arxiv.org/abs/2512.01278v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.01278v1",
                "title": "Accelerating Large-Scale Reasoning Model Inference with Sparse Self-Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Large-Scale Reasoning Model Inference with Sparse Self-Speculative Decoding"
                },
                "updated": "2025-12-01T04:50:55Z",
                "updated_parsed": [
                    2025,
                    12,
                    1,
                    4,
                    50,
                    55,
                    0,
                    335,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.01278v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.01278v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Reasoning language models have demonstrated remarkable capabilities on challenging tasks by generating elaborate chain-of-thought (CoT) solutions. However, such lengthy generation shifts the inference bottleneck from compute-bound to memory-bound. To generate each token, the model applies full attention to all previously generated tokens, requiring memory access to an increasingly large KV-Cache. Consequently, longer generations demand more memory access for every step, leading to substantial pressure on memory bandwidth.\n  To address this, we introduce SparseSpec, a speculative decoding framework that reuses the same model as the draft and target models (i.e., self-speculation). SparseSpec features a novel sparse attention mechanism, PillarAttn, as the draft model, which accurately selects critical tokens via elegantly reusing information from the verification stage. Furthermore, SparseSpec co-designs self-speculation with three system innovations: (1) a unified scheduler to batch token drafting and verification, (2) delayed verification for CPU/GPU overlap, and (3) dynamic KV-Cache management to maximize memory utilization. Across various models and datasets, SparseSpec outperforms state-of-the-art solutions, with an up to 2.13x throughput speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning language models have demonstrated remarkable capabilities on challenging tasks by generating elaborate chain-of-thought (CoT) solutions. However, such lengthy generation shifts the inference bottleneck from compute-bound to memory-bound. To generate each token, the model applies full attention to all previously generated tokens, requiring memory access to an increasingly large KV-Cache. Consequently, longer generations demand more memory access for every step, leading to substantial pressure on memory bandwidth.\n  To address this, we introduce SparseSpec, a speculative decoding framework that reuses the same model as the draft and target models (i.e., self-speculation). SparseSpec features a novel sparse attention mechanism, PillarAttn, as the draft model, which accurately selects critical tokens via elegantly reusing information from the verification stage. Furthermore, SparseSpec co-designs self-speculation with three system innovations: (1) a unified scheduler to batch token drafting and verification, (2) delayed verification for CPU/GPU overlap, and (3) dynamic KV-Cache management to maximize memory utilization. Across various models and datasets, SparseSpec outperforms state-of-the-art solutions, with an up to 2.13x throughput speedup."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-01T04:50:55Z",
                "published_parsed": [
                    2025,
                    12,
                    1,
                    4,
                    50,
                    55,
                    0,
                    335,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Yilong Zhao"
                    },
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Kan Zhu"
                    },
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Chi-Chih Chang"
                    },
                    {
                        "name": "Chaofan Lin"
                    },
                    {
                        "name": "Jongseok Park"
                    },
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    },
                    {
                        "name": "Mingyu Gao"
                    },
                    {
                        "name": "Baris Kasikci"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica"
            },
            {
                "id": "http://arxiv.org/abs/2304.10805v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2304.10805v3",
                "title": "EPLKG: Efficient Prompt Learning with Knowledge Graph",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EPLKG: Efficient Prompt Learning with Knowledge Graph"
                },
                "updated": "2025-11-30T14:24:30Z",
                "updated_parsed": [
                    2025,
                    11,
                    30,
                    14,
                    24,
                    30,
                    6,
                    334,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2304.10805v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2304.10805v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large-scale pre-trained models such as CLIP excel in transferability and robust generalization across diverse datasets. However, adapting these models to new datasets or domains is computationally costly, especially in low-resource or few-shot settings, and existing prompt-learning methods often lack interpretability. We introduce Efficient Prompt Learning with Knowledge Graph (EPLKG), which uses a knowledge graph to curate diverse, interpretable prompts and, where KG coverage is limited, augments this bank with LLM-generated human-readable visual descriptions. EPLKG operates entirely on cached CLIP image and text embeddings and employs a lightweight Gumbel-Softmax module to select a single prompt per image-class pair, enabling low-memory, fast training. Across 11 benchmarks, EPLKG reduces per-image training time by up to 45 percent and peak GPU memory by around 30 to 40 percent compared to strong prompt-learning baselines, while keeping the average base-new harmonic-mean accuracy within 2 percentage points, thereby improving the efficiency of model adaptation without sacrificing competitive performance or interpretability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale pre-trained models such as CLIP excel in transferability and robust generalization across diverse datasets. However, adapting these models to new datasets or domains is computationally costly, especially in low-resource or few-shot settings, and existing prompt-learning methods often lack interpretability. We introduce Efficient Prompt Learning with Knowledge Graph (EPLKG), which uses a knowledge graph to curate diverse, interpretable prompts and, where KG coverage is limited, augments this bank with LLM-generated human-readable visual descriptions. EPLKG operates entirely on cached CLIP image and text embeddings and employs a lightweight Gumbel-Softmax module to select a single prompt per image-class pair, enabling low-memory, fast training. Across 11 benchmarks, EPLKG reduces per-image training time by up to 45 percent and peak GPU memory by around 30 to 40 percent compared to strong prompt-learning baselines, while keeping the average base-new harmonic-mean accuracy within 2 percentage points, thereby improving the efficiency of model adaptation without sacrificing competitive performance or interpretability."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2023-04-21T08:22:58Z",
                "published_parsed": [
                    2023,
                    4,
                    21,
                    8,
                    22,
                    58,
                    4,
                    111,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "YongTaek Lim"
                    },
                    {
                        "name": "Suho Kang"
                    },
                    {
                        "name": "Yewon Kim"
                    },
                    {
                        "name": "Dokyung Yoon"
                    },
                    {
                        "name": "KyungWoo Song"
                    }
                ],
                "author_detail": {
                    "name": "KyungWoo Song"
                },
                "author": "KyungWoo Song"
            },
            {
                "id": "http://arxiv.org/abs/2512.00903v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.00903v1",
                "title": "SwiftVLA: Unlocking Spatiotemporal Dynamics for Lightweight VLA Models at Minimal Overhead",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SwiftVLA: Unlocking Spatiotemporal Dynamics for Lightweight VLA Models at Minimal Overhead"
                },
                "updated": "2025-11-30T14:10:28Z",
                "updated_parsed": [
                    2025,
                    11,
                    30,
                    14,
                    10,
                    28,
                    6,
                    334,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.00903v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.00903v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Vision-Language-Action (VLA) models built on pretrained Vision-Language Models (VLMs) show strong potential but are limited in practicality due to their large parameter counts. To mitigate this issue, using a lightweight VLM has been explored, but it compromises spatiotemporal reasoning. Although some methods suggest that incorporating additional 3D inputs can help, they usually rely on large VLMs to fuse 3D and 2D inputs and still lack temporal understanding. Therefore, we propose SwiftVLA, an architecture that enhances a compact model with 4D understanding while preserving design efficiency. Specifically, our approach features a pretrained 4D visual geometry transformer with a temporal cache that extracts 4D features from 2D images. Then, to enhance the VLM's ability to exploit both 2D images and 4D features, we introduce Fusion Tokens, a set of learnable tokens trained with a future prediction objective to generate unified representations for action generation. Finally, we introduce a mask-and-reconstruct strategy that masks 4D inputs to the VLM and trains the VLA to reconstruct them, enabling the VLM to learn effective 4D representations and allowing the 4D branch to be dropped at inference with minimal performance loss. Experiments in real and simulated environments show that SwiftVLA outperforms lightweight baselines and rivals VLAs up to 7 times larger, achieving comparable performance on edge devices while being 18 times faster and reducing memory footprint by 12 times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) models built on pretrained Vision-Language Models (VLMs) show strong potential but are limited in practicality due to their large parameter counts. To mitigate this issue, using a lightweight VLM has been explored, but it compromises spatiotemporal reasoning. Although some methods suggest that incorporating additional 3D inputs can help, they usually rely on large VLMs to fuse 3D and 2D inputs and still lack temporal understanding. Therefore, we propose SwiftVLA, an architecture that enhances a compact model with 4D understanding while preserving design efficiency. Specifically, our approach features a pretrained 4D visual geometry transformer with a temporal cache that extracts 4D features from 2D images. Then, to enhance the VLM's ability to exploit both 2D images and 4D features, we introduce Fusion Tokens, a set of learnable tokens trained with a future prediction objective to generate unified representations for action generation. Finally, we introduce a mask-and-reconstruct strategy that masks 4D inputs to the VLM and trains the VLA to reconstruct them, enabling the VLM to learn effective 4D representations and allowing the 4D branch to be dropped at inference with minimal performance loss. Experiments in real and simulated environments show that SwiftVLA outperforms lightweight baselines and rivals VLAs up to 7 times larger, achieving comparable performance on edge devices while being 18 times faster and reducing memory footprint by 12 times."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-30T14:10:28Z",
                "published_parsed": [
                    2025,
                    11,
                    30,
                    14,
                    10,
                    28,
                    6,
                    334,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Chaojun Ni"
                    },
                    {
                        "name": "Cheng Chen"
                    },
                    {
                        "name": "Xiaofeng Wang"
                    },
                    {
                        "name": "Zheng Zhu"
                    },
                    {
                        "name": "Wenzhao Zheng"
                    },
                    {
                        "name": "Boyuan Wang"
                    },
                    {
                        "name": "Tianrun Chen"
                    },
                    {
                        "name": "Guosheng Zhao"
                    },
                    {
                        "name": "Haoyun Li"
                    },
                    {
                        "name": "Zhehao Dong"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Yun Ye"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Guan Huang"
                    },
                    {
                        "name": "Wenjun Mei"
                    }
                ],
                "author_detail": {
                    "name": "Wenjun Mei"
                },
                "author": "Wenjun Mei"
            },
            {
                "id": "http://arxiv.org/abs/2512.00891v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.00891v1",
                "title": "Accelerating Streaming Video Large Language Models via Hierarchical Token Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Streaming Video Large Language Models via Hierarchical Token Compression"
                },
                "updated": "2025-11-30T13:44:28Z",
                "updated_parsed": [
                    2025,
                    11,
                    30,
                    13,
                    44,
                    28,
                    6,
                    334,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.00891v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.00891v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Streaming Video Large Language Models (VideoLLMs) have demonstrated impressive performance across various video understanding tasks, but they face significant challenges in real-time deployment due to the high computational cost of processing dense visual tokens from continuous video streams. In streaming video scenarios, the primary bottleneck lies in the Vision Transformer (ViT) encoding stage, where redundant processing of temporally similar frames leads to inefficiency. Additionally, inflated token sequences during LLM pre-filling further exacerbate latency and memory overhead. To address these challenges, we propose \\textbf{S}treaming \\textbf{T}oken \\textbf{C}ompression (\\textbf{STC}), a plug-and-play hierarchical framework that seamlessly integrates into existing streaming VideoLLMs, optimizing both ViT encoding and LLM pre-filling stages to accelerate processing. STC introduces two token-level accelerators: \\textbf{STC-Cacher}, which reduces ViT encoding overhead by caching and reusing features from temporally similar frames, and \\textbf{STC-Pruner}, which compresses the visual token sequence before it enters the LLM, preserving only the most salient tokens based on both spatial and temporal relevance. Extensive experiments on four baseline streaming VideoLLMs across five benchmarks demonstrate that STC outperforms other compression methods. Notably, STC retains up to \\textbf{99\\%} of accuracy on the ReKV framework while reducing ViT encoding latency and LLM pre-filling latency by \\textbf{24.5\\%} and \\textbf{45.3\\%}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streaming Video Large Language Models (VideoLLMs) have demonstrated impressive performance across various video understanding tasks, but they face significant challenges in real-time deployment due to the high computational cost of processing dense visual tokens from continuous video streams. In streaming video scenarios, the primary bottleneck lies in the Vision Transformer (ViT) encoding stage, where redundant processing of temporally similar frames leads to inefficiency. Additionally, inflated token sequences during LLM pre-filling further exacerbate latency and memory overhead. To address these challenges, we propose \\textbf{S}treaming \\textbf{T}oken \\textbf{C}ompression (\\textbf{STC}), a plug-and-play hierarchical framework that seamlessly integrates into existing streaming VideoLLMs, optimizing both ViT encoding and LLM pre-filling stages to accelerate processing. STC introduces two token-level accelerators: \\textbf{STC-Cacher}, which reduces ViT encoding overhead by caching and reusing features from temporally similar frames, and \\textbf{STC-Pruner}, which compresses the visual token sequence before it enters the LLM, preserving only the most salient tokens based on both spatial and temporal relevance. Extensive experiments on four baseline streaming VideoLLMs across five benchmarks demonstrate that STC outperforms other compression methods. Notably, STC retains up to \\textbf{99\\%} of accuracy on the ReKV framework while reducing ViT encoding latency and LLM pre-filling latency by \\textbf{24.5\\%} and \\textbf{45.3\\%}."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-30T13:44:28Z",
                "published_parsed": [
                    2025,
                    11,
                    30,
                    13,
                    44,
                    28,
                    6,
                    334,
                    0
                ],
                "arxiv_comment": "Code is avaliable at \\url{https://github.com/lern-to-write/STC}",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Yiyu Wang"
                    },
                    {
                        "name": "Xuyang Liu"
                    },
                    {
                        "name": "Xiyan Gui"
                    },
                    {
                        "name": "Xinying Lin"
                    },
                    {
                        "name": "Boxue Yang"
                    },
                    {
                        "name": "Chenfei Liao"
                    },
                    {
                        "name": "Tailai Chen"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2512.00722v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.00722v1",
                "title": "SpeContext: Enabling Efficient Long-context Reasoning with Speculative Context Sparsity in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpeContext: Enabling Efficient Long-context Reasoning with Speculative Context Sparsity in LLMs"
                },
                "updated": "2025-11-30T04:32:43Z",
                "updated_parsed": [
                    2025,
                    11,
                    30,
                    4,
                    32,
                    43,
                    6,
                    334,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.00722v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.00722v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In this paper, we point out that the objective of the retrieval algorithms is to align with the LLM, which is similar to the objective of knowledge distillation in LLMs. We analyze the similarity in information focus between the distilled language model(DLM) and the original LLM from the perspective of information theory, and thus propose a novel paradigm that leverages a DLM as the retrieval algorithm. Based on the insight, we present SpeContext, an algorithm and system co-design for long-context reasoning. (1) At the algorithm level, SpeContext proposes lightweight retrieval head based on the head-level attention weights of DLM, achieving > 90% parameters reduction by pruning the redundancy. (2) At the system level, SpeContext designs an asynchronous prefetch dataflow via the elastic loading strategy, effectively overlapping KV cache retrieval with the LLM computation. (3) At the compilation level, SpeContext constructs the theoretical memory model and implements an adaptive memory management system to achieve acceleration by maximizing GPU memory utilization. We deploy and evaluate SpeContext in two resourceconstrained environments, cloud and edge. Extensive experiments show that, compared with the Huggingface framework, SpeContext achieves up to 24.89x throughput improvement in cloud and 10.06x speedup in edge with negligible accuracy loss, pushing the Pareto frontier of accuracy and throughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we point out that the objective of the retrieval algorithms is to align with the LLM, which is similar to the objective of knowledge distillation in LLMs. We analyze the similarity in information focus between the distilled language model(DLM) and the original LLM from the perspective of information theory, and thus propose a novel paradigm that leverages a DLM as the retrieval algorithm. Based on the insight, we present SpeContext, an algorithm and system co-design for long-context reasoning. (1) At the algorithm level, SpeContext proposes lightweight retrieval head based on the head-level attention weights of DLM, achieving > 90% parameters reduction by pruning the redundancy. (2) At the system level, SpeContext designs an asynchronous prefetch dataflow via the elastic loading strategy, effectively overlapping KV cache retrieval with the LLM computation. (3) At the compilation level, SpeContext constructs the theoretical memory model and implements an adaptive memory management system to achieve acceleration by maximizing GPU memory utilization. We deploy and evaluate SpeContext in two resourceconstrained environments, cloud and edge. Extensive experiments show that, compared with the Huggingface framework, SpeContext achieves up to 24.89x throughput improvement in cloud and 10.06x speedup in edge with negligible accuracy loss, pushing the Pareto frontier of accuracy and throughput."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-30T04:32:43Z",
                "published_parsed": [
                    2025,
                    11,
                    30,
                    4,
                    32,
                    43,
                    6,
                    334,
                    0
                ],
                "arxiv_comment": "Accepted by ASPLOS 2026",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Jiaming Xu"
                    },
                    {
                        "name": "Jiayi Pan"
                    },
                    {
                        "name": "Hanzhen Wang"
                    },
                    {
                        "name": "Yongkang Zhou"
                    },
                    {
                        "name": "Jiancai Ye"
                    },
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Guohao Dai"
                    }
                ],
                "author_detail": {
                    "name": "Guohao Dai"
                },
                "author": "Guohao Dai"
            },
            {
                "id": "http://arxiv.org/abs/2512.00719v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.00719v1",
                "title": "SIMPLE: Disaggregating Sampling from GPU Inference into a Decision Plane for Faster Distributed LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SIMPLE: Disaggregating Sampling from GPU Inference into a Decision Plane for Faster Distributed LLM Serving"
                },
                "updated": "2025-11-30T04:15:34Z",
                "updated_parsed": [
                    2025,
                    11,
                    30,
                    4,
                    15,
                    34,
                    6,
                    334,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.00719v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.00719v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As large language models (LLMs) scale out with tensor parallelism (TP) and pipeline parallelism (PP) and production stacks have aggressively optimized the data plane (attention/GEMM and KV cache), sampling, the decision plane that turns logits into tokens, becomes a new bottleneck. This creates a structural holdout: sampling neither expands with TP nor balances across PP stages, so its share of iteration time grows as GPUs get faster and it caps pipeline frequency at the last stage. We present SIMPLE, a stage-agnostic, sequence-parallel, overlappable decision plane that disaggregates sampling into a CPU-side service and shrinks its runtime footprint back to a minor, hidden role. SIMPLE combines: (1) sequence-parallel sampling, which shards work along the batch dimension and removes vocabulary-axis collectives; (2) a CPU-based algorithm with column-wise penalties and truncation-first filtering to realize single-pass, linear-time kernels; and (3) speculative hot-vocab sampling (SHVS), which samples on a small hot set with rejection-correctness and uses a simple sizing model to choose the hot-vocab size that maximizes throughput. In evaluation, SIMPLE improves end-to-end throughput by up to 96% and reduces P95 latency by 20-65%. Crucially, SIMPLE requires no user-side code changes and composes with existing data-plane optimizations, unlocking scaling benefits that compound with future GPU generations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) scale out with tensor parallelism (TP) and pipeline parallelism (PP) and production stacks have aggressively optimized the data plane (attention/GEMM and KV cache), sampling, the decision plane that turns logits into tokens, becomes a new bottleneck. This creates a structural holdout: sampling neither expands with TP nor balances across PP stages, so its share of iteration time grows as GPUs get faster and it caps pipeline frequency at the last stage. We present SIMPLE, a stage-agnostic, sequence-parallel, overlappable decision plane that disaggregates sampling into a CPU-side service and shrinks its runtime footprint back to a minor, hidden role. SIMPLE combines: (1) sequence-parallel sampling, which shards work along the batch dimension and removes vocabulary-axis collectives; (2) a CPU-based algorithm with column-wise penalties and truncation-first filtering to realize single-pass, linear-time kernels; and (3) speculative hot-vocab sampling (SHVS), which samples on a small hot set with rejection-correctness and uses a simple sizing model to choose the hot-vocab size that maximizes throughput. In evaluation, SIMPLE improves end-to-end throughput by up to 96% and reduces P95 latency by 20-65%. Crucially, SIMPLE requires no user-side code changes and composes with existing data-plane optimizations, unlocking scaling benefits that compound with future GPU generations."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-30T04:15:34Z",
                "published_parsed": [
                    2025,
                    11,
                    30,
                    4,
                    15,
                    34,
                    6,
                    334,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Bohan Zhao"
                    },
                    {
                        "name": "Zane Cao"
                    },
                    {
                        "name": "Yongchao He"
                    }
                ],
                "author_detail": {
                    "name": "Yongchao He"
                },
                "author": "Yongchao He"
            },
            {
                "id": "http://arxiv.org/abs/2512.00635v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.00635v1",
                "title": "Extended Abstract: Synthesizable Low-overhead Circuit-level Countermeasures and Pro-Active Detection Techniques for Power and EM SCA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extended Abstract: Synthesizable Low-overhead Circuit-level Countermeasures and Pro-Active Detection Techniques for Power and EM SCA"
                },
                "updated": "2025-11-29T21:12:22Z",
                "updated_parsed": [
                    2025,
                    11,
                    29,
                    21,
                    12,
                    22,
                    5,
                    333,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.00635v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.00635v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The gamut of todays internet-connected embedded devices has led to increased concerns regarding the security and confidentiality of data. Most internet-connected embedded devices employ mathematically secure cryptographic algorithms to address security vulnerabilities. Despite such mathematical guarantees, as these algorithms are often implemented in silicon, they leak critical information in terms of power consumption, electromagnetic (EM) radiation, timing, cache hits and misses, photonic emission and so on, leading to side-channel analysis (SCA) attacks. This thesis focuses on low overhead generic circuit-level yet synthesizable countermeasures against power and EM SCA. Existing countermeasures (including proposed) still have relatively high overhead which bars them from being used in energy-constraint IoT devices. We propose a zero-overhead integrated inductive sensor which is able to detect i)EM SCA ii) Clock glitch-based Fault Injection Attack (FIA), and iii) Voltage-glitch based Fault Injection Attack by using a simple ML algorithm. Advent of quantum computer research will open new possibilities for theoretical attacks against existing cryptographic protocols. National Institute of Standard & Technology (NIST) has standardized post-quantum cryptographic algorithms to secure crypto-systems against quantum adversary. I contribute to the standardization procedure by introducing the first silicon-verified Saber (a NIST finalist modulo Learning with Rounding scheme) which consumes lowest energy and area till date amongst all the candidates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The gamut of todays internet-connected embedded devices has led to increased concerns regarding the security and confidentiality of data. Most internet-connected embedded devices employ mathematically secure cryptographic algorithms to address security vulnerabilities. Despite such mathematical guarantees, as these algorithms are often implemented in silicon, they leak critical information in terms of power consumption, electromagnetic (EM) radiation, timing, cache hits and misses, photonic emission and so on, leading to side-channel analysis (SCA) attacks. This thesis focuses on low overhead generic circuit-level yet synthesizable countermeasures against power and EM SCA. Existing countermeasures (including proposed) still have relatively high overhead which bars them from being used in energy-constraint IoT devices. We propose a zero-overhead integrated inductive sensor which is able to detect i)EM SCA ii) Clock glitch-based Fault Injection Attack (FIA), and iii) Voltage-glitch based Fault Injection Attack by using a simple ML algorithm. Advent of quantum computer research will open new possibilities for theoretical attacks against existing cryptographic protocols. National Institute of Standard & Technology (NIST) has standardized post-quantum cryptographic algorithms to secure crypto-systems against quantum adversary. I contribute to the standardization procedure by introducing the first silicon-verified Saber (a NIST finalist modulo Learning with Rounding scheme) which consumes lowest energy and area till date amongst all the candidates."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-29T21:12:22Z",
                "published_parsed": [
                    2025,
                    11,
                    29,
                    21,
                    12,
                    22,
                    5,
                    333,
                    0
                ],
                "arxiv_comment": "This extended abstract is archived for educational purposes as an example for different PhD forum competitions. Total page is 3",
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Archisman Ghosh"
                    }
                ],
                "author_detail": {
                    "name": "Archisman Ghosh"
                },
                "author": "Archisman Ghosh"
            },
            {
                "id": "http://arxiv.org/abs/2512.00504v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.00504v1",
                "title": "G-KV: Decoding-Time KV Cache Eviction with Global Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "G-KV: Decoding-Time KV Cache Eviction with Global Attention"
                },
                "updated": "2025-11-29T14:21:33Z",
                "updated_parsed": [
                    2025,
                    11,
                    29,
                    14,
                    21,
                    33,
                    5,
                    333,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.00504v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.00504v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent reasoning large language models (LLMs) excel in complex tasks but encounter significant computational and memory challenges due to long sequence lengths. KV cache compression has emerged as an effective approach to greatly enhance the efficiency of reasoning. However, existing methods often focus on prompt compression or token eviction with local attention score, overlooking the long-term importance of tokens. We propose G-KV, a KV cache eviction method that employs a global scoring mechanism, combining local and historical attention scores to more accurately assess token importance. Additionally, we introduce post-training techniques, including reinforcement learning and distillation, to optimize models for compressed KV cache settings. The code of this paper is available on: https://github.com/microsoft/G-KV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent reasoning large language models (LLMs) excel in complex tasks but encounter significant computational and memory challenges due to long sequence lengths. KV cache compression has emerged as an effective approach to greatly enhance the efficiency of reasoning. However, existing methods often focus on prompt compression or token eviction with local attention score, overlooking the long-term importance of tokens. We propose G-KV, a KV cache eviction method that employs a global scoring mechanism, combining local and historical attention scores to more accurately assess token importance. Additionally, we introduce post-training techniques, including reinforcement learning and distillation, to optimize models for compressed KV cache settings. The code of this paper is available on: https://github.com/microsoft/G-KV."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-29T14:21:33Z",
                "published_parsed": [
                    2025,
                    11,
                    29,
                    14,
                    21,
                    33,
                    5,
                    333,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Mengqi Liao"
                    },
                    {
                        "name": "Lu Wang"
                    },
                    {
                        "name": "Chaoyun Zhang"
                    },
                    {
                        "name": "Zekai Shen"
                    },
                    {
                        "name": "Xiaowei Mao"
                    },
                    {
                        "name": "Si Qin"
                    },
                    {
                        "name": "Qingwei Lin"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    },
                    {
                        "name": "Dongmei Zhang"
                    },
                    {
                        "name": "Huaiyu Wan"
                    }
                ],
                "author_detail": {
                    "name": "Huaiyu Wan"
                },
                "author": "Huaiyu Wan"
            },
            {
                "id": "http://arxiv.org/abs/2512.00300v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.00300v1",
                "title": "TGSFormer: Scalable Temporal Gaussian Splatting for Embodied Semantic Scene Completion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TGSFormer: Scalable Temporal Gaussian Splatting for Embodied Semantic Scene Completion"
                },
                "updated": "2025-11-29T03:47:14Z",
                "updated_parsed": [
                    2025,
                    11,
                    29,
                    3,
                    47,
                    14,
                    5,
                    333,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.00300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.00300v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Embodied 3D Semantic Scene Completion (SSC) infers dense geometry and semantics from continuous egocentric observations. Most existing Gaussian-based methods rely on random initialization of many primitives within predefined spatial bounds, resulting in redundancy and poor scalability to unbounded scenes. Recent depth-guided approach alleviates this issue but remains local, suffering from latency and memory overhead as scale increases. To overcome these challenges, we propose TGSFormer, a scalable Temporal Gaussian Splatting framework for embodied SSC. It maintains a persistent Gaussian memory for temporal prediction, without relying on image coherence or frame caches. For temporal fusion, a Dual Temporal Encoder jointly processes current and historical Gaussian features through confidence-aware cross-attention. Subsequently, a Confidence-aware Voxel Fusion module merges overlapping primitives into voxel-aligned representations, regulating density and maintaining compactness. Extensive experiments demonstrate that TGSFormer achieves state-of-the-art results on both local and embodied SSC benchmarks, offering superior accuracy and scalability with significantly fewer primitives while maintaining consistent long-term scene integrity. The code will be released upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied 3D Semantic Scene Completion (SSC) infers dense geometry and semantics from continuous egocentric observations. Most existing Gaussian-based methods rely on random initialization of many primitives within predefined spatial bounds, resulting in redundancy and poor scalability to unbounded scenes. Recent depth-guided approach alleviates this issue but remains local, suffering from latency and memory overhead as scale increases. To overcome these challenges, we propose TGSFormer, a scalable Temporal Gaussian Splatting framework for embodied SSC. It maintains a persistent Gaussian memory for temporal prediction, without relying on image coherence or frame caches. For temporal fusion, a Dual Temporal Encoder jointly processes current and historical Gaussian features through confidence-aware cross-attention. Subsequently, a Confidence-aware Voxel Fusion module merges overlapping primitives into voxel-aligned representations, regulating density and maintaining compactness. Extensive experiments demonstrate that TGSFormer achieves state-of-the-art results on both local and embodied SSC benchmarks, offering superior accuracy and scalability with significantly fewer primitives while maintaining consistent long-term scene integrity. The code will be released upon acceptance."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-29T03:47:14Z",
                "published_parsed": [
                    2025,
                    11,
                    29,
                    3,
                    47,
                    14,
                    5,
                    333,
                    0
                ],
                "arxiv_comment": "14 pages, 10 figures",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Rui Qian"
                    },
                    {
                        "name": "Haozhi Cao"
                    },
                    {
                        "name": "Tianchen Deng"
                    },
                    {
                        "name": "Tianxin Hu"
                    },
                    {
                        "name": "Weixiang Guo"
                    },
                    {
                        "name": "Shenghai Yuan"
                    },
                    {
                        "name": "Lihua Xie"
                    }
                ],
                "author_detail": {
                    "name": "Lihua Xie"
                },
                "author": "Lihua Xie"
            },
            {
                "id": "http://arxiv.org/abs/2512.05134v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05134v1",
                "title": "InvarDiff: Cross-Scale Invariance Caching for Accelerated Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InvarDiff: Cross-Scale Invariance Caching for Accelerated Diffusion Models"
                },
                "updated": "2025-11-29T02:34:23Z",
                "updated_parsed": [
                    2025,
                    11,
                    29,
                    2,
                    34,
                    23,
                    5,
                    333,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05134v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05134v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion models deliver high-fidelity synthesis but remain slow due to iterative sampling. We empirically observe there exists feature invariance in deterministic sampling, and present InvarDiff, a training-free acceleration method that exploits the relative temporal invariance across timestep-scale and layer-scale. From a few deterministic runs, we compute a per-timestep, per-layer, per-module binary cache plan matrix and use a re-sampling correction to avoid drift when consecutive caches occur. Using quantile-based change metrics, this matrix specifies which module at which step is reused rather than recomputed. The same invariance criterion is applied at the step scale to enable cross-timestep caching, deciding whether an entire step can reuse cached results. During inference, InvarDiff performs step-first and layer-wise caching guided by this matrix. When applied to DiT and FLUX, our approach reduces redundant compute while preserving fidelity. Experiments show that InvarDiff achieves $2$-$3\\times$ end-to-end speed-ups with minimal impact on standard quality metrics. Qualitatively, we observe almost no degradation in visual quality compared with full computations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models deliver high-fidelity synthesis but remain slow due to iterative sampling. We empirically observe there exists feature invariance in deterministic sampling, and present InvarDiff, a training-free acceleration method that exploits the relative temporal invariance across timestep-scale and layer-scale. From a few deterministic runs, we compute a per-timestep, per-layer, per-module binary cache plan matrix and use a re-sampling correction to avoid drift when consecutive caches occur. Using quantile-based change metrics, this matrix specifies which module at which step is reused rather than recomputed. The same invariance criterion is applied at the step scale to enable cross-timestep caching, deciding whether an entire step can reuse cached results. During inference, InvarDiff performs step-first and layer-wise caching guided by this matrix. When applied to DiT and FLUX, our approach reduces redundant compute while preserving fidelity. Experiments show that InvarDiff achieves $2$-$3\\times$ end-to-end speed-ups with minimal impact on standard quality metrics. Qualitatively, we observe almost no degradation in visual quality compared with full computations."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-29T02:34:23Z",
                "published_parsed": [
                    2025,
                    11,
                    29,
                    2,
                    34,
                    23,
                    5,
                    333,
                    0
                ],
                "arxiv_comment": "8 pages main, 8 pages appendix, 16 figures, 5 tables. Code: https://github.com/zihaowu25/InvarDiff",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Zihao Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zihao Wu"
                },
                "author": "Zihao Wu"
            },
            {
                "id": "http://arxiv.org/abs/2408.10104v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2408.10104v2",
                "title": "Multi-Mode Lens for Momentum Microscopy and XPEEM: Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Mode Lens for Momentum Microscopy and XPEEM: Theory"
                },
                "updated": "2025-11-28T21:55:41Z",
                "updated_parsed": [
                    2025,
                    11,
                    28,
                    21,
                    55,
                    41,
                    4,
                    332,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2408.10104v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2408.10104v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The strong electric field between the sample and the extractor is the core of cathode lenses and a pivotal determinant of high resolution. Nevertheless, fields in the range of 3-8 kV/mm can be a source of complications. Local field enhancement at sharp edges or microscopic protrusions of cleaved samples may result in field emission or flashovers. Moreover, slow background electrons are drawn into the microscope column, where they contribute to space charge effects. A novel front lens configuration, optimized through ray-tracing simulations, significantly reduces the field at the sample and allows even for zero field or retarding field, which serves to suppress space charge effects. One or several annular electrodes, situated in a concentric position relative to the extractor, serve to form an additional lens within the gap between the sample and the extractor. The refractory power of this lens, and consequently the field at the sample surface, can be modified by adjusting the potentials of the annular electrodes. The imaging properties and aberrations of this gap lens have been investigated with regard to momentum imaging and XPEEM. The study encompasses the energy range from the few-eV level for laser-ARPES to 6 keV, for hard X-ray ARPES. The additional converging lens situated in close proximity to the sample exhibits a reduced field curvature of the k-image in the backfocal plane. This allows for the acquisition of larger fields of view in both momentum and real-space imaging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The strong electric field between the sample and the extractor is the core of cathode lenses and a pivotal determinant of high resolution. Nevertheless, fields in the range of 3-8 kV/mm can be a source of complications. Local field enhancement at sharp edges or microscopic protrusions of cleaved samples may result in field emission or flashovers. Moreover, slow background electrons are drawn into the microscope column, where they contribute to space charge effects. A novel front lens configuration, optimized through ray-tracing simulations, significantly reduces the field at the sample and allows even for zero field or retarding field, which serves to suppress space charge effects. One or several annular electrodes, situated in a concentric position relative to the extractor, serve to form an additional lens within the gap between the sample and the extractor. The refractory power of this lens, and consequently the field at the sample surface, can be modified by adjusting the potentials of the annular electrodes. The imaging properties and aberrations of this gap lens have been investigated with regard to momentum imaging and XPEEM. The study encompasses the energy range from the few-eV level for laser-ARPES to 6 keV, for hard X-ray ARPES. The additional converging lens situated in close proximity to the sample exhibits a reduced field curvature of the k-image in the backfocal plane. This allows for the acquisition of larger fields of view in both momentum and real-space imaging."
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-08-19T15:47:17Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    15,
                    47,
                    17,
                    0,
                    232,
                    0
                ],
                "arxiv_comment": "17 pages, 4 figures, 44 references",
                "arxiv_primary_category": {
                    "term": "physics.app-ph"
                },
                "authors": [
                    {
                        "name": "Olena Tkach"
                    },
                    {
                        "name": "Gerd Schoenhense"
                    }
                ],
                "author_detail": {
                    "name": "Gerd Schoenhense"
                },
                "author": "Gerd Schoenhense"
            },
            {
                "id": "http://arxiv.org/abs/2511.23070v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.23070v1",
                "title": "Buffer replay enhances the robustness of multimodal learning under missing-modality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Buffer replay enhances the robustness of multimodal learning under missing-modality"
                },
                "updated": "2025-11-28T10:55:31Z",
                "updated_parsed": [
                    2025,
                    11,
                    28,
                    10,
                    55,
                    31,
                    4,
                    332,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.23070v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.23070v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Missing modalities consistently lead to significant performance degradation in multimodal models. Existing approaches either synthesize missing modalities at high computational cost or apply prompt-based fine-tuning that relies only on adjacent-layer features and overlooks long-distance contextual information, which may offer additional tolerance to errors when one or more modalities are missing. To address this, we introduce REplay Prompting (REP): (1) construct modality-wise feature buffers via a residual bypass to cache early-layer representations and replay them in deeper layers, mitigating information loss as network depth increases; (2) employ a private-shared feature decoupling strategy, where private buffers preserve modality-specific signals and shared buffers encode cross-modal semantics; and (3) design a task-aware dynamic initialization mechanism to configure these buffers differently, improving stability and generalization under diverse missing-modality conditions. Experiments on vision-language, vision-language-audio, and temporal multimodal benchmarks demonstrate that REP consistently outperforms prior methods under both single- and multi-modality missing scenarios, while introducing only negligible parameter overhead. These results establish REP as a lightweight and effective paradigm for robust multimodal learning in challenging missing-modality environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Missing modalities consistently lead to significant performance degradation in multimodal models. Existing approaches either synthesize missing modalities at high computational cost or apply prompt-based fine-tuning that relies only on adjacent-layer features and overlooks long-distance contextual information, which may offer additional tolerance to errors when one or more modalities are missing. To address this, we introduce REplay Prompting (REP): (1) construct modality-wise feature buffers via a residual bypass to cache early-layer representations and replay them in deeper layers, mitigating information loss as network depth increases; (2) employ a private-shared feature decoupling strategy, where private buffers preserve modality-specific signals and shared buffers encode cross-modal semantics; and (3) design a task-aware dynamic initialization mechanism to configure these buffers differently, improving stability and generalization under diverse missing-modality conditions. Experiments on vision-language, vision-language-audio, and temporal multimodal benchmarks demonstrate that REP consistently outperforms prior methods under both single- and multi-modality missing scenarios, while introducing only negligible parameter overhead. These results establish REP as a lightweight and effective paradigm for robust multimodal learning in challenging missing-modality environments."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-28T10:55:31Z",
                "published_parsed": [
                    2025,
                    11,
                    28,
                    10,
                    55,
                    31,
                    4,
                    332,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Hongye Zhu"
                    },
                    {
                        "name": "Xuan Liu"
                    },
                    {
                        "name": "Yanwen Ba"
                    },
                    {
                        "name": "Jingye Xue"
                    },
                    {
                        "name": "Shigeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shigeng Zhang"
                },
                "author": "Shigeng Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2511.23011v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.23011v1",
                "title": "Cohet: A CXL-Driven Coherent Heterogeneous Computing Framework with Hardware-Calibrated Full-System Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cohet: A CXL-Driven Coherent Heterogeneous Computing Framework with Hardware-Calibrated Full-System Simulation"
                },
                "updated": "2025-11-28T09:22:37Z",
                "updated_parsed": [
                    2025,
                    11,
                    28,
                    9,
                    22,
                    37,
                    4,
                    332,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.23011v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.23011v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Conventional heterogeneous computing systems built on PCIe interconnects suffer from inefficient fine-grained host-device interactions and complex programming models. In recent years, many proprietary and open cache-coherent interconnect standards have emerged, among which compute express link (CXL) prevails in the open-standard domain after acquiring several competing solutions. Although CXL-based coherent heterogeneous computing holds the potential to fundamentally transform the collaborative computing mode of CPUs and XPUs, research in this direction remains hampered by the scarcity of available CXL-supported platforms, immature software/hardware ecosystems, and unclear application prospects. This paper presents Cohet, the first CXL-driven coherent heterogeneous computing framework. Cohet decouples the compute and memory resources to form unbiased CPU and XPU pools which share a single unified and coherent memory pool. It exposes a standard malloc/mmap interface to both CPU and XPU compute threads, leaving the OS dealing with smart memory allocation and management of heterogeneous resources. To facilitate Cohet research, we also present a full-system cycle-level simulator named SimCXL, which is capable of modeling all CXL sub-protocols and device types. SimCXL has been rigorously calibrated against a real CXL testbed with various CXL memory and accelerators, showing an average simulation error of 3%. Our evaluation reveals that CXL.cache reduces latency by 68% and increases bandwidth by 14.4x compared to DMA transfers at cacheline granularity. Building upon these insights, we demonstrate the benefits of Cohet with two killer apps, which are remote atomic operation (RAO) and remote procedure call (RPC). Compared to PCIe-NIC design, CXL-NIC achieves a 5.5 to 40.2x speedup for RAO offloading and an average speedup of 1.86x for RPC (de)serialization offloading.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional heterogeneous computing systems built on PCIe interconnects suffer from inefficient fine-grained host-device interactions and complex programming models. In recent years, many proprietary and open cache-coherent interconnect standards have emerged, among which compute express link (CXL) prevails in the open-standard domain after acquiring several competing solutions. Although CXL-based coherent heterogeneous computing holds the potential to fundamentally transform the collaborative computing mode of CPUs and XPUs, research in this direction remains hampered by the scarcity of available CXL-supported platforms, immature software/hardware ecosystems, and unclear application prospects. This paper presents Cohet, the first CXL-driven coherent heterogeneous computing framework. Cohet decouples the compute and memory resources to form unbiased CPU and XPU pools which share a single unified and coherent memory pool. It exposes a standard malloc/mmap interface to both CPU and XPU compute threads, leaving the OS dealing with smart memory allocation and management of heterogeneous resources. To facilitate Cohet research, we also present a full-system cycle-level simulator named SimCXL, which is capable of modeling all CXL sub-protocols and device types. SimCXL has been rigorously calibrated against a real CXL testbed with various CXL memory and accelerators, showing an average simulation error of 3%. Our evaluation reveals that CXL.cache reduces latency by 68% and increases bandwidth by 14.4x compared to DMA transfers at cacheline granularity. Building upon these insights, we demonstrate the benefits of Cohet with two killer apps, which are remote atomic operation (RAO) and remote procedure call (RPC). Compared to PCIe-NIC design, CXL-NIC achieves a 5.5 to 40.2x speedup for RAO offloading and an average speedup of 1.86x for RPC (de)serialization offloading."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-28T09:22:37Z",
                "published_parsed": [
                    2025,
                    11,
                    28,
                    9,
                    22,
                    37,
                    4,
                    332,
                    0
                ],
                "arxiv_comment": "Accepted by HPCA 2026",
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Yanjing Wang"
                    },
                    {
                        "name": "Lizhou Wu"
                    },
                    {
                        "name": "Sunfeng Gao"
                    },
                    {
                        "name": "Yibo Tang"
                    },
                    {
                        "name": "Junhui Luo"
                    },
                    {
                        "name": "Zicong Wang"
                    },
                    {
                        "name": "Yang Ou"
                    },
                    {
                        "name": "Dezun Dong"
                    },
                    {
                        "name": "Nong Xiao"
                    },
                    {
                        "name": "Mingche Lai"
                    }
                ],
                "author_detail": {
                    "name": "Mingche Lai"
                },
                "author": "Mingche Lai"
            },
            {
                "id": "http://arxiv.org/abs/2511.22973v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.22973v1",
                "title": "BlockVid: Block Diffusion for High-Quality and Consistent Minute-Long Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BlockVid: Block Diffusion for High-Quality and Consistent Minute-Long Video Generation"
                },
                "updated": "2025-11-28T08:25:59Z",
                "updated_parsed": [
                    2025,
                    11,
                    28,
                    8,
                    25,
                    59,
                    4,
                    332,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.22973v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.22973v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Generating minute-long videos is a critical step toward developing world models, providing a foundation for realistic extended scenes and advanced AI simulators. The emerging semi-autoregressive (block diffusion) paradigm integrates the strengths of diffusion and autoregressive models, enabling arbitrary-length video generation and improving inference efficiency through KV caching and parallel sampling. However, it yet faces two enduring challenges: (i) KV-cache-induced long-horizon error accumulation, and (ii) the lack of fine-grained long-video benchmarks and coherence-aware metrics. To overcome these limitations, we propose BlockVid, a novel block diffusion framework equipped with semantic-aware sparse KV cache, an effective training strategy called Block Forcing, and dedicated chunk-wise noise scheduling and shuffling to reduce error propagation and enhance temporal consistency. We further introduce LV-Bench, a fine-grained benchmark for minute-long videos, complete with new metrics evaluating long-range coherence. Extensive experiments on VBench and LV-Bench demonstrate that BlockVid consistently outperforms existing methods in generating high-quality, coherent minute-long videos. In particular, it achieves a 22.2% improvement on VDE Subject and a 19.4% improvement on VDE Clarity in LV-Bench over the state of the art approaches. Project website: https://ziplab.co/BlockVid. Inferix (Code): https://github.com/alibaba-damo-academy/Inferix.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating minute-long videos is a critical step toward developing world models, providing a foundation for realistic extended scenes and advanced AI simulators. The emerging semi-autoregressive (block diffusion) paradigm integrates the strengths of diffusion and autoregressive models, enabling arbitrary-length video generation and improving inference efficiency through KV caching and parallel sampling. However, it yet faces two enduring challenges: (i) KV-cache-induced long-horizon error accumulation, and (ii) the lack of fine-grained long-video benchmarks and coherence-aware metrics. To overcome these limitations, we propose BlockVid, a novel block diffusion framework equipped with semantic-aware sparse KV cache, an effective training strategy called Block Forcing, and dedicated chunk-wise noise scheduling and shuffling to reduce error propagation and enhance temporal consistency. We further introduce LV-Bench, a fine-grained benchmark for minute-long videos, complete with new metrics evaluating long-range coherence. Extensive experiments on VBench and LV-Bench demonstrate that BlockVid consistently outperforms existing methods in generating high-quality, coherent minute-long videos. In particular, it achieves a 22.2% improvement on VDE Subject and a 19.4% improvement on VDE Clarity in LV-Bench over the state of the art approaches. Project website: https://ziplab.co/BlockVid. Inferix (Code): https://github.com/alibaba-damo-academy/Inferix."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-28T08:25:59Z",
                "published_parsed": [
                    2025,
                    11,
                    28,
                    8,
                    25,
                    59,
                    4,
                    332,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Shuning Chang"
                    },
                    {
                        "name": "Yuanyu He"
                    },
                    {
                        "name": "Yizeng Han"
                    },
                    {
                        "name": "Jiasheng Tang"
                    },
                    {
                        "name": "Fan Wang"
                    },
                    {
                        "name": "Bohan Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Bohan Zhuang"
                },
                "author": "Bohan Zhuang"
            },
            {
                "id": "http://arxiv.org/abs/2511.22889v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.22889v1",
                "title": "The Immutable Tensor Architecture: A Pure Dataflow Approach for Secure, Energy-Efficient AI Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Immutable Tensor Architecture: A Pure Dataflow Approach for Secure, Energy-Efficient AI Inference"
                },
                "updated": "2025-11-28T05:36:12Z",
                "updated_parsed": [
                    2025,
                    11,
                    28,
                    5,
                    36,
                    12,
                    4,
                    332,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.22889v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.22889v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The deployment of Large Language Models (LLMs) on consumer edge devices is throttled by the \"Memory Wall\" -- the prohibitive bandwidth and energy cost of fetching gigabytes of model weights from DRAM for every token generated. Current architectures (GPUs, NPUs) treat model weights as mutable software data, incurring massive energy penalties to maintain general-purpose programmability. We propose The Immutable Tensor Architecture (ITA), a paradigm shift that treats model weights not as data, but as physical circuit topology. By encoding parameters directly into the metal interconnects and logic of mature-node ASICs (28nm/40nm), ITA eliminates the memory hierarchy entirely. We present a \"Split-Brain\" system design where a host CPU manages dynamic KV-cache operations while the ITA ASIC acts as a stateless, ROM-embedded dataflow engine.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of Large Language Models (LLMs) on consumer edge devices is throttled by the \"Memory Wall\" -- the prohibitive bandwidth and energy cost of fetching gigabytes of model weights from DRAM for every token generated. Current architectures (GPUs, NPUs) treat model weights as mutable software data, incurring massive energy penalties to maintain general-purpose programmability. We propose The Immutable Tensor Architecture (ITA), a paradigm shift that treats model weights not as data, but as physical circuit topology. By encoding parameters directly into the metal interconnects and logic of mature-node ASICs (28nm/40nm), ITA eliminates the memory hierarchy entirely. We present a \"Split-Brain\" system design where a host CPU manages dynamic KV-cache operations while the ITA ASIC acts as a stateless, ROM-embedded dataflow engine."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-28T05:36:12Z",
                "published_parsed": [
                    2025,
                    11,
                    28,
                    5,
                    36,
                    12,
                    4,
                    332,
                    0
                ],
                "arxiv_comment": "Code and data can be found here: https://github.com/fanglioc/ita-fpga-prototype",
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Fang Li"
                    }
                ],
                "author_detail": {
                    "name": "Fang Li"
                },
                "author": "Fang Li"
            },
            {
                "id": "http://arxiv.org/abs/2511.22880v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.22880v1",
                "title": "Serving Heterogeneous LoRA Adapters in Distributed LLM Inference Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving Heterogeneous LoRA Adapters in Distributed LLM Inference Systems"
                },
                "updated": "2025-11-28T05:04:02Z",
                "updated_parsed": [
                    2025,
                    11,
                    28,
                    5,
                    4,
                    2,
                    4,
                    332,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.22880v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.22880v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Low-Rank Adaptation (LoRA) has become the de facto method for parameter-efficient fine-tuning of large language models (LLMs), enabling rapid adaptation to diverse domains. In production, LoRA-based models are served at scale, creating multi-tenant environments with hundreds of adapters sharing a base model. However, state-of-the-art serving systems co-batch heterogeneous adapters without accounting for rank (size) variability, leading to severe performance skew, which ultimately requires adding more GPUs to satisfy service-level objectives (SLOs). Existing optimizations, focused on loading, caching, and kernel execution, ignore this heterogeneity, leaving GPU resources underutilized. We present LoRAServe, a workload-aware dynamic adapter placement and routing framework designed to tame rank diversity in LoRA serving. By dynamically rebalancing adapters across GPUs and leveraging GPU Direct RDMA for remote access, LoRAServe maximizes throughput and minimizes tail latency under real-world workload drift. Evaluations on production traces from Company X show that LoRAServe elicits up to 2$\\times$ higher throughput, up to 9$\\times$ lower TTFT, while using up to 50% fewer GPUs under SLO constraints compared to state-of-the-art systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) has become the de facto method for parameter-efficient fine-tuning of large language models (LLMs), enabling rapid adaptation to diverse domains. In production, LoRA-based models are served at scale, creating multi-tenant environments with hundreds of adapters sharing a base model. However, state-of-the-art serving systems co-batch heterogeneous adapters without accounting for rank (size) variability, leading to severe performance skew, which ultimately requires adding more GPUs to satisfy service-level objectives (SLOs). Existing optimizations, focused on loading, caching, and kernel execution, ignore this heterogeneity, leaving GPU resources underutilized. We present LoRAServe, a workload-aware dynamic adapter placement and routing framework designed to tame rank diversity in LoRA serving. By dynamically rebalancing adapters across GPUs and leveraging GPU Direct RDMA for remote access, LoRAServe maximizes throughput and minimizes tail latency under real-world workload drift. Evaluations on production traces from Company X show that LoRAServe elicits up to 2$\\times$ higher throughput, up to 9$\\times$ lower TTFT, while using up to 50% fewer GPUs under SLO constraints compared to state-of-the-art systems."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-28T05:04:02Z",
                "published_parsed": [
                    2025,
                    11,
                    28,
                    5,
                    4,
                    2,
                    4,
                    332,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Shashwat Jaiswal"
                    },
                    {
                        "name": "Shrikara Arun"
                    },
                    {
                        "name": "Anjaly Parayil"
                    },
                    {
                        "name": "Ankur Mallick"
                    },
                    {
                        "name": "Spyros Mastorakis"
                    },
                    {
                        "name": "Alind Khare"
                    },
                    {
                        "name": "Chloi Alverti"
                    },
                    {
                        "name": "Renee St Amant"
                    },
                    {
                        "name": "Chetan Bansal"
                    },
                    {
                        "name": "Victor Rühle"
                    },
                    {
                        "name": "Josep Torrellas"
                    }
                ],
                "author_detail": {
                    "name": "Josep Torrellas"
                },
                "author": "Josep Torrellas"
            },
            {
                "id": "http://arxiv.org/abs/2511.22857v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.22857v1",
                "title": "GLOW: Global Illumination-Aware Inverse Rendering of Indoor Scenes Captured with Dynamic Co-Located Light & Camera",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GLOW: Global Illumination-Aware Inverse Rendering of Indoor Scenes Captured with Dynamic Co-Located Light & Camera"
                },
                "updated": "2025-11-28T03:24:12Z",
                "updated_parsed": [
                    2025,
                    11,
                    28,
                    3,
                    24,
                    12,
                    4,
                    332,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.22857v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.22857v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Inverse rendering of indoor scenes remains challenging due to the ambiguity between reflectance and lighting, exacerbated by inter-reflections among multiple objects. While natural illumination-based methods struggle to resolve this ambiguity, co-located light-camera setups offer better disentanglement as lighting can be easily calibrated via Structure-from-Motion. However, such setups introduce additional complexities like strong inter-reflections, dynamic shadows, near-field lighting, and moving specular highlights, which existing approaches fail to handle. We present GLOW, a Global Illumination-aware Inverse Rendering framework designed to address these challenges. GLOW integrates a neural implicit surface representation with a neural radiance cache to approximate global illumination, jointly optimizing geometry and reflectance through carefully designed regularization and initialization. We then introduce a dynamic radiance cache that adapts to sharp lighting discontinuities from near-field motion, and a surface-angle-weighted radiometric loss to suppress specular artifacts common in flashlight captures. Experiments show that GLOW substantially outperforms prior methods in material reflectance estimation under both natural and co-located illumination.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inverse rendering of indoor scenes remains challenging due to the ambiguity between reflectance and lighting, exacerbated by inter-reflections among multiple objects. While natural illumination-based methods struggle to resolve this ambiguity, co-located light-camera setups offer better disentanglement as lighting can be easily calibrated via Structure-from-Motion. However, such setups introduce additional complexities like strong inter-reflections, dynamic shadows, near-field lighting, and moving specular highlights, which existing approaches fail to handle. We present GLOW, a Global Illumination-aware Inverse Rendering framework designed to address these challenges. GLOW integrates a neural implicit surface representation with a neural radiance cache to approximate global illumination, jointly optimizing geometry and reflectance through carefully designed regularization and initialization. We then introduce a dynamic radiance cache that adapts to sharp lighting discontinuities from near-field motion, and a surface-angle-weighted radiometric loss to suppress specular artifacts common in flashlight captures. Experiments show that GLOW substantially outperforms prior methods in material reflectance estimation under both natural and co-located illumination."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-28T03:24:12Z",
                "published_parsed": [
                    2025,
                    11,
                    28,
                    3,
                    24,
                    12,
                    4,
                    332,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Jiaye Wu"
                    },
                    {
                        "name": "Saeed Hadadan"
                    },
                    {
                        "name": "Geng Lin"
                    },
                    {
                        "name": "Peihan Tu"
                    },
                    {
                        "name": "Matthias Zwicker"
                    },
                    {
                        "name": "David Jacobs"
                    },
                    {
                        "name": "Roni Sengupta"
                    }
                ],
                "author_detail": {
                    "name": "Roni Sengupta"
                },
                "author": "Roni Sengupta"
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2512.10959v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10959v1",
                "title": "StereoSpace: Depth-Free Synthesis of Stereo Geometry via End-to-End Diffusion in a Canonical Space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StereoSpace: Depth-Free Synthesis of Stereo Geometry via End-to-End Diffusion in a Canonical Space"
                },
                "updated": "2025-12-11T18:59:59Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    18,
                    59,
                    59,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10959v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10959v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We introduce StereoSpace, a diffusion-based framework for monocular-to-stereo synthesis that models geometry purely through viewpoint conditioning, without explicit depth or warping. A canonical rectified space and the conditioning guide the generator to infer correspondences and fill disocclusions end-to-end. To ensure fair and leakage-free evaluation, we introduce an end-to-end protocol that excludes any ground truth or proxy geometry estimates at test time. The protocol emphasizes metrics reflecting downstream relevance: iSQoE for perceptual comfort and MEt3R for geometric consistency. StereoSpace surpasses other methods from the warp & inpaint, latent-warping, and warped-conditioning categories, achieving sharp parallax and strong robustness on layered and non-Lambertian scenes. This establishes viewpoint-conditioned diffusion as a scalable, depth-free solution for stereo generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce StereoSpace, a diffusion-based framework for monocular-to-stereo synthesis that models geometry purely through viewpoint conditioning, without explicit depth or warping. A canonical rectified space and the conditioning guide the generator to infer correspondences and fill disocclusions end-to-end. To ensure fair and leakage-free evaluation, we introduce an end-to-end protocol that excludes any ground truth or proxy geometry estimates at test time. The protocol emphasizes metrics reflecting downstream relevance: iSQoE for perceptual comfort and MEt3R for geometric consistency. StereoSpace surpasses other methods from the warp & inpaint, latent-warping, and warped-conditioning categories, achieving sharp parallax and strong robustness on layered and non-Lambertian scenes. This establishes viewpoint-conditioned diffusion as a scalable, depth-free solution for stereo generation."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T18:59:59Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    18,
                    59,
                    59,
                    3,
                    345,
                    0
                ],
                "arxiv_comment": "Project page: https://hf.co/spaces/prs-eth/stereospace_web",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Tjark Behrens"
                    },
                    {
                        "name": "Anton Obukhov"
                    },
                    {
                        "name": "Bingxin Ke"
                    },
                    {
                        "name": "Fabio Tosi"
                    },
                    {
                        "name": "Matteo Poggi"
                    },
                    {
                        "name": "Konrad Schindler"
                    }
                ],
                "author_detail": {
                    "name": "Konrad Schindler"
                },
                "author": "Konrad Schindler"
            },
            {
                "id": "http://arxiv.org/abs/2512.10954v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10954v1",
                "title": "Group Diffusion: Enhancing Image Generation by Unlocking Cross-Sample Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Group Diffusion: Enhancing Image Generation by Unlocking Cross-Sample Collaboration"
                },
                "updated": "2025-12-11T18:59:55Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    18,
                    59,
                    55,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10954v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10954v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In this work, we explore an untapped signal in diffusion model inference. While all previous methods generate images independently at inference, we instead ask if samples can be generated collaboratively. We propose Group Diffusion, unlocking the attention mechanism to be shared across images, rather than limited to just the patches within an image. This enables images to be jointly denoised at inference time, learning both intra and inter-image correspondence. We observe a clear scaling effect - larger group sizes yield stronger cross-sample attention and better generation quality. Furthermore, we introduce a qualitative measure to capture this behavior and show that its strength closely correlates with FID. Built on standard diffusion transformers, our GroupDiff achieves up to 32.2% FID improvement on ImageNet-256x256. Our work reveals cross-sample inference as an effective, previously unexplored mechanism for generative modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we explore an untapped signal in diffusion model inference. While all previous methods generate images independently at inference, we instead ask if samples can be generated collaboratively. We propose Group Diffusion, unlocking the attention mechanism to be shared across images, rather than limited to just the patches within an image. This enables images to be jointly denoised at inference time, learning both intra and inter-image correspondence. We observe a clear scaling effect - larger group sizes yield stronger cross-sample attention and better generation quality. Furthermore, we introduce a qualitative measure to capture this behavior and show that its strength closely correlates with FID. Built on standard diffusion transformers, our GroupDiff achieves up to 32.2% FID improvement on ImageNet-256x256. Our work reveals cross-sample inference as an effective, previously unexplored mechanism for generative modeling."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T18:59:55Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    18,
                    59,
                    55,
                    3,
                    345,
                    0
                ],
                "arxiv_comment": "Project Page: https://sichengmo.github.io/GroupDiff/",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Sicheng Mo"
                    },
                    {
                        "name": "Thao Nguyen"
                    },
                    {
                        "name": "Richard Zhang"
                    },
                    {
                        "name": "Nick Kolkin"
                    },
                    {
                        "name": "Siddharth Srinivasan Iyer"
                    },
                    {
                        "name": "Eli Shechtman"
                    },
                    {
                        "name": "Krishna Kumar Singh"
                    },
                    {
                        "name": "Yong Jae Lee"
                    },
                    {
                        "name": "Bolei Zhou"
                    },
                    {
                        "name": "Yuheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Yuheng Li"
                },
                "author": "Yuheng Li"
            },
            {
                "id": "http://arxiv.org/abs/2512.10950v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10950v1",
                "title": "E-RayZer: Self-supervised 3D Reconstruction as Spatial Visual Pre-training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "E-RayZer: Self-supervised 3D Reconstruction as Spatial Visual Pre-training"
                },
                "updated": "2025-12-11T18:59:53Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    18,
                    59,
                    53,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10950v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10950v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Self-supervised pre-training has revolutionized foundation models for languages, individual 2D images and videos, but remains largely unexplored for learning 3D-aware representations from multi-view images. In this paper, we present E-RayZer, a self-supervised large 3D Vision model that learns truly 3D-aware representations directly from unlabeled images. Unlike prior self-supervised methods such as RayZer that infer 3D indirectly through latent-space view synthesis, E-RayZer operates directly in 3D space, performing self-supervised 3D reconstruction with Explicit geometry. This formulation eliminates shortcut solutions and yields representations that are geometrically grounded. To ensure convergence and scalability, we introduce a novel fine-grained learning curriculum that organizes training from easy to hard samples and harmonizes heterogeneous data sources in an entirely unsupervised manner. Experiments demonstrate that E-RayZer significantly outperforms RayZer on pose estimation, matches or sometimes surpasses fully supervised reconstruction models such as VGGT. Furthermore, its learned representations outperform leading visual pre-training models (e.g., DINOv3, CroCo v2, VideoMAE V2, and RayZer) when transferring to 3D downstream tasks, establishing E-RayZer as a new paradigm for 3D-aware visual pre-training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-supervised pre-training has revolutionized foundation models for languages, individual 2D images and videos, but remains largely unexplored for learning 3D-aware representations from multi-view images. In this paper, we present E-RayZer, a self-supervised large 3D Vision model that learns truly 3D-aware representations directly from unlabeled images. Unlike prior self-supervised methods such as RayZer that infer 3D indirectly through latent-space view synthesis, E-RayZer operates directly in 3D space, performing self-supervised 3D reconstruction with Explicit geometry. This formulation eliminates shortcut solutions and yields representations that are geometrically grounded. To ensure convergence and scalability, we introduce a novel fine-grained learning curriculum that organizes training from easy to hard samples and harmonizes heterogeneous data sources in an entirely unsupervised manner. Experiments demonstrate that E-RayZer significantly outperforms RayZer on pose estimation, matches or sometimes surpasses fully supervised reconstruction models such as VGGT. Furthermore, its learned representations outperform leading visual pre-training models (e.g., DINOv3, CroCo v2, VideoMAE V2, and RayZer) when transferring to 3D downstream tasks, establishing E-RayZer as a new paradigm for 3D-aware visual pre-training."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T18:59:53Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    18,
                    59,
                    53,
                    3,
                    345,
                    0
                ],
                "arxiv_comment": "Project website: https://qitaozhao.github.io/E-RayZer",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Qitao Zhao"
                    },
                    {
                        "name": "Hao Tan"
                    },
                    {
                        "name": "Qianqian Wang"
                    },
                    {
                        "name": "Sai Bi"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Kalyan Sunkavalli"
                    },
                    {
                        "name": "Shubham Tulsiani"
                    },
                    {
                        "name": "Hanwen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Hanwen Jiang"
                },
                "author": "Hanwen Jiang"
            },
            {
                "id": "http://arxiv.org/abs/2512.10947v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10947v1",
                "title": "Towards Efficient and Effective Multi-Camera Encoding for End-to-End Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Efficient and Effective Multi-Camera Encoding for End-to-End Driving"
                },
                "updated": "2025-12-11T18:59:46Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    18,
                    59,
                    46,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10947v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10947v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We present Flex, an efficient and effective scene encoder that addresses the computational bottleneck of processing high-volume multi-camera data in end-to-end autonomous driving. Flex employs a small set of learnable scene tokens to jointly encode information from all image tokens across different cameras and timesteps. By design, our approach is geometry-agnostic, learning a compact scene representation directly from data without relying on the explicit 3D inductive biases, such as Bird-Eye-View (BEV), occupancy or tri-plane representations, which are common in prior work. This holistic encoding strategy aggressively compresses the visual input for the downstream Large Language Model (LLM) based policy model. Evaluated on a large-scale proprietary dataset of 20,000 driving hours, our Flex achieves 2.2x greater inference throughput while improving driving performance by a large margin compared to state-of-the-art methods. Furthermore, we show that these compact scene tokens develop an emergent capability for scene decomposition without any explicit supervision. Our findings challenge the prevailing assumption that 3D priors are necessary, demonstrating that a data-driven, joint encoding strategy offers a more scalable, efficient and effective path for future autonomous driving systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Flex, an efficient and effective scene encoder that addresses the computational bottleneck of processing high-volume multi-camera data in end-to-end autonomous driving. Flex employs a small set of learnable scene tokens to jointly encode information from all image tokens across different cameras and timesteps. By design, our approach is geometry-agnostic, learning a compact scene representation directly from data without relying on the explicit 3D inductive biases, such as Bird-Eye-View (BEV), occupancy or tri-plane representations, which are common in prior work. This holistic encoding strategy aggressively compresses the visual input for the downstream Large Language Model (LLM) based policy model. Evaluated on a large-scale proprietary dataset of 20,000 driving hours, our Flex achieves 2.2x greater inference throughput while improving driving performance by a large margin compared to state-of-the-art methods. Furthermore, we show that these compact scene tokens develop an emergent capability for scene decomposition without any explicit supervision. Our findings challenge the prevailing assumption that 3D priors are necessary, demonstrating that a data-driven, joint encoding strategy offers a more scalable, efficient and effective path for future autonomous driving systems."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T18:59:46Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    18,
                    59,
                    46,
                    3,
                    345,
                    0
                ],
                "arxiv_comment": "Project Page: https://jiawei-yang.github.io/Flex/",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Jiawei Yang"
                    },
                    {
                        "name": "Ziyu Chen"
                    },
                    {
                        "name": "Yurong You"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Yuxiao Chen"
                    },
                    {
                        "name": "Boyi Li"
                    },
                    {
                        "name": "Boris Ivanovic"
                    },
                    {
                        "name": "Marco Pavone"
                    },
                    {
                        "name": "Yue Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yue Wang"
                },
                "author": "Yue Wang"
            },
            {
                "id": "http://arxiv.org/abs/2512.10942v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10942v1",
                "title": "VL-JEPA: Joint Embedding Predictive Architecture for Vision-language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VL-JEPA: Joint Embedding Predictive Architecture for Vision-language"
                },
                "updated": "2025-12-11T18:59:22Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    18,
                    59,
                    22,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10942v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10942v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We introduce VL-JEPA, a vision-language model built on a Joint Embedding Predictive Architecture (JEPA). Instead of autoregressively generating tokens as in classical VLMs, VL-JEPA predicts continuous embeddings of the target texts. By learning in an abstract representation space, the model focuses on task-relevant semantics while abstracting away surface-level linguistic variability. In a strictly controlled comparison against standard token-space VLM training with the same vision encoder and training data, VL-JEPA achieves stronger performance while having 50% fewer trainable parameters. At inference time, a lightweight text decoder is invoked only when needed to translate VL-JEPA predicted embeddings into text. We show that VL-JEPA natively supports selective decoding that reduces the number of decoding operations by 2.85x while maintaining similar performance compared to non-adaptive uniform decoding. Beyond generation, the VL-JEPA's embedding space naturally supports open-vocabulary classification, text-to-video retrieval, and discriminative VQA without any architecture modification. On eight video classification and eight video retrieval datasets, the average performance VL-JEPA surpasses that of CLIP, SigLIP2, and Perception Encoder. At the same time, the model achieves comparable performance as classical VLMs (InstructBLIP, QwenVL) on four VQA datasets: GQA, TallyQA, POPE and POPEv2, despite only having 1.6B parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce VL-JEPA, a vision-language model built on a Joint Embedding Predictive Architecture (JEPA). Instead of autoregressively generating tokens as in classical VLMs, VL-JEPA predicts continuous embeddings of the target texts. By learning in an abstract representation space, the model focuses on task-relevant semantics while abstracting away surface-level linguistic variability. In a strictly controlled comparison against standard token-space VLM training with the same vision encoder and training data, VL-JEPA achieves stronger performance while having 50% fewer trainable parameters. At inference time, a lightweight text decoder is invoked only when needed to translate VL-JEPA predicted embeddings into text. We show that VL-JEPA natively supports selective decoding that reduces the number of decoding operations by 2.85x while maintaining similar performance compared to non-adaptive uniform decoding. Beyond generation, the VL-JEPA's embedding space naturally supports open-vocabulary classification, text-to-video retrieval, and discriminative VQA without any architecture modification. On eight video classification and eight video retrieval datasets, the average performance VL-JEPA surpasses that of CLIP, SigLIP2, and Perception Encoder. At the same time, the model achieves comparable performance as classical VLMs (InstructBLIP, QwenVL) on four VQA datasets: GQA, TallyQA, POPE and POPEv2, despite only having 1.6B parameters."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T18:59:22Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    18,
                    59,
                    22,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Delong Chen"
                    },
                    {
                        "name": "Mustafa Shukor"
                    },
                    {
                        "name": "Theo Moutakanni"
                    },
                    {
                        "name": "Willy Chung"
                    },
                    {
                        "name": "Jade Yu"
                    },
                    {
                        "name": "Tejaswi Kasarla"
                    },
                    {
                        "name": "Allen Bolourchi"
                    },
                    {
                        "name": "Yann LeCun"
                    },
                    {
                        "name": "Pascale Fung"
                    }
                ],
                "author_detail": {
                    "name": "Pascale Fung"
                },
                "author": "Pascale Fung"
            },
            {
                "id": "http://arxiv.org/abs/2512.10931v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10931v1",
                "title": "Asynchronous Reasoning: Training-Free Interactive Thinking LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Asynchronous Reasoning: Training-Free Interactive Thinking LLMs"
                },
                "updated": "2025-12-11T18:57:02Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    18,
                    57,
                    2,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10931v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10931v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Many state-of-the-art LLMs are trained to think before giving their answer. Reasoning can greatly improve language model capabilities and safety, but it also makes them less interactive: given a new input, a model must stop thinking before it can respond. Real-world use cases such as voice-based or embedded assistants require an LLM agent to respond and adapt to additional information in real time, which is incompatible with sequential interactions. In contrast, humans can listen, think, and act asynchronously: we begin thinking about the problem while reading it and continue thinking while formulating the answer. In this work, we augment LLMs capable of reasoning to operate in a similar way without additional training. Our method uses the properties of rotary embeddings to enable LLMs built for sequential interactions to simultaneously think, listen, and generate outputs. We evaluate our approach on math, commonsense, and safety reasoning and find that it can generate accurate thinking-augmented answers in real time, reducing time to first non-thinking token from minutes to <= 5s. and the overall real-time delays by 6-11x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many state-of-the-art LLMs are trained to think before giving their answer. Reasoning can greatly improve language model capabilities and safety, but it also makes them less interactive: given a new input, a model must stop thinking before it can respond. Real-world use cases such as voice-based or embedded assistants require an LLM agent to respond and adapt to additional information in real time, which is incompatible with sequential interactions. In contrast, humans can listen, think, and act asynchronously: we begin thinking about the problem while reading it and continue thinking while formulating the answer. In this work, we augment LLMs capable of reasoning to operate in a similar way without additional training. Our method uses the properties of rotary embeddings to enable LLMs built for sequential interactions to simultaneously think, listen, and generate outputs. We evaluate our approach on math, commonsense, and safety reasoning and find that it can generate accurate thinking-augmented answers in real time, reducing time to first non-thinking token from minutes to <= 5s. and the overall real-time delays by 6-11x."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T18:57:02Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    18,
                    57,
                    2,
                    3,
                    345,
                    0
                ],
                "arxiv_comment": "Preprint, work in progress",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "George Yakushev"
                    },
                    {
                        "name": "Nataliia Babina"
                    },
                    {
                        "name": "Masoud Vahid Dastgerdi"
                    },
                    {
                        "name": "Vyacheslav Zhdanovskiy"
                    },
                    {
                        "name": "Alina Shutova"
                    },
                    {
                        "name": "Denis Kuznedelev"
                    }
                ],
                "author_detail": {
                    "name": "Denis Kuznedelev"
                },
                "author": "Denis Kuznedelev"
            },
            {
                "id": "http://arxiv.org/abs/2512.10927v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10927v1",
                "title": "FoundationMotion: Auto-Labeling and Reasoning about Spatial Movement in Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FoundationMotion: Auto-Labeling and Reasoning about Spatial Movement in Videos"
                },
                "updated": "2025-12-11T18:53:15Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    18,
                    53,
                    15,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10927v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10927v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Motion understanding is fundamental to physical reasoning, enabling models to infer dynamics and predict future states. However, state-of-the-art models still struggle on recent motion benchmarks, primarily due to the scarcity of large-scale, fine-grained motion datasets. Existing motion datasets are often constructed from costly manual annotation, severely limiting scalability. To address this challenge, we introduce FoundationMotion, a fully automated data curation pipeline that constructs large-scale motion datasets. Our approach first detects and tracks objects in videos to extract their trajectories, then leverages these trajectories and video frames with Large Language Models (LLMs) to generate fine-grained captions and diverse question-answer pairs about motion and spatial reasoning. Using datasets produced by this pipeline, we fine-tune open-source models including NVILA-Video-15B and Qwen2.5-7B, achieving substantial improvements in motion understanding without compromising performance on other tasks. Notably, our models outperform strong closed-source baselines like Gemini-2.5 Flash and large open-source models such as Qwen2.5-VL-72B across diverse motion understanding datasets and benchmarks. FoundationMotion thus provides a scalable solution for curating fine-grained motion datasets that enable effective fine-tuning of diverse models to enhance motion understanding and spatial reasoning capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motion understanding is fundamental to physical reasoning, enabling models to infer dynamics and predict future states. However, state-of-the-art models still struggle on recent motion benchmarks, primarily due to the scarcity of large-scale, fine-grained motion datasets. Existing motion datasets are often constructed from costly manual annotation, severely limiting scalability. To address this challenge, we introduce FoundationMotion, a fully automated data curation pipeline that constructs large-scale motion datasets. Our approach first detects and tracks objects in videos to extract their trajectories, then leverages these trajectories and video frames with Large Language Models (LLMs) to generate fine-grained captions and diverse question-answer pairs about motion and spatial reasoning. Using datasets produced by this pipeline, we fine-tune open-source models including NVILA-Video-15B and Qwen2.5-7B, achieving substantial improvements in motion understanding without compromising performance on other tasks. Notably, our models outperform strong closed-source baselines like Gemini-2.5 Flash and large open-source models such as Qwen2.5-VL-72B across diverse motion understanding datasets and benchmarks. FoundationMotion thus provides a scalable solution for curating fine-grained motion datasets that enable effective fine-tuning of diverse models to enhance motion understanding and spatial reasoning capabilities."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T18:53:15Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    18,
                    53,
                    15,
                    3,
                    345,
                    0
                ],
                "arxiv_comment": "Code is available at https://github.com/Wolfv0/FoundationMotion/tree/main",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Yulu Gan"
                    },
                    {
                        "name": "Ligeng Zhu"
                    },
                    {
                        "name": "Dandan Shan"
                    },
                    {
                        "name": "Baifeng Shi"
                    },
                    {
                        "name": "Hongxu Yin"
                    },
                    {
                        "name": "Boris Ivanovic"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Trevor Darrell"
                    },
                    {
                        "name": "Jitendra Malik"
                    },
                    {
                        "name": "Marco Pavone"
                    },
                    {
                        "name": "Boyi Li"
                    }
                ],
                "author_detail": {
                    "name": "Boyi Li"
                },
                "author": "Boyi Li"
            },
            {
                "id": "http://arxiv.org/abs/2512.06982v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.06982v2",
                "title": "LLM-Driven Composite Neural Architecture Search for Multi-Source RL State Encoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Driven Composite Neural Architecture Search for Multi-Source RL State Encoding"
                },
                "updated": "2025-12-11T18:52:44Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    18,
                    52,
                    44,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.06982v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.06982v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Designing state encoders for reinforcement learning (RL) with multiple information sources -- such as sensor measurements, time-series signals, image observations, and textual instructions -- remains underexplored and often requires manual design. We formalize this challenge as a problem of composite neural architecture search (NAS), where multiple source-specific modules and a fusion module are jointly optimized. Existing NAS methods overlook useful side information from the intermediate outputs of these modules -- such as their representation quality -- limiting sample efficiency in multi-source RL settings. To address this, we propose an LLM-driven NAS pipeline in which the LLM serves as a neural architecture design agent, leveraging language-model priors and intermediate-output signals to guide sample-efficient search for high-performing composite state encoders. On a mixed-autonomy traffic control task, our approach discovers higher-performing architectures with fewer candidate evaluations than traditional NAS baselines and the LLM-based GENIUS framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing state encoders for reinforcement learning (RL) with multiple information sources -- such as sensor measurements, time-series signals, image observations, and textual instructions -- remains underexplored and often requires manual design. We formalize this challenge as a problem of composite neural architecture search (NAS), where multiple source-specific modules and a fusion module are jointly optimized. Existing NAS methods overlook useful side information from the intermediate outputs of these modules -- such as their representation quality -- limiting sample efficiency in multi-source RL settings. To address this, we propose an LLM-driven NAS pipeline in which the LLM serves as a neural architecture design agent, leveraging language-model priors and intermediate-output signals to guide sample-efficient search for high-performing composite state encoders. On a mixed-autonomy traffic control task, our approach discovers higher-performing architectures with fewer candidate evaluations than traditional NAS baselines and the LLM-based GENIUS framework."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-07T20:25:07Z",
                "published_parsed": [
                    2025,
                    12,
                    7,
                    20,
                    25,
                    7,
                    6,
                    341,
                    0
                ],
                "arxiv_comment": "NeurIPS 2025 Workshop on Bridging Language, Agent, and World Models for Reasoning and Planning",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Yu Yu"
                    },
                    {
                        "name": "Qian Xie"
                    },
                    {
                        "name": "Nairen Cao"
                    },
                    {
                        "name": "Li Jin"
                    }
                ],
                "author_detail": {
                    "name": "Li Jin"
                },
                "author": "Li Jin"
            },
            {
                "id": "http://arxiv.org/abs/2504.20101v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2504.20101v4",
                "title": "PlanetServe: A Decentralized, Scalable, and Privacy-Preserving Overlay for Democratizing Large Language Model Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PlanetServe: A Decentralized, Scalable, and Privacy-Preserving Overlay for Democratizing Large Language Model Serving"
                },
                "updated": "2025-12-11T18:49:32Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    18,
                    49,
                    32,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2504.20101v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2504.20101v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "While significant progress has been made in research and development on open-source and cost-efficient large-language models (LLMs), serving scalability remains a critical challenge, particularly for small organizations and individuals seeking to deploy and test their LLM innovations. Inspired by peer-to-peer networks that leverage decentralized overlay nodes to increase throughput and availability, we propose GenTorrent, an LLM serving overlay that harnesses computing resources from decentralized contributors. We identify four key research problems inherent to enabling such a decentralized infrastructure: 1) overlay network organization; 2) LLM communication privacy; 3) overlay forwarding for resource efficiency; and 4) verification of serving quality. This work presents the first systematic study of these fundamental problems in the context of decentralized LLM serving. Evaluation results from a prototype implemented on a set of decentralized nodes demonstrate that GenTorrent achieves a latency reduction of over 50% compared to the baseline design without overlay forwarding. Furthermore, the security features introduce minimal overhead to serving latency and throughput. We believe this work pioneers a new direction for democratizing and scaling future AI serving capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While significant progress has been made in research and development on open-source and cost-efficient large-language models (LLMs), serving scalability remains a critical challenge, particularly for small organizations and individuals seeking to deploy and test their LLM innovations. Inspired by peer-to-peer networks that leverage decentralized overlay nodes to increase throughput and availability, we propose GenTorrent, an LLM serving overlay that harnesses computing resources from decentralized contributors. We identify four key research problems inherent to enabling such a decentralized infrastructure: 1) overlay network organization; 2) LLM communication privacy; 3) overlay forwarding for resource efficiency; and 4) verification of serving quality. This work presents the first systematic study of these fundamental problems in the context of decentralized LLM serving. Evaluation results from a prototype implemented on a set of decentralized nodes demonstrate that GenTorrent achieves a latency reduction of over 50% compared to the baseline design without overlay forwarding. Furthermore, the security features introduce minimal overhead to serving latency and throughput. We believe this work pioneers a new direction for democratizing and scaling future AI serving capabilities."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-04-27T01:08:25Z",
                "published_parsed": [
                    2025,
                    4,
                    27,
                    1,
                    8,
                    25,
                    6,
                    117,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Fei Fang"
                    },
                    {
                        "name": "Yifan Hua"
                    },
                    {
                        "name": "Shengze Wang"
                    },
                    {
                        "name": "Ruilin Zhou"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Chen Qian"
                    },
                    {
                        "name": "Xiaoxue Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoxue Zhang"
                },
                "author": "Xiaoxue Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2512.10922v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10922v1",
                "title": "SparseSwaps: Tractable LLM Pruning Mask Refinement at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseSwaps: Tractable LLM Pruning Mask Refinement at Scale"
                },
                "updated": "2025-12-11T18:47:48Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    18,
                    47,
                    48,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10922v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10922v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The resource requirements of Neural Networks can be significantly reduced through pruning -- the removal of seemingly less important parameters. However, with the rise of Large Language Models (LLMs), full retraining to recover pruning-induced performance degradation is often prohibitive and classical approaches such as global magnitude pruning are suboptimal on Transformer architectures. State-of-the-art methods hence solve a layer-wise mask selection problem, the problem of finding a pruning mask which minimizes the per-layer pruning error on a small set of calibration data. Exactly solving this problem to optimality using Integer Programming (IP) solvers is computationally infeasible due to its combinatorial nature and the size of the search space, and existing approaches therefore rely on approximations or heuristics. In this work, we demonstrate that the mask selection problem can be made drastically more tractable at LLM scale. To that end, we decouple the rows by enforcing equal sparsity levels per row. This allows us to derive optimal 1-swaps (exchanging one kept and one pruned weight) that can be computed efficiently using the Gram matrix of the calibration data. Using these observations, we propose a tractable and simple 1-swap algorithm that warm starts from any pruning mask, runs efficiently on GPUs at LLM scale, and is essentially hyperparameter-free. We demonstrate that our approach reduces per-layer pruning error by up to 60% over Wanda (Sun et al., 2023) and consistently improves perplexity and zero-shot accuracy across state-of-the-art GPT architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The resource requirements of Neural Networks can be significantly reduced through pruning -- the removal of seemingly less important parameters. However, with the rise of Large Language Models (LLMs), full retraining to recover pruning-induced performance degradation is often prohibitive and classical approaches such as global magnitude pruning are suboptimal on Transformer architectures. State-of-the-art methods hence solve a layer-wise mask selection problem, the problem of finding a pruning mask which minimizes the per-layer pruning error on a small set of calibration data. Exactly solving this problem to optimality using Integer Programming (IP) solvers is computationally infeasible due to its combinatorial nature and the size of the search space, and existing approaches therefore rely on approximations or heuristics. In this work, we demonstrate that the mask selection problem can be made drastically more tractable at LLM scale. To that end, we decouple the rows by enforcing equal sparsity levels per row. This allows us to derive optimal 1-swaps (exchanging one kept and one pruned weight) that can be computed efficiently using the Gram matrix of the calibration data. Using these observations, we propose a tractable and simple 1-swap algorithm that warm starts from any pruning mask, runs efficiently on GPUs at LLM scale, and is essentially hyperparameter-free. We demonstrate that our approach reduces per-layer pruning error by up to 60% over Wanda (Sun et al., 2023) and consistently improves perplexity and zero-shot accuracy across state-of-the-art GPT architectures."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T18:47:48Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    18,
                    47,
                    48,
                    3,
                    345,
                    0
                ],
                "arxiv_comment": "15 pages, 2 figures, 4 tables",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Max Zimmer"
                    },
                    {
                        "name": "Christophe Roux"
                    },
                    {
                        "name": "Moritz Wagner"
                    },
                    {
                        "name": "Deborah Hendrych"
                    },
                    {
                        "name": "Sebastian Pokutta"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Pokutta"
                },
                "author": "Sebastian Pokutta"
            },
            {
                "id": "http://arxiv.org/abs/2512.10918v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10918v1",
                "title": "CompanionCast: A Multi-Agent Conversational AI Framework with Spatial Audio for Social Co-Viewing Experiences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CompanionCast: A Multi-Agent Conversational AI Framework with Spatial Audio for Social Co-Viewing Experiences"
                },
                "updated": "2025-12-11T18:44:44Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    18,
                    44,
                    44,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10918v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10918v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Social presence is central to the enjoyment of watching content together, yet modern media consumption is increasingly solitary. We investigate whether multi-agent conversational AI systems can recreate the dynamics of shared viewing experiences across diverse content types. We present CompanionCast, a general framework for orchestrating multiple role-specialized AI agents that respond to video content using multimodal inputs, speech synthesis, and spatial audio. Distinctly, CompanionCast integrates an LLM-as-a-Judge module that iteratively scores and refines conversations across five dimensions (relevance, authenticity, engagement, diversity, personality consistency). We validate this framework through sports viewing, a domain with rich dynamics and strong social traditions, where a pilot study with soccer fans suggests that multi-agent interaction improves perceived social presence compared to solo viewing. We contribute: (1) a generalizable framework for orchestrating multi-agent conversations around multimodal video content, (2) a novel evaluator-agent pipeline for conversation quality control, and (3) exploratory evidence of increased social presence in AI-mediated co-viewing. We discuss challenges and future directions for applying this approach to diverse viewing contexts including entertainment, education, and collaborative watching experiences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Social presence is central to the enjoyment of watching content together, yet modern media consumption is increasingly solitary. We investigate whether multi-agent conversational AI systems can recreate the dynamics of shared viewing experiences across diverse content types. We present CompanionCast, a general framework for orchestrating multiple role-specialized AI agents that respond to video content using multimodal inputs, speech synthesis, and spatial audio. Distinctly, CompanionCast integrates an LLM-as-a-Judge module that iteratively scores and refines conversations across five dimensions (relevance, authenticity, engagement, diversity, personality consistency). We validate this framework through sports viewing, a domain with rich dynamics and strong social traditions, where a pilot study with soccer fans suggests that multi-agent interaction improves perceived social presence compared to solo viewing. We contribute: (1) a generalizable framework for orchestrating multi-agent conversations around multimodal video content, (2) a novel evaluator-agent pipeline for conversation quality control, and (3) exploratory evidence of increased social presence in AI-mediated co-viewing. We discuss challenges and future directions for applying this approach to diverse viewing contexts including entertainment, education, and collaborative watching experiences."
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T18:44:44Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    18,
                    44,
                    44,
                    3,
                    345,
                    0
                ],
                "arxiv_comment": "11 pages",
                "arxiv_primary_category": {
                    "term": "cs.HC"
                },
                "authors": [
                    {
                        "name": "Yiyang Wang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Tica Lin"
                    },
                    {
                        "name": "Vishnu Raj"
                    },
                    {
                        "name": "Josh Kimball"
                    },
                    {
                        "name": "Alex Cabral"
                    },
                    {
                        "name": "Josiah Hester"
                    }
                ],
                "author_detail": {
                    "name": "Josiah Hester"
                },
                "author": "Josiah Hester"
            },
            {
                "id": "http://arxiv.org/abs/2512.10903v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10903v1",
                "title": "Multi-Granular Node Pruning for Circuit Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Granular Node Pruning for Circuit Discovery"
                },
                "updated": "2025-12-11T18:32:15Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    18,
                    32,
                    15,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10903v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10903v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Circuit discovery aims to identify minimal subnetworks that are responsible for specific behaviors in large language models (LLMs). Existing approaches primarily rely on iterative edge pruning, which is computationally expensive and limited to coarse-grained units such as attention heads or MLP blocks, overlooking finer structures like individual neurons. We propose a node-level pruning framework for circuit discovery that addresses both scalability and granularity limitations. Our method introduces learnable masks across multiple levels of granularity, from entire blocks to individual neurons, within a unified optimization objective. Granularity-specific sparsity penalties guide the pruning process, allowing a comprehensive compression in a single fine-tuning run. Empirically, our approach identifies circuits that are smaller in nodes than those discovered by prior methods; moreover, we demonstrate that many neurons deemed important by coarse methods are actually irrelevant, while still maintaining task performance. Furthermore, our method has a significantly lower memory footprint, 5-10x, as it does not require keeping intermediate activations in the memory to work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Circuit discovery aims to identify minimal subnetworks that are responsible for specific behaviors in large language models (LLMs). Existing approaches primarily rely on iterative edge pruning, which is computationally expensive and limited to coarse-grained units such as attention heads or MLP blocks, overlooking finer structures like individual neurons. We propose a node-level pruning framework for circuit discovery that addresses both scalability and granularity limitations. Our method introduces learnable masks across multiple levels of granularity, from entire blocks to individual neurons, within a unified optimization objective. Granularity-specific sparsity penalties guide the pruning process, allowing a comprehensive compression in a single fine-tuning run. Empirically, our approach identifies circuits that are smaller in nodes than those discovered by prior methods; moreover, we demonstrate that many neurons deemed important by coarse methods are actually irrelevant, while still maintaining task performance. Furthermore, our method has a significantly lower memory footprint, 5-10x, as it does not require keeping intermediate activations in the memory to work."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T18:32:15Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    18,
                    32,
                    15,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Muhammad Umair Haider"
                    },
                    {
                        "name": "Hammad Rizwan"
                    },
                    {
                        "name": "Hassan Sajjad"
                    },
                    {
                        "name": "A. B. Siddique"
                    }
                ],
                "author_detail": {
                    "name": "A. B. Siddique"
                },
                "author": "A. B. Siddique"
            },
            {
                "id": "http://arxiv.org/abs/2512.10895v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10895v1",
                "title": "LLMs Can Assist with Proposal Selection at Large User Facilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs Can Assist with Proposal Selection at Large User Facilities"
                },
                "updated": "2025-12-11T18:23:56Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    18,
                    23,
                    56,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10895v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10895v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We explore how large language models (LLMs) can enhance the proposal selection process at large user facilities, offering a scalable, consistent, and cost-effective alternative to traditional human review. Proposal selection depends on assessing the relative strength among submitted proposals; however, traditional human scoring often suffers from weak inter-proposal correlations and is subject to reviewer bias and inconsistency. A pairwise preference-based approach is logically superior, providing a more rigorous and internally consistent basis for ranking, but its quadratic workload makes it impractical for human reviewers. We address this limitation using LLMs. Leveraging the uniquely well-curated proposals and publication records from three beamlines at the Spallation Neutron Source (SNS), Oak Ridge National Laboratory (ORNL), we show that the LLM rankings correlate strongly with the human rankings (Spearman $ρ\\simeq 0.2-0.8$, improving to $\\geq 0.5$ after 10\\% outlier removal). Moreover, LLM performance is no worse than that of human reviewers in identifying proposals with high publication potential, while costing over two orders of magnitude less. Beyond ranking, LLMs enable advanced analyses that are challenging for humans, such as quantitative assessment of proposal similarity via embedding models, which provides information crucial for review committees.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We explore how large language models (LLMs) can enhance the proposal selection process at large user facilities, offering a scalable, consistent, and cost-effective alternative to traditional human review. Proposal selection depends on assessing the relative strength among submitted proposals; however, traditional human scoring often suffers from weak inter-proposal correlations and is subject to reviewer bias and inconsistency. A pairwise preference-based approach is logically superior, providing a more rigorous and internally consistent basis for ranking, but its quadratic workload makes it impractical for human reviewers. We address this limitation using LLMs. Leveraging the uniquely well-curated proposals and publication records from three beamlines at the Spallation Neutron Source (SNS), Oak Ridge National Laboratory (ORNL), we show that the LLM rankings correlate strongly with the human rankings (Spearman $ρ\\simeq 0.2-0.8$, improving to $\\geq 0.5$ after 10\\% outlier removal). Moreover, LLM performance is no worse than that of human reviewers in identifying proposals with high publication potential, while costing over two orders of magnitude less. Beyond ranking, LLMs enable advanced analyses that are challenging for humans, such as quantitative assessment of proposal similarity via embedding models, which provides information crucial for review committees."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T18:23:56Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    18,
                    23,
                    56,
                    3,
                    345,
                    0
                ],
                "arxiv_comment": "9 pages, 8figures",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Lijie Ding"
                    },
                    {
                        "name": "Janell Thomson"
                    },
                    {
                        "name": "Jon Taylor"
                    },
                    {
                        "name": "Changwoo Do"
                    }
                ],
                "author_detail": {
                    "name": "Changwoo Do"
                },
                "author": "Changwoo Do"
            },
            {
                "id": "http://arxiv.org/abs/2512.10894v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10894v1",
                "title": "DuetSVG: Unified Multimodal SVG Generation with Internal Visual Guidance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DuetSVG: Unified Multimodal SVG Generation with Internal Visual Guidance"
                },
                "updated": "2025-12-11T18:23:03Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    18,
                    23,
                    3,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10894v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10894v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent vision-language model (VLM)-based approaches have achieved impressive results on SVG generation. However, because they generate only text and lack visual signals during decoding, they often struggle with complex semantics and fail to produce visually appealing or geometrically coherent SVGs. We introduce DuetSVG, a unified multimodal model that jointly generates image tokens and corresponding SVG tokens in an end-to-end manner. DuetSVG is trained on both image and SVG datasets. At inference, we apply a novel test-time scaling strategy that leverages the model's native visual predictions as guidance to improve SVG decoding quality. Extensive experiments show that our method outperforms existing methods, producing visually faithful, semantically aligned, and syntactically clean SVGs across a wide range of applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent vision-language model (VLM)-based approaches have achieved impressive results on SVG generation. However, because they generate only text and lack visual signals during decoding, they often struggle with complex semantics and fail to produce visually appealing or geometrically coherent SVGs. We introduce DuetSVG, a unified multimodal model that jointly generates image tokens and corresponding SVG tokens in an end-to-end manner. DuetSVG is trained on both image and SVG datasets. At inference, we apply a novel test-time scaling strategy that leverages the model's native visual predictions as guidance to improve SVG decoding quality. Extensive experiments show that our method outperforms existing methods, producing visually faithful, semantically aligned, and syntactically clean SVGs across a wide range of applications."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T18:23:03Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    18,
                    23,
                    3,
                    3,
                    345,
                    0
                ],
                "arxiv_comment": "Project page: https://intchous.github.io/DuetSVG-site",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Peiying Zhang"
                    },
                    {
                        "name": "Nanxuan Zhao"
                    },
                    {
                        "name": "Matthew Fisher"
                    },
                    {
                        "name": "Yiran Xu"
                    },
                    {
                        "name": "Jing Liao"
                    },
                    {
                        "name": "Difan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Difan Liu"
                },
                "author": "Difan Liu"
            },
            {
                "id": "http://arxiv.org/abs/2506.18156v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.18156v3",
                "title": "AI Through the Human Lens: Investigating Cognitive Theories in Machine Psychology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Through the Human Lens: Investigating Cognitive Theories in Machine Psychology"
                },
                "updated": "2025-12-11T18:18:42Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    18,
                    18,
                    42,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.18156v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.18156v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We investigate whether Large Language Models (LLMs) exhibit human-like cognitive patterns under four established frameworks from psychology: Thematic Apperception Test (TAT), Framing Bias, Moral Foundations Theory (MFT), and Cognitive Dissonance. We evaluated several proprietary and open-source models using structured prompts and automated scoring. Our findings reveal that these models often produce coherent narratives, show susceptibility to positive framing, exhibit moral judgments aligned with Liberty/Oppression concerns, and demonstrate self-contradictions tempered by extensive rationalization. Such behaviors mirror human cognitive tendencies yet are shaped by their training data and alignment methods. We discuss the implications for AI transparency, ethical deployment, and future work that bridges cognitive psychology and AI safety",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate whether Large Language Models (LLMs) exhibit human-like cognitive patterns under four established frameworks from psychology: Thematic Apperception Test (TAT), Framing Bias, Moral Foundations Theory (MFT), and Cognitive Dissonance. We evaluated several proprietary and open-source models using structured prompts and automated scoring. Our findings reveal that these models often produce coherent narratives, show susceptibility to positive framing, exhibit moral judgments aligned with Liberty/Oppression concerns, and demonstrate self-contradictions tempered by extensive rationalization. Such behaviors mirror human cognitive tendencies yet are shaped by their training data and alignment methods. We discuss the implications for AI transparency, ethical deployment, and future work that bridges cognitive psychology and AI safety"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-22T19:58:19Z",
                "published_parsed": [
                    2025,
                    6,
                    22,
                    19,
                    58,
                    19,
                    6,
                    173,
                    0
                ],
                "arxiv_comment": "Accepted to IJCNLP-AACL 2025 Student Research Workshop",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Akash Kundu"
                    },
                    {
                        "name": "Rishika Goswami"
                    }
                ],
                "author_detail": {
                    "name": "Rishika Goswami"
                },
                "author": "Rishika Goswami"
            },
            {
                "id": "http://arxiv.org/abs/2512.00846v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.00846v2",
                "title": "AFRAgent : An Adaptive Feature Renormalization Based High Resolution Aware GUI agent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AFRAgent : An Adaptive Feature Renormalization Based High Resolution Aware GUI agent"
                },
                "updated": "2025-12-11T18:16:38Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    18,
                    16,
                    38,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.00846v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.00846v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "There is a growing demand for mobile user interface (UI) automation, driven by its broad applications across industries. With the advent of visual language models (VLMs), GUI automation has progressed from generating text-based instructions for humans to autonomously executing tasks, thus optimizing automation workflows. Recent approaches leverage VLMs for this problem due to their ability to 1) process on-screen content directly, 2) remain independent of device-specific APIs by utilizing human actions (e.g., clicks, typing), and 3) apply real-world contextual knowledge for task understanding. However, these models often have trouble accurately identifying widgets and determining actions due to limited spatial information in vision encoder features. Additionally, top-performing models are often large, requiring extensive training and resulting in inference delays. In this work, we introduce AFRAgent, an instruct-BLIP-based multimodal architecture that achieves superior performance in GUI automation while being less than one-fourth the size of its nearest competitor. To enhance image embeddings in the large language model (LLM) pipeline, we propose an adaptive feature renormalization-based (a token-level affine transformation) technique that effectively enriches low-resolution image embeddings and fuses high-resolution details. We evaluate AFRAgent on Meta-GUI and AITW benchmarks, establishing a new state-of-the-art baseline for smartphone automation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is a growing demand for mobile user interface (UI) automation, driven by its broad applications across industries. With the advent of visual language models (VLMs), GUI automation has progressed from generating text-based instructions for humans to autonomously executing tasks, thus optimizing automation workflows. Recent approaches leverage VLMs for this problem due to their ability to 1) process on-screen content directly, 2) remain independent of device-specific APIs by utilizing human actions (e.g., clicks, typing), and 3) apply real-world contextual knowledge for task understanding. However, these models often have trouble accurately identifying widgets and determining actions due to limited spatial information in vision encoder features. Additionally, top-performing models are often large, requiring extensive training and resulting in inference delays. In this work, we introduce AFRAgent, an instruct-BLIP-based multimodal architecture that achieves superior performance in GUI automation while being less than one-fourth the size of its nearest competitor. To enhance image embeddings in the large language model (LLM) pipeline, we propose an adaptive feature renormalization-based (a token-level affine transformation) technique that effectively enriches low-resolution image embeddings and fuses high-resolution details. We evaluate AFRAgent on Meta-GUI and AITW benchmarks, establishing a new state-of-the-art baseline for smartphone automation."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-30T11:32:54Z",
                "published_parsed": [
                    2025,
                    11,
                    30,
                    11,
                    32,
                    54,
                    6,
                    334,
                    0
                ],
                "arxiv_comment": "Accepted at WACV 2026 Conference",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Neeraj Anand"
                    },
                    {
                        "name": "Rishabh Jain"
                    },
                    {
                        "name": "Sohan Patnaik"
                    },
                    {
                        "name": "Balaji Krishnamurthy"
                    },
                    {
                        "name": "Mausoom Sarkar"
                    }
                ],
                "author_detail": {
                    "name": "Mausoom Sarkar"
                },
                "author": "Mausoom Sarkar"
            },
            {
                "id": "http://arxiv.org/abs/2512.10886v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10886v1",
                "title": "Physics-Informed Learning of Flow Distribution and Receiver Heat Losses in Parabolic Trough Solar Fields",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physics-Informed Learning of Flow Distribution and Receiver Heat Losses in Parabolic Trough Solar Fields"
                },
                "updated": "2025-12-11T18:16:26Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    18,
                    16,
                    26,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10886v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10886v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Parabolic trough Concentrating Solar Power (CSP) plants operate large hydraulic networks of collector loops that must deliver a uniform outlet temperature despite spatially heterogeneous optical performance, heat losses, and pressure drops. While loop temperatures are measured, loop-level mass flows and receiver heat-loss parameters are unobserved, making it impossible to diagnose hydraulic imbalances or receiver degradation using standard monitoring tools.\n  We present a physics-informed learning framework that infers (i) loop-level mass-flow ratios and (ii) time-varying receiver heat-transfer coefficients directly from routine operational data. The method exploits nocturnal homogenization periods -- when hot oil is circulated through a non-irradiated field -- to isolate hydraulic and thermal-loss effects. A differentiable conjugate heat-transfer model is discretized and embedded into an end-to-end learning pipeline optimized using historical plant data from the 50 MW Andasol 3 solar field.\n  The model accurately reconstructs loop temperatures (RMSE $<2^\\circ$C) and produces physically meaningful estimates of loop imbalances and receiver heat losses. Comparison against drone-based infrared thermography (QScan) shows strong correspondence, correctly identifying all areas with high-loss receivers. This demonstrates that noisy real-world CSP operational data contain enough information to recover latent physical parameters when combined with appropriate modeling and differentiable optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parabolic trough Concentrating Solar Power (CSP) plants operate large hydraulic networks of collector loops that must deliver a uniform outlet temperature despite spatially heterogeneous optical performance, heat losses, and pressure drops. While loop temperatures are measured, loop-level mass flows and receiver heat-loss parameters are unobserved, making it impossible to diagnose hydraulic imbalances or receiver degradation using standard monitoring tools.\n  We present a physics-informed learning framework that infers (i) loop-level mass-flow ratios and (ii) time-varying receiver heat-transfer coefficients directly from routine operational data. The method exploits nocturnal homogenization periods -- when hot oil is circulated through a non-irradiated field -- to isolate hydraulic and thermal-loss effects. A differentiable conjugate heat-transfer model is discretized and embedded into an end-to-end learning pipeline optimized using historical plant data from the 50 MW Andasol 3 solar field.\n  The model accurately reconstructs loop temperatures (RMSE $<2^\\circ$C) and produces physically meaningful estimates of loop imbalances and receiver heat losses. Comparison against drone-based infrared thermography (QScan) shows strong correspondence, correctly identifying all areas with high-loss receivers. This demonstrates that noisy real-world CSP operational data contain enough information to recover latent physical parameters when combined with appropriate modeling and differentiable optimization."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T18:16:26Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    18,
                    16,
                    26,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Stefan Matthes"
                    },
                    {
                        "name": "Markus Schramm"
                    }
                ],
                "author_detail": {
                    "name": "Markus Schramm"
                },
                "author": "Markus Schramm"
            },
            {
                "id": "http://arxiv.org/abs/2512.08898v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08898v2",
                "title": "Self-lensing of moving gravitational-wave sources can break the microlensing crossing timescale degeneracy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-lensing of moving gravitational-wave sources can break the microlensing crossing timescale degeneracy"
                },
                "updated": "2025-12-11T18:13:21Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    18,
                    13,
                    21,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08898v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08898v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "When a moving gravitational-wave (GW) source travels behind a massive astrophysical object, its signal is gravitationally lensed, showing a waveform distortion similar to a Paczyński curve. We present a first study of the lensing signature of a massive black hole (MBH) on a frequency-dependent GW signal from a moving binary merger. For both light and GW sources in a Keplerian circular orbit around a MBH lens, the self-lensing geometry breaks the microlensing degeneracy in the Einstein radius crossing timescale $t_{\\rm E}$. The duration of the curve ($2 t_{\\rm E}$) becomes independent on the MBH mass $M_{\\rm MBH}$, and provides a direct measure of the distance $d_{\\rm LS}$ to the MBH. However, $M_{\\rm MBH}$ remains unknown. We show that, in GW signals, the redshifted mass $M_{{\\rm MBH},z}$ can additionally be obtained from the interference pattern, by measuring the modulation period $T$, the GW frequency $f$, and $t_{\\rm E}$: $M_{{\\rm MBH},z}\\simeq 2.5\\times 10^6\\,M_\\odot\\,(t_{\\rm E}/[100\\,{\\rm s}])\\,(f\\,T)^{-1}$. If this lensing signature is not considered, it may be confused with other waveform distortions, especially in the modeling of overlapping signals in next generation ground-based GW detectors. The observation of one of these curves and its associated parameters may help (1) constrain the orbital distance $d_{\\rm LS}$ of sources, especially around low-mass MBHs at the center of star clusters and galaxies, (2) additionally estimate the mass $M_{{\\rm MBH},z}$ of these MBHs, and (3) infer the orbital inclination of the binary. Simultaneously obtaining $d_{\\rm LS}$ and $M_{{\\rm MBH},z}$ through self-lensing can help constrain the astrophysical environments where GW signals come from.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When a moving gravitational-wave (GW) source travels behind a massive astrophysical object, its signal is gravitationally lensed, showing a waveform distortion similar to a Paczyński curve. We present a first study of the lensing signature of a massive black hole (MBH) on a frequency-dependent GW signal from a moving binary merger. For both light and GW sources in a Keplerian circular orbit around a MBH lens, the self-lensing geometry breaks the microlensing degeneracy in the Einstein radius crossing timescale $t_{\\rm E}$. The duration of the curve ($2 t_{\\rm E}$) becomes independent on the MBH mass $M_{\\rm MBH}$, and provides a direct measure of the distance $d_{\\rm LS}$ to the MBH. However, $M_{\\rm MBH}$ remains unknown. We show that, in GW signals, the redshifted mass $M_{{\\rm MBH},z}$ can additionally be obtained from the interference pattern, by measuring the modulation period $T$, the GW frequency $f$, and $t_{\\rm E}$: $M_{{\\rm MBH},z}\\simeq 2.5\\times 10^6\\,M_\\odot\\,(t_{\\rm E}/[100\\,{\\rm s}])\\,(f\\,T)^{-1}$. If this lensing signature is not considered, it may be confused with other waveform distortions, especially in the modeling of overlapping signals in next generation ground-based GW detectors. The observation of one of these curves and its associated parameters may help (1) constrain the orbital distance $d_{\\rm LS}$ of sources, especially around low-mass MBHs at the center of star clusters and galaxies, (2) additionally estimate the mass $M_{{\\rm MBH},z}$ of these MBHs, and (3) infer the orbital inclination of the binary. Simultaneously obtaining $d_{\\rm LS}$ and $M_{{\\rm MBH},z}$ through self-lensing can help constrain the astrophysical environments where GW signals come from."
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T18:38:47Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    18,
                    38,
                    47,
                    1,
                    343,
                    0
                ],
                "arxiv_comment": "9 pages, 6 figures. Submitted version. Comments welcome",
                "arxiv_primary_category": {
                    "term": "astro-ph.HE"
                },
                "authors": [
                    {
                        "name": "Helena Ubach"
                    }
                ],
                "author_detail": {
                    "name": "Helena Ubach"
                },
                "author": "Helena Ubach"
            },
            {
                "id": "http://arxiv.org/abs/2512.10882v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10882v1",
                "title": "Computational emotion analysis with multimodal LLMs: Current evidence on an emerging methodological opportunity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational emotion analysis with multimodal LLMs: Current evidence on an emerging methodological opportunity"
                },
                "updated": "2025-12-11T18:11:46Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    18,
                    11,
                    46,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10882v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10882v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Emotions are central to politics and analyzing their role in political communication has a long tradition. As research increasingly leverages audio-visual materials to analyze the display of emotions, the emergence of multimodal generative AI promises great advances. However, we lack evidence about the effectiveness of multimodal AI in emotion analysis. This paper addresses this gap by evaluating current multimodal large language models (mLLMs) in video-based analysis of emotional arousal in two complementary data sets of human-labeled video recordings. I find that under ideal circumstances, mLLMs' emotional arousal ratings are highly reliable and show little to know indication of demographic bias. However, in recordings of speakers in real-world parliamentary debates, mLLMs' arousal ratings fail to deliver on this promise with potential negative consequences for downstream statistical inferences. This study therefore underscores the need for continued, thorough evaluation of emerging generative AI methods in political analysis and contributes a suitable replicable framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emotions are central to politics and analyzing their role in political communication has a long tradition. As research increasingly leverages audio-visual materials to analyze the display of emotions, the emergence of multimodal generative AI promises great advances. However, we lack evidence about the effectiveness of multimodal AI in emotion analysis. This paper addresses this gap by evaluating current multimodal large language models (mLLMs) in video-based analysis of emotional arousal in two complementary data sets of human-labeled video recordings. I find that under ideal circumstances, mLLMs' emotional arousal ratings are highly reliable and show little to know indication of demographic bias. However, in recordings of speakers in real-world parliamentary debates, mLLMs' arousal ratings fail to deliver on this promise with potential negative consequences for downstream statistical inferences. This study therefore underscores the need for continued, thorough evaluation of emerging generative AI methods in political analysis and contributes a suitable replicable framework."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T18:11:46Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    18,
                    11,
                    46,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Hauke Licht"
                    }
                ],
                "author_detail": {
                    "name": "Hauke Licht"
                },
                "author": "Hauke Licht"
            },
            {
                "id": "http://arxiv.org/abs/2512.10876v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10876v1",
                "title": "Diversity in the haziness and chemistry of temperate sub-Neptunes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diversity in the haziness and chemistry of temperate sub-Neptunes"
                },
                "updated": "2025-12-11T18:04:43Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    18,
                    4,
                    43,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10876v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10876v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1038/s41550-025-02723-3",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Recent transit observations of K2-18b and TOI-270d revealed strong molecular absorption signatures, lending credence to the idea that temperate sub-Neptunes (T$_\\mathrm{eq}$=250-400K) have upper atmospheres mostly free of aerosols. These observations also indicated higher-than-expected CO$_2$ abundances on both planets, implying bulk compositions with high water mass fractions. However, it remains unclear whether these findings hold true for all temperate sub-Neptunes. Here, we present the JWST NIRSpec/PRISM 0.7-5.4$\\mathbfμ$m transmission spectrum of a third temperate sub-Neptune, the 2.4R$_\\oplus$ planet LP 791-18c (T$_\\mathrm{eq}$=355K), which is even more favorable for atmospheric characterization thanks to its small M6 host star. Intriguingly, despite LP 791-18c's radius, mass, and equilibrium temperature being in between those of K2-18b and TOI-270d, we find a drastically different transmission spectrum. While we also detect methane on LP 791-18c, its transit spectrum is dominated by strong haze scattering and there is no discernible CO$_2$ absorption. Overall, we infer a deep metal-enriched atmosphere (246-415$\\times$solar) for LP 791-18c, with a CO$_2$-to-CH$_4$ ratio smaller than 0.07 (at 2$σ$), indicating less H$_2$O in the deep envelope of LP 791-18c and implying a relatively dry formation inside the water ice-line. These results show that sub-Neptunes that are near-analogues in density and temperature can show drastically different aerosols and envelope chemistry, and are intrinsically diverse beyond a simple temperature dependence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent transit observations of K2-18b and TOI-270d revealed strong molecular absorption signatures, lending credence to the idea that temperate sub-Neptunes (T$_\\mathrm{eq}$=250-400K) have upper atmospheres mostly free of aerosols. These observations also indicated higher-than-expected CO$_2$ abundances on both planets, implying bulk compositions with high water mass fractions. However, it remains unclear whether these findings hold true for all temperate sub-Neptunes. Here, we present the JWST NIRSpec/PRISM 0.7-5.4$\\mathbfμ$m transmission spectrum of a third temperate sub-Neptune, the 2.4R$_\\oplus$ planet LP 791-18c (T$_\\mathrm{eq}$=355K), which is even more favorable for atmospheric characterization thanks to its small M6 host star. Intriguingly, despite LP 791-18c's radius, mass, and equilibrium temperature being in between those of K2-18b and TOI-270d, we find a drastically different transmission spectrum. While we also detect methane on LP 791-18c, its transit spectrum is dominated by strong haze scattering and there is no discernible CO$_2$ absorption. Overall, we infer a deep metal-enriched atmosphere (246-415$\\times$solar) for LP 791-18c, with a CO$_2$-to-CH$_4$ ratio smaller than 0.07 (at 2$σ$), indicating less H$_2$O in the deep envelope of LP 791-18c and implying a relatively dry formation inside the water ice-line. These results show that sub-Neptunes that are near-analogues in density and temperature can show drastically different aerosols and envelope chemistry, and are intrinsically diverse beyond a simple temperature dependence."
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T18:04:43Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    18,
                    4,
                    43,
                    3,
                    345,
                    0
                ],
                "arxiv_comment": "Paper published in Nature Astronomy at https://www.nature.com/articles/s41550-025-02723-3. This preprint is the original submitted version before peer-review",
                "arxiv_primary_category": {
                    "term": "astro-ph.EP"
                },
                "authors": [
                    {
                        "name": "Pierre-Alexis Roy"
                    },
                    {
                        "name": "Björn Benneke"
                    },
                    {
                        "name": "Marylou Fournier-Tondreau"
                    },
                    {
                        "name": "Louis-Philippe Coulombe"
                    },
                    {
                        "name": "Caroline Piaulet-Ghorayeb"
                    },
                    {
                        "name": "David Lafrenière"
                    },
                    {
                        "name": "Romain Allart"
                    },
                    {
                        "name": "Nicolas B. Cowan"
                    },
                    {
                        "name": "Lisa Dang"
                    },
                    {
                        "name": "Doug Johnstone"
                    },
                    {
                        "name": "Adam B. Langeveld"
                    },
                    {
                        "name": "Stefan Pelletier"
                    },
                    {
                        "name": "Michael Radica"
                    },
                    {
                        "name": "Jake Taylor"
                    },
                    {
                        "name": "Loïc Albert"
                    },
                    {
                        "name": "René Doyon"
                    },
                    {
                        "name": "Laura Flagg"
                    },
                    {
                        "name": "Ray Jayawardhana"
                    },
                    {
                        "name": "Ryan J. MacDonald"
                    },
                    {
                        "name": "Jake D. Turner"
                    }
                ],
                "author_detail": {
                    "name": "Jake D. Turner"
                },
                "author": "Jake D. Turner",
                "arxiv_doi": "10.1038/s41550-025-02723-3"
            },
            {
                "id": "http://arxiv.org/abs/2512.10858v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10858v1",
                "title": "Scaling Behavior of Discrete Diffusion Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Behavior of Discrete Diffusion Language Models"
                },
                "updated": "2025-12-11T17:54:10Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    17,
                    54,
                    10,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10858v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10858v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Modern LLM pre-training consumes vast amounts of compute and training data, making the scaling behavior, or scaling laws, of different models a key distinguishing factor. Discrete diffusion language models (DLMs) have been proposed as an alternative to autoregressive language models (ALMs). However, their scaling behavior has not yet been fully explored, with prior work suggesting that they require more data and compute to match the performance of ALMs.\n  We study the scaling behavior of DLMs on different noise types by smoothly interpolating between masked and uniform diffusion while paying close attention to crucial hyperparameters such as batch size and learning rate. Our experiments reveal that the scaling behavior of DLMs strongly depends on the noise type and is considerably different from ALMs. While all noise types converge to similar loss values in compute-bound scaling, we find that uniform diffusion requires more parameters and less data for compute-efficient training compared to masked diffusion, making them a promising candidate in data-bound settings. We scale our uniform diffusion model up to 10B parameters trained for $10^{22}$ FLOPs, confirming the predicted scaling behavior and making it the largest publicly known uniform diffusion model to date.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern LLM pre-training consumes vast amounts of compute and training data, making the scaling behavior, or scaling laws, of different models a key distinguishing factor. Discrete diffusion language models (DLMs) have been proposed as an alternative to autoregressive language models (ALMs). However, their scaling behavior has not yet been fully explored, with prior work suggesting that they require more data and compute to match the performance of ALMs.\n  We study the scaling behavior of DLMs on different noise types by smoothly interpolating between masked and uniform diffusion while paying close attention to crucial hyperparameters such as batch size and learning rate. Our experiments reveal that the scaling behavior of DLMs strongly depends on the noise type and is considerably different from ALMs. While all noise types converge to similar loss values in compute-bound scaling, we find that uniform diffusion requires more parameters and less data for compute-efficient training compared to masked diffusion, making them a promising candidate in data-bound settings. We scale our uniform diffusion model up to 10B parameters trained for $10^{22}$ FLOPs, confirming the predicted scaling behavior and making it the largest publicly known uniform diffusion model to date."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T17:54:10Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    17,
                    54,
                    10,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Dimitri von Rütte"
                    },
                    {
                        "name": "Janis Fluri"
                    },
                    {
                        "name": "Omead Pooladzandi"
                    },
                    {
                        "name": "Bernhard Schölkopf"
                    },
                    {
                        "name": "Thomas Hofmann"
                    },
                    {
                        "name": "Antonio Orvieto"
                    }
                ],
                "author_detail": {
                    "name": "Antonio Orvieto"
                },
                "author": "Antonio Orvieto"
            },
            {
                "id": "http://arxiv.org/abs/2512.10847v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10847v1",
                "title": "Large Language Models for Superconductor Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Superconductor Discovery"
                },
                "updated": "2025-12-11T17:32:38Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    17,
                    32,
                    38,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10847v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10847v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) offer new opportunities for automated data extraction and property prediction across materials science, yet their use in superconductivity research remains limited. Here we construct a large experimental database of 78,203 records, covering 19,058 unique compositions, extracted from scientific literature using an LLM-driven workflow. Each entry includes chemical composition, critical temperature, measurement pressure, structural descriptors, and critical fields. We fine-tune several open-source LLMs for three tasks: (i) classifying superconductors vs. non-superconductors, (ii) predicting the superconducting transition temperature directly from composition or structure-informed inputs, and (iii) inverse design of candidate compositions conditioned on target Tc. The fine-tuned LLMs achieve performance comparable to traditional feature-based models and in some cases exceed them, while substantially outperforming their base versions and capturing meaningful chemical and structural trends. The inverse-design model generates chemically plausible compositions, including 28% novel candidates not seen in training. Finally, applying the trained predictors to the GNoME database identifies unreported materials with predicted Tc > 10 K. Although unverified, these candidates illustrate how integrating an LLM-driven workflow can enable scalable hypothesis generation for superconductivity discovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) offer new opportunities for automated data extraction and property prediction across materials science, yet their use in superconductivity research remains limited. Here we construct a large experimental database of 78,203 records, covering 19,058 unique compositions, extracted from scientific literature using an LLM-driven workflow. Each entry includes chemical composition, critical temperature, measurement pressure, structural descriptors, and critical fields. We fine-tune several open-source LLMs for three tasks: (i) classifying superconductors vs. non-superconductors, (ii) predicting the superconducting transition temperature directly from composition or structure-informed inputs, and (iii) inverse design of candidate compositions conditioned on target Tc. The fine-tuned LLMs achieve performance comparable to traditional feature-based models and in some cases exceed them, while substantially outperforming their base versions and capturing meaningful chemical and structural trends. The inverse-design model generates chemically plausible compositions, including 28% novel candidates not seen in training. Finally, applying the trained predictors to the GNoME database identifies unreported materials with predicted Tc > 10 K. Although unverified, these candidates illustrate how integrating an LLM-driven workflow can enable scalable hypothesis generation for superconductivity discovery."
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T17:32:38Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    17,
                    32,
                    38,
                    3,
                    345,
                    0
                ],
                "arxiv_comment": "15 pages, 6 figures",
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci"
                },
                "authors": [
                    {
                        "name": "Suman Itani"
                    },
                    {
                        "name": "Yibo Zhang"
                    },
                    {
                        "name": "Ranjit Itani"
                    },
                    {
                        "name": "Jiadong Zang"
                    }
                ],
                "author_detail": {
                    "name": "Jiadong Zang"
                },
                "author": "Jiadong Zang"
            },
            {
                "id": "http://arxiv.org/abs/2407.18360v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2407.18360v4",
                "title": "Evaluating Organizational Effectiveness: A New Strategy to Leverage Multisite Randomized Trials for Valid Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Organizational Effectiveness: A New Strategy to Leverage Multisite Randomized Trials for Valid Assessment"
                },
                "updated": "2025-12-11T17:31:10Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    17,
                    31,
                    10,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2407.18360v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2407.18360v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Determining which organizations are more effective in implementing an intervention program is essential for theoretically and empirically characterizing exemplary practice and for intervening to enhance the capacity of ineffective ones. Yet sites differ in their local ecological conditions including client composition, alternative programs, and community context. Applying the causal inference framework, this study proposes a formal mathematical definition for the local relative effectiveness of an organization attributable solely to malleable organizational practice. Capitalizing on multisite randomized trials, the identification leverages observed control group outcomes that capture some of the confounding impacts of otherwise unmeasured contextual variation. We propose a two-step mixed-effects modeling (2SME) procedure that adjusts for pre-existing between-site variation. A series of Monte Carlo simulations reveals its superior performance in comparison with conventional methods. We apply the new strategy to an evaluation of Job Corps centers nationwide serving disadvantaged youths.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Determining which organizations are more effective in implementing an intervention program is essential for theoretically and empirically characterizing exemplary practice and for intervening to enhance the capacity of ineffective ones. Yet sites differ in their local ecological conditions including client composition, alternative programs, and community context. Applying the causal inference framework, this study proposes a formal mathematical definition for the local relative effectiveness of an organization attributable solely to malleable organizational practice. Capitalizing on multisite randomized trials, the identification leverages observed control group outcomes that capture some of the confounding impacts of otherwise unmeasured contextual variation. We propose a two-step mixed-effects modeling (2SME) procedure that adjusts for pre-existing between-site variation. A series of Monte Carlo simulations reveals its superior performance in comparison with conventional methods. We apply the new strategy to an evaluation of Job Corps centers nationwide serving disadvantaged youths."
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-07-25T19:50:33Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    19,
                    50,
                    33,
                    3,
                    207,
                    0
                ],
                "arxiv_comment": "To appear in the American Journal of Evaluation",
                "arxiv_primary_category": {
                    "term": "stat.AP"
                },
                "authors": [
                    {
                        "name": "Guanglei Hong"
                    },
                    {
                        "name": "Jonah Deutsch"
                    },
                    {
                        "name": "Peter Kress"
                    },
                    {
                        "name": "Jose Eos Trinidad"
                    },
                    {
                        "name": "Zhengyan Xu"
                    }
                ],
                "author_detail": {
                    "name": "Zhengyan Xu"
                },
                "arxiv_affiliation": "University of Pennsylvania",
                "author": "Zhengyan Xu"
            },
            {
                "id": "http://arxiv.org/abs/2307.08975v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2307.08975v3",
                "title": "A Bayesian Framework for Multivariate Differential Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Bayesian Framework for Multivariate Differential Analysis"
                },
                "updated": "2025-12-11T17:28:40Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    17,
                    28,
                    40,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2307.08975v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2307.08975v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Differential analysis is a routine procedure in the statistical analysis toolbox across many applied fields, including quantitative proteomics, the main illustration of the present paper. The state-of-the-art limma approach uses a hierarchical formulation with moderated-variance estimators for each analyte directly injected into the t-statistic. While standard hypothesis testing strategies are recognised for their low computational cost, allowing for quick extraction of the most differential among thousands of elements, they generally overlook key aspects such as handling missing values, inter-element correlations, and uncertainty quantification. The present paper proposes a fully Bayesian framework for differential analysis, leveraging a conjugate hierarchical formulation for both the mean and the variance. Inference is performed by computing the posterior distribution of compared experimental conditions and sampling from the distribution of differences. This approach provides well-calibrated uncertainty quantification at a similar computational cost as hypothesis testing by leveraging closed-form equations. Furthermore, a natural extension enables multivariate differential analysis that accounts for possible inter-element correlations. We also demonstrate that, in this Bayesian treatment, missing data should generally be ignored in univariate settings, and further derive a tailored approximation that handles multiple imputation for the multivariate setting. We argue that probabilistic statements in terms of effect size and associated uncertainty are better suited to practical decision-making. Therefore, we finally propose simple and intuitive inference criteria, such as the overlap coefficient, which express group similarity as a probability rather than traditional, and often misleading, p-values.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differential analysis is a routine procedure in the statistical analysis toolbox across many applied fields, including quantitative proteomics, the main illustration of the present paper. The state-of-the-art limma approach uses a hierarchical formulation with moderated-variance estimators for each analyte directly injected into the t-statistic. While standard hypothesis testing strategies are recognised for their low computational cost, allowing for quick extraction of the most differential among thousands of elements, they generally overlook key aspects such as handling missing values, inter-element correlations, and uncertainty quantification. The present paper proposes a fully Bayesian framework for differential analysis, leveraging a conjugate hierarchical formulation for both the mean and the variance. Inference is performed by computing the posterior distribution of compared experimental conditions and sampling from the distribution of differences. This approach provides well-calibrated uncertainty quantification at a similar computational cost as hypothesis testing by leveraging closed-form equations. Furthermore, a natural extension enables multivariate differential analysis that accounts for possible inter-element correlations. We also demonstrate that, in this Bayesian treatment, missing data should generally be ignored in univariate settings, and further derive a tailored approximation that handles multiple imputation for the multivariate setting. We argue that probabilistic statements in terms of effect size and associated uncertainty are better suited to practical decision-making. Therefore, we finally propose simple and intuitive inference criteria, such as the overlap coefficient, which express group similarity as a probability rather than traditional, and often misleading, p-values."
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2023-07-18T05:14:29Z",
                "published_parsed": [
                    2023,
                    7,
                    18,
                    5,
                    14,
                    29,
                    1,
                    199,
                    0
                ],
                "arxiv_comment": "31 pages, 11 figures, 9 tables",
                "arxiv_primary_category": {
                    "term": "stat.ME"
                },
                "authors": [
                    {
                        "name": "Marie Chion"
                    },
                    {
                        "name": "Arthur Leroy"
                    }
                ],
                "author_detail": {
                    "name": "Arthur Leroy"
                },
                "author": "Arthur Leroy"
            },
            {
                "id": "http://arxiv.org/abs/2512.09015v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.09015v2",
                "title": "Luxical: High-Speed Lexical-Dense Text Embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Luxical: High-Speed Lexical-Dense Text Embeddings"
                },
                "updated": "2025-12-11T17:14:51Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    17,
                    14,
                    51,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.09015v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.09015v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Frontier language model quality increasingly hinges on our ability to organize web-scale text corpora for training. Today's dominant tools trade off speed and flexibility: lexical classifiers (e.g., FastText) are fast but limited to producing classification output scores, while the vector-valued outputs of transformer text embedding models flexibly support numerous workflows (e.g., clustering, classification, and retrieval) but are computationally expensive to produce. We introduce Luxical, a library for high-speed \"lexical-dense\" text embeddings that aims to recover the best properties of both approaches for web-scale text organization. Luxical combines sparse TF--IDF features, a small ReLU network, and a knowledge distillation training regimen to approximate large transformer embedding models at a fraction of their operational cost. In this technical report, we describe the Luxical architecture and training objective and evaluate a concrete Luxical model in two disparate applications: a targeted webcrawl document retrieval test and an end-to-end language model data curation task grounded in text classification. In these tasks we demonstrate speedups ranging from 3x to 100x over varying-sized neural baselines, and comparable to FastText model inference during the data curation task. On these evaluations, the tested Luxical model illustrates favorable compute/quality trade-offs for large-scale text organization, matching the quality of neural baselines. Luxical is available as open-source software at https://github.com/datologyai/luxical.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Frontier language model quality increasingly hinges on our ability to organize web-scale text corpora for training. Today's dominant tools trade off speed and flexibility: lexical classifiers (e.g., FastText) are fast but limited to producing classification output scores, while the vector-valued outputs of transformer text embedding models flexibly support numerous workflows (e.g., clustering, classification, and retrieval) but are computationally expensive to produce. We introduce Luxical, a library for high-speed \"lexical-dense\" text embeddings that aims to recover the best properties of both approaches for web-scale text organization. Luxical combines sparse TF--IDF features, a small ReLU network, and a knowledge distillation training regimen to approximate large transformer embedding models at a fraction of their operational cost. In this technical report, we describe the Luxical architecture and training objective and evaluate a concrete Luxical model in two disparate applications: a targeted webcrawl document retrieval test and an end-to-end language model data curation task grounded in text classification. In these tasks we demonstrate speedups ranging from 3x to 100x over varying-sized neural baselines, and comparable to FastText model inference during the data curation task. On these evaluations, the tested Luxical model illustrates favorable compute/quality trade-offs for large-scale text organization, matching the quality of neural baselines. Luxical is available as open-source software at https://github.com/datologyai/luxical."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T18:58:44Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    18,
                    58,
                    44,
                    1,
                    343,
                    0
                ],
                "arxiv_comment": "9 pages, 6 figures (v2 fixes typos only)",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "DatologyAI"
                    },
                    {
                        "name": ":"
                    },
                    {
                        "name": "Luke Merrick"
                    },
                    {
                        "name": "Alex Fang"
                    },
                    {
                        "name": "Aldo Carranza"
                    },
                    {
                        "name": "Alvin Deng"
                    },
                    {
                        "name": "Amro Abbas"
                    },
                    {
                        "name": "Brett Larsen"
                    },
                    {
                        "name": "Cody Blakeney"
                    },
                    {
                        "name": "Darren Teh"
                    },
                    {
                        "name": "David Schwab"
                    },
                    {
                        "name": "Fan Pan"
                    },
                    {
                        "name": "Haakon Mongstad"
                    },
                    {
                        "name": "Haoli Yin"
                    },
                    {
                        "name": "Jack Urbanek"
                    },
                    {
                        "name": "Jason Lee"
                    },
                    {
                        "name": "Jason Telanoff"
                    },
                    {
                        "name": "Josh Wills"
                    },
                    {
                        "name": "Kaleigh Mentzer"
                    },
                    {
                        "name": "Paul Burstein"
                    },
                    {
                        "name": "Parth Doshi"
                    },
                    {
                        "name": "Paul Burnstein"
                    },
                    {
                        "name": "Pratyush Maini"
                    },
                    {
                        "name": "Ricardo Monti"
                    },
                    {
                        "name": "Rishabh Adiga"
                    },
                    {
                        "name": "Scott Loftin"
                    },
                    {
                        "name": "Siddharth Joshi"
                    },
                    {
                        "name": "Spandan Das"
                    },
                    {
                        "name": "Tony Jiang"
                    },
                    {
                        "name": "Vineeth Dorna"
                    },
                    {
                        "name": "Zhengping Wang"
                    },
                    {
                        "name": "Bogdan Gaza"
                    },
                    {
                        "name": "Ari Morcos"
                    },
                    {
                        "name": "Matthew Leavitt"
                    }
                ],
                "author_detail": {
                    "name": "Matthew Leavitt"
                },
                "author": "Matthew Leavitt"
            },
            {
                "id": "http://arxiv.org/abs/2512.10818v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10818v1",
                "title": "Self-Ensemble Post Learning for Noisy Domain Generalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Ensemble Post Learning for Noisy Domain Generalization"
                },
                "updated": "2025-12-11T17:09:35Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    17,
                    9,
                    35,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10818v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10818v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "While computer vision and machine learning have made great progress, their robustness is still challenged by two key issues: data distribution shift and label noise. When domain generalization (DG) encounters noise, noisy labels further exacerbate the emergence of spurious features in deep layers, i.e. spurious feature enlargement, leading to a degradation in the performance of existing algorithms. This paper, starting from domain generalization, explores how to make existing methods rework when meeting noise. We find that the latent features inside the model have certain discriminative capabilities, and different latent features focus on different parts of the image. Based on these observations, we propose the Self-Ensemble Post Learning approach (SEPL) to diversify features which can be leveraged. Specifically, SEPL consists of two parts: feature probing training and prediction ensemble inference. It leverages intermediate feature representations within the model architecture, training multiple probing classifiers to fully exploit the capabilities of pre-trained models, while the final predictions are obtained through the integration of outputs from these diverse classification heads. Considering the presence of noisy labels, we employ semi-supervised algorithms to train probing classifiers. Given that different probing classifiers focus on different areas, we integrate their predictions using a crowdsourcing inference approach. Extensive experimental evaluations demonstrate that the proposed method not only enhances the robustness of existing methods but also exhibits significant potential for real-world applications with high flexibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While computer vision and machine learning have made great progress, their robustness is still challenged by two key issues: data distribution shift and label noise. When domain generalization (DG) encounters noise, noisy labels further exacerbate the emergence of spurious features in deep layers, i.e. spurious feature enlargement, leading to a degradation in the performance of existing algorithms. This paper, starting from domain generalization, explores how to make existing methods rework when meeting noise. We find that the latent features inside the model have certain discriminative capabilities, and different latent features focus on different parts of the image. Based on these observations, we propose the Self-Ensemble Post Learning approach (SEPL) to diversify features which can be leveraged. Specifically, SEPL consists of two parts: feature probing training and prediction ensemble inference. It leverages intermediate feature representations within the model architecture, training multiple probing classifiers to fully exploit the capabilities of pre-trained models, while the final predictions are obtained through the integration of outputs from these diverse classification heads. Considering the presence of noisy labels, we employ semi-supervised algorithms to train probing classifiers. Given that different probing classifiers focus on different areas, we integrate their predictions using a crowdsourcing inference approach. Extensive experimental evaluations demonstrate that the proposed method not only enhances the robustness of existing methods but also exhibits significant potential for real-world applications with high flexibility."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T17:09:35Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    17,
                    9,
                    35,
                    3,
                    345,
                    0
                ],
                "arxiv_comment": "18 pages",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Wang Lu"
                    },
                    {
                        "name": "Jindong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jindong Wang"
                },
                "author": "Jindong Wang"
            },
            {
                "id": "http://arxiv.org/abs/2507.18699v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2507.18699v2",
                "title": "The z = 9.625 Cosmic Gems Galaxy was a \"Compact Blue Monster\" Propelled by Massive Star Clusters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The z = 9.625 Cosmic Gems Galaxy was a \"Compact Blue Monster\" Propelled by Massive Star Clusters"
                },
                "updated": "2025-12-11T17:04:19Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    17,
                    4,
                    19,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2507.18699v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2507.18699v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The recent discovery of five massive stellar clusters at z=9.625 in the Cosmic Gems has raised the question about the formation mechanism of star clusters in the first half Gyr after the Big-Bang. We infer the total stellar mass in clusters by normalizing and integrating the stellar cluster mass function (SCMF, dn(M)/dM ~ (n$_0$) $M^β$), assuming three different slopes $β$ = -1.5, -2.0 and -2.5 and different lower-mass limits between $10^2$ and $10^5$ Msun. The total integrated cluster stellar mass is compared to the stellar mass inferred from the counter-image of the Cosmic Gems, which provides the best, modestly magnified ($μ$ = 1.84$\\pm$0.05) representation of the entire galaxy. The delensed stellar mass of the Cosmic Gems galaxy is estimated as 3.5$_{-1.8}^{+3.3}$ x$10^7$ Msun, with an effective radius of Reff = 103$_{-15}^{+13}$ parsec and a stellar surface mass density of $Σ$mass = 520$_{-225}^{+340}$ Msun pc$^{-2}$. Accounting for normalization uncertainties - including different lensing magnification scenarios for the arc - a modified SCMF, combined with a significantly high star cluster formation efficiency (approaching 100%), appears to be a necessary condition to explain the relatively short formation timescale of both the star clusters and the counter-image, without exceeding the galaxy's stellar mass. By extrapolating the physical properties at the peak of the burst we find that in its recent past (<~ 30 Myr) the Cosmic Gems galaxy has likely experienced a specific star formation rate (sSFR) exceeding 25 Gyr$^{-1}$ and luminosity approaching the ``blue monster'' regime (M$_{UV}$ < -20). Our study provides insights into the extreme clustered nature of star formation in early galaxies and shed light into the formation of bound star clusters that might survive to z = 0 as globular clusters, older than 13 Gyr.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent discovery of five massive stellar clusters at z=9.625 in the Cosmic Gems has raised the question about the formation mechanism of star clusters in the first half Gyr after the Big-Bang. We infer the total stellar mass in clusters by normalizing and integrating the stellar cluster mass function (SCMF, dn(M)/dM ~ (n$_0$) $M^β$), assuming three different slopes $β$ = -1.5, -2.0 and -2.5 and different lower-mass limits between $10^2$ and $10^5$ Msun. The total integrated cluster stellar mass is compared to the stellar mass inferred from the counter-image of the Cosmic Gems, which provides the best, modestly magnified ($μ$ = 1.84$\\pm$0.05) representation of the entire galaxy. The delensed stellar mass of the Cosmic Gems galaxy is estimated as 3.5$_{-1.8}^{+3.3}$ x$10^7$ Msun, with an effective radius of Reff = 103$_{-15}^{+13}$ parsec and a stellar surface mass density of $Σ$mass = 520$_{-225}^{+340}$ Msun pc$^{-2}$. Accounting for normalization uncertainties - including different lensing magnification scenarios for the arc - a modified SCMF, combined with a significantly high star cluster formation efficiency (approaching 100%), appears to be a necessary condition to explain the relatively short formation timescale of both the star clusters and the counter-image, without exceeding the galaxy's stellar mass. By extrapolating the physical properties at the peak of the burst we find that in its recent past (<~ 30 Myr) the Cosmic Gems galaxy has likely experienced a specific star formation rate (sSFR) exceeding 25 Gyr$^{-1}$ and luminosity approaching the ``blue monster'' regime (M$_{UV}$ < -20). Our study provides insights into the extreme clustered nature of star formation in early galaxies and shed light into the formation of bound star clusters that might survive to z = 0 as globular clusters, older than 13 Gyr."
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-07-24T18:00:03Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    18,
                    0,
                    3,
                    3,
                    205,
                    0
                ],
                "arxiv_comment": "11 pages, 8 figures, A&A accepted; see also the companion work Messa et al. 2025b (arXiv:2507.18705)",
                "arxiv_primary_category": {
                    "term": "astro-ph.GA"
                },
                "authors": [
                    {
                        "name": "E. Vanzella"
                    },
                    {
                        "name": "M. Messa"
                    },
                    {
                        "name": "A. Adamo"
                    },
                    {
                        "name": "F. Loiacono"
                    },
                    {
                        "name": "M. Oguri"
                    },
                    {
                        "name": "K. Sharon"
                    },
                    {
                        "name": "L. D. Bradley"
                    },
                    {
                        "name": "P. Bergamini"
                    },
                    {
                        "name": "M. Meneghetti"
                    },
                    {
                        "name": "A. Claeyssens"
                    },
                    {
                        "name": "B. Welch"
                    },
                    {
                        "name": "M. Bradac"
                    },
                    {
                        "name": "A. Zanella"
                    },
                    {
                        "name": "A. Bolamperti"
                    },
                    {
                        "name": "F. Calura"
                    },
                    {
                        "name": "T. Y-Y. Hsiao"
                    },
                    {
                        "name": "E. Zackrisson"
                    },
                    {
                        "name": "M. Ricotti"
                    },
                    {
                        "name": "L. Christensen"
                    },
                    {
                        "name": "J. M. Diego"
                    },
                    {
                        "name": "F. E. Bauer"
                    },
                    {
                        "name": "X. Xu"
                    },
                    {
                        "name": "S. Fujimoto"
                    },
                    {
                        "name": "C. Grillo"
                    },
                    {
                        "name": "M. Lombardi"
                    },
                    {
                        "name": "P. Rosati"
                    },
                    {
                        "name": "T. Resseguier"
                    },
                    {
                        "name": "A. Zitrin"
                    },
                    {
                        "name": "A. Bik"
                    },
                    {
                        "name": "J. Richard"
                    },
                    {
                        "name": "Abdurro'uf"
                    },
                    {
                        "name": "R. Bhatawdekar"
                    },
                    {
                        "name": "D. Coe"
                    },
                    {
                        "name": "B. Frye"
                    },
                    {
                        "name": "A. K. Inoue Y. Jimenez-Teja"
                    },
                    {
                        "name": "C. Norman"
                    },
                    {
                        "name": "J. R. Rigby"
                    },
                    {
                        "name": "M. Trenti"
                    },
                    {
                        "name": "T. Hashimoto"
                    }
                ],
                "author_detail": {
                    "name": "T. Hashimoto"
                },
                "author": "T. Hashimoto"
            },
            {
                "id": "http://arxiv.org/abs/2512.10810v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10810v1",
                "title": "Complexity and multi-functional variants of the Quantum-to-Quantum Bernoulli Factories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Complexity and multi-functional variants of the Quantum-to-Quantum Bernoulli Factories"
                },
                "updated": "2025-12-11T16:56:32Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    16,
                    56,
                    32,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10810v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10810v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "A Bernoulli factory is a model for randomness manipulation that transforms an initial Bernoulli random variable into another Bernoulli variable by applying a predetermined function relating the output bias to the input one. In literature, quantum-to-quantum Bernoulli factory schemes have been proposed, which encode both the input and output variables using qubit amplitudes. This fundamental concept can serve as a subroutine for quantum algorithms that involve Bayesian inference and Monte Carlo methods, or that require data encryption, like in blind quantum computation. In this work, we present a characterisation of the complexity of the quantum-to-quantum Bernoulli factory by providing a lower bound on the required number of qubits needed to implement the protocol, an upper bound on the success probability and the quantum circuit that saturates the bounds. We also formalise and analyse two different variants of the original problem that address the possibility of increasing the number of input biases or the number of functions implemented by the quantum-to-quantum Bernoulli factory. The obtained results can be used as a framework for randomness manipulation via such an approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Bernoulli factory is a model for randomness manipulation that transforms an initial Bernoulli random variable into another Bernoulli variable by applying a predetermined function relating the output bias to the input one. In literature, quantum-to-quantum Bernoulli factory schemes have been proposed, which encode both the input and output variables using qubit amplitudes. This fundamental concept can serve as a subroutine for quantum algorithms that involve Bayesian inference and Monte Carlo methods, or that require data encryption, like in blind quantum computation. In this work, we present a characterisation of the complexity of the quantum-to-quantum Bernoulli factory by providing a lower bound on the required number of qubits needed to implement the protocol, an upper bound on the success probability and the quantum circuit that saturates the bounds. We also formalise and analyse two different variants of the original problem that address the possibility of increasing the number of input biases or the number of functions implemented by the quantum-to-quantum Bernoulli factory. The obtained results can be used as a framework for randomness manipulation via such an approach."
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T16:56:32Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    16,
                    56,
                    32,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph"
                },
                "authors": [
                    {
                        "name": "Francesco Hoch"
                    },
                    {
                        "name": "Taira Giordani"
                    },
                    {
                        "name": "Gonzalo Carvacho"
                    },
                    {
                        "name": "Nicolò Spagnolo"
                    },
                    {
                        "name": "Fabio Sciarrino"
                    }
                ],
                "author_detail": {
                    "name": "Fabio Sciarrino"
                },
                "author": "Fabio Sciarrino"
            },
            {
                "id": "http://arxiv.org/abs/2512.10805v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10805v1",
                "title": "Interpretable and Steerable Concept Bottleneck Sparse Autoencoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpretable and Steerable Concept Bottleneck Sparse Autoencoders"
                },
                "updated": "2025-12-11T16:48:07Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    16,
                    48,
                    7,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10805v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Sparse autoencoders (SAEs) promise a unified approach for mechanistic interpretability, concept discovery, and model steering in LLMs and LVLMs. However, realizing this potential requires that the learned features be both interpretable and steerable. To that end, we introduce two new computationally inexpensive interpretability and steerability metrics and conduct a systematic analysis on LVLMs. Our analysis uncovers two observations; (i) a majority of SAE neurons exhibit either low interpretability or low steerability or both, rendering them ineffective for downstream use; and (ii) due to the unsupervised nature of SAEs, user-desired concepts are often absent in the learned dictionary, thus limiting their practical utility. To address these limitations, we propose Concept Bottleneck Sparse Autoencoders (CB-SAE) - a novel post-hoc framework that prunes low-utility neurons and augments the latent space with a lightweight concept bottleneck aligned to a user-defined concept set. The resulting CB-SAE improves interpretability by +32.1% and steerability by +14.5% across LVLMs and image generation tasks. We will make our code and model weights available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse autoencoders (SAEs) promise a unified approach for mechanistic interpretability, concept discovery, and model steering in LLMs and LVLMs. However, realizing this potential requires that the learned features be both interpretable and steerable. To that end, we introduce two new computationally inexpensive interpretability and steerability metrics and conduct a systematic analysis on LVLMs. Our analysis uncovers two observations; (i) a majority of SAE neurons exhibit either low interpretability or low steerability or both, rendering them ineffective for downstream use; and (ii) due to the unsupervised nature of SAEs, user-desired concepts are often absent in the learned dictionary, thus limiting their practical utility. To address these limitations, we propose Concept Bottleneck Sparse Autoencoders (CB-SAE) - a novel post-hoc framework that prunes low-utility neurons and augments the latent space with a lightweight concept bottleneck aligned to a user-defined concept set. The resulting CB-SAE improves interpretability by +32.1% and steerability by +14.5% across LVLMs and image generation tasks. We will make our code and model weights available."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T16:48:07Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    16,
                    48,
                    7,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Akshay Kulkarni"
                    },
                    {
                        "name": "Tsui-Wei Weng"
                    },
                    {
                        "name": "Vivek Narayanaswamy"
                    },
                    {
                        "name": "Shusen Liu"
                    },
                    {
                        "name": "Wesam A. Sakla"
                    },
                    {
                        "name": "Kowshik Thopalli"
                    }
                ],
                "author_detail": {
                    "name": "Kowshik Thopalli"
                },
                "author": "Kowshik Thopalli"
            },
            {
                "id": "http://arxiv.org/abs/2512.10803v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10803v1",
                "title": "Detection of GW200105 with a targeted eccentric search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detection of GW200105 with a targeted eccentric search"
                },
                "updated": "2025-12-11T16:46:37Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    16,
                    46,
                    37,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10803v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10803v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The neutron star -- black hole (NSBH) binary GW200105 was recently found to have significant residual orbital eccentricity at a gravitational-wave frequency of 20 Hz~\\cite{Morras:2025xfu}. The event was originally identified with moderate significance by matched-filter searches that employ non-eccentric templates. The neglect of relevant physical effects, such as orbital eccentricity, can severely reduce the sensitivity of the search and, consequently, also the significance of an event candidate. Here, we present a targeted eccentric search for GW200105. The eccentric search identifies GW200105 as the most significant event with a signal-to-noise ratio of $13.4$ and a false alarm rate of less than 1 in 1000 years.\n  The best-matching template parameters are consistent with the Bayesian inference result, supporting the interpretation of GW200105 as an NSBH that formed through dynamical mechanisms and not isolated binary evolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The neutron star -- black hole (NSBH) binary GW200105 was recently found to have significant residual orbital eccentricity at a gravitational-wave frequency of 20 Hz~\\cite{Morras:2025xfu}. The event was originally identified with moderate significance by matched-filter searches that employ non-eccentric templates. The neglect of relevant physical effects, such as orbital eccentricity, can severely reduce the sensitivity of the search and, consequently, also the significance of an event candidate. Here, we present a targeted eccentric search for GW200105. The eccentric search identifies GW200105 as the most significant event with a signal-to-noise ratio of $13.4$ and a false alarm rate of less than 1 in 1000 years.\n  The best-matching template parameters are consistent with the Bayesian inference result, supporting the interpretation of GW200105 as an NSBH that formed through dynamical mechanisms and not isolated binary evolution."
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T16:46:37Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    16,
                    46,
                    37,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc"
                },
                "authors": [
                    {
                        "name": "Khun Sang Phukon"
                    },
                    {
                        "name": "Patricia Schmidt"
                    },
                    {
                        "name": "Gonzalo Morras"
                    },
                    {
                        "name": "Geraint Pratten"
                    }
                ],
                "author_detail": {
                    "name": "Geraint Pratten"
                },
                "author": "Geraint Pratten"
            },
            {
                "id": "http://arxiv.org/abs/2401.11255v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2401.11255v2",
                "title": "Visualization Generation with Large Language Models: An Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visualization Generation with Large Language Models: An Evaluation"
                },
                "updated": "2025-12-11T16:41:18Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    16,
                    41,
                    18,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2401.11255v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2401.11255v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The frequent need for analysts to create visualizations to derive insights from data has driven extensive research into the generation of natural Language to Visualization (NL2VIS). While recent progress in large language models (LLMs) suggests their potential to effectively support NL2VIS tasks, existing studies lack a systematic investigation into the performance of different LLMs under various prompt strategies. This paper addresses this gap and contributes a crucial baseline evaluation of LLMs' capabilities in generating visualization specifications of NL2VIS tasks. Our evaluation utilizes the nvBench dataset, employing six representative LLMs and eight distinct prompt strategies to evaluate their performance in generating six target chart types using the Vega-Lite visualization specification. We assess model performance with multiple metrics, including vis accuracy, validity and legality. Our results reveal substantial performance disparities across prompt strategies, chart types, and LLMs. Furthermore, based on the evaluation results, we uncover several counterintuitive behaviors across these dimensions, and propose directions for enhancing the NL2VIS benchmark to better support future NL2VIS research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The frequent need for analysts to create visualizations to derive insights from data has driven extensive research into the generation of natural Language to Visualization (NL2VIS). While recent progress in large language models (LLMs) suggests their potential to effectively support NL2VIS tasks, existing studies lack a systematic investigation into the performance of different LLMs under various prompt strategies. This paper addresses this gap and contributes a crucial baseline evaluation of LLMs' capabilities in generating visualization specifications of NL2VIS tasks. Our evaluation utilizes the nvBench dataset, employing six representative LLMs and eight distinct prompt strategies to evaluate their performance in generating six target chart types using the Vega-Lite visualization specification. We assess model performance with multiple metrics, including vis accuracy, validity and legality. Our results reveal substantial performance disparities across prompt strategies, chart types, and LLMs. Furthermore, based on the evaluation results, we uncover several counterintuitive behaviors across these dimensions, and propose directions for enhancing the NL2VIS benchmark to better support future NL2VIS research."
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-01-20T15:28:22Z",
                "published_parsed": [
                    2024,
                    1,
                    20,
                    15,
                    28,
                    22,
                    5,
                    20,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC"
                },
                "authors": [
                    {
                        "name": "Xinyu Wang"
                    },
                    {
                        "name": "Chenwei Liang"
                    },
                    {
                        "name": "Shunyuan Zheng"
                    },
                    {
                        "name": "Jinyuan Liang"
                    },
                    {
                        "name": "Guozheng Li"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Chi Harold Liu"
                    }
                ],
                "author_detail": {
                    "name": "Chi Harold Liu"
                },
                "author": "Chi Harold Liu"
            },
            {
                "id": "http://arxiv.org/abs/2512.10793v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10793v1",
                "title": "LabelFusion: Learning to Fuse LLMs and Transformer Classifiers for Robust Text Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LabelFusion: Learning to Fuse LLMs and Transformer Classifiers for Robust Text Classification"
                },
                "updated": "2025-12-11T16:39:07Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    16,
                    39,
                    7,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10793v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10793v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "LabelFusion is a fusion ensemble for text classification that learns to combine a traditional transformer-based classifier (e.g., RoBERTa) with one or more Large Language Models (LLMs such as OpenAI GPT, Google Gemini, or DeepSeek) to deliver accurate and cost-aware predictions across multi-class and multi-label tasks. The package provides a simple high-level interface (AutoFusionClassifier) that trains the full pipeline end-to-end with minimal configuration, and a flexible API for advanced users. Under the hood, LabelFusion integrates vector signals from both sources by concatenating the ML backbone's embeddings with the LLM-derived per-class scores -- obtained through structured prompt-engineering strategies -- and feeds this joint representation into a compact multi-layer perceptron (FusionMLP) that produces the final prediction. This learned fusion approach captures complementary strengths of LLM reasoning and traditional transformer-based classifiers, yielding robust performance across domains -- achieving 92.4% accuracy on AG News and 92.3% on 10-class Reuters 21578 topic classification -- while enabling practical trade-offs between accuracy, latency, and cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LabelFusion is a fusion ensemble for text classification that learns to combine a traditional transformer-based classifier (e.g., RoBERTa) with one or more Large Language Models (LLMs such as OpenAI GPT, Google Gemini, or DeepSeek) to deliver accurate and cost-aware predictions across multi-class and multi-label tasks. The package provides a simple high-level interface (AutoFusionClassifier) that trains the full pipeline end-to-end with minimal configuration, and a flexible API for advanced users. Under the hood, LabelFusion integrates vector signals from both sources by concatenating the ML backbone's embeddings with the LLM-derived per-class scores -- obtained through structured prompt-engineering strategies -- and feeds this joint representation into a compact multi-layer perceptron (FusionMLP) that produces the final prediction. This learned fusion approach captures complementary strengths of LLM reasoning and traditional transformer-based classifiers, yielding robust performance across domains -- achieving 92.4% accuracy on AG News and 92.3% on 10-class Reuters 21578 topic classification -- while enabling practical trade-offs between accuracy, latency, and cost."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T16:39:07Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    16,
                    39,
                    7,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Michael Schlee"
                    },
                    {
                        "name": "Christoph Weisser"
                    },
                    {
                        "name": "Timo Kivimäki"
                    },
                    {
                        "name": "Melchizedek Mashiku"
                    },
                    {
                        "name": "Benjamin Saefken"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Saefken"
                },
                "author": "Benjamin Saefken"
            },
            {
                "id": "http://arxiv.org/abs/2508.15432v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.15432v3",
                "title": "SyGra: A Unified Graph-Based Framework for Scalable Generation, Quality Tagging, and Management of Synthetic Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SyGra: A Unified Graph-Based Framework for Scalable Generation, Quality Tagging, and Management of Synthetic Data"
                },
                "updated": "2025-12-11T16:38:00Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    16,
                    38,
                    0,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.15432v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.15432v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The advancement of large language models (LLMs) is critically dependent on the availability of high-quality datasets for Supervised Fine-Tuning (SFT), alignment tasks like Direct Preference Optimization (DPO), etc. In this work, we present a comprehensive synthetic data generation framework that facilitates scalable, configurable, and high-fidelity generation of synthetic data tailored for these training paradigms. Our approach employs a modular and configuration-based pipeline capable of modeling complex dialogue flows with minimal manual intervention. This framework uses a dual-stage quality tagging mechanism, combining heuristic rules and LLM-based evaluations, to automatically filter and score data extracted from OASST-formatted conversations, ensuring the curation of high-quality dialogue samples. The resulting datasets are structured under a flexible schema supporting both SFT and DPO use cases, enabling seamless integration into diverse training workflows. Together, these innovations offer a robust solution for generating and managing synthetic conversational data at scale, significantly reducing the overhead of data preparation in LLM training pipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advancement of large language models (LLMs) is critically dependent on the availability of high-quality datasets for Supervised Fine-Tuning (SFT), alignment tasks like Direct Preference Optimization (DPO), etc. In this work, we present a comprehensive synthetic data generation framework that facilitates scalable, configurable, and high-fidelity generation of synthetic data tailored for these training paradigms. Our approach employs a modular and configuration-based pipeline capable of modeling complex dialogue flows with minimal manual intervention. This framework uses a dual-stage quality tagging mechanism, combining heuristic rules and LLM-based evaluations, to automatically filter and score data extracted from OASST-formatted conversations, ensuring the curation of high-quality dialogue samples. The resulting datasets are structured under a flexible schema supporting both SFT and DPO use cases, enabling seamless integration into diverse training workflows. Together, these innovations offer a robust solution for generating and managing synthetic conversational data at scale, significantly reducing the overhead of data preparation in LLM training pipelines."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-21T10:35:41Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    10,
                    35,
                    41,
                    3,
                    233,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Bidyapati Pradhan"
                    },
                    {
                        "name": "Surajit Dasgupta"
                    },
                    {
                        "name": "Amit Kumar Saha"
                    },
                    {
                        "name": "Omkar Anustoop"
                    },
                    {
                        "name": "Sriram Puttagunta"
                    },
                    {
                        "name": "Vipul Mittal"
                    },
                    {
                        "name": "Gopal Sarda"
                    }
                ],
                "author_detail": {
                    "name": "Gopal Sarda"
                },
                "author": "Gopal Sarda"
            },
            {
                "id": "http://arxiv.org/abs/2512.10792v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10792v1",
                "title": "Physics-Informed Learning of Microvascular Flow Models using Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physics-Informed Learning of Microvascular Flow Models using Graph Neural Networks"
                },
                "updated": "2025-12-11T16:37:14Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    16,
                    37,
                    14,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10792v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10792v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The simulation of microcirculatory blood flow in realistic vascular architectures poses significant challenges due to the multiscale nature of the problem and the topological complexity of capillary networks. In this work, we propose a novel deep learning-based reduced-order modeling strategy, leveraging Graph Neural Networks (GNNs) trained on synthetic microvascular graphs to approximate hemodynamic quantities on anatomically realistic domains. Our method combines algorithms for synthetic vascular generation with a physics-informed training procedure that integrates graph topological information and local flow dynamics. To ensure the physical reliability of the learned surrogates, we incorporate a physics-informed loss functional derived from the governing equations, allowing enforcement of mass conservation and rheological constraints. The resulting GNN architecture demonstrates robust generalization capabilities across diverse network configurations. The GNN formulation is validated on benchmark problems with linear and nonlinear rheology, showing accurate pressure and velocity field reconstruction with substantial computational gains over full-order solvers. The methodology showcases significant generalization capabilities with respect to vascular complexity, as highlighted by tests on data from the mouse cerebral cortex. This work establishes a new class of graph-based surrogate models for microvascular flow, grounded in physical laws and equipped with inductive biases that mirror mass conservation and rheological models, opening new directions for real-time inference in vascular modeling and biomedical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The simulation of microcirculatory blood flow in realistic vascular architectures poses significant challenges due to the multiscale nature of the problem and the topological complexity of capillary networks. In this work, we propose a novel deep learning-based reduced-order modeling strategy, leveraging Graph Neural Networks (GNNs) trained on synthetic microvascular graphs to approximate hemodynamic quantities on anatomically realistic domains. Our method combines algorithms for synthetic vascular generation with a physics-informed training procedure that integrates graph topological information and local flow dynamics. To ensure the physical reliability of the learned surrogates, we incorporate a physics-informed loss functional derived from the governing equations, allowing enforcement of mass conservation and rheological constraints. The resulting GNN architecture demonstrates robust generalization capabilities across diverse network configurations. The GNN formulation is validated on benchmark problems with linear and nonlinear rheology, showing accurate pressure and velocity field reconstruction with substantial computational gains over full-order solvers. The methodology showcases significant generalization capabilities with respect to vascular complexity, as highlighted by tests on data from the mouse cerebral cortex. This work establishes a new class of graph-based surrogate models for microvascular flow, grounded in physical laws and equipped with inductive biases that mirror mass conservation and rheological models, opening new directions for real-time inference in vascular modeling and biomedical applications."
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T16:37:14Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    16,
                    37,
                    14,
                    3,
                    345,
                    0
                ],
                "arxiv_comment": "28 pages, 8 figures",
                "arxiv_primary_category": {
                    "term": "math.NA"
                },
                "authors": [
                    {
                        "name": "Paolo Botta"
                    },
                    {
                        "name": "Piermario Vitullo"
                    },
                    {
                        "name": "Thomas Ventimiglia"
                    },
                    {
                        "name": "Andreas Linninger"
                    },
                    {
                        "name": "Paolo Zunino"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Zunino"
                },
                "author": "Paolo Zunino"
            },
            {
                "id": "http://arxiv.org/abs/2512.10785v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10785v1",
                "title": "Developing and Evaluating a Large Language Model-Based Automated Feedback System Grounded in Evidence-Centered Design for Supporting Physics Problem Solving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing and Evaluating a Large Language Model-Based Automated Feedback System Grounded in Evidence-Centered Design for Supporting Physics Problem Solving"
                },
                "updated": "2025-12-11T16:29:38Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    16,
                    29,
                    38,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10785v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10785v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Generative AI offers new opportunities for individualized and adaptive learning, particularly through large language model (LLM)-based feedback systems. While LLMs can produce effective feedback for relatively straightforward conceptual tasks, delivering high-quality feedback for tasks that require advanced domain expertise, such as physics problem solving, remains a substantial challenge. This study presents the design of an LLM-based feedback system for physics problem solving grounded in evidence-centered design (ECD) and evaluates its performance within the German Physics Olympiad. Participants assessed the usefulness and accuracy of the generated feedback, which was generally perceived as useful and highly accurate. However, an in-depth analysis revealed that the feedback contained factual errors in 20% of cases; errors that often went unnoticed by the students. We discuss the risks associated with uncritical reliance on LLM-based feedback systems and outline potential directions for generating more adaptive and reliable LLM-based feedback in the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI offers new opportunities for individualized and adaptive learning, particularly through large language model (LLM)-based feedback systems. While LLMs can produce effective feedback for relatively straightforward conceptual tasks, delivering high-quality feedback for tasks that require advanced domain expertise, such as physics problem solving, remains a substantial challenge. This study presents the design of an LLM-based feedback system for physics problem solving grounded in evidence-centered design (ECD) and evaluates its performance within the German Physics Olympiad. Participants assessed the usefulness and accuracy of the generated feedback, which was generally perceived as useful and highly accurate. However, an in-depth analysis revealed that the feedback contained factual errors in 20% of cases; errors that often went unnoticed by the students. We discuss the risks associated with uncritical reliance on LLM-based feedback systems and outline potential directions for generating more adaptive and reliable LLM-based feedback in the future."
                },
                "tags": [
                    {
                        "term": "physics.ed-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T16:29:38Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    16,
                    29,
                    38,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "physics.ed-ph"
                },
                "authors": [
                    {
                        "name": "Holger Maus"
                    },
                    {
                        "name": "Paul Tschisgale"
                    },
                    {
                        "name": "Fabian Kieser"
                    },
                    {
                        "name": "Stefan Petersen"
                    },
                    {
                        "name": "Peter Wulff"
                    }
                ],
                "author_detail": {
                    "name": "Peter Wulff"
                },
                "author": "Peter Wulff"
            },
            {
                "id": "http://arxiv.org/abs/2512.10780v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10780v1",
                "title": "Script Gap: Evaluating LLM Triage on Indian Languages in Native vs Roman Scripts in a Real World Setting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Script Gap: Evaluating LLM Triage on Indian Languages in Native vs Roman Scripts in a Real World Setting"
                },
                "updated": "2025-12-11T16:15:42Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    16,
                    15,
                    42,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10780v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10780v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) are increasingly deployed in high-stakes clinical applications in India. In many such settings, speakers of Indian languages frequently communicate using romanized text rather than native scripts, yet existing research rarely evaluates this orthographic variation using real-world data. We investigate how romanization impacts the reliability of LLMs in a critical domain: maternal and newborn healthcare triage. We benchmark leading LLMs on a real-world dataset of user-generated queries spanning five Indian languages and Nepali. Our results reveal consistent degradation in performance for romanized messages, with F1 scores trailing those of native scripts by 5-12 points. At our partner maternal health organization in India, this gap could cause nearly 2 million excess errors in triage. Crucially, this performance gap by scripts is not due to a failure in clinical reasoning. We demonstrate that LLMs often correctly infer the semantic intent of romanized queries. Nevertheless, their final classification outputs remain brittle in the presence of orthographic noise in romanized inputs. Our findings highlight a critical safety blind spot in LLM-based health systems: models that appear to understand romanized input may still fail to act on it reliably.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed in high-stakes clinical applications in India. In many such settings, speakers of Indian languages frequently communicate using romanized text rather than native scripts, yet existing research rarely evaluates this orthographic variation using real-world data. We investigate how romanization impacts the reliability of LLMs in a critical domain: maternal and newborn healthcare triage. We benchmark leading LLMs on a real-world dataset of user-generated queries spanning five Indian languages and Nepali. Our results reveal consistent degradation in performance for romanized messages, with F1 scores trailing those of native scripts by 5-12 points. At our partner maternal health organization in India, this gap could cause nearly 2 million excess errors in triage. Crucially, this performance gap by scripts is not due to a failure in clinical reasoning. We demonstrate that LLMs often correctly infer the semantic intent of romanized queries. Nevertheless, their final classification outputs remain brittle in the presence of orthographic noise in romanized inputs. Our findings highlight a critical safety blind spot in LLM-based health systems: models that appear to understand romanized input may still fail to act on it reliably."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T16:15:42Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    16,
                    15,
                    42,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Manurag Khullar"
                    },
                    {
                        "name": "Utkarsh Desai"
                    },
                    {
                        "name": "Poorva Malviya"
                    },
                    {
                        "name": "Aman Dalmia"
                    },
                    {
                        "name": "Zheyuan Ryan Shi"
                    }
                ],
                "author_detail": {
                    "name": "Zheyuan Ryan Shi"
                },
                "author": "Zheyuan Ryan Shi"
            },
            {
                "id": "http://arxiv.org/abs/2512.10770v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10770v1",
                "title": "Template-Free Retrosynthesis with Graph-Prior Augmented Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Template-Free Retrosynthesis with Graph-Prior Augmented Transformers"
                },
                "updated": "2025-12-11T16:08:32Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    16,
                    8,
                    32,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10770v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10770v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Retrosynthesis reaction prediction seeks to infer plausible reactant molecules for a given product and is a central problem in computer-aided organic synthesis. Despite recent progress, many existing models still fall short of the accuracy and robustness required for practical deployment. This work studies a template-free, Transformer-based framework that eliminates reliance on handcrafted reaction templates or additional chemical rule engines. The model injects molecular graph information into the attention mechanism to jointly exploit \\SMILES\\ sequences and structural cues, and further applies a paired data augmentation strategy to enhance training diversity and scale. On the USPTO-50K benchmark, our proposed approach achieves state-of-the-art performance among template-free methods and substantially outperforming a vanilla Transformer baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrosynthesis reaction prediction seeks to infer plausible reactant molecules for a given product and is a central problem in computer-aided organic synthesis. Despite recent progress, many existing models still fall short of the accuracy and robustness required for practical deployment. This work studies a template-free, Transformer-based framework that eliminates reliance on handcrafted reaction templates or additional chemical rule engines. The model injects molecular graph information into the attention mechanism to jointly exploit \\SMILES\\ sequences and structural cues, and further applies a paired data augmentation strategy to enhance training diversity and scale. On the USPTO-50K benchmark, our proposed approach achieves state-of-the-art performance among template-free methods and substantially outperforming a vanilla Transformer baseline."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T16:08:32Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    16,
                    8,
                    32,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Youjun Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Youjun Zhao"
                },
                "author": "Youjun Zhao"
            },
            {
                "id": "http://arxiv.org/abs/2512.06112v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.06112v2",
                "title": "WAM-Flow: Parallel Coarse-to-Fine Motion Planning via Discrete Flow Matching for Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WAM-Flow: Parallel Coarse-to-Fine Motion Planning via Discrete Flow Matching for Autonomous Driving"
                },
                "updated": "2025-12-11T16:06:13Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    16,
                    6,
                    13,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.06112v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.06112v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We introduce WAM-Flow, a vision-language-action (VLA) model that casts ego-trajectory planning as discrete flow matching over a structured token space. In contrast to autoregressive decoders, WAM-Flow performs fully parallel, bidirectional denoising, enabling coarse-to-fine refinement with a tunable compute-accuracy trade-off. Specifically, the approach combines a metric-aligned numerical tokenizer that preserves scalar geometry via triplet-margin learning, a geometry-aware flow objective and a simulator-guided GRPO alignment that integrates safety, ego progress, and comfort rewards while retaining parallel generation. A multi-stage adaptation converts a pre-trained auto-regressive backbone (Janus-1.5B) from causal decoding to non-causal flow model and strengthens road-scene competence through continued multimodal pretraining. Thanks to the inherent nature of consistency model training and parallel decoding inference, WAM-Flow achieves superior closed-loop performance against autoregressive and diffusion-based VLA baselines, with 1-step inference attaining 89.1 PDMS and 5-step inference reaching 90.3 PDMS on NAVSIM v1 benchmark. These results establish discrete flow matching as a new promising paradigm for end-to-end autonomous driving. The code will be publicly available soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce WAM-Flow, a vision-language-action (VLA) model that casts ego-trajectory planning as discrete flow matching over a structured token space. In contrast to autoregressive decoders, WAM-Flow performs fully parallel, bidirectional denoising, enabling coarse-to-fine refinement with a tunable compute-accuracy trade-off. Specifically, the approach combines a metric-aligned numerical tokenizer that preserves scalar geometry via triplet-margin learning, a geometry-aware flow objective and a simulator-guided GRPO alignment that integrates safety, ego progress, and comfort rewards while retaining parallel generation. A multi-stage adaptation converts a pre-trained auto-regressive backbone (Janus-1.5B) from causal decoding to non-causal flow model and strengthens road-scene competence through continued multimodal pretraining. Thanks to the inherent nature of consistency model training and parallel decoding inference, WAM-Flow achieves superior closed-loop performance against autoregressive and diffusion-based VLA baselines, with 1-step inference attaining 89.1 PDMS and 5-step inference reaching 90.3 PDMS on NAVSIM v1 benchmark. These results establish discrete flow matching as a new promising paradigm for end-to-end autonomous driving. The code will be publicly available soon."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T19:36:46Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    19,
                    36,
                    46,
                    4,
                    339,
                    0
                ],
                "arxiv_comment": "18 pages, 11 figures. Code & Model: https://github.com/fudan-generative-vision/WAM-Flow",
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Yifang Xu"
                    },
                    {
                        "name": "Jiahao Cui"
                    },
                    {
                        "name": "Feipeng Cai"
                    },
                    {
                        "name": "Zhihao Zhu"
                    },
                    {
                        "name": "Hanlin Shang"
                    },
                    {
                        "name": "Shan Luan"
                    },
                    {
                        "name": "Mingwang Xu"
                    },
                    {
                        "name": "Neng Zhang"
                    },
                    {
                        "name": "Yaoyi Li"
                    },
                    {
                        "name": "Jia Cai"
                    },
                    {
                        "name": "Siyu Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Siyu Zhu"
                },
                "author": "Siyu Zhu"
            },
            {
                "id": "http://arxiv.org/abs/2511.10419v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.10419v2",
                "title": "Principal component analysis in econometrics: a selective inference perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Principal component analysis in econometrics: a selective inference perspective"
                },
                "updated": "2025-12-11T16:05:43Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    16,
                    5,
                    43,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.10419v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.10419v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We study the long-standing problem of determining the number of principal components in econometric applications from a selective inference perspective. We consider i.i.d. observations from a $p$-dimensional random vector with $p<n$ and define the ``true'' dimensionality as the rank of the population covariance matrix. Building on the sequential testing viewpoint, we propose a data-driven procedure that estimates $\\rank(Σ_X)$ using a statistic that depends on the eigenvalues of the sample covariance matrix. While the test statistic shares the functional form of its fixed design counterpart Choi et al. (2017), our analysis departs from the non-stochastic setting by treating the design as random and by avoiding parametric Gaussian assumptions. Under a locally defined null hypothesis, we establish asymptotically exact type~I error controls in the sequential testing procedure, with simulation results indicating empirical validity of the proposed method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the long-standing problem of determining the number of principal components in econometric applications from a selective inference perspective. We consider i.i.d. observations from a $p$-dimensional random vector with $p<n$ and define the ``true'' dimensionality as the rank of the population covariance matrix. Building on the sequential testing viewpoint, we propose a data-driven procedure that estimates $\\rank(Σ_X)$ using a statistic that depends on the eigenvalues of the sample covariance matrix. While the test statistic shares the functional form of its fixed design counterpart Choi et al. (2017), our analysis departs from the non-stochastic setting by treating the design as random and by avoiding parametric Gaussian assumptions. Under a locally defined null hypothesis, we establish asymptotically exact type~I error controls in the sequential testing procedure, with simulation results indicating empirical validity of the proposed method."
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-13T15:40:38Z",
                "published_parsed": [
                    2025,
                    11,
                    13,
                    15,
                    40,
                    38,
                    3,
                    317,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM"
                },
                "authors": [
                    {
                        "name": "Yasuyuki Matsumura"
                    },
                    {
                        "name": "Chisato Tachibana"
                    }
                ],
                "author_detail": {
                    "name": "Chisato Tachibana"
                },
                "author": "Chisato Tachibana"
            },
            {
                "id": "http://arxiv.org/abs/2512.10765v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10765v1",
                "title": "Blood Pressure Prediction for Coronary Artery Disease Diagnosis using Coronary Computed Tomography Angiography",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Blood Pressure Prediction for Coronary Artery Disease Diagnosis using Coronary Computed Tomography Angiography"
                },
                "updated": "2025-12-11T16:03:35Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    16,
                    3,
                    35,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10765v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10765v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Computational fluid dynamics (CFD) based simulation of coronary blood flow provides valuable hemodynamic markers, such as pressure gradients, for diagnosing coronary artery disease (CAD). However, CFD is computationally expensive, time-consuming, and difficult to integrate into large-scale clinical workflows. These limitations restrict the availability of labeled hemodynamic data for training AI models and hinder broad adoption of non-invasive, physiology based CAD assessment. To address these challenges, we develop an end to end pipeline that automates coronary geometry extraction from coronary computed tomography angiography (CCTA), streamlines simulation data generation, and enables efficient learning of coronary blood pressure distributions. The pipeline reduces the manual burden associated with traditional CFD workflows while producing consistent training data. We further introduce a diffusion-based regression model designed to predict coronary blood pressure directly from CCTA derived features, bypassing the need for slow CFD computation during inference. Evaluated on a dataset of simulated coronary hemodynamics, the proposed model achieves state of the art performance, with an R2 of 64.42%, a root mean squared error of 0.0974, and a normalized RMSE of 0.154, outperforming several baseline approaches. This work provides a scalable and accessible framework for rapid, non-invasive blood pressure prediction to support CAD diagnosis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational fluid dynamics (CFD) based simulation of coronary blood flow provides valuable hemodynamic markers, such as pressure gradients, for diagnosing coronary artery disease (CAD). However, CFD is computationally expensive, time-consuming, and difficult to integrate into large-scale clinical workflows. These limitations restrict the availability of labeled hemodynamic data for training AI models and hinder broad adoption of non-invasive, physiology based CAD assessment. To address these challenges, we develop an end to end pipeline that automates coronary geometry extraction from coronary computed tomography angiography (CCTA), streamlines simulation data generation, and enables efficient learning of coronary blood pressure distributions. The pipeline reduces the manual burden associated with traditional CFD workflows while producing consistent training data. We further introduce a diffusion-based regression model designed to predict coronary blood pressure directly from CCTA derived features, bypassing the need for slow CFD computation during inference. Evaluated on a dataset of simulated coronary hemodynamics, the proposed model achieves state of the art performance, with an R2 of 64.42%, a root mean squared error of 0.0974, and a normalized RMSE of 0.154, outperforming several baseline approaches. This work provides a scalable and accessible framework for rapid, non-invasive blood pressure prediction to support CAD diagnosis."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T16:03:35Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    16,
                    3,
                    35,
                    3,
                    345,
                    0
                ],
                "arxiv_comment": "19 pages, 9 figures",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Rene Lisasi"
                    },
                    {
                        "name": "Michele Esposito"
                    },
                    {
                        "name": "Chen Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Chen Zhao"
                },
                "author": "Chen Zhao"
            },
            {
                "id": "http://arxiv.org/abs/2505.14932v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.14932v2",
                "title": "FOL-Traces: Verified First-Order Logic Reasoning Traces at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FOL-Traces: Verified First-Order Logic Reasoning Traces at Scale"
                },
                "updated": "2025-12-11T16:02:17Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    16,
                    2,
                    17,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.14932v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.14932v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Reasoning in language models is difficult to evaluate: natural-language traces are unverifiable, symbolic datasets too small, and most benchmarks conflate heuristics with inference. We present FOL-Traces, the first large-scale dataset of programmatically verified reasoning traces, enabling rigorous evaluation of structured logical inference. We also propose two challenging and comprehensive diagnostic tasks-masked operation prediction and step completion-that directly probe syntactic awareness and process fidelity. FOL-Traces serves as a scalable testbed for rigorously studying how models perform structured logical inference. Systematic experiments with 5 reasoning LLMs show that the dataset remains challenging: models only reach around 45.7% accuracy on masked operation prediction and around 27% on two-step completion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning in language models is difficult to evaluate: natural-language traces are unverifiable, symbolic datasets too small, and most benchmarks conflate heuristics with inference. We present FOL-Traces, the first large-scale dataset of programmatically verified reasoning traces, enabling rigorous evaluation of structured logical inference. We also propose two challenging and comprehensive diagnostic tasks-masked operation prediction and step completion-that directly probe syntactic awareness and process fidelity. FOL-Traces serves as a scalable testbed for rigorously studying how models perform structured logical inference. Systematic experiments with 5 reasoning LLMs show that the dataset remains challenging: models only reach around 45.7% accuracy on masked operation prediction and around 27% on two-step completion."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-20T21:38:28Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    21,
                    38,
                    28,
                    1,
                    140,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Isabelle Lee"
                    },
                    {
                        "name": "Sarah Liaw"
                    },
                    {
                        "name": "Dani Yogatama"
                    }
                ],
                "author_detail": {
                    "name": "Dani Yogatama"
                },
                "author": "Dani Yogatama"
            },
            {
                "id": "http://arxiv.org/abs/2512.10756v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10756v1",
                "title": "OPV: Outcome-based Process Verifier for Efficient Long Chain-of-Thought Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OPV: Outcome-based Process Verifier for Efficient Long Chain-of-Thought Verification"
                },
                "updated": "2025-12-11T15:47:38Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    15,
                    47,
                    38,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10756v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10756v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) have achieved significant progress in solving complex reasoning tasks by Reinforcement Learning with Verifiable Rewards (RLVR). This advancement is also inseparable from the oversight automated by reliable verifiers. However, current outcome-based verifiers (OVs) are unable to inspect the unreliable intermediate steps in the long reasoning chains of thought (CoTs). Meanwhile, current process-based verifiers (PVs) have difficulties in reliably detecting errors in the complex long CoTs, limited by the scarcity of high-quality annotations due to the prohibitive costs of human annotations. Therefore, we propose the Outcome-based Process Verifier (OPV), which verifies the rationale process of summarized outcomes from long CoTs to achieve both accurate and efficient verification and enable large-scale annotation. To empower the proposed verifier, we adopt an iterative active learning framework with expert annotations to progressively improve the verification capability of OPV with fewer annotation costs. Specifically, in each iteration, the most uncertain cases of the current best OPV are annotated and then subsequently used to train a new OPV through Rejection Fine-Tuning (RFT) and RLVR for the next round. Extensive experiments demonstrate OPV's superior performance and broad applicability. It achieves new state-of-the-art results on our held-out OPV-Bench, outperforming much larger open-source models such as Qwen3-Max-Preview with an F1 score of 83.1 compared to 76.3. Furthermore, OPV effectively detects false positives within synthetic dataset, closely align with expert assessment. When collaborating with policy models, OPV consistently yields performance gains, e.g., raising the accuracy of DeepSeek-R1-Distill-Qwen-32B from 55.2% to 73.3% on AIME2025 as the compute budget scales.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved significant progress in solving complex reasoning tasks by Reinforcement Learning with Verifiable Rewards (RLVR). This advancement is also inseparable from the oversight automated by reliable verifiers. However, current outcome-based verifiers (OVs) are unable to inspect the unreliable intermediate steps in the long reasoning chains of thought (CoTs). Meanwhile, current process-based verifiers (PVs) have difficulties in reliably detecting errors in the complex long CoTs, limited by the scarcity of high-quality annotations due to the prohibitive costs of human annotations. Therefore, we propose the Outcome-based Process Verifier (OPV), which verifies the rationale process of summarized outcomes from long CoTs to achieve both accurate and efficient verification and enable large-scale annotation. To empower the proposed verifier, we adopt an iterative active learning framework with expert annotations to progressively improve the verification capability of OPV with fewer annotation costs. Specifically, in each iteration, the most uncertain cases of the current best OPV are annotated and then subsequently used to train a new OPV through Rejection Fine-Tuning (RFT) and RLVR for the next round. Extensive experiments demonstrate OPV's superior performance and broad applicability. It achieves new state-of-the-art results on our held-out OPV-Bench, outperforming much larger open-source models such as Qwen3-Max-Preview with an F1 score of 83.1 compared to 76.3. Furthermore, OPV effectively detects false positives within synthetic dataset, closely align with expert assessment. When collaborating with policy models, OPV consistently yields performance gains, e.g., raising the accuracy of DeepSeek-R1-Distill-Qwen-32B from 55.2% to 73.3% on AIME2025 as the compute budget scales."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T15:47:38Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    15,
                    47,
                    38,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Zijian Wu"
                    },
                    {
                        "name": "Lingkai Kong"
                    },
                    {
                        "name": "Wenwei Zhang"
                    },
                    {
                        "name": "Songyang Gao"
                    },
                    {
                        "name": "Yuzhe Gu"
                    },
                    {
                        "name": "Zhongrui Cai"
                    },
                    {
                        "name": "Tianyou Ma"
                    },
                    {
                        "name": "Yuhong Liu"
                    },
                    {
                        "name": "Zhi Wang"
                    },
                    {
                        "name": "Runyuan Ma"
                    },
                    {
                        "name": "Guangyu Wang"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Kai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Chen"
                },
                "author": "Kai Chen"
            },
            {
                "id": "http://arxiv.org/abs/2512.10753v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10753v1",
                "title": "Quantifying displacement: a gentrification's consequence via persistent homology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantifying displacement: a gentrification's consequence via persistent homology"
                },
                "updated": "2025-12-11T15:45:49Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    15,
                    45,
                    49,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10753v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10753v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Gentrification is the process by which wealthier individuals move into a previously lower-income neighbourhood. Among the effects of this multi-faceted phenomenon are rising living costs, cultural and social changes-where local traditions, businesses, and community networks are replaced or diluted by new, more affluent lifestyles-and population displacement, where long-term, lower-income residents are priced out by rising rents and property taxes. Despite its relevance, quantifying displacement presents difficulties stemming from lack of information on motives for relocation and from the fact that a long time-span must be analysed: displacement is a gradual process (leases end or conditions change at different times), impossible to capture in one data snapshot. We introduce a novel tool to overcome these difficulties. Using only publicly available address change data, we construct four cubical complexes which simultaneously incorporate geographical and temporal information of people moving, and then analyse them building on Topological Data Analysis tools. Finally, we demonstrate the potential of this method through a 20-year case study of Madrid, Spain. The results reveal its ability to capture population displacement and to identify the specific neighbourhoods and years affected--patterns that cannot be inferred from raw address change data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gentrification is the process by which wealthier individuals move into a previously lower-income neighbourhood. Among the effects of this multi-faceted phenomenon are rising living costs, cultural and social changes-where local traditions, businesses, and community networks are replaced or diluted by new, more affluent lifestyles-and population displacement, where long-term, lower-income residents are priced out by rising rents and property taxes. Despite its relevance, quantifying displacement presents difficulties stemming from lack of information on motives for relocation and from the fact that a long time-span must be analysed: displacement is a gradual process (leases end or conditions change at different times), impossible to capture in one data snapshot. We introduce a novel tool to overcome these difficulties. Using only publicly available address change data, we construct four cubical complexes which simultaneously incorporate geographical and temporal information of people moving, and then analyse them building on Topological Data Analysis tools. Finally, we demonstrate the potential of this method through a 20-year case study of Madrid, Spain. The results reveal its ability to capture population displacement and to identify the specific neighbourhoods and years affected--patterns that cannot be inferred from raw address change data."
                },
                "tags": [
                    {
                        "term": "cs.CG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T15:45:49Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    15,
                    45,
                    49,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CG"
                },
                "authors": [
                    {
                        "name": "Rita Rodríguez Vázquez"
                    },
                    {
                        "name": "Manuel Cuerno"
                    }
                ],
                "author_detail": {
                    "name": "Manuel Cuerno"
                },
                "author": "Manuel Cuerno"
            },
            {
                "id": "http://arxiv.org/abs/2512.10750v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10750v1",
                "title": "LDP: Parameter-Efficient Fine-Tuning of Multimodal LLM for Medical Report Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LDP: Parameter-Efficient Fine-Tuning of Multimodal LLM for Medical Report Generation"
                },
                "updated": "2025-12-11T15:43:33Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    15,
                    43,
                    33,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10750v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10750v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Colonoscopic polyp diagnosis is pivotal for early colorectal cancer detection, yet traditional automated reporting suffers from inconsistencies and hallucinations due to the scarcity of high-quality multimodal medical data. To bridge this gap, we propose LDP, a novel framework leveraging multimodal large language models (MLLMs) for professional polyp diagnosis report generation. Specifically, we curate MMEndo, a multimodal endoscopic dataset comprising expert-annotated colonoscopy image-text pairs. We fine-tune the Qwen2-VL-7B backbone using Parameter-Efficient Fine-Tuning (LoRA) and align it with clinical standards via Direct Preference Optimization (DPO). Extensive experiments show that our LDP outperforms existing baselines on both automated metrics and rigorous clinical expert evaluations (achieving a Physician Score of 7.2/10), significantly reducing training computational costs by 833x compared to full fine-tuning. The proposed solution offers a scalable, clinically viable path for primary healthcare, with additional validation on the IU-XRay dataset confirming its robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Colonoscopic polyp diagnosis is pivotal for early colorectal cancer detection, yet traditional automated reporting suffers from inconsistencies and hallucinations due to the scarcity of high-quality multimodal medical data. To bridge this gap, we propose LDP, a novel framework leveraging multimodal large language models (MLLMs) for professional polyp diagnosis report generation. Specifically, we curate MMEndo, a multimodal endoscopic dataset comprising expert-annotated colonoscopy image-text pairs. We fine-tune the Qwen2-VL-7B backbone using Parameter-Efficient Fine-Tuning (LoRA) and align it with clinical standards via Direct Preference Optimization (DPO). Extensive experiments show that our LDP outperforms existing baselines on both automated metrics and rigorous clinical expert evaluations (achieving a Physician Score of 7.2/10), significantly reducing training computational costs by 833x compared to full fine-tuning. The proposed solution offers a scalable, clinically viable path for primary healthcare, with additional validation on the IU-XRay dataset confirming its robustness."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T15:43:33Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    15,
                    43,
                    33,
                    3,
                    345,
                    0
                ],
                "arxiv_comment": "Work in progress",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Tianyu Zhou"
                    },
                    {
                        "name": "Junyi Tang"
                    },
                    {
                        "name": "Zehui Li"
                    },
                    {
                        "name": "Dahong Qian"
                    },
                    {
                        "name": "Suncheng Xiang"
                    }
                ],
                "author_detail": {
                    "name": "Suncheng Xiang"
                },
                "author": "Suncheng Xiang"
            },
            {
                "id": "http://arxiv.org/abs/2305.14255v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2305.14255v2",
                "title": "Augmented match weighted estimators for average treatment effects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Augmented match weighted estimators for average treatment effects"
                },
                "updated": "2025-12-11T15:42:08Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    15,
                    42,
                    8,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2305.14255v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2305.14255v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Propensity score matching (PSM) and augmented inverse propensity weighting (AIPW) are widely used in observational studies to estimate causal effects. The two approaches present complementary features. The AIPW estimator is doubly robust and locally efficient but can be unstable when the propensity scores are close to zero or one due to weighting by the inverse of the propensity score. On the other hand, PSM circumvents the instability of propensity score weighting but it hinges on the correctness of the propensity score model and cannot attain the semiparametric efficiency bound. Besides, the fixed number of matches, K, renders PSM nonsmooth and thus invalidates standard nonparametric bootstrap inference.\n  This article presents novel augmented match weighted (AMW) estimators that combine the advantages of matching and weighting estimators. AMW adheres to the form of AIPW for its double robustness and local efficiency but it mitigates the instability due to weighting. We replace inverse propensity weights with matching weights resulting from PSM with unfixed K. Meanwhile, we propose a new cross-validation procedure to select K that minimizes the mean squared error anchored around an unbiased estimator of the causal estimand. Besides, we derive the limiting distribution for the AMW estimators showing that they enjoy the double robustness property and can achieve the semiparametric efficiency bound if both nuisance models are correct. As a byproduct of unfixed K which smooths the AMW estimators, nonparametric bootstrap can be adopted for variance estimation and inference. Furthermore, simulation studies and real data applications support that the AMW estimators are stable with extreme propensity scores and their variances can be obtained by naive bootstrap.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Propensity score matching (PSM) and augmented inverse propensity weighting (AIPW) are widely used in observational studies to estimate causal effects. The two approaches present complementary features. The AIPW estimator is doubly robust and locally efficient but can be unstable when the propensity scores are close to zero or one due to weighting by the inverse of the propensity score. On the other hand, PSM circumvents the instability of propensity score weighting but it hinges on the correctness of the propensity score model and cannot attain the semiparametric efficiency bound. Besides, the fixed number of matches, K, renders PSM nonsmooth and thus invalidates standard nonparametric bootstrap inference.\n  This article presents novel augmented match weighted (AMW) estimators that combine the advantages of matching and weighting estimators. AMW adheres to the form of AIPW for its double robustness and local efficiency but it mitigates the instability due to weighting. We replace inverse propensity weights with matching weights resulting from PSM with unfixed K. Meanwhile, we propose a new cross-validation procedure to select K that minimizes the mean squared error anchored around an unbiased estimator of the causal estimand. Besides, we derive the limiting distribution for the AMW estimators showing that they enjoy the double robustness property and can achieve the semiparametric efficiency bound if both nuisance models are correct. As a byproduct of unfixed K which smooths the AMW estimators, nonparametric bootstrap can be adopted for variance estimation and inference. Furthermore, simulation studies and real data applications support that the AMW estimators are stable with extreme propensity scores and their variances can be obtained by naive bootstrap."
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2023-05-23T17:10:08Z",
                "published_parsed": [
                    2023,
                    5,
                    23,
                    17,
                    10,
                    8,
                    1,
                    143,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME"
                },
                "authors": [
                    {
                        "name": "Tanchumin Xu"
                    },
                    {
                        "name": "Yunshu Zhang"
                    },
                    {
                        "name": "Shu Yang"
                    }
                ],
                "author_detail": {
                    "name": "Shu Yang"
                },
                "author": "Shu Yang"
            },
            {
                "id": "http://arxiv.org/abs/2510.14573v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.14573v2",
                "title": "State-Space Models for Tabular Prior-Data Fitted Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State-Space Models for Tabular Prior-Data Fitted Networks"
                },
                "updated": "2025-12-11T15:36:53Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    15,
                    36,
                    53,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.14573v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.14573v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent advancements in foundation models for tabular data, such as TabPFN, demonstrated that pretrained Transformer architectures can approximate Bayesian inference with high predictive performance. However, Transformers suffer from quadratic complexity with respect to sequence length, motivating the exploration of more efficient sequence models. In this work, we investigate the potential of using Hydra, a bidirectional linear-time structured state space model (SSM), as an alternative to Transformers in TabPFN. A key challenge lies in SSM's inherent sensitivity to the order of input tokens - an undesirable property for tabular datasets where the row order is semantically meaningless. We investigate to what extent a bidirectional approach can preserve efficiency and enable symmetric context aggregation. Our experiments show that this approach reduces the order-dependence, achieving predictive performance competitive to the original TabPFN model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in foundation models for tabular data, such as TabPFN, demonstrated that pretrained Transformer architectures can approximate Bayesian inference with high predictive performance. However, Transformers suffer from quadratic complexity with respect to sequence length, motivating the exploration of more efficient sequence models. In this work, we investigate the potential of using Hydra, a bidirectional linear-time structured state space model (SSM), as an alternative to Transformers in TabPFN. A key challenge lies in SSM's inherent sensitivity to the order of input tokens - an undesirable property for tabular datasets where the row order is semantically meaningless. We investigate to what extent a bidirectional approach can preserve efficiency and enable symmetric context aggregation. Our experiments show that this approach reduces the order-dependence, achieving predictive performance competitive to the original TabPFN model."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-16T11:31:51Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    11,
                    31,
                    51,
                    3,
                    289,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "arxiv_journal_ref": "International Conference on Machine Learning (ICML), 1st ICML Workshop on Foundation Models for Structured Data, 2025",
                "authors": [
                    {
                        "name": "Felix Koch"
                    },
                    {
                        "name": "Marcel Wever"
                    },
                    {
                        "name": "Fabian Raisch"
                    },
                    {
                        "name": "Benjamin Tischler"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Tischler"
                },
                "author": "Benjamin Tischler"
            },
            {
                "id": "http://arxiv.org/abs/2504.01951v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2504.01951v2",
                "title": "The LLM Wears Prada: Analysing Gender Bias and Stereotypes through Online Shopping Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The LLM Wears Prada: Analysing Gender Bias and Stereotypes through Online Shopping Data"
                },
                "updated": "2025-12-11T15:33:30Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    15,
                    33,
                    30,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2504.01951v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2504.01951v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "With the wide and cross-domain adoption of Large Language Models, it becomes crucial to assess to which extent the statistical correlations in training data, which underlie their impressive performance, hide subtle and potentially troubling biases. Gender bias in LLMs has been widely investigated from the perspectives of works, hobbies, and emotions typically associated with a specific gender. In this study, we introduce a novel perspective. We investigate whether LLMs can predict an individual's gender based solely on online shopping histories and whether these predictions are influenced by gender biases and stereotypes. Using a dataset of historical online purchases from users in the United States, we evaluate the ability of six LLMs to classify gender and we then analyze their reasoning and products-gender co-occurrences. Results indicate that while models can infer gender with moderate accuracy, their decisions are often rooted in stereotypical associations between product categories and gender. Furthermore, explicit instructions to avoid bias reduce the certainty of model predictions, but do not eliminate stereotypical patterns. Our findings highlight the persistent nature of gender biases in LLMs and emphasize the need for robust bias-mitigation strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the wide and cross-domain adoption of Large Language Models, it becomes crucial to assess to which extent the statistical correlations in training data, which underlie their impressive performance, hide subtle and potentially troubling biases. Gender bias in LLMs has been widely investigated from the perspectives of works, hobbies, and emotions typically associated with a specific gender. In this study, we introduce a novel perspective. We investigate whether LLMs can predict an individual's gender based solely on online shopping histories and whether these predictions are influenced by gender biases and stereotypes. Using a dataset of historical online purchases from users in the United States, we evaluate the ability of six LLMs to classify gender and we then analyze their reasoning and products-gender co-occurrences. Results indicate that while models can infer gender with moderate accuracy, their decisions are often rooted in stereotypical associations between product categories and gender. Furthermore, explicit instructions to avoid bias reduce the certainty of model predictions, but do not eliminate stereotypical patterns. Our findings highlight the persistent nature of gender biases in LLMs and emphasize the need for robust bias-mitigation strategies."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-04-02T17:56:08Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    17,
                    56,
                    8,
                    2,
                    92,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Massimiliano Luca"
                    },
                    {
                        "name": "Ciro Beneduce"
                    },
                    {
                        "name": "Bruno Lepri"
                    },
                    {
                        "name": "Jacopo Staiano"
                    }
                ],
                "author_detail": {
                    "name": "Jacopo Staiano"
                },
                "author": "Jacopo Staiano"
            },
            {
                "id": "http://arxiv.org/abs/2512.10745v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10745v1",
                "title": "PMB-NN: Physiology-Centred Hybrid AI for Personalized Hemodynamic Monitoring from Photoplethysmography",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PMB-NN: Physiology-Centred Hybrid AI for Personalized Hemodynamic Monitoring from Photoplethysmography"
                },
                "updated": "2025-12-11T15:32:50Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    15,
                    32,
                    50,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10745v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10745v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Continuous monitoring of blood pressure (BP) and hemodynamic parameters such as peripheral resistance (R) and arterial compliance (C) are critical for early vascular dysfunction detection. While photoplethysmography (PPG) wearables has gained popularity, existing data-driven methods for BP estimation lack interpretability. We advanced our previously proposed physiology-centered hybrid AI method-Physiological Model-Based Neural Network (PMB-NN)-in blood pressure estimation, that unifies deep learning with a 2-element Windkessel based model parameterized by R and C acting as physics constraints. The PMB-NN model was trained in a subject-specific manner using PPG-derived timing features, while demographic information was used to infer an intermediate variable: cardiac output. We validated the model on 10 healthy adults performing static and cycling activities across two days for model's day-to-day robustness, benchmarked against deep learning (DL) models (FCNN, CNN-LSTM, Transformer) and standalone Windkessel based physiological model (PM). Validation was conducted on three perspectives: accuracy, interpretability and plausibility. PMB-NN achieved systolic BP accuracy (MAE: 7.2 mmHg) comparable to DL benchmarks, diastolic performance (MAE: 3.9 mmHg) lower than DL models. However, PMB-NN exhibited higher physiological plausibility than both DL baselines and PM, suggesting that the hybrid architecture unifies and enhances the respective merits of physiological principles and data-driven techniques. Beyond BP, PMB-NN identified R (ME: 0.15 mmHg$\\cdot$s/ml) and C (ME: -0.35 ml/mmHg) during training with accuracy similar to PM, demonstrating that the embedded physiological constraints confer interpretability to the hybrid AI framework. These results position PMB-NN as a balanced, physiologically grounded alternative to purely data-driven approaches for daily hemodynamic monitoring.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continuous monitoring of blood pressure (BP) and hemodynamic parameters such as peripheral resistance (R) and arterial compliance (C) are critical for early vascular dysfunction detection. While photoplethysmography (PPG) wearables has gained popularity, existing data-driven methods for BP estimation lack interpretability. We advanced our previously proposed physiology-centered hybrid AI method-Physiological Model-Based Neural Network (PMB-NN)-in blood pressure estimation, that unifies deep learning with a 2-element Windkessel based model parameterized by R and C acting as physics constraints. The PMB-NN model was trained in a subject-specific manner using PPG-derived timing features, while demographic information was used to infer an intermediate variable: cardiac output. We validated the model on 10 healthy adults performing static and cycling activities across two days for model's day-to-day robustness, benchmarked against deep learning (DL) models (FCNN, CNN-LSTM, Transformer) and standalone Windkessel based physiological model (PM). Validation was conducted on three perspectives: accuracy, interpretability and plausibility. PMB-NN achieved systolic BP accuracy (MAE: 7.2 mmHg) comparable to DL benchmarks, diastolic performance (MAE: 3.9 mmHg) lower than DL models. However, PMB-NN exhibited higher physiological plausibility than both DL baselines and PM, suggesting that the hybrid architecture unifies and enhances the respective merits of physiological principles and data-driven techniques. Beyond BP, PMB-NN identified R (ME: 0.15 mmHg$\\cdot$s/ml) and C (ME: -0.35 ml/mmHg) during training with accuracy similar to PM, demonstrating that the embedded physiological constraints confer interpretability to the hybrid AI framework. These results position PMB-NN as a balanced, physiologically grounded alternative to purely data-driven approaches for daily hemodynamic monitoring."
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T15:32:50Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    15,
                    32,
                    50,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph"
                },
                "authors": [
                    {
                        "name": "Yaowen Zhang"
                    },
                    {
                        "name": "Libera Fresiello"
                    },
                    {
                        "name": "Peter H. Veltink"
                    },
                    {
                        "name": "Dirk W. Donker"
                    },
                    {
                        "name": "Ying Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ying Wang"
                },
                "author": "Ying Wang"
            },
            {
                "id": "http://arxiv.org/abs/2512.10739v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10739v1",
                "title": "Long-horizon Reasoning Agent for Olympiad-Level Mathematical Problem Solving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-horizon Reasoning Agent for Olympiad-Level Mathematical Problem Solving"
                },
                "updated": "2025-12-11T15:26:28Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    15,
                    26,
                    28,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10739v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10739v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) have achieved significant progress in solving complex reasoning tasks by Reinforcement Learning with Verifiable Rewards (RLVR). This advancement is also inseparable from the oversight automated by reliable verifiers. However, current outcome-based verifiers (OVs) are unable to inspect the unreliable intermediate steps in the long reasoning chains of thought (CoTs). Meanwhile, current process-based verifiers (PVs) have difficulties in reliably detecting errors in the complex long CoTs, limited by the scarcity of high-quality annotations due to the prohibitive costs of human annotations. Therefore, we propose the \\textbf{O}utcome-based \\textbf{P}rocess \\textbf{V}erifier (OPV), which verifies the rationale process of summarized outcomes from long CoTs to achieve both accurate and efficient verification and enable large-scale annotation. To empower the proposed verifier, we adopt an iterative active learning framework with expert annotations to progressively improve the verification capability of OPV with fewer annotation costs. Specifically, in each iteration, the most uncertain cases of the current best OPV are annotated and then subsequently used to train a new OPV through Rejection Fine-Tuning (RFT) and RLVR for the next round. Extensive experiments demonstrate OPV's superior performance and broad applicability. It achieves new state-of-the-art results on our held-out \\textsc{\\thisbench}, outperforming much larger open-source models such as Qwen3-Max-Preview with an F1 score of 83.1 compared to 76.3. Furthermore, OPV effectively detects false positives within synthetic dataset, closely align with expert assessment. When collaborating with policy models, OPV consistently yields performance gains, e.g., raising the accuracy of DeepSeek-R1-Distill-Qwen-32B from 55.2\\% to 73.3\\% on AIME2025 as the compute budget scales.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved significant progress in solving complex reasoning tasks by Reinforcement Learning with Verifiable Rewards (RLVR). This advancement is also inseparable from the oversight automated by reliable verifiers. However, current outcome-based verifiers (OVs) are unable to inspect the unreliable intermediate steps in the long reasoning chains of thought (CoTs). Meanwhile, current process-based verifiers (PVs) have difficulties in reliably detecting errors in the complex long CoTs, limited by the scarcity of high-quality annotations due to the prohibitive costs of human annotations. Therefore, we propose the \\textbf{O}utcome-based \\textbf{P}rocess \\textbf{V}erifier (OPV), which verifies the rationale process of summarized outcomes from long CoTs to achieve both accurate and efficient verification and enable large-scale annotation. To empower the proposed verifier, we adopt an iterative active learning framework with expert annotations to progressively improve the verification capability of OPV with fewer annotation costs. Specifically, in each iteration, the most uncertain cases of the current best OPV are annotated and then subsequently used to train a new OPV through Rejection Fine-Tuning (RFT) and RLVR for the next round. Extensive experiments demonstrate OPV's superior performance and broad applicability. It achieves new state-of-the-art results on our held-out \\textsc{\\thisbench}, outperforming much larger open-source models such as Qwen3-Max-Preview with an F1 score of 83.1 compared to 76.3. Furthermore, OPV effectively detects false positives within synthetic dataset, closely align with expert assessment. When collaborating with policy models, OPV consistently yields performance gains, e.g., raising the accuracy of DeepSeek-R1-Distill-Qwen-32B from 55.2\\% to 73.3\\% on AIME2025 as the compute budget scales."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T15:26:28Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    15,
                    26,
                    28,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Songyang Gao"
                    },
                    {
                        "name": "Yuzhe Gu"
                    },
                    {
                        "name": "Zijian Wu"
                    },
                    {
                        "name": "Lingkai Kong"
                    },
                    {
                        "name": "Wenwei Zhang"
                    },
                    {
                        "name": "Zhongrui Cai"
                    },
                    {
                        "name": "Fan Zheng"
                    },
                    {
                        "name": "Tianyou Ma"
                    },
                    {
                        "name": "Junhao Shen"
                    },
                    {
                        "name": "Haiteng Zhao"
                    },
                    {
                        "name": "Duanyang Zhang"
                    },
                    {
                        "name": "Huilun Zhang"
                    },
                    {
                        "name": "Kuikun Liu"
                    },
                    {
                        "name": "Chengqi Lyu"
                    },
                    {
                        "name": "Yanhui Duan"
                    },
                    {
                        "name": "Chiyu Chen"
                    },
                    {
                        "name": "Ningsheng Ma"
                    },
                    {
                        "name": "Jianfei Gao"
                    },
                    {
                        "name": "Han Lyu"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Kai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Chen"
                },
                "author": "Kai Chen"
            },
            {
                "id": "http://arxiv.org/abs/2512.10734v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10734v1",
                "title": "Textual Data Bias Detection and Mitigation - An Extensible Pipeline with Experimental Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Textual Data Bias Detection and Mitigation - An Extensible Pipeline with Experimental Evaluation"
                },
                "updated": "2025-12-11T15:18:59Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    15,
                    18,
                    59,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10734v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10734v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Textual data used to train large language models (LLMs) exhibits multifaceted bias manifestations encompassing harmful language and skewed demographic distributions. Regulations such as the European AI Act require identifying and mitigating biases against protected groups in data, with the ultimate goal of preventing unfair model outputs. However, practical guidance and operationalization are lacking. We propose a comprehensive data bias detection and mitigation pipeline comprising four components that address two data bias types, namely representation bias and (explicit) stereotypes for a configurable sensitive attribute. First, we leverage LLM-generated word lists created based on quality criteria to detect relevant group labels. Second, representation bias is quantified using the Demographic Representation Score. Third, we detect and mitigate stereotypes using sociolinguistically informed filtering. Finally, we compensate representation bias through Grammar- and Context-Aware Counterfactual Data Augmentation. We conduct a two-fold evaluation using the examples of gender, religion and age. First, the effectiveness of each individual component on data debiasing is evaluated through human validation and baseline comparison. The findings demonstrate that we successfully reduce representation bias and (explicit) stereotypes in a text dataset. Second, the effect of data debiasing on model bias reduction is evaluated by bias benchmarking of several models (0.6B-8B parameters), fine-tuned on the debiased text dataset. This evaluation reveals that LLMs fine-tuned on debiased data do not consistently show improved performance on bias benchmarks, exposing critical gaps in current evaluation methodologies and highlighting the need for targeted data manipulation to address manifested model bias.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Textual data used to train large language models (LLMs) exhibits multifaceted bias manifestations encompassing harmful language and skewed demographic distributions. Regulations such as the European AI Act require identifying and mitigating biases against protected groups in data, with the ultimate goal of preventing unfair model outputs. However, practical guidance and operationalization are lacking. We propose a comprehensive data bias detection and mitigation pipeline comprising four components that address two data bias types, namely representation bias and (explicit) stereotypes for a configurable sensitive attribute. First, we leverage LLM-generated word lists created based on quality criteria to detect relevant group labels. Second, representation bias is quantified using the Demographic Representation Score. Third, we detect and mitigate stereotypes using sociolinguistically informed filtering. Finally, we compensate representation bias through Grammar- and Context-Aware Counterfactual Data Augmentation. We conduct a two-fold evaluation using the examples of gender, religion and age. First, the effectiveness of each individual component on data debiasing is evaluated through human validation and baseline comparison. The findings demonstrate that we successfully reduce representation bias and (explicit) stereotypes in a text dataset. Second, the effect of data debiasing on model bias reduction is evaluated by bias benchmarking of several models (0.6B-8B parameters), fine-tuned on the debiased text dataset. This evaluation reveals that LLMs fine-tuned on debiased data do not consistently show improved performance on bias benchmarks, exposing critical gaps in current evaluation methodologies and highlighting the need for targeted data manipulation to address manifested model bias."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T15:18:59Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    15,
                    18,
                    59,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Rebekka Görge"
                    },
                    {
                        "name": "Sujan Sai Gannamaneni"
                    },
                    {
                        "name": "Tabea Naeven"
                    },
                    {
                        "name": "Hammam Abdelwahab"
                    },
                    {
                        "name": "Héctor Allende-Cid"
                    },
                    {
                        "name": "Armin B. Cremers"
                    },
                    {
                        "name": "Lennard Helmer"
                    },
                    {
                        "name": "Michael Mock"
                    },
                    {
                        "name": "Anna Schmitz"
                    },
                    {
                        "name": "Songkai Xue"
                    },
                    {
                        "name": "Elif Yildirir"
                    },
                    {
                        "name": "Maximilian Poretschkin"
                    },
                    {
                        "name": "Stefan Wrobel"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Wrobel"
                },
                "author": "Stefan Wrobel"
            },
            {
                "id": "http://arxiv.org/abs/2502.09255v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2502.09255v2",
                "title": "Bayesian Matrix Factor Models for Demographic Analysis Across Age and Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Matrix Factor Models for Demographic Analysis Across Age and Time"
                },
                "updated": "2025-12-11T15:15:49Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    15,
                    15,
                    49,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2502.09255v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2502.09255v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Analyzing demographic data collected across multiple populations, time periods, and age groups is challenging due to the interplay of high dimensionality, demographic heterogeneity among groups, and stochastic variability within smaller groups. This paper proposes a Bayesian matrix factor model to address these challenges. By factorizing count data matrices as the product of low-dimensional latent age and time factors, the model achieves a parsimonious representation that mitigates overfitting and remains computationally feasible even when hundreds of populations are involved. Informative priors enforce smoothness in the age factors and allow for the dynamic evolution of the time factors. A straightforward Markov chain Monte Carlo algorithm is developed for posterior inference. Applying the model to Austrian district-level migration data from 2002 to 2023 demonstrates its ability to accurately reconstruct complex demographic processes using only a fraction of the parameters required by conventional demographic factor models. A forecasting exercise shows that the proposed model consistently outperforms standard benchmarks. Beyond statistical demography, the framework holds promise for a wide range of applications involving noisy, heterogeneous, and high-dimensional non-Gaussian matrix-valued data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analyzing demographic data collected across multiple populations, time periods, and age groups is challenging due to the interplay of high dimensionality, demographic heterogeneity among groups, and stochastic variability within smaller groups. This paper proposes a Bayesian matrix factor model to address these challenges. By factorizing count data matrices as the product of low-dimensional latent age and time factors, the model achieves a parsimonious representation that mitigates overfitting and remains computationally feasible even when hundreds of populations are involved. Informative priors enforce smoothness in the age factors and allow for the dynamic evolution of the time factors. A straightforward Markov chain Monte Carlo algorithm is developed for posterior inference. Applying the model to Austrian district-level migration data from 2002 to 2023 demonstrates its ability to accurately reconstruct complex demographic processes using only a fraction of the parameters required by conventional demographic factor models. A forecasting exercise shows that the proposed model consistently outperforms standard benchmarks. Beyond statistical demography, the framework holds promise for a wide range of applications involving noisy, heterogeneous, and high-dimensional non-Gaussian matrix-valued data."
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-02-13T12:10:11Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    12,
                    10,
                    11,
                    3,
                    44,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP"
                },
                "authors": [
                    {
                        "name": "Gregor Zens"
                    }
                ],
                "author_detail": {
                    "name": "Gregor Zens"
                },
                "author": "Gregor Zens"
            },
            {
                "id": "http://arxiv.org/abs/2512.10725v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10725v1",
                "title": "Video Depth Propagation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Depth Propagation"
                },
                "updated": "2025-12-11T15:08:37Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    15,
                    8,
                    37,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10725v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10725v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Depth estimation in videos is essential for visual perception in real-world applications. However, existing methods either rely on simple frame-by-frame monocular models, leading to temporal inconsistencies and inaccuracies, or use computationally demanding temporal modeling, unsuitable for real-time applications. These limitations significantly restrict general applicability and performance in practical settings. To address this, we propose VeloDepth, an efficient and robust online video depth estimation pipeline that effectively leverages spatiotemporal priors from previous depth predictions and performs deep feature propagation. Our method introduces a novel Propagation Module that refines and propagates depth features and predictions using flow-based warping coupled with learned residual corrections. In addition, our design structurally enforces temporal consistency, resulting in stable depth predictions across consecutive frames with improved efficiency. Comprehensive zero-shot evaluation on multiple benchmarks demonstrates the state-of-the-art temporal consistency and competitive accuracy of VeloDepth, alongside its significantly faster inference compared to existing video-based depth estimators. VeloDepth thus provides a practical, efficient, and accurate solution for real-time depth estimation suitable for diverse perception tasks. Code and models are available at https://github.com/lpiccinelli-eth/velodepth",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Depth estimation in videos is essential for visual perception in real-world applications. However, existing methods either rely on simple frame-by-frame monocular models, leading to temporal inconsistencies and inaccuracies, or use computationally demanding temporal modeling, unsuitable for real-time applications. These limitations significantly restrict general applicability and performance in practical settings. To address this, we propose VeloDepth, an efficient and robust online video depth estimation pipeline that effectively leverages spatiotemporal priors from previous depth predictions and performs deep feature propagation. Our method introduces a novel Propagation Module that refines and propagates depth features and predictions using flow-based warping coupled with learned residual corrections. In addition, our design structurally enforces temporal consistency, resulting in stable depth predictions across consecutive frames with improved efficiency. Comprehensive zero-shot evaluation on multiple benchmarks demonstrates the state-of-the-art temporal consistency and competitive accuracy of VeloDepth, alongside its significantly faster inference compared to existing video-based depth estimators. VeloDepth thus provides a practical, efficient, and accurate solution for real-time depth estimation suitable for diverse perception tasks. Code and models are available at https://github.com/lpiccinelli-eth/velodepth"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T15:08:37Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    15,
                    8,
                    37,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Luigi Piccinelli"
                    },
                    {
                        "name": "Thiemo Wandel"
                    },
                    {
                        "name": "Christos Sakaridis"
                    },
                    {
                        "name": "Wim Abbeloos"
                    },
                    {
                        "name": "Luc Van Gool"
                    }
                ],
                "author_detail": {
                    "name": "Luc Van Gool"
                },
                "author": "Luc Van Gool"
            },
            {
                "id": "http://arxiv.org/abs/2512.10717v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10717v1",
                "title": "Dynamic sparse graphs with overlapping communities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic sparse graphs with overlapping communities"
                },
                "updated": "2025-12-11T14:56:05Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    14,
                    56,
                    5,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10717v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10717v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Dynamic community detection in networks addresses the challenge of tracking how groups of interconnected nodes evolve, merge, and dissolve within time-evolving networks. Here, we propose a novel statistical framework for sparse networks with power-law degree distribution and dynamic overlapping community structure. Using a Bayesian Nonparametric framework, we build on the idea to represent the graph as an exchangeable point process on the plane. We base the model construction on vectors of completely random measures and a latent Markov process for the time-evolving node affiliations. This construction provides a flexible and interpretable approach to model dynamic communities, naturally generalizing existing overlapping block models to the sparse and scale-free regimes. We provide the asymptotic properties of the model concerning sparsity and power-law behavior and propose inference through an approximate procedure which we validate empirically. We show how the model can uncover interpretable community trajectories in a real-world network.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic community detection in networks addresses the challenge of tracking how groups of interconnected nodes evolve, merge, and dissolve within time-evolving networks. Here, we propose a novel statistical framework for sparse networks with power-law degree distribution and dynamic overlapping community structure. Using a Bayesian Nonparametric framework, we build on the idea to represent the graph as an exchangeable point process on the plane. We base the model construction on vectors of completely random measures and a latent Markov process for the time-evolving node affiliations. This construction provides a flexible and interpretable approach to model dynamic communities, naturally generalizing existing overlapping block models to the sparse and scale-free regimes. We provide the asymptotic properties of the model concerning sparsity and power-law behavior and propose inference through an approximate procedure which we validate empirically. We show how the model can uncover interpretable community trajectories in a real-world network."
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T14:56:05Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    14,
                    56,
                    5,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME"
                },
                "authors": [
                    {
                        "name": "Antreas Laos"
                    },
                    {
                        "name": "Xenia Miscouridou"
                    },
                    {
                        "name": "Francesca Panero"
                    }
                ],
                "author_detail": {
                    "name": "Francesca Panero"
                },
                "author": "Francesca Panero"
            },
            {
                "id": "http://arxiv.org/abs/2506.06158v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.06158v3",
                "title": "ENMA: Tokenwise Autoregression for Generative Neural PDE Operators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ENMA: Tokenwise Autoregression for Generative Neural PDE Operators"
                },
                "updated": "2025-12-11T14:55:09Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    14,
                    55,
                    9,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.06158v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.06158v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Solving time-dependent parametric partial differential equations (PDEs) remains a fundamental challenge for neural solvers, particularly when generalizing across a wide range of physical parameters and dynamics. When data is uncertain or incomplete-as is often the case-a natural approach is to turn to generative models. We introduce ENMA, a generative neural operator designed to model spatio-temporal dynamics arising from physical phenomena. ENMA predicts future dynamics in a compressed latent space using a generative masked autoregressive transformer trained with flow matching loss, enabling tokenwise generation. Irregularly sampled spatial observations are encoded into uniform latent representations via attention mechanisms and further compressed through a spatio-temporal convolutional encoder. This allows ENMA to perform in-context learning at inference time by conditioning on either past states of the target trajectory or auxiliary context trajectories with similar dynamics. The result is a robust and adaptable framework that generalizes to new PDE regimes and supports one-shot surrogate modeling of time-dependent parametric PDEs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Solving time-dependent parametric partial differential equations (PDEs) remains a fundamental challenge for neural solvers, particularly when generalizing across a wide range of physical parameters and dynamics. When data is uncertain or incomplete-as is often the case-a natural approach is to turn to generative models. We introduce ENMA, a generative neural operator designed to model spatio-temporal dynamics arising from physical phenomena. ENMA predicts future dynamics in a compressed latent space using a generative masked autoregressive transformer trained with flow matching loss, enabling tokenwise generation. Irregularly sampled spatial observations are encoded into uniform latent representations via attention mechanisms and further compressed through a spatio-temporal convolutional encoder. This allows ENMA to perform in-context learning at inference time by conditioning on either past states of the target trajectory or auxiliary context trajectories with similar dynamics. The result is a robust and adaptable framework that generalizes to new PDE regimes and supports one-shot surrogate modeling of time-dependent parametric PDEs."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-06T15:25:14Z",
                "published_parsed": [
                    2025,
                    6,
                    6,
                    15,
                    25,
                    14,
                    4,
                    157,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Armand Kassaï Koupaï"
                    },
                    {
                        "name": "Lise Le Boudec"
                    },
                    {
                        "name": "Louis Serrano"
                    },
                    {
                        "name": "Patrick Gallinari"
                    }
                ],
                "author_detail": {
                    "name": "Patrick Gallinari"
                },
                "author": "Patrick Gallinari"
            },
            {
                "id": "http://arxiv.org/abs/2510.05802v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.05802v2",
                "title": "Assessing the Effects of Monetary Shocks on Macroeconomic Stars: A SMUC-IV Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing the Effects of Monetary Shocks on Macroeconomic Stars: A SMUC-IV Framework"
                },
                "updated": "2025-12-11T14:50:37Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    14,
                    50,
                    37,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.05802v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.05802v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This paper proposes a structural multivariate unobserved components model with external instrument (SMUC-IV) to investigate the effects of monetary policy shocks on key U.S. macroeconomic \"stars\"-namely, the level of potential output, the growth rate of potential output, trend inflation, and the neutral interest rate. A key feature of our approach is the use of an external instrument to identify monetary policy shocks within the multivariate unobserved components modeling framework. We develop an MCMC estimation method to facilitate posterior inference within our proposed SMUC-IV framework. In addition, we propose an marginal likelihood estimator to enable model comparison across alternative specifications. Our empirical analysis shows that contractionary monetary policy shocks have significant negative effects on the macroeconomic stars, highlighting the nonzero long-run effects of transitory monetary policy shocks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a structural multivariate unobserved components model with external instrument (SMUC-IV) to investigate the effects of monetary policy shocks on key U.S. macroeconomic \"stars\"-namely, the level of potential output, the growth rate of potential output, trend inflation, and the neutral interest rate. A key feature of our approach is the use of an external instrument to identify monetary policy shocks within the multivariate unobserved components modeling framework. We develop an MCMC estimation method to facilitate posterior inference within our proposed SMUC-IV framework. In addition, we propose an marginal likelihood estimator to enable model comparison across alternative specifications. Our empirical analysis shows that contractionary monetary policy shocks have significant negative effects on the macroeconomic stars, highlighting the nonzero long-run effects of transitory monetary policy shocks."
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-07T11:19:13Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    11,
                    19,
                    13,
                    1,
                    280,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM"
                },
                "authors": [
                    {
                        "name": "Bowen Fu"
                    },
                    {
                        "name": "Chenghan Hou"
                    },
                    {
                        "name": "Jan Prüser"
                    }
                ],
                "author_detail": {
                    "name": "Jan Prüser"
                },
                "author": "Jan Prüser"
            },
            {
                "id": "http://arxiv.org/abs/2512.10713v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10713v1",
                "title": "PACIFIC: a framework for generating benchmarks to check Precise Automatically Checked Instruction Following In Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PACIFIC: a framework for generating benchmarks to check Precise Automatically Checked Instruction Following In Code"
                },
                "updated": "2025-12-11T14:49:56Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    14,
                    49,
                    56,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10713v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10713v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Model (LLM)-based code assistants have emerged as a powerful application of generative AI, demonstrating impressive capabilities in code generation and comprehension. A key requirement for these systems is their ability to accurately follow user instructions. We present Precise Automatically Checked Instruction Following In Code (PACIFIC), a novel framework designed to automatically generate benchmarks that rigorously assess sequential instruction-following and code dry-running capabilities in LLMs, while allowing control over benchmark difficulty. PACIFIC produces benchmark variants with clearly defined expected outputs, enabling straightforward and reliable evaluation through simple output comparisons. In contrast to existing approaches that often rely on tool usage or agentic behavior, our work isolates and evaluates the LLM's intrinsic ability to reason through code behavior step-by-step without execution (dry running) and to follow instructions. Furthermore, our framework mitigates training data contamination by facilitating effortless generation of novel benchmark variations. We validate our framework by generating a suite of benchmarks spanning a range of difficulty levels and evaluating multiple state-of-the-art LLMs. Our results demonstrate that PACIFIC can produce increasingly challenging benchmarks that effectively differentiate instruction-following and dry running capabilities, even among advanced models. Overall, our framework offers a scalable, contamination-resilient methodology for assessing core competencies of LLMs in code-related tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM)-based code assistants have emerged as a powerful application of generative AI, demonstrating impressive capabilities in code generation and comprehension. A key requirement for these systems is their ability to accurately follow user instructions. We present Precise Automatically Checked Instruction Following In Code (PACIFIC), a novel framework designed to automatically generate benchmarks that rigorously assess sequential instruction-following and code dry-running capabilities in LLMs, while allowing control over benchmark difficulty. PACIFIC produces benchmark variants with clearly defined expected outputs, enabling straightforward and reliable evaluation through simple output comparisons. In contrast to existing approaches that often rely on tool usage or agentic behavior, our work isolates and evaluates the LLM's intrinsic ability to reason through code behavior step-by-step without execution (dry running) and to follow instructions. Furthermore, our framework mitigates training data contamination by facilitating effortless generation of novel benchmark variations. We validate our framework by generating a suite of benchmarks spanning a range of difficulty levels and evaluating multiple state-of-the-art LLMs. Our results demonstrate that PACIFIC can produce increasingly challenging benchmarks that effectively differentiate instruction-following and dry running capabilities, even among advanced models. Overall, our framework offers a scalable, contamination-resilient methodology for assessing core competencies of LLMs in code-related tasks."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T14:49:56Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    14,
                    49,
                    56,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Itay Dreyfuss"
                    },
                    {
                        "name": "Antonio Abu Nassar"
                    },
                    {
                        "name": "Samuel Ackerman"
                    },
                    {
                        "name": "Axel Ben David"
                    },
                    {
                        "name": "Rami Katan"
                    },
                    {
                        "name": "Orna Raz"
                    },
                    {
                        "name": "Marcel Zalmanovici"
                    }
                ],
                "author_detail": {
                    "name": "Marcel Zalmanovici"
                },
                "author": "Marcel Zalmanovici"
            },
            {
                "id": "http://arxiv.org/abs/2510.21737v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.21737v2",
                "title": "From Factoid Questions to Data Product Requests: Benchmarking Data Product Discovery over Tables and Text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Factoid Questions to Data Product Requests: Benchmarking Data Product Discovery over Tables and Text"
                },
                "updated": "2025-12-11T14:47:03Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    14,
                    47,
                    3,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.21737v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.21737v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Data products are reusable, self-contained assets designed for specific business use cases. Automating their discovery and generation is of great industry interest, as it enables discovery in large data lakes and supports analytical Data Product Requests (DPRs). Currently, there is no benchmark established specifically for data product discovery. Existing datasets focus on answering single factoid questions over individual tables rather than collecting multiple data assets for broader, coherent products. To address this gap, we introduce DPBench, the first user-request-driven data product benchmark over hybrid table-text corpora. Our framework systematically repurposes existing table-text QA datasets by clustering related tables and passages into coherent data products, generating professional-level analytical requests that span both data sources, and validating benchmark quality through multi-LLM evaluation. DPBench preserves full provenance while producing actionable, analyst-like data product requests. Baseline experiments with hybrid retrieval methods establish the feasibility of DPR evaluation, reveal current limitations, and point to new opportunities for automatic data product discovery research.\n  Code and datasets are available at: https://anonymous.4open.science/r/data-product-benchmark-BBA7/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data products are reusable, self-contained assets designed for specific business use cases. Automating their discovery and generation is of great industry interest, as it enables discovery in large data lakes and supports analytical Data Product Requests (DPRs). Currently, there is no benchmark established specifically for data product discovery. Existing datasets focus on answering single factoid questions over individual tables rather than collecting multiple data assets for broader, coherent products. To address this gap, we introduce DPBench, the first user-request-driven data product benchmark over hybrid table-text corpora. Our framework systematically repurposes existing table-text QA datasets by clustering related tables and passages into coherent data products, generating professional-level analytical requests that span both data sources, and validating benchmark quality through multi-LLM evaluation. DPBench preserves full provenance while producing actionable, analyst-like data product requests. Baseline experiments with hybrid retrieval methods establish the feasibility of DPR evaluation, reveal current limitations, and point to new opportunities for automatic data product discovery research.\n  Code and datasets are available at: https://anonymous.4open.science/r/data-product-benchmark-BBA7/"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-30T23:07:36Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    23,
                    7,
                    36,
                    1,
                    273,
                    0
                ],
                "arxiv_comment": "9 pages, 1 figure, 2 tables",
                "arxiv_primary_category": {
                    "term": "cs.IR"
                },
                "authors": [
                    {
                        "name": "Liangliang Zhang"
                    },
                    {
                        "name": "Nandana Mihindukulasooriya"
                    },
                    {
                        "name": "Niharika S. D'Souza"
                    },
                    {
                        "name": "Sola Shirai"
                    },
                    {
                        "name": "Sarthak Dash"
                    },
                    {
                        "name": "Yao Ma"
                    },
                    {
                        "name": "Horst Samulowitz"
                    }
                ],
                "author_detail": {
                    "name": "Horst Samulowitz"
                },
                "author": "Horst Samulowitz"
            },
            {
                "id": "http://arxiv.org/abs/2512.10696v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10696v1",
                "title": "Remember Me, Refine Me: A Dynamic Procedural Memory Framework for Experience-Driven Agent Evolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Remember Me, Refine Me: A Dynamic Procedural Memory Framework for Experience-Driven Agent Evolution"
                },
                "updated": "2025-12-11T14:40:01Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    14,
                    40,
                    1,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10696v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10696v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Procedural memory enables large language model (LLM) agents to internalize \"how-to\" knowledge, theoretically reducing redundant trial-and-error. However, existing frameworks predominantly suffer from a \"passive accumulation\" paradigm, treating memory as a static append-only archive. To bridge the gap between static storage and dynamic reasoning, we propose $\\textbf{ReMe}$ ($\\textit{Remember Me, Refine Me}$), a comprehensive framework for experience-driven agent evolution. ReMe innovates across the memory lifecycle via three mechanisms: 1) $\\textit{multi-faceted distillation}$, which extracts fine-grained experiences by recognizing success patterns, analyzing failure triggers and generating comparative insights; 2) $\\textit{context-adaptive reuse}$, which tailors historical insights to new contexts via scenario-aware indexing; and 3) $\\textit{utility-based refinement}$, which autonomously adds valid memories and prunes outdated ones to maintain a compact, high-quality experience pool. Extensive experiments on BFCL-V3 and AppWorld demonstrate that ReMe establishes a new state-of-the-art in agent memory system. Crucially, we observe a significant memory-scaling effect: Qwen3-8B equipped with ReMe outperforms larger, memoryless Qwen3-14B, suggesting that self-evolving memory provides a computation-efficient pathway for lifelong learning. We release our code and the $\\texttt{reme.library}$ dataset to facilitate further research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Procedural memory enables large language model (LLM) agents to internalize \"how-to\" knowledge, theoretically reducing redundant trial-and-error. However, existing frameworks predominantly suffer from a \"passive accumulation\" paradigm, treating memory as a static append-only archive. To bridge the gap between static storage and dynamic reasoning, we propose $\\textbf{ReMe}$ ($\\textit{Remember Me, Refine Me}$), a comprehensive framework for experience-driven agent evolution. ReMe innovates across the memory lifecycle via three mechanisms: 1) $\\textit{multi-faceted distillation}$, which extracts fine-grained experiences by recognizing success patterns, analyzing failure triggers and generating comparative insights; 2) $\\textit{context-adaptive reuse}$, which tailors historical insights to new contexts via scenario-aware indexing; and 3) $\\textit{utility-based refinement}$, which autonomously adds valid memories and prunes outdated ones to maintain a compact, high-quality experience pool. Extensive experiments on BFCL-V3 and AppWorld demonstrate that ReMe establishes a new state-of-the-art in agent memory system. Crucially, we observe a significant memory-scaling effect: Qwen3-8B equipped with ReMe outperforms larger, memoryless Qwen3-14B, suggesting that self-evolving memory provides a computation-efficient pathway for lifelong learning. We release our code and the $\\texttt{reme.library}$ dataset to facilitate further research."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T14:40:01Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    14,
                    40,
                    1,
                    3,
                    345,
                    0
                ],
                "arxiv_comment": "16 pages, 9 figures, 9 tables",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Zouying Cao"
                    },
                    {
                        "name": "Jiaji Deng"
                    },
                    {
                        "name": "Li Yu"
                    },
                    {
                        "name": "Weikang Zhou"
                    },
                    {
                        "name": "Zhaoyang Liu"
                    },
                    {
                        "name": "Bolin Ding"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao"
            },
            {
                "id": "http://arxiv.org/abs/2512.10690v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10690v1",
                "title": "On the ground state of the nonlinear Schr{ö}dinger equation: asymptotic behavior at the endpoint powers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the ground state of the nonlinear Schr{ö}dinger equation: asymptotic behavior at the endpoint powers"
                },
                "updated": "2025-12-11T14:35:45Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    14,
                    35,
                    45,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10690v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10690v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We consider the ground states of the nonlinear Schr{ö}dinger equation, which stand for radially symmetric and exponentially decaying solutions on the full space. We investigate their behaviors at both endpoint powers of the nonlinearity, up to some rescaling to infer non-trivial limits. One case corresponds to the limit towards a Gaussian function called Gausson, which is the ground state of the stationary logarithmic Schr{ö}dinger equation. The other case, for dimension at least three, corresponds to the limit towards the Aubin-Talenti algebraic soliton. We prove strong convergence with explicit bounds for both cases, and provide detailed asymptotics. These theoretical results are illustrated with numerical approximations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the ground states of the nonlinear Schr{ö}dinger equation, which stand for radially symmetric and exponentially decaying solutions on the full space. We investigate their behaviors at both endpoint powers of the nonlinearity, up to some rescaling to infer non-trivial limits. One case corresponds to the limit towards a Gaussian function called Gausson, which is the ground state of the stationary logarithmic Schr{ö}dinger equation. The other case, for dimension at least three, corresponds to the limit towards the Aubin-Talenti algebraic soliton. We prove strong convergence with explicit bounds for both cases, and provide detailed asymptotics. These theoretical results are illustrated with numerical approximations."
                },
                "tags": [
                    {
                        "term": "math.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T14:35:45Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    14,
                    35,
                    45,
                    3,
                    345,
                    0
                ],
                "arxiv_comment": "43 pages, 6 figures",
                "arxiv_primary_category": {
                    "term": "math.AP"
                },
                "authors": [
                    {
                        "name": "Rémi Carles"
                    },
                    {
                        "name": "Quentin Chauleur"
                    },
                    {
                        "name": "Guillaume Ferriere"
                    },
                    {
                        "name": "Dmitry Pelinovsky"
                    }
                ],
                "author_detail": {
                    "name": "Dmitry Pelinovsky"
                },
                "arxiv_affiliation": "LPP",
                "author": "Dmitry Pelinovsky"
            },
            {
                "id": "http://arxiv.org/abs/2512.10687v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10687v1",
                "title": "Challenges of Evaluating LLM Safety for User Welfare",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Challenges of Evaluating LLM Safety for User Welfare"
                },
                "updated": "2025-12-11T14:34:40Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    14,
                    34,
                    40,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10687v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10687v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Safety evaluations of large language models (LLMs) typically focus on universal risks like dangerous capabilities or undesirable propensities. However, millions use LLMs for personal advice on high-stakes topics like finance and health, where harms are context-dependent rather than universal. While frameworks like the OECD's AI classification recognize the need to assess individual risks, user-welfare safety evaluations remain underdeveloped. We argue that developing such evaluations is non-trivial due to fundamental questions about accounting for user context in evaluation design. In this exploratory study, we evaluated advice on finance and health from GPT-5, Claude Sonnet 4, and Gemini 2.5 Pro across user profiles of varying vulnerability. First, we demonstrate that evaluators must have access to rich user context: identical LLM responses were rated significantly safer by context-blind evaluators than by those aware of user circumstances, with safety scores for high-vulnerability users dropping from safe (5/7) to somewhat unsafe (3/7). One might assume this gap could be addressed by creating realistic user prompts containing key contextual information. However, our second study challenges this: we rerun the evaluation on prompts containing context users report they would disclose, finding no significant improvement. Our work establishes that effective user-welfare safety evaluation requires evaluators to assess responses against diverse user profiles, as realistic user context disclosure alone proves insufficient, particularly for vulnerable populations. By demonstrating a methodology for context-aware evaluation, this study provides both a starting point for such assessments and foundational evidence that evaluating individual welfare demands approaches distinct from existing universal-risk frameworks. We publish our code and dataset to aid future developments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safety evaluations of large language models (LLMs) typically focus on universal risks like dangerous capabilities or undesirable propensities. However, millions use LLMs for personal advice on high-stakes topics like finance and health, where harms are context-dependent rather than universal. While frameworks like the OECD's AI classification recognize the need to assess individual risks, user-welfare safety evaluations remain underdeveloped. We argue that developing such evaluations is non-trivial due to fundamental questions about accounting for user context in evaluation design. In this exploratory study, we evaluated advice on finance and health from GPT-5, Claude Sonnet 4, and Gemini 2.5 Pro across user profiles of varying vulnerability. First, we demonstrate that evaluators must have access to rich user context: identical LLM responses were rated significantly safer by context-blind evaluators than by those aware of user circumstances, with safety scores for high-vulnerability users dropping from safe (5/7) to somewhat unsafe (3/7). One might assume this gap could be addressed by creating realistic user prompts containing key contextual information. However, our second study challenges this: we rerun the evaluation on prompts containing context users report they would disclose, finding no significant improvement. Our work establishes that effective user-welfare safety evaluation requires evaluators to assess responses against diverse user profiles, as realistic user context disclosure alone proves insufficient, particularly for vulnerable populations. By demonstrating a methodology for context-aware evaluation, this study provides both a starting point for such assessments and foundational evidence that evaluating individual welfare demands approaches distinct from existing universal-risk frameworks. We publish our code and dataset to aid future developments."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T14:34:40Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    14,
                    34,
                    40,
                    3,
                    345,
                    0
                ],
                "arxiv_comment": "Paper accepted at IASEAI'26; please cite that peer-reviewed version instead",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Manon Kempermann"
                    },
                    {
                        "name": "Sai Suresh Macharla Vasu"
                    },
                    {
                        "name": "Mahalakshmi Raveenthiran"
                    },
                    {
                        "name": "Theo Farrell"
                    },
                    {
                        "name": "Ingmar Weber"
                    }
                ],
                "author_detail": {
                    "name": "Ingmar Weber"
                },
                "author": "Ingmar Weber"
            },
            {
                "id": "http://arxiv.org/abs/2512.10683v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10683v1",
                "title": "Optimal transport unlocks end-to-end learning for single-molecule localization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal transport unlocks end-to-end learning for single-molecule localization"
                },
                "updated": "2025-12-11T14:30:16Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    14,
                    30,
                    16,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10683v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10683v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Single-molecule localization microscopy (SMLM) allows reconstructing biology-relevant structures beyond the diffraction limit by detecting and localizing individual fluorophores -- fluorescent molecules stained onto the observed specimen -- over time to reconstruct super-resolved images. Currently, efficient SMLM requires non-overlapping emitting fluorophores, leading to long acquisition times that hinders live-cell imaging. Recent deep-learning approaches can handle denser emissions, but they rely on variants of non-maximum suppression (NMS) layers, which are unfortunately non-differentiable and may discard true positives with their local fusion strategy. In this presentation, we reformulate the SMLM training objective as a set-matching problem, deriving an optimal-transport loss that eliminates the need for NMS during inference and enables end-to-end training. Additionally, we propose an iterative neural network that integrates knowledge of the microscope's optical system inside our model. Experiments on synthetic benchmarks and real biological data show that both our new loss function and architecture surpass the state of the art at moderate and high emitter densities. Code is available at https://github.com/RSLLES/SHOT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Single-molecule localization microscopy (SMLM) allows reconstructing biology-relevant structures beyond the diffraction limit by detecting and localizing individual fluorophores -- fluorescent molecules stained onto the observed specimen -- over time to reconstruct super-resolved images. Currently, efficient SMLM requires non-overlapping emitting fluorophores, leading to long acquisition times that hinders live-cell imaging. Recent deep-learning approaches can handle denser emissions, but they rely on variants of non-maximum suppression (NMS) layers, which are unfortunately non-differentiable and may discard true positives with their local fusion strategy. In this presentation, we reformulate the SMLM training objective as a set-matching problem, deriving an optimal-transport loss that eliminates the need for NMS during inference and enables end-to-end training. Additionally, we propose an iterative neural network that integrates knowledge of the microscope's optical system inside our model. Experiments on synthetic benchmarks and real biological data show that both our new loss function and architecture surpass the state of the art at moderate and high emitter densities. Code is available at https://github.com/RSLLES/SHOT."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T14:30:16Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    14,
                    30,
                    16,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Romain Seailles"
                    },
                    {
                        "name": "Jean-Baptiste Masson"
                    },
                    {
                        "name": "Jean Ponce"
                    },
                    {
                        "name": "Julien Mairal"
                    }
                ],
                "author_detail": {
                    "name": "Julien Mairal"
                },
                "arxiv_affiliation": "LJK",
                "author": "Julien Mairal"
            },
            {
                "id": "http://arxiv.org/abs/2404.18708v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2404.18708v2",
                "title": "The Spatial Semantics of Iconic Gesture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Spatial Semantics of Iconic Gesture"
                },
                "updated": "2025-12-11T14:29:42Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    14,
                    29,
                    42,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2404.18708v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2404.18708v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The current multimodal turn in linguistic theory leaves a crucial question unanswered: what is the meaning of iconic gestures, and how does it compose with speech meaning? We argue for a separation of linguistic and visual levels of meaning and introduce a spatial gesture semantics that closes this gap. Iconicity is differentiated into three aspects: Firstly, an interpretation of the form of a gesture in terms of a translation from kinematic gesture annotations into vector sequences (iconic model). Secondly, a truth-functional evaluation of the iconic model within spatially extended domains (embedding). Since a simple embedding is too strong, we identify a number of transformations that can be applied to iconic models, namely rotation, scaling, perspective fixation, and quotation of handshape. Thirdly, the linguistic description or classification of an iconic model (informational evaluation). Since the informational evaluation of an iconic gesture is a heuristic act, it needs a place in a semantic theory of visual communication. Informational evaluation lifts a gesture to a quasi-linguistic level that can interact with verbal content. This interaction is either vacuous, or regimented by usual lexicon-driven inferences discussed in dynamic semantic frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current multimodal turn in linguistic theory leaves a crucial question unanswered: what is the meaning of iconic gestures, and how does it compose with speech meaning? We argue for a separation of linguistic and visual levels of meaning and introduce a spatial gesture semantics that closes this gap. Iconicity is differentiated into three aspects: Firstly, an interpretation of the form of a gesture in terms of a translation from kinematic gesture annotations into vector sequences (iconic model). Secondly, a truth-functional evaluation of the iconic model within spatially extended domains (embedding). Since a simple embedding is too strong, we identify a number of transformations that can be applied to iconic models, namely rotation, scaling, perspective fixation, and quotation of handshape. Thirdly, the linguistic description or classification of an iconic model (informational evaluation). Since the informational evaluation of an iconic gesture is a heuristic act, it needs a place in a semantic theory of visual communication. Informational evaluation lifts a gesture to a quasi-linguistic level that can interact with verbal content. This interaction is either vacuous, or regimented by usual lexicon-driven inferences discussed in dynamic semantic frameworks."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-04-29T13:58:03Z",
                "published_parsed": [
                    2024,
                    4,
                    29,
                    13,
                    58,
                    3,
                    0,
                    120,
                    0
                ],
                "arxiv_comment": "52 pages, 38 figures, in review",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Andy Lücking"
                    },
                    {
                        "name": "Alexander Henlein"
                    },
                    {
                        "name": "Alexander Mehler"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Mehler"
                },
                "author": "Alexander Mehler"
            },
            {
                "id": "http://arxiv.org/abs/2512.05414v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05414v3",
                "title": "LMSpell: Neural Spell Checking for Low-Resource Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LMSpell: Neural Spell Checking for Low-Resource Languages"
                },
                "updated": "2025-12-11T14:22:42Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    14,
                    22,
                    42,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05414v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05414v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Spell correction is still a challenging problem for low-resource languages (LRLs). While pretrained language models (PLMs) have been employed for spell correction, their use is still limited to a handful of languages, and there has been no proper comparison across PLMs. We present the first empirical study on the effectiveness of PLMs for spell correction, which includes LRLs. We find that Large Language Models (LLMs) outperform their counterparts (encoder-based and encoder-decoder) when the fine-tuning dataset is large. This observation holds even in languages for which the LLM is not pre-trained. We release LMSpell, an easy- to use spell correction toolkit across PLMs. It includes an evaluation function that compensates for the hallucination of LLMs. Further, we present a case study with Sinhala to shed light on the plight of spell correction for LRLs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spell correction is still a challenging problem for low-resource languages (LRLs). While pretrained language models (PLMs) have been employed for spell correction, their use is still limited to a handful of languages, and there has been no proper comparison across PLMs. We present the first empirical study on the effectiveness of PLMs for spell correction, which includes LRLs. We find that Large Language Models (LLMs) outperform their counterparts (encoder-based and encoder-decoder) when the fine-tuning dataset is large. This observation holds even in languages for which the LLM is not pre-trained. We release LMSpell, an easy- to use spell correction toolkit across PLMs. It includes an evaluation function that compensates for the hallucination of LLMs. Further, we present a case study with Sinhala to shed light on the plight of spell correction for LRLs."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T04:14:09Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    4,
                    14,
                    9,
                    4,
                    339,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Akesh Gunathilake"
                    },
                    {
                        "name": "Nadil Karunarathna"
                    },
                    {
                        "name": "Tharusha Bandaranayake"
                    },
                    {
                        "name": "Nisansa de Silva"
                    },
                    {
                        "name": "Surangika Ranathunga"
                    }
                ],
                "author_detail": {
                    "name": "Surangika Ranathunga"
                },
                "author": "Surangika Ranathunga"
            },
            {
                "id": "http://arxiv.org/abs/2511.12195v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.12195v2",
                "title": "High-impact Scientific Software in Astronomy and its creators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-impact Scientific Software in Astronomy and its creators"
                },
                "updated": "2025-12-11T14:20:27Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    14,
                    20,
                    27,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.12195v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.12195v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.3847/25c2cfeb.fe9c1f84",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "In the last decades, scientific software has graduated from a hidden side-product to a first-class member of the astrophysics literature. We aim to quantify the activity and impact of software development for astronomy, using a systematic survey. Starting from the Astrophysics Source Code Library and the Journal of Open Source Software, we analyse 3432 public git-based scientific software packages. Paper abstract text analysis suggests seven dominant themes: cosmology, data reduction pipelines, exoplanets, hydrodynamic simulations, radiative transfer spectra simulation, statistical inference and galaxies. We present key individual software contributors, their affiliated institutes and countries of high-impact software in astronomy & astrophysics. We consider the number of citations to papers using the software and the number of person-days from their git repositories, as proxies for impact and complexity, respectively. We find that half of the mapped development is through US-affiliated institutes, and a large number of high-impact projects are led by a single person. Our results indicate that there are currently over 200 people active on any given day to improve software in astronomy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the last decades, scientific software has graduated from a hidden side-product to a first-class member of the astrophysics literature. We aim to quantify the activity and impact of software development for astronomy, using a systematic survey. Starting from the Astrophysics Source Code Library and the Journal of Open Source Software, we analyse 3432 public git-based scientific software packages. Paper abstract text analysis suggests seven dominant themes: cosmology, data reduction pipelines, exoplanets, hydrodynamic simulations, radiative transfer spectra simulation, statistical inference and galaxies. We present key individual software contributors, their affiliated institutes and countries of high-impact software in astronomy & astrophysics. We consider the number of citations to papers using the software and the number of person-days from their git repositories, as proxies for impact and complexity, respectively. We find that half of the mapped development is through US-affiliated institutes, and a large number of high-impact projects are led by a single person. Our results indicate that there are currently over 200 people active on any given day to improve software in astronomy."
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-15T13:04:26Z",
                "published_parsed": [
                    2025,
                    11,
                    15,
                    13,
                    4,
                    26,
                    5,
                    319,
                    0
                ],
                "arxiv_comment": "This is metascience - research about research in astrophysics. Published in BAAS",
                "arxiv_primary_category": {
                    "term": "astro-ph.IM"
                },
                "arxiv_journal_ref": "Bulletin of the AAS, 2025, Dec 11, Volume 57",
                "authors": [
                    {
                        "name": "Johannes Buchner"
                    }
                ],
                "author_detail": {
                    "name": "Johannes Buchner"
                },
                "author": "Johannes Buchner",
                "arxiv_doi": "10.3847/25c2cfeb.fe9c1f84"
            },
            {
                "id": "http://arxiv.org/abs/2512.10674v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10674v1",
                "title": "Geo6DPose: Fast Zero-Shot 6D Object Pose Estimation via Geometry-Filtered Feature Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geo6DPose: Fast Zero-Shot 6D Object Pose Estimation via Geometry-Filtered Feature Matching"
                },
                "updated": "2025-12-11T14:20:17Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    14,
                    20,
                    17,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10674v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10674v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent progress in zero-shot 6D object pose estimation has been driven largely by large-scale models and cloud-based inference. However, these approaches often introduce high latency, elevated energy consumption, and deployment risks related to connectivity, cost, and data governance; factors that conflict with the practical constraints of real-world robotics, where compute is limited and on-device inference is frequently required. We introduce Geo6DPose, a lightweight, fully local, and training-free pipeline for zero-shot 6D pose estimation that trades model scale for geometric reliability. Our method combines foundation model visual features with a geometric filtering strategy: Similarity maps are computed between onboarded template DINO descriptors and scene patches, and mutual correspondences are established by projecting scene patch centers to 3D and template descriptors to the object model coordinate system. Final poses are recovered via correspondence-driven RANSAC and ranked using a weighted geometric alignment metric that jointly accounts for reprojection consistency and spatial support, improving robustness to noise, clutter, and partial visibility. Geo6DPose achieves sub-second inference on a single commodity GPU while matching the average recall of significantly larger zero-shot baselines (53.7 AR, 1.08 FPS). It requires no training, fine-tuning, or network access, and remains compatible with evolving foundation backbones, advancing practical, fully local 6D perception for robotic deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent progress in zero-shot 6D object pose estimation has been driven largely by large-scale models and cloud-based inference. However, these approaches often introduce high latency, elevated energy consumption, and deployment risks related to connectivity, cost, and data governance; factors that conflict with the practical constraints of real-world robotics, where compute is limited and on-device inference is frequently required. We introduce Geo6DPose, a lightweight, fully local, and training-free pipeline for zero-shot 6D pose estimation that trades model scale for geometric reliability. Our method combines foundation model visual features with a geometric filtering strategy: Similarity maps are computed between onboarded template DINO descriptors and scene patches, and mutual correspondences are established by projecting scene patch centers to 3D and template descriptors to the object model coordinate system. Final poses are recovered via correspondence-driven RANSAC and ranked using a weighted geometric alignment metric that jointly accounts for reprojection consistency and spatial support, improving robustness to noise, clutter, and partial visibility. Geo6DPose achieves sub-second inference on a single commodity GPU while matching the average recall of significantly larger zero-shot baselines (53.7 AR, 1.08 FPS). It requires no training, fine-tuning, or network access, and remains compatible with evolving foundation backbones, advancing practical, fully local 6D perception for robotic deployment."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T14:20:17Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    14,
                    20,
                    17,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Javier Villena Toro"
                    },
                    {
                        "name": "Mehdi Tarkian"
                    }
                ],
                "author_detail": {
                    "name": "Mehdi Tarkian"
                },
                "author": "Mehdi Tarkian"
            },
            {
                "id": "http://arxiv.org/abs/2512.10668v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10668v1",
                "title": "XDen-1K: A Density Field Dataset of Real-World Objects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XDen-1K: A Density Field Dataset of Real-World Objects"
                },
                "updated": "2025-12-11T14:15:42Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    14,
                    15,
                    42,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10668v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10668v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "A deep understanding of the physical world is a central goal for embodied AI and realistic simulation. While current models excel at capturing an object's surface geometry and appearance, they largely neglect its internal physical properties. This omission is critical, as properties like volumetric density are fundamental for predicting an object's center of mass, stability, and interaction dynamics in applications ranging from robotic manipulation to physical simulation. The primary bottleneck has been the absence of large-scale, real-world data. To bridge this gap, we introduce XDen-1K, the first large-scale, multi-modal dataset designed for real-world physical property estimation, with a particular focus on volumetric density. The core of this dataset consists of 1,000 real-world objects across 148 categories, for which we provide comprehensive multi-modal data, including a high-resolution 3D geometric model with part-level annotations and a corresponding set of real-world biplanar X-ray scans. Building upon this data, we introduce a novel optimization framework that recovers a high-fidelity volumetric density field of each object from its sparse X-ray views. To demonstrate its practical value, we add X-ray images as a conditioning signal to an existing segmentation network and perform volumetric segmentation. Furthermore, we conduct experiments on downstream robotics tasks. The results show that leveraging the dataset can effectively improve the accuracy of center-of-mass estimation and the success rate of robotic manipulation. We believe XDen-1K will serve as a foundational resource and a challenging new benchmark, catalyzing future research in physically grounded visual inference and embodied AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A deep understanding of the physical world is a central goal for embodied AI and realistic simulation. While current models excel at capturing an object's surface geometry and appearance, they largely neglect its internal physical properties. This omission is critical, as properties like volumetric density are fundamental for predicting an object's center of mass, stability, and interaction dynamics in applications ranging from robotic manipulation to physical simulation. The primary bottleneck has been the absence of large-scale, real-world data. To bridge this gap, we introduce XDen-1K, the first large-scale, multi-modal dataset designed for real-world physical property estimation, with a particular focus on volumetric density. The core of this dataset consists of 1,000 real-world objects across 148 categories, for which we provide comprehensive multi-modal data, including a high-resolution 3D geometric model with part-level annotations and a corresponding set of real-world biplanar X-ray scans. Building upon this data, we introduce a novel optimization framework that recovers a high-fidelity volumetric density field of each object from its sparse X-ray views. To demonstrate its practical value, we add X-ray images as a conditioning signal to an existing segmentation network and perform volumetric segmentation. Furthermore, we conduct experiments on downstream robotics tasks. The results show that leveraging the dataset can effectively improve the accuracy of center-of-mass estimation and the success rate of robotic manipulation. We believe XDen-1K will serve as a foundational resource and a challenging new benchmark, catalyzing future research in physically grounded visual inference and embodied AI."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T14:15:42Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    14,
                    15,
                    42,
                    3,
                    345,
                    0
                ],
                "arxiv_comment": "10 pages, 7 figures",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Jingxuan Zhang"
                    },
                    {
                        "name": "Tianqi Yu"
                    },
                    {
                        "name": "Yatu Zhang"
                    },
                    {
                        "name": "Jinze Wu"
                    },
                    {
                        "name": "Kaixin Yao"
                    },
                    {
                        "name": "Jingyang Liu"
                    },
                    {
                        "name": "Yuyao Zhang"
                    },
                    {
                        "name": "Jiayuan Gu"
                    },
                    {
                        "name": "Jingyi Yu"
                    }
                ],
                "author_detail": {
                    "name": "Jingyi Yu"
                },
                "author": "Jingyi Yu"
            },
            {
                "id": "http://arxiv.org/abs/2512.10665v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10665v1",
                "title": "On the Dynamics of Multi-Agent LLM Communities Driven by Value Diversity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Dynamics of Multi-Agent LLM Communities Driven by Value Diversity"
                },
                "updated": "2025-12-11T14:13:53Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    14,
                    13,
                    53,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10665v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10665v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As Large Language Models (LLM) based multi-agent systems become increasingly prevalent, the collective behaviors, e.g., collective intelligence, of such artificial communities have drawn growing attention. This work aims to answer a fundamental question: How does diversity of values shape the collective behavior of AI communities? Using naturalistic value elicitation grounded in the prevalent Schwartz's Theory of Basic Human Values, we constructed multi-agent simulations where communities with varying numbers of agents engaged in open-ended interactions and constitution formation. The results show that value diversity enhances value stability, fosters emergent behaviors, and brings more creative principles developed by the agents themselves without external guidance. However, these effects also show diminishing returns: extreme heterogeneity induces instability. This work positions value diversity as a new axis of future AI capability, bridging AI ability and sociological studies of institutional emergence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLM) based multi-agent systems become increasingly prevalent, the collective behaviors, e.g., collective intelligence, of such artificial communities have drawn growing attention. This work aims to answer a fundamental question: How does diversity of values shape the collective behavior of AI communities? Using naturalistic value elicitation grounded in the prevalent Schwartz's Theory of Basic Human Values, we constructed multi-agent simulations where communities with varying numbers of agents engaged in open-ended interactions and constitution formation. The results show that value diversity enhances value stability, fosters emergent behaviors, and brings more creative principles developed by the agents themselves without external guidance. However, these effects also show diminishing returns: extreme heterogeneity induces instability. This work positions value diversity as a new axis of future AI capability, bridging AI ability and sociological studies of institutional emergence."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T14:13:53Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    14,
                    13,
                    53,
                    3,
                    345,
                    0
                ],
                "arxiv_comment": "Working Paper",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Muhua Huang"
                    },
                    {
                        "name": "Qinlin Zhao"
                    },
                    {
                        "name": "Xiaoyuan Yi"
                    },
                    {
                        "name": "Xing Xie"
                    }
                ],
                "author_detail": {
                    "name": "Xing Xie"
                },
                "author": "Xing Xie"
            },
            {
                "id": "http://arxiv.org/abs/2512.10655v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10655v1",
                "title": "CAPTAIN: Semantic Feature Injection for Memorization Mitigation in Text-to-Image Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAPTAIN: Semantic Feature Injection for Memorization Mitigation in Text-to-Image Diffusion Models"
                },
                "updated": "2025-12-11T14:01:47Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    14,
                    1,
                    47,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10655v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10655v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion models can unintentionally reproduce training examples, raising privacy and copyright concerns as these systems are increasingly deployed at scale. Existing inference-time mitigation methods typically manipulate classifier-free guidance (CFG) or perturb prompt embeddings; however, they often struggle to reduce memorization without compromising alignment with the conditioning prompt. We introduce CAPTAIN, a training-free framework that mitigates memorization by directly modifying latent features during denoising. CAPTAIN first applies frequency-based noise initialization to reduce the tendency to replicate memorized patterns early in the denoising process. It then identifies the optimal denoising timesteps for feature injection and localizes memorized regions. Finally, CAPTAIN injects semantically aligned features from non-memorized reference images into localized latent regions, suppressing memorization while preserving prompt fidelity and visual quality. Our experiments show that CAPTAIN achieves substantial reductions in memorization compared to CFG-based baselines while maintaining strong alignment with the intended prompt.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models can unintentionally reproduce training examples, raising privacy and copyright concerns as these systems are increasingly deployed at scale. Existing inference-time mitigation methods typically manipulate classifier-free guidance (CFG) or perturb prompt embeddings; however, they often struggle to reduce memorization without compromising alignment with the conditioning prompt. We introduce CAPTAIN, a training-free framework that mitigates memorization by directly modifying latent features during denoising. CAPTAIN first applies frequency-based noise initialization to reduce the tendency to replicate memorized patterns early in the denoising process. It then identifies the optimal denoising timesteps for feature injection and localizes memorized regions. Finally, CAPTAIN injects semantically aligned features from non-memorized reference images into localized latent regions, suppressing memorization while preserving prompt fidelity and visual quality. Our experiments show that CAPTAIN achieves substantial reductions in memorization compared to CFG-based baselines while maintaining strong alignment with the intended prompt."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T14:01:47Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    14,
                    1,
                    47,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Tong Zhang"
                    },
                    {
                        "name": "Carlos Hinojosa"
                    },
                    {
                        "name": "Bernard Ghanem"
                    }
                ],
                "author_detail": {
                    "name": "Bernard Ghanem"
                },
                "author": "Bernard Ghanem"
            },
            {
                "id": "http://arxiv.org/abs/2508.08139v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.08139v2",
                "title": "Can LLMs Detect Their Confabulations? Estimating Reliability in Uncertainty-Aware Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Detect Their Confabulations? Estimating Reliability in Uncertainty-Aware Language Models"
                },
                "updated": "2025-12-11T13:49:54Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    13,
                    49,
                    54,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.08139v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.08139v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) are prone to generating fluent but incorrect content, known as confabulation, which poses increasing risks in multi-turn or agentic applications where outputs may be reused as context. In this work, we investigate how in-context information influences model behavior and whether LLMs can identify their unreliable responses. We propose a reliability estimation that leverages token-level uncertainty to guide the aggregation of internal model representations. Specifically, we compute aleatoric and epistemic uncertainty from output logits to identify salient tokens and aggregate their hidden states into compact representations for response-level reliability prediction. Through controlled experiments on open QA benchmarks, we find that correct in-context information improves both answer accuracy and model confidence, while misleading context often induces confidently incorrect responses, revealing a misalignment between uncertainty and correctness. Our probing-based method captures these shifts in model behavior and improves the detection of unreliable outputs across multiple open-source LLMs. These results underscore the limitations of direct uncertainty signals and highlight the potential of uncertainty-guided probing for reliability-aware generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are prone to generating fluent but incorrect content, known as confabulation, which poses increasing risks in multi-turn or agentic applications where outputs may be reused as context. In this work, we investigate how in-context information influences model behavior and whether LLMs can identify their unreliable responses. We propose a reliability estimation that leverages token-level uncertainty to guide the aggregation of internal model representations. Specifically, we compute aleatoric and epistemic uncertainty from output logits to identify salient tokens and aggregate their hidden states into compact representations for response-level reliability prediction. Through controlled experiments on open QA benchmarks, we find that correct in-context information improves both answer accuracy and model confidence, while misleading context often induces confidently incorrect responses, revealing a misalignment between uncertainty and correctness. Our probing-based method captures these shifts in model behavior and improves the detection of unreliable outputs across multiple open-source LLMs. These results underscore the limitations of direct uncertainty signals and highlight the potential of uncertainty-guided probing for reliability-aware generation."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-11T16:12:36Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    16,
                    12,
                    36,
                    0,
                    223,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Tianyi Zhou"
                    },
                    {
                        "name": "Johanne Medina"
                    },
                    {
                        "name": "Sanjay Chawla"
                    }
                ],
                "author_detail": {
                    "name": "Sanjay Chawla"
                },
                "author": "Sanjay Chawla"
            },
            {
                "id": "http://arxiv.org/abs/2510.08158v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.08158v2",
                "title": "Beyond Over-Refusal: Scenario-Based Diagnostics and Post-Hoc Mitigation for Exaggerated Refusals in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Over-Refusal: Scenario-Based Diagnostics and Post-Hoc Mitigation for Exaggerated Refusals in LLMs"
                },
                "updated": "2025-12-11T13:48:33Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    13,
                    48,
                    33,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.08158v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.08158v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) frequently produce false refusals, declining benign requests that contain terms resembling unsafe queries. We address this challenge by introducing two comprehensive benchmarks: the Exaggerated Safety Benchmark (XSB) for single-turn prompts, annotated with \"Focus\" keywords that identify refusal-inducing triggers, and the Multi-turn Scenario-based Exaggerated Safety Benchmark (MS-XSB), which systematically evaluates refusal calibration in realistic, context-rich dialog settings. Our benchmarks reveal that exaggerated refusals persist across diverse recent LLMs and are especially pronounced in complex, multi-turn scenarios. To mitigate these failures, we leverage post-hoc explanation methods to identify refusal triggers and deploy three lightweight, model-agnostic approaches, ignore-word instructions, prompt rephrasing, and attention steering, at inference time, all without retraining or parameter access. Experiments on four instruction-tuned Llama models demonstrate that these strategies substantially improve compliance on safe prompts while maintaining robust safety protections. Our findings establish a reproducible framework for diagnosing and mitigating exaggerated refusals, highlighting practical pathways to safer and more helpful LLM deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) frequently produce false refusals, declining benign requests that contain terms resembling unsafe queries. We address this challenge by introducing two comprehensive benchmarks: the Exaggerated Safety Benchmark (XSB) for single-turn prompts, annotated with \"Focus\" keywords that identify refusal-inducing triggers, and the Multi-turn Scenario-based Exaggerated Safety Benchmark (MS-XSB), which systematically evaluates refusal calibration in realistic, context-rich dialog settings. Our benchmarks reveal that exaggerated refusals persist across diverse recent LLMs and are especially pronounced in complex, multi-turn scenarios. To mitigate these failures, we leverage post-hoc explanation methods to identify refusal triggers and deploy three lightweight, model-agnostic approaches, ignore-word instructions, prompt rephrasing, and attention steering, at inference time, all without retraining or parameter access. Experiments on four instruction-tuned Llama models demonstrate that these strategies substantially improve compliance on safe prompts while maintaining robust safety protections. Our findings establish a reproducible framework for diagnosing and mitigating exaggerated refusals, highlighting practical pathways to safer and more helpful LLM deployments."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-09T12:38:16Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    12,
                    38,
                    16,
                    3,
                    282,
                    0
                ],
                "arxiv_comment": "Errors in the paper",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Shuzhou Yuan"
                    },
                    {
                        "name": "Ercong Nie"
                    },
                    {
                        "name": "Yinuo Sun"
                    },
                    {
                        "name": "Chenxuan Zhao"
                    },
                    {
                        "name": "William LaCroix"
                    },
                    {
                        "name": "Michael Färber"
                    }
                ],
                "author_detail": {
                    "name": "Michael Färber"
                },
                "author": "Michael Färber"
            },
            {
                "id": "http://arxiv.org/abs/2505.01730v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.01730v2",
                "title": "PASCAL: Precise and Efficient ANN- SNN Conversion using Spike Accumulation and Adaptive Layerwise Activation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PASCAL: Precise and Efficient ANN- SNN Conversion using Spike Accumulation and Adaptive Layerwise Activation"
                },
                "updated": "2025-12-11T13:45:50Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    13,
                    45,
                    50,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.01730v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.01730v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Spiking Neural Networks (SNNs) have been put forward as an energy-efficient alternative to Artificial Neural Networks (ANNs) since they perform sparse Accumulate operations instead of the power-hungry Multiply-and-Accumulate operations. ANN-SNN conversion is a widely used method to realize deep SNNs with accuracy comparable to that of ANNs.~\\citeauthor{bu2023optimal} recently proposed the Quantization-Clip-Floor-Shift (QCFS) activation as an alternative to ReLU to minimize the accuracy loss during ANN-SNN conversion. Nevertheless, SNN inferencing requires a large number of timesteps to match the accuracy of the source ANN for real-world datasets. In this work, we propose PASCAL, which performs ANN-SNN conversion in such a way that the resulting SNN is mathematically equivalent to an ANN with QCFS-activation, thereby yielding similar accuracy as the source ANN with minimal inference timesteps. In addition, we propose a systematic method to configure the quantization step of QCFS activation in a layerwise manner, which effectively determines the optimal number of timesteps per layer for the converted SNN. Our results show that the ResNet-34 SNN obtained using PASCAL achieves an accuracy of $\\approx$74\\% on ImageNet with a 64$\\times$ reduction in the number of inference timesteps compared to existing approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spiking Neural Networks (SNNs) have been put forward as an energy-efficient alternative to Artificial Neural Networks (ANNs) since they perform sparse Accumulate operations instead of the power-hungry Multiply-and-Accumulate operations. ANN-SNN conversion is a widely used method to realize deep SNNs with accuracy comparable to that of ANNs.~\\citeauthor{bu2023optimal} recently proposed the Quantization-Clip-Floor-Shift (QCFS) activation as an alternative to ReLU to minimize the accuracy loss during ANN-SNN conversion. Nevertheless, SNN inferencing requires a large number of timesteps to match the accuracy of the source ANN for real-world datasets. In this work, we propose PASCAL, which performs ANN-SNN conversion in such a way that the resulting SNN is mathematically equivalent to an ANN with QCFS-activation, thereby yielding similar accuracy as the source ANN with minimal inference timesteps. In addition, we propose a systematic method to configure the quantization step of QCFS activation in a layerwise manner, which effectively determines the optimal number of timesteps per layer for the converted SNN. Our results show that the ResNet-34 SNN obtained using PASCAL achieves an accuracy of $\\approx$74\\% on ImageNet with a 64$\\times$ reduction in the number of inference timesteps compared to existing approaches."
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-03T07:55:29Z",
                "published_parsed": [
                    2025,
                    5,
                    3,
                    7,
                    55,
                    29,
                    5,
                    123,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE"
                },
                "arxiv_journal_ref": "Transactions on Machine Learning Research, 2025",
                "authors": [
                    {
                        "name": "Pranav Ramesh"
                    },
                    {
                        "name": "Gopalakrishnan Srinivasan"
                    }
                ],
                "author_detail": {
                    "name": "Gopalakrishnan Srinivasan"
                },
                "author": "Gopalakrishnan Srinivasan"
            },
            {
                "id": "http://arxiv.org/abs/2512.10638v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10638v1",
                "title": "A Spiking Neural Network Implementation of Gaussian Belief Propagation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Spiking Neural Network Implementation of Gaussian Belief Propagation"
                },
                "updated": "2025-12-11T13:43:42Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    13,
                    43,
                    42,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10638v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10638v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Bayesian inference offers a principled account of information processing in natural agents. However, it remains an open question how neural mechanisms perform their abstract operations. We investigate a hypothesis where a distributed form of Bayesian inference, namely message passing on factor graphs, is performed by a simulated network of leaky-integrate-and-fire neurons. Specifically, we perform Gaussian belief propagation by encoding messages that come into factor nodes as spike-based signals, propagating these signals through a spiking neural network (SNN) and decoding the spike-based signal back to an outgoing message. Three core linear operations, equality (branching), addition, and multiplication, are realized in networks of leaky integrate-and-fire models. Validation against the standard sum-product algorithm shows accurate message updates, while applications to Kalman filtering and Bayesian linear regression demonstrate the framework's potential for both static and dynamic inference tasks. Our results provide a step toward biologically grounded, neuromorphic implementations of probabilistic reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian inference offers a principled account of information processing in natural agents. However, it remains an open question how neural mechanisms perform their abstract operations. We investigate a hypothesis where a distributed form of Bayesian inference, namely message passing on factor graphs, is performed by a simulated network of leaky-integrate-and-fire neurons. Specifically, we perform Gaussian belief propagation by encoding messages that come into factor nodes as spike-based signals, propagating these signals through a spiking neural network (SNN) and decoding the spike-based signal back to an outgoing message. Three core linear operations, equality (branching), addition, and multiplication, are realized in networks of leaky integrate-and-fire models. Validation against the standard sum-product algorithm shows accurate message updates, while applications to Kalman filtering and Bayesian linear regression demonstrate the framework's potential for both static and dynamic inference tasks. Our results provide a step toward biologically grounded, neuromorphic implementations of probabilistic reasoning."
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T13:43:42Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    13,
                    43,
                    42,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE"
                },
                "authors": [
                    {
                        "name": "Sepideh Adamiat"
                    },
                    {
                        "name": "Wouter M. Kouw"
                    },
                    {
                        "name": "Bert de Vries"
                    }
                ],
                "author_detail": {
                    "name": "Bert de Vries"
                },
                "author": "Bert de Vries"
            },
            {
                "id": "http://arxiv.org/abs/2512.10630v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10630v1",
                "title": "From Data Scarcity to Data Care: Reimagining Language Technologies for Serbian and other Low-Resource Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Data Scarcity to Data Care: Reimagining Language Technologies for Serbian and other Low-Resource Languages"
                },
                "updated": "2025-12-11T13:29:25Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    13,
                    29,
                    25,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10630v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10630v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models are commonly trained on dominant languages like English, and their representation of low resource languages typically reflects cultural and linguistic biases present in the source language materials. Using the Serbian language as a case, this study examines the structural, historical, and sociotechnical factors shaping language technology development for low resource languages in the AI age. Drawing on semi structured interviews with ten scholars and practitioners, including linguists, digital humanists, and AI developers, it traces challenges rooted in historical destruction of Serbian textual heritage, intensified by contemporary issues that drive reductive, engineering first approaches prioritizing functionality over linguistic nuance. These include superficial transliteration, reliance on English-trained models, data bias, and dataset curation lacking cultural specificity. To address these challenges, the study proposes Data Care, a framework grounded in CARE principles (Collective Benefit, Authority to Control, Responsibility, and Ethics), that reframes bias mitigation from a post hoc technical fix to an integral component of corpus design, annotation, and governance, and positions Data Care as a replicable model for building inclusive, sustainable, and culturally grounded language technologies in contexts where traditional LLM development reproduces existing power imbalances and cultural blind spots.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models are commonly trained on dominant languages like English, and their representation of low resource languages typically reflects cultural and linguistic biases present in the source language materials. Using the Serbian language as a case, this study examines the structural, historical, and sociotechnical factors shaping language technology development for low resource languages in the AI age. Drawing on semi structured interviews with ten scholars and practitioners, including linguists, digital humanists, and AI developers, it traces challenges rooted in historical destruction of Serbian textual heritage, intensified by contemporary issues that drive reductive, engineering first approaches prioritizing functionality over linguistic nuance. These include superficial transliteration, reliance on English-trained models, data bias, and dataset curation lacking cultural specificity. To address these challenges, the study proposes Data Care, a framework grounded in CARE principles (Collective Benefit, Authority to Control, Responsibility, and Ethics), that reframes bias mitigation from a post hoc technical fix to an integral component of corpus design, annotation, and governance, and positions Data Care as a replicable model for building inclusive, sustainable, and culturally grounded language technologies in contexts where traditional LLM development reproduces existing power imbalances and cultural blind spots."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T13:29:25Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    13,
                    29,
                    25,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Smiljana Antonijevic Ubois"
                    }
                ],
                "author_detail": {
                    "name": "Smiljana Antonijevic Ubois"
                },
                "author": "Smiljana Antonijevic Ubois"
            },
            {
                "id": "http://arxiv.org/abs/2504.11840v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2504.11840v2",
                "title": "GT-SNT: A Linear-Time Transformer for Large-Scale Graphs via Spiking Node Tokenization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GT-SNT: A Linear-Time Transformer for Large-Scale Graphs via Spiking Node Tokenization"
                },
                "updated": "2025-12-11T13:28:05Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    13,
                    28,
                    5,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2504.11840v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2504.11840v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Graph Transformers (GTs), which integrate message passing and self-attention mechanisms simultaneously, have achieved promising empirical results in graph prediction tasks. However, the design of scalable and topology-aware node tokenization has lagged behind other modalities. This gap becomes critical as the quadratic complexity of full attention renders them impractical on large-scale graphs. Recently, Spiking Neural Networks (SNNs), as brain-inspired models, provided an energy-saving scheme to convert input intensity into discrete spike-based representations through event-driven spiking neurons. Inspired by these characteristics, we propose a linear-time Graph Transformer with Spiking Node Tokenization (GT-SNT) for node classification. By integrating multi-step feature propagation with SNNs, spiking node tokenization generates compact, locality-aware spike count embeddings as node tokens to avoid predefined codebooks and their utilization issues. The codebook guided self-attention leverages these tokens to perform node-to-token attention for linear-time global context aggregation. In experiments, we compare GT-SNT with other state-of-the-art baselines on node classification datasets ranging from small to large. Experimental results show that GT-SNT achieves comparable performances on most datasets and reaches up to 130x faster inference speed compared to other GTs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Transformers (GTs), which integrate message passing and self-attention mechanisms simultaneously, have achieved promising empirical results in graph prediction tasks. However, the design of scalable and topology-aware node tokenization has lagged behind other modalities. This gap becomes critical as the quadratic complexity of full attention renders them impractical on large-scale graphs. Recently, Spiking Neural Networks (SNNs), as brain-inspired models, provided an energy-saving scheme to convert input intensity into discrete spike-based representations through event-driven spiking neurons. Inspired by these characteristics, we propose a linear-time Graph Transformer with Spiking Node Tokenization (GT-SNT) for node classification. By integrating multi-step feature propagation with SNNs, spiking node tokenization generates compact, locality-aware spike count embeddings as node tokens to avoid predefined codebooks and their utilization issues. The codebook guided self-attention leverages these tokens to perform node-to-token attention for linear-time global context aggregation. In experiments, we compare GT-SNT with other state-of-the-art baselines on node classification datasets ranging from small to large. Experimental results show that GT-SNT achieves comparable performances on most datasets and reaches up to 130x faster inference speed compared to other GTs."
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-04-16T07:57:42Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    7,
                    57,
                    42,
                    2,
                    106,
                    0
                ],
                "arxiv_comment": "Accepted by AAAI 2026; Code is available at https://github.com/Zhhuizhe/GT-SNT",
                "arxiv_primary_category": {
                    "term": "cs.NE"
                },
                "authors": [
                    {
                        "name": "Huizhe Zhang"
                    },
                    {
                        "name": "Jintang Li"
                    },
                    {
                        "name": "Yuchang Zhu"
                    },
                    {
                        "name": "Huazhen Zhong"
                    },
                    {
                        "name": "Liang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Liang Chen"
                },
                "author": "Liang Chen"
            },
            {
                "id": "http://arxiv.org/abs/2512.10628v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10628v1",
                "title": "K-Track: Kalman-Enhanced Tracking for Accelerating Deep Point Trackers on Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "K-Track: Kalman-Enhanced Tracking for Accelerating Deep Point Trackers on Edge Devices"
                },
                "updated": "2025-12-11T13:26:58Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    13,
                    26,
                    58,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10628v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10628v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Point tracking in video sequences is a foundational capability for real-world computer vision applications, including robotics, autonomous systems, augmented reality, and video analysis. While recent deep learning-based trackers achieve state-of-the-art accuracy on challenging benchmarks, their reliance on per-frame GPU inference poses a major barrier to deployment on resource-constrained edge devices, where compute, power, and connectivity are limited. We introduce K-Track (Kalman-enhanced Tracking), a general-purpose, tracker-agnostic acceleration framework designed to bridge this deployment gap. K-Track reduces inference cost by combining sparse deep learning keyframe updates with lightweight Kalman filtering for intermediate frame prediction, using principled Bayesian uncertainty propagation to maintain temporal coherence. This hybrid strategy enables 5-10X speedup while retaining over 85% of the original trackers' accuracy. We evaluate K-Track across multiple state-of-the-art point trackers and demonstrate real-time performance on edge platforms such as the NVIDIA Jetson Nano and RTX Titan. By preserving accuracy while dramatically lowering computational requirements, K-Track provides a practical path toward deploying high-quality point tracking in real-world, resource-limited settings, closing the gap between modern tracking algorithms and deployable vision systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Point tracking in video sequences is a foundational capability for real-world computer vision applications, including robotics, autonomous systems, augmented reality, and video analysis. While recent deep learning-based trackers achieve state-of-the-art accuracy on challenging benchmarks, their reliance on per-frame GPU inference poses a major barrier to deployment on resource-constrained edge devices, where compute, power, and connectivity are limited. We introduce K-Track (Kalman-enhanced Tracking), a general-purpose, tracker-agnostic acceleration framework designed to bridge this deployment gap. K-Track reduces inference cost by combining sparse deep learning keyframe updates with lightweight Kalman filtering for intermediate frame prediction, using principled Bayesian uncertainty propagation to maintain temporal coherence. This hybrid strategy enables 5-10X speedup while retaining over 85% of the original trackers' accuracy. We evaluate K-Track across multiple state-of-the-art point trackers and demonstrate real-time performance on edge platforms such as the NVIDIA Jetson Nano and RTX Titan. By preserving accuracy while dramatically lowering computational requirements, K-Track provides a practical path toward deploying high-quality point tracking in real-world, resource-limited settings, closing the gap between modern tracking algorithms and deployable vision systems."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T13:26:58Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    13,
                    26,
                    58,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Bishoy Galoaa"
                    },
                    {
                        "name": "Pau Closas"
                    },
                    {
                        "name": "Sarah Ostadabbas"
                    }
                ],
                "author_detail": {
                    "name": "Sarah Ostadabbas"
                },
                "author": "Sarah Ostadabbas"
            },
            {
                "id": "http://arxiv.org/abs/2512.09724v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.09724v2",
                "title": "Bayesian Model Selection with an Application to Cosmology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Model Selection with an Application to Cosmology"
                },
                "updated": "2025-12-11T13:15:36Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    13,
                    15,
                    36,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.09724v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.09724v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We investigate cosmological parameter inference and model selection from a Bayesian perspective. Type Ia supernova data from the Dark Energy Survey (DES-SN5YR) are used to test the $Λ$CDM, $w$CDM, and CPL cosmological models. Posterior inference is performed via Hamiltonian Monte Carlo using the No-U-Turn Sampler (NUTS) implemented in NumPyro and analyzed with ArviZ in Python. Bayesian model comparison is conducted through Bayes factors computed using the bridgesampling library in R. The results indicate that all three models demonstrate similar predictive performance, but $w$CDM shows stronger evidence relative to $Λ$CDM and CPL. We conclude that, under the assumptions and data used in this study, $w$CDM provides a better description of cosmological expansion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate cosmological parameter inference and model selection from a Bayesian perspective. Type Ia supernova data from the Dark Energy Survey (DES-SN5YR) are used to test the $Λ$CDM, $w$CDM, and CPL cosmological models. Posterior inference is performed via Hamiltonian Monte Carlo using the No-U-Turn Sampler (NUTS) implemented in NumPyro and analyzed with ArviZ in Python. Bayesian model comparison is conducted through Bayes factors computed using the bridgesampling library in R. The results indicate that all three models demonstrate similar predictive performance, but $w$CDM shows stronger evidence relative to $Λ$CDM and CPL. We conclude that, under the assumptions and data used in this study, $w$CDM provides a better description of cosmological expansion."
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-10T15:06:09Z",
                "published_parsed": [
                    2025,
                    12,
                    10,
                    15,
                    6,
                    9,
                    2,
                    344,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP"
                },
                "authors": [
                    {
                        "name": "Nikoloz Gigiberia"
                    }
                ],
                "author_detail": {
                    "name": "Nikoloz Gigiberia"
                },
                "author": "Nikoloz Gigiberia"
            },
            {
                "id": "http://arxiv.org/abs/2512.10616v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10616v1",
                "title": "Stellar masses and mass ratios for Gaia open cluster members",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stellar masses and mass ratios for Gaia open cluster members"
                },
                "updated": "2025-12-11T13:12:35Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    13,
                    12,
                    35,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10616v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10616v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Context: Unresolved binaries in star clusters can bias stellar and cluster mass estimates, making their proper treatment essential for studying cluster dynamics and evolution. Aims: We aim to develop a fast and robust framework for jointly deriving stellar masses and multiplicity statistics of member stars, together with optimal cluster parameters. Methods: We use Gaia DR3 parallaxes together with multi-band photometry of open cluster (OC) members to infer stellar masses and binary mass-ratios through simulation-based inference (SBI), while iteratively fitting the cluster parameters. The validation of our SBI framework on simulated clusters demonstrates that the inclusion of infrared photometry significantly improves the detection of low mass-ratio binaries. The minimum mass-ratio threshold for reliably identifying unresolved binaries depends on cluster properties and the available photometry, but typically lies below $q=0.5$. Results: Applying our method to 42 well-populated OCs, we derive a catalogue of stellar masses and mass-ratios for 27201 stars, achieving typical uncertainties of 0.08 in $q$ and $0.01\\,\\mathrm{M}_\\odot$ in the primary stellar mass. We analyse the archetype OCs M67 and NGC 2360 in detail, including mass segregation and mass-ratio distribution among other characteristics, while deriving multiplicity fractions for the rest of the sample. We find evidence that the high mass-ratio ($q\\geq 0.6$) binary fraction shows a strong correlation with the age and a weak anti-correlation with the cluster metallicity. Furthermore, the variation of the binary fraction with stellar mass in OCs shows strong accordance with the observed dependence for field stars heavier than $\\gtrsim0.6\\,\\mathrm{M}_\\odot$. Conclusions: Our work paves a path for future population-level investigations of multiplicity statistics and precision stellar masses in extended samples of OCs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context: Unresolved binaries in star clusters can bias stellar and cluster mass estimates, making their proper treatment essential for studying cluster dynamics and evolution. Aims: We aim to develop a fast and robust framework for jointly deriving stellar masses and multiplicity statistics of member stars, together with optimal cluster parameters. Methods: We use Gaia DR3 parallaxes together with multi-band photometry of open cluster (OC) members to infer stellar masses and binary mass-ratios through simulation-based inference (SBI), while iteratively fitting the cluster parameters. The validation of our SBI framework on simulated clusters demonstrates that the inclusion of infrared photometry significantly improves the detection of low mass-ratio binaries. The minimum mass-ratio threshold for reliably identifying unresolved binaries depends on cluster properties and the available photometry, but typically lies below $q=0.5$. Results: Applying our method to 42 well-populated OCs, we derive a catalogue of stellar masses and mass-ratios for 27201 stars, achieving typical uncertainties of 0.08 in $q$ and $0.01\\,\\mathrm{M}_\\odot$ in the primary stellar mass. We analyse the archetype OCs M67 and NGC 2360 in detail, including mass segregation and mass-ratio distribution among other characteristics, while deriving multiplicity fractions for the rest of the sample. We find evidence that the high mass-ratio ($q\\geq 0.6$) binary fraction shows a strong correlation with the age and a weak anti-correlation with the cluster metallicity. Furthermore, the variation of the binary fraction with stellar mass in OCs shows strong accordance with the observed dependence for field stars heavier than $\\gtrsim0.6\\,\\mathrm{M}_\\odot$. Conclusions: Our work paves a path for future population-level investigations of multiplicity statistics and precision stellar masses in extended samples of OCs."
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T13:12:35Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    13,
                    12,
                    35,
                    3,
                    345,
                    0
                ],
                "arxiv_comment": "13 pages main text, 8 pages appendix and 26 figures. Accepted in A&A. Before archiving at the CDS completes, tables are available at https://drive.google.com/drive/folders/1zkPjxyPd7ZV1OOp9k10l3XfCFZqs9-4N?usp=sharing",
                "arxiv_primary_category": {
                    "term": "astro-ph.GA"
                },
                "authors": [
                    {
                        "name": "Sagar Malhotra"
                    },
                    {
                        "name": "Alfred Castro-Ginard"
                    },
                    {
                        "name": "Friedrich Anders"
                    },
                    {
                        "name": "Carme Jordi"
                    },
                    {
                        "name": "Judit Donada"
                    },
                    {
                        "name": "Xavier Luri"
                    },
                    {
                        "name": "Lola Balaguer-Núñez"
                    },
                    {
                        "name": "Songmei Qin"
                    },
                    {
                        "name": "Yueyue Jiang"
                    },
                    {
                        "name": "Andrija Župić"
                    }
                ],
                "author_detail": {
                    "name": "Andrija Župić"
                },
                "author": "Andrija Župić"
            },
            {
                "id": "http://arxiv.org/abs/2512.09443v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.09443v2",
                "title": "Advancing Mathematical Research via Human-AI Interactive Theorem Proving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Mathematical Research via Human-AI Interactive Theorem Proving"
                },
                "updated": "2025-12-11T13:10:50Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    13,
                    10,
                    50,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.09443v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.09443v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We investigate how large language models can be used as research tools in scientific computing while preserving mathematical rigor. We propose a human-in-the-loop workflow for interactive theorem proving and discovery with LLMs. Human experts retain control over problem formulation and admissible assumptions, while the model searches for proofs or contradictions, proposes candidate properties and theorems, and helps construct structures and parameters that satisfy explicit constraints, supported by numerical experiments and simple verification checks. Experts treat these outputs as raw material, further refine them, and organize the results into precise statements and rigorous proofs. We instantiate this workflow in a case study on the connection between manifold optimization and Grover's quantum search algorithm, where the pipeline helps identify invariant subspaces, explore Grover-compatible retractions, and obtain convergence guarantees for the retraction-based gradient method. The framework provides a practical template for integrating large language models into frontier mathematical research, enabling faster exploration of proof space and algorithm design while maintaining transparent reasoning responsibilities. Although illustrated on manifold optimization problems in quantum computing, the principles extend to other core areas of scientific computing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate how large language models can be used as research tools in scientific computing while preserving mathematical rigor. We propose a human-in-the-loop workflow for interactive theorem proving and discovery with LLMs. Human experts retain control over problem formulation and admissible assumptions, while the model searches for proofs or contradictions, proposes candidate properties and theorems, and helps construct structures and parameters that satisfy explicit constraints, supported by numerical experiments and simple verification checks. Experts treat these outputs as raw material, further refine them, and organize the results into precise statements and rigorous proofs. We instantiate this workflow in a case study on the connection between manifold optimization and Grover's quantum search algorithm, where the pipeline helps identify invariant subspaces, explore Grover-compatible retractions, and obtain convergence guarantees for the retraction-based gradient method. The framework provides a practical template for integrating large language models into frontier mathematical research, enabling faster exploration of proof space and algorithm design while maintaining transparent reasoning responsibilities. Although illustrated on manifold optimization problems in quantum computing, the principles extend to other core areas of scientific computing."
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-10T09:16:27Z",
                "published_parsed": [
                    2025,
                    12,
                    10,
                    9,
                    16,
                    27,
                    2,
                    344,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC"
                },
                "authors": [
                    {
                        "name": "Chenyi Li"
                    },
                    {
                        "name": "Zhijian Lai"
                    },
                    {
                        "name": "Dong An"
                    },
                    {
                        "name": "Jiang Hu"
                    },
                    {
                        "name": "Zaiwen Wen"
                    }
                ],
                "author_detail": {
                    "name": "Zaiwen Wen"
                },
                "author": "Zaiwen Wen"
            },
            {
                "id": "http://arxiv.org/abs/2512.10611v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10611v1",
                "title": "Phythesis: Physics-Guided Evolutionary Scene Synthesis for Energy-Efficient Data Center Design via LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phythesis: Physics-Guided Evolutionary Scene Synthesis for Energy-Efficient Data Center Design via LLMs"
                },
                "updated": "2025-12-11T13:04:44Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    13,
                    4,
                    44,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10611v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10611v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Data center (DC) infrastructure serves as the backbone to support the escalating demand for computing capacity. Traditional design methodologies that blend human expertise with specialized simulation tools scale poorly with the increasing system complexity. Recent studies adopt generative artificial intelligence to design plausible human-centric indoor layouts. However, they do not consider the underlying physics, making them unsuitable for the DC design that sets quantifiable operational objectives and strict physical constraints. To bridge the gap, we propose Phythesis, a novel framework that synergizes large language models (LLMs) and physics-guided evolutionary optimization to automate simulation-ready (SimReady) scene synthesis for energy-efficient DC design. Phythesis employs an iterative bi-level optimization architecture, where (i) the LLM-driven optimization level generates physically plausible three-dimensional layouts and self-criticizes them to refine the scene topology, and (ii) the physics-informed optimization level identifies the optimal asset parameters and selects the best asset combination. Experiments on three generation scales show that Phythesis achieves 57.3% generation success rate increase and 11.5% power usage effectiveness (PUE) improvement, compared with the vanilla LLM-based solution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data center (DC) infrastructure serves as the backbone to support the escalating demand for computing capacity. Traditional design methodologies that blend human expertise with specialized simulation tools scale poorly with the increasing system complexity. Recent studies adopt generative artificial intelligence to design plausible human-centric indoor layouts. However, they do not consider the underlying physics, making them unsuitable for the DC design that sets quantifiable operational objectives and strict physical constraints. To bridge the gap, we propose Phythesis, a novel framework that synergizes large language models (LLMs) and physics-guided evolutionary optimization to automate simulation-ready (SimReady) scene synthesis for energy-efficient DC design. Phythesis employs an iterative bi-level optimization architecture, where (i) the LLM-driven optimization level generates physically plausible three-dimensional layouts and self-criticizes them to refine the scene topology, and (ii) the physics-informed optimization level identifies the optimal asset parameters and selects the best asset combination. Experiments on three generation scales show that Phythesis achieves 57.3% generation success rate increase and 11.5% power usage effectiveness (PUE) improvement, compared with the vanilla LLM-based solution."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T13:04:44Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    13,
                    4,
                    44,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Minghao LI"
                    },
                    {
                        "name": "Ruihang Wang"
                    },
                    {
                        "name": "Rui Tan"
                    },
                    {
                        "name": "Yonggang Wen"
                    }
                ],
                "author_detail": {
                    "name": "Yonggang Wen"
                },
                "author": "Yonggang Wen"
            },
            {
                "id": "http://arxiv.org/abs/2512.10610v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10610v1",
                "title": "Thinking While Driving: A Concurrent Framework for Real-Time, LLM-Based Adaptive Routing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thinking While Driving: A Concurrent Framework for Real-Time, LLM-Based Adaptive Routing"
                },
                "updated": "2025-12-11T13:04:37Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    13,
                    4,
                    37,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10610v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10610v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We present Thinking While Driving, a concurrent routing framework that integrates LLMs into a graph-based traffic environment. Unlike approaches that require agents to stop and deliberate, our system enables LLM-based route planning while agents are moving, significantly reducing intersection wait times. Under high traffic, agents average just 0.75 seconds of decision latency. To coordinate many agents in real-time, we implement a non-blocking asynchronous architecture using Unity coroutines and a dedicated request manager. The environment is a weighted undirected graph with live congestion metrics, updated continuously by the agents to enable shared perception. Our results show LLM-driven agents can dynamically adapt to traffic, reroute around congestion, and exhibit behaviors beyond static pathfinding, all while maintaining real-time performance. This work provides a reproducible framework for future research in adaptive routing and multi-agent cooperation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Thinking While Driving, a concurrent routing framework that integrates LLMs into a graph-based traffic environment. Unlike approaches that require agents to stop and deliberate, our system enables LLM-based route planning while agents are moving, significantly reducing intersection wait times. Under high traffic, agents average just 0.75 seconds of decision latency. To coordinate many agents in real-time, we implement a non-blocking asynchronous architecture using Unity coroutines and a dedicated request manager. The environment is a weighted undirected graph with live congestion metrics, updated continuously by the agents to enable shared perception. Our results show LLM-driven agents can dynamically adapt to traffic, reroute around congestion, and exhibit behaviors beyond static pathfinding, all while maintaining real-time performance. This work provides a reproducible framework for future research in adaptive routing and multi-agent cooperation."
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T13:04:37Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    13,
                    4,
                    37,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA"
                },
                "authors": [
                    {
                        "name": "Xiaopei Tan"
                    },
                    {
                        "name": "Muyang Fan"
                    }
                ],
                "author_detail": {
                    "name": "Muyang Fan"
                },
                "author": "Muyang Fan"
            },
            {
                "id": "http://arxiv.org/abs/2512.10605v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10605v1",
                "title": "LEO-RobotAgent: A General-purpose Robotic Agent for Language-driven Embodied Operator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LEO-RobotAgent: A General-purpose Robotic Agent for Language-driven Embodied Operator"
                },
                "updated": "2025-12-11T12:58:36Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    12,
                    58,
                    36,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10605v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10605v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We propose LEO-RobotAgent, a general-purpose language-driven intelligent agent framework for robots. Under this framework, LLMs can operate different types of robots to complete unpredictable complex tasks across various scenarios. This framework features strong generalization, robustness, and efficiency. The application-level system built around it can fully enhance bidirectional human-robot intent understanding and lower the threshold for human-robot interaction. Regarding robot task planning, the vast majority of existing studies focus on the application of large models in single-task scenarios and for single robot types. These algorithms often have complex structures and lack generalizability. Thus, the proposed LEO-RobotAgent framework is designed with a streamlined structure as much as possible, enabling large models to independently think, plan, and act within this clear framework. We provide a modular and easily registrable toolset, allowing large models to flexibly call various tools to meet different requirements. Meanwhile, the framework incorporates a human-robot interaction mechanism, enabling the algorithm to collaborate with humans like a partner. Experiments have verified that this framework can be easily adapted to mainstream robot platforms including unmanned aerial vehicles (UAVs), robotic arms, and wheeled robot, and efficiently execute a variety of carefully designed tasks with different complexity levels. Our code is available at https://github.com/LegendLeoChen/LEO-RobotAgent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose LEO-RobotAgent, a general-purpose language-driven intelligent agent framework for robots. Under this framework, LLMs can operate different types of robots to complete unpredictable complex tasks across various scenarios. This framework features strong generalization, robustness, and efficiency. The application-level system built around it can fully enhance bidirectional human-robot intent understanding and lower the threshold for human-robot interaction. Regarding robot task planning, the vast majority of existing studies focus on the application of large models in single-task scenarios and for single robot types. These algorithms often have complex structures and lack generalizability. Thus, the proposed LEO-RobotAgent framework is designed with a streamlined structure as much as possible, enabling large models to independently think, plan, and act within this clear framework. We provide a modular and easily registrable toolset, allowing large models to flexibly call various tools to meet different requirements. Meanwhile, the framework incorporates a human-robot interaction mechanism, enabling the algorithm to collaborate with humans like a partner. Experiments have verified that this framework can be easily adapted to mainstream robot platforms including unmanned aerial vehicles (UAVs), robotic arms, and wheeled robot, and efficiently execute a variety of carefully designed tasks with different complexity levels. Our code is available at https://github.com/LegendLeoChen/LEO-RobotAgent."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T12:58:36Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    12,
                    58,
                    36,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Lihuang Chen"
                    },
                    {
                        "name": "Xiangyu Luo"
                    },
                    {
                        "name": "Jun Meng"
                    }
                ],
                "author_detail": {
                    "name": "Jun Meng"
                },
                "author": "Jun Meng"
            },
            {
                "id": "http://arxiv.org/abs/2512.10602v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10602v1",
                "title": "Uncertainty-Preserving QBNNs: Multi-Level Quantization of SVI-Based Bayesian Neural Networks for Image Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty-Preserving QBNNs: Multi-Level Quantization of SVI-Based Bayesian Neural Networks for Image Classification"
                },
                "updated": "2025-12-11T12:51:42Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    12,
                    51,
                    42,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10602v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10602v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Bayesian Neural Networks (BNNs) provide principled uncertainty quantification but suffer from substantial computational and memory overhead compared to deterministic networks. While quantization techniques have successfully reduced resource requirements in standard deep learning models, their application to probabilistic models remains largely unexplored. We introduce a systematic multi-level quantization framework for Stochastic Variational Inference based BNNs that distinguishes between three quantization strategies: Variational Parameter Quantization (VPQ), Sampled Parameter Quantization (SPQ), and Joint Quantization (JQ). Our logarithmic quantization for variance parameters, and specialized activation functions to preserve the distributional structure are essential for calibrated uncertainty estimation. Through comprehensive experiments on Dirty-MNIST, we demonstrate that BNNs can be quantized down to 4-bit precision while maintaining both classification accuracy and uncertainty disentanglement. At 4 bits, Joint Quantization achieves up to 8x memory reduction compared to floating-point implementations with minimal degradation in epistemic and aleatoric uncertainty estimation. These results enable deployment of BNNs on resource-constrained edge devices and provide design guidelines for future analog \"Bayesian Machines\" operating at inherently low precision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Neural Networks (BNNs) provide principled uncertainty quantification but suffer from substantial computational and memory overhead compared to deterministic networks. While quantization techniques have successfully reduced resource requirements in standard deep learning models, their application to probabilistic models remains largely unexplored. We introduce a systematic multi-level quantization framework for Stochastic Variational Inference based BNNs that distinguishes between three quantization strategies: Variational Parameter Quantization (VPQ), Sampled Parameter Quantization (SPQ), and Joint Quantization (JQ). Our logarithmic quantization for variance parameters, and specialized activation functions to preserve the distributional structure are essential for calibrated uncertainty estimation. Through comprehensive experiments on Dirty-MNIST, we demonstrate that BNNs can be quantized down to 4-bit precision while maintaining both classification accuracy and uncertainty disentanglement. At 4 bits, Joint Quantization achieves up to 8x memory reduction compared to floating-point implementations with minimal degradation in epistemic and aleatoric uncertainty estimation. These results enable deployment of BNNs on resource-constrained edge devices and provide design guidelines for future analog \"Bayesian Machines\" operating at inherently low precision."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T12:51:42Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    12,
                    51,
                    42,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Hendrik Borras"
                    },
                    {
                        "name": "Yong Wu"
                    },
                    {
                        "name": "Bernhard Klein"
                    },
                    {
                        "name": "Holger Fröning"
                    }
                ],
                "author_detail": {
                    "name": "Holger Fröning"
                },
                "author": "Holger Fröning"
            },
            {
                "id": "http://arxiv.org/abs/2406.14669v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2406.14669v2",
                "title": "Comprehensive survey of hybrid equations of state in neutron star mergers and constraints on the hadron-quark phase transition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comprehensive survey of hybrid equations of state in neutron star mergers and constraints on the hadron-quark phase transition"
                },
                "updated": "2025-12-11T12:46:29Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    12,
                    46,
                    29,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2406.14669v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2406.14669v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We perform an extensive study of equation of state (EoS) models featuring a phase transition from hadronic to deconfined quark matter in neutron star merger simulations. We employ three different hadronic EoSs, a constant speed of sound parameterization for the quark phase and a Maxwell construction to generate a large sample of hybrid EoS models. We systematically vary the onset density and density jump of the phase transition as well as the quark matter stiffness and simulate binary neutron star mergers to infer how the properties of the phase transition affect the gravitational-wave signal. In total we simulate mergers with 245 different hybrid EoS models. In particular, we explore in which scenarios a phase transition would be detectable by a characteristically increased postmerger gravitational-wave frequency compared to an estimate from the inspiral signal assuming a purely hadronic EoS. We find that the density jump at the transition (latent heat) has the largest impact on the gravitational-wave frequencies, while the influence of the stiffness of quark matter is smaller. We quantify which range of phase transition properties would be compatible with a certain magnitude or absence of the gravitational-wave postmerger frequency shift. By means of these dependencies, a future detection will thus directly yield constraints on the allowed features of the hadron-quark phase transition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We perform an extensive study of equation of state (EoS) models featuring a phase transition from hadronic to deconfined quark matter in neutron star merger simulations. We employ three different hadronic EoSs, a constant speed of sound parameterization for the quark phase and a Maxwell construction to generate a large sample of hybrid EoS models. We systematically vary the onset density and density jump of the phase transition as well as the quark matter stiffness and simulate binary neutron star mergers to infer how the properties of the phase transition affect the gravitational-wave signal. In total we simulate mergers with 245 different hybrid EoS models. In particular, we explore in which scenarios a phase transition would be detectable by a characteristically increased postmerger gravitational-wave frequency compared to an estimate from the inspiral signal assuming a purely hadronic EoS. We find that the density jump at the transition (latent heat) has the largest impact on the gravitational-wave frequencies, while the influence of the stiffness of quark matter is smaller. We quantify which range of phase transition properties would be compatible with a certain magnitude or absence of the gravitational-wave postmerger frequency shift. By means of these dependencies, a future detection will thus directly yield constraints on the allowed features of the hadron-quark phase transition."
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-06-20T18:43:42Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    18,
                    43,
                    42,
                    3,
                    172,
                    0
                ],
                "arxiv_comment": "16 pages, 12 figures, accepted for publication in Phys. Rev. D",
                "arxiv_primary_category": {
                    "term": "astro-ph.HE"
                },
                "authors": [
                    {
                        "name": "Sebastian Blacker"
                    },
                    {
                        "name": "Andreas Bauswein"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Bauswein"
                },
                "author": "Andreas Bauswein"
            },
            {
                "id": "http://arxiv.org/abs/2506.05191v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.05191v2",
                "title": "MokA: Multimodal Low-Rank Adaptation for MLLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MokA: Multimodal Low-Rank Adaptation for MLLMs"
                },
                "updated": "2025-12-11T12:37:26Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    12,
                    37,
                    26,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.05191v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.05191v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In this paper, we reveal that most current efficient multimodal fine-tuning methods are hindered by a key limitation: they are directly borrowed from LLMs, often neglecting the intrinsic differences of multimodal scenarios and even affecting the full utilization of all modalities. Inspired by our empirical observation, we argue that unimodal adaptation and cross-modal adaptation are two essential parts for the effective fine-tuning of MLLMs. From this perspective, we propose Multimodal low-rank Adaptation (MokA), a multimodal-aware efficient fine-tuning strategy that takes multimodal characteristics into consideration. It compresses unimodal information by modality-specific parameters while explicitly enhancing cross-modal interaction, ensuring both unimodal and cross-modal adaptation. Extensive experiments cover three representative multimodal scenarios (audio-visual-text, visual-text, and speech-text), and multiple LLM backbones (LLaMA2/3, Qwen2, Qwen2.5-VL, etc). Consistent improvements indicate the efficacy and versatility of the proposed method. Ablation studies and efficiency evaluation are also conducted to fully asses our method. Overall, we think MokA provides a more targeted solution for efficient adaptation of MLLMs, paving the way for further exploration. The project page is at https://gewu-lab.github.io/MokA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we reveal that most current efficient multimodal fine-tuning methods are hindered by a key limitation: they are directly borrowed from LLMs, often neglecting the intrinsic differences of multimodal scenarios and even affecting the full utilization of all modalities. Inspired by our empirical observation, we argue that unimodal adaptation and cross-modal adaptation are two essential parts for the effective fine-tuning of MLLMs. From this perspective, we propose Multimodal low-rank Adaptation (MokA), a multimodal-aware efficient fine-tuning strategy that takes multimodal characteristics into consideration. It compresses unimodal information by modality-specific parameters while explicitly enhancing cross-modal interaction, ensuring both unimodal and cross-modal adaptation. Extensive experiments cover three representative multimodal scenarios (audio-visual-text, visual-text, and speech-text), and multiple LLM backbones (LLaMA2/3, Qwen2, Qwen2.5-VL, etc). Consistent improvements indicate the efficacy and versatility of the proposed method. Ablation studies and efficiency evaluation are also conducted to fully asses our method. Overall, we think MokA provides a more targeted solution for efficient adaptation of MLLMs, paving the way for further exploration. The project page is at https://gewu-lab.github.io/MokA."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-05T16:04:08Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    16,
                    4,
                    8,
                    3,
                    156,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Yake Wei"
                    },
                    {
                        "name": "Yu Miao"
                    },
                    {
                        "name": "Dongzhan Zhou"
                    },
                    {
                        "name": "Di Hu"
                    }
                ],
                "author_detail": {
                    "name": "Di Hu"
                },
                "author": "Di Hu"
            },
            {
                "id": "http://arxiv.org/abs/2512.10576v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10576v1",
                "title": "ESS: An Offload-Centric Latent-Cache Management Architecture for DeepSeek-V3.2-Exp",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ESS: An Offload-Centric Latent-Cache Management Architecture for DeepSeek-V3.2-Exp"
                },
                "updated": "2025-12-11T12:06:00Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    12,
                    6,
                    0,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10576v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10576v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "DeepSeek-V3.2-Exp introduces a sparse attention mechanism that significantly reduces inference latency in long-context scenarios. Although the overall throughput has improved greatly, the Decode-stage of PD disaggregation remains to be a major bottleneck. This bottleneck primarily stems from the conflict between linear growth of Latent-Cache with sequence length and the limited GPU memory capacity, which constrains the feasible batch-size and thereby suppresses Decode-stage throughput.\n  To address this challenge, we propose ESS (Extended Sparse Server), an offload-centric system design tailored for DeepSeek-V3.2-Exp. ESS selectively offloads Latent-Cache to CPU memory while preserving latency-critical components on GPU. By freeing up GPU memory, ESS effectively decoupling batch-size scaling from GPU memory constraints. This design significantly improves Decode-stage throughput, thereby reducing deployment costs in real-world settings.\n  Our high-fidelity simulations show that ESS delivers 69.4\\% throughput improvement at 32K context length and up to 123\\% throughput improvement at 128K, demonstrating its effectiveness for large-context inference workloads. These results highlight ESS as a practical and scalable solution for long-context LLM serving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepSeek-V3.2-Exp introduces a sparse attention mechanism that significantly reduces inference latency in long-context scenarios. Although the overall throughput has improved greatly, the Decode-stage of PD disaggregation remains to be a major bottleneck. This bottleneck primarily stems from the conflict between linear growth of Latent-Cache with sequence length and the limited GPU memory capacity, which constrains the feasible batch-size and thereby suppresses Decode-stage throughput.\n  To address this challenge, we propose ESS (Extended Sparse Server), an offload-centric system design tailored for DeepSeek-V3.2-Exp. ESS selectively offloads Latent-Cache to CPU memory while preserving latency-critical components on GPU. By freeing up GPU memory, ESS effectively decoupling batch-size scaling from GPU memory constraints. This design significantly improves Decode-stage throughput, thereby reducing deployment costs in real-world settings.\n  Our high-fidelity simulations show that ESS delivers 69.4\\% throughput improvement at 32K context length and up to 123\\% throughput improvement at 128K, demonstrating its effectiveness for large-context inference workloads. These results highlight ESS as a practical and scalable solution for long-context LLM serving."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T12:06:00Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    12,
                    6,
                    0,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Xinhang Chen"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Jiahuan He"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Jianming Zhang"
                    },
                    {
                        "name": "Wenlong Zhou"
                    },
                    {
                        "name": "Xiao Li"
                    },
                    {
                        "name": "Pai Zeng"
                    },
                    {
                        "name": "Shiyong Li"
                    },
                    {
                        "name": "Yuanpan Qian"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Zhaogeng Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhaogeng Li"
                },
                "author": "Zhaogeng Li"
            },
            {
                "id": "http://arxiv.org/abs/2512.10575v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10575v1",
                "title": "RoleRMBench & RoleRM: Towards Reward Modeling for Profile-Based Role Play in Dialogue Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoleRMBench & RoleRM: Towards Reward Modeling for Profile-Based Role Play in Dialogue Systems"
                },
                "updated": "2025-12-11T12:04:46Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    12,
                    4,
                    46,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10575v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10575v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Reward modeling has become a cornerstone of aligning large language models (LLMs) with human preferences. Yet, when extended to subjective and open-ended domains such as role play, existing reward models exhibit severe degradation, struggling to capture nuanced and persona-grounded human judgments. To address this gap, we introduce RoleRMBench, the first systematic benchmark for reward modeling in role-playing dialogue, covering seven fine-grained capabilities from narrative management to role consistency and engagement. Evaluation on RoleRMBench reveals large and consistent gaps between general-purpose reward models and human judgment, particularly in narrative and stylistic dimensions. We further propose RoleRM, a reward model trained with Continuous Implicit Preferences (CIP), which reformulates subjective evaluation as continuous consistent pairwise supervision under multiple structuring strategies. Comprehensive experiments show that RoleRM surpasses strong open- and closed-source reward models by over 24% on average, demonstrating substantial gains in narrative coherence and stylistic fidelity. Our findings highlight the importance of continuous preference representation and annotation consistency, establishing a foundation for subjective alignment in human-centered dialogue systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward modeling has become a cornerstone of aligning large language models (LLMs) with human preferences. Yet, when extended to subjective and open-ended domains such as role play, existing reward models exhibit severe degradation, struggling to capture nuanced and persona-grounded human judgments. To address this gap, we introduce RoleRMBench, the first systematic benchmark for reward modeling in role-playing dialogue, covering seven fine-grained capabilities from narrative management to role consistency and engagement. Evaluation on RoleRMBench reveals large and consistent gaps between general-purpose reward models and human judgment, particularly in narrative and stylistic dimensions. We further propose RoleRM, a reward model trained with Continuous Implicit Preferences (CIP), which reformulates subjective evaluation as continuous consistent pairwise supervision under multiple structuring strategies. Comprehensive experiments show that RoleRM surpasses strong open- and closed-source reward models by over 24% on average, demonstrating substantial gains in narrative coherence and stylistic fidelity. Our findings highlight the importance of continuous preference representation and annotation consistency, establishing a foundation for subjective alignment in human-centered dialogue systems."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T12:04:46Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    12,
                    4,
                    46,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Hang Ding"
                    },
                    {
                        "name": "Qiming Feng"
                    },
                    {
                        "name": "Dongqi Liu"
                    },
                    {
                        "name": "Qi Zhao"
                    },
                    {
                        "name": "Tao Yao"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Dongsheng Chen"
                    },
                    {
                        "name": "Jian Li"
                    },
                    {
                        "name": "Zhenye Gan"
                    },
                    {
                        "name": "Jiangning Zhang"
                    },
                    {
                        "name": "Chengjie Wang"
                    },
                    {
                        "name": "Yabiao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yabiao Wang"
                },
                "author": "Yabiao Wang"
            },
            {
                "id": "http://arxiv.org/abs/2512.05647v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05647v2",
                "title": "A Greek Government Decisions Dataset for Public-Sector Analysis and Insight",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Greek Government Decisions Dataset for Public-Sector Analysis and Insight"
                },
                "updated": "2025-12-11T11:58:25Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    11,
                    58,
                    25,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05647v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05647v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We introduce an open, machine-readable corpus of Greek government decisions sourced from the national transparency platform Diavgeia. The resource comprises 1 million decisions, featuring and high-quality raw text extracted from PDFs. It is released with raw extracted text in Markdown format, alongside a fully reproducible extraction pipeline. Beyond the core dataset, we conduct qualitative analyses to explore boilerplate patterns and design a retrieval-augmented generation (RAG) task by formulating a set of representative questions, creating high-quality answers, and evaluating a baseline RAG system on its ability to retrieve and reason over public decisions. This evaluation demonstrates the potential of large-scale public-sector corpora to support advanced information access and transparency through structured retrieval and reasoning over governmental documents, and highlights how such a RAG pipeline could simulate a chat-based assistant capable of interactively answering questions about public decisions. Due to its scale, quality, and domain coverage, the corpus can also serve as high-value pre-training or fine-tuning material for new Language Models (LMs) and Large Language Models (LLMs) respectively, including specialized models for legal and governmental domains, and as a foundation for novel approaches in domain adaptation, knowledge-grounded generation, and explainable AI. Finally, we discuss limitations, outline future directions, and make both the data and the code accessible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce an open, machine-readable corpus of Greek government decisions sourced from the national transparency platform Diavgeia. The resource comprises 1 million decisions, featuring and high-quality raw text extracted from PDFs. It is released with raw extracted text in Markdown format, alongside a fully reproducible extraction pipeline. Beyond the core dataset, we conduct qualitative analyses to explore boilerplate patterns and design a retrieval-augmented generation (RAG) task by formulating a set of representative questions, creating high-quality answers, and evaluating a baseline RAG system on its ability to retrieve and reason over public decisions. This evaluation demonstrates the potential of large-scale public-sector corpora to support advanced information access and transparency through structured retrieval and reasoning over governmental documents, and highlights how such a RAG pipeline could simulate a chat-based assistant capable of interactively answering questions about public decisions. Due to its scale, quality, and domain coverage, the corpus can also serve as high-value pre-training or fine-tuning material for new Language Models (LMs) and Large Language Models (LLMs) respectively, including specialized models for legal and governmental domains, and as a foundation for novel approaches in domain adaptation, knowledge-grounded generation, and explainable AI. Finally, we discuss limitations, outline future directions, and make both the data and the code accessible."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T11:47:33Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    11,
                    47,
                    33,
                    4,
                    339,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Giorgos Antoniou"
                    },
                    {
                        "name": "Giorgos Filandrianos"
                    },
                    {
                        "name": "Aggelos Vlachos"
                    },
                    {
                        "name": "Giorgos Stamou"
                    },
                    {
                        "name": "Lampros Kollimenos"
                    },
                    {
                        "name": "Konstantinos Skianis"
                    },
                    {
                        "name": "Michalis Vazirgiannis"
                    }
                ],
                "author_detail": {
                    "name": "Michalis Vazirgiannis"
                },
                "author": "Michalis Vazirgiannis"
            },
            {
                "id": "http://arxiv.org/abs/2512.02899v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.02899v2",
                "title": "Glance: Accelerating Diffusion Models with 1 Sample",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Glance: Accelerating Diffusion Models with 1 Sample"
                },
                "updated": "2025-12-11T11:53:22Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    11,
                    53,
                    22,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.02899v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.02899v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion models have achieved remarkable success in image generation, yet their deployment remains constrained by the heavy computational cost and the need for numerous inference steps. Previous efforts on fewer-step distillation attempt to skip redundant steps by training compact student models, yet they often suffer from heavy retraining costs and degraded generalization. In this work, we take a different perspective: we accelerate smartly, not evenly, applying smaller speedups to early semantic stages and larger ones to later redundant phases. We instantiate this phase-aware strategy with two experts that specialize in slow and fast denoising phases. Surprisingly, instead of investing massive effort in retraining student models, we find that simply equipping the base model with lightweight LoRA adapters achieves both efficient acceleration and strong generalization. We refer to these two adapters as Slow-LoRA and Fast-LoRA. Through extensive experiments, our method achieves up to 5 acceleration over the base model while maintaining comparable visual quality across diverse benchmarks. Remarkably, the LoRA experts are trained with only 1 samples on a single V100 within one hour, yet the resulting models generalize strongly on unseen prompts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have achieved remarkable success in image generation, yet their deployment remains constrained by the heavy computational cost and the need for numerous inference steps. Previous efforts on fewer-step distillation attempt to skip redundant steps by training compact student models, yet they often suffer from heavy retraining costs and degraded generalization. In this work, we take a different perspective: we accelerate smartly, not evenly, applying smaller speedups to early semantic stages and larger ones to later redundant phases. We instantiate this phase-aware strategy with two experts that specialize in slow and fast denoising phases. Surprisingly, instead of investing massive effort in retraining student models, we find that simply equipping the base model with lightweight LoRA adapters achieves both efficient acceleration and strong generalization. We refer to these two adapters as Slow-LoRA and Fast-LoRA. Through extensive experiments, our method achieves up to 5 acceleration over the base model while maintaining comparable visual quality across diverse benchmarks. Remarkably, the LoRA experts are trained with only 1 samples on a single V100 within one hour, yet the resulting models generalize strongly on unseen prompts."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-02T16:05:21Z",
                "published_parsed": [
                    2025,
                    12,
                    2,
                    16,
                    5,
                    21,
                    1,
                    336,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Zhuobai Dong"
                    },
                    {
                        "name": "Rui Zhao"
                    },
                    {
                        "name": "Songjie Wu"
                    },
                    {
                        "name": "Junchao Yi"
                    },
                    {
                        "name": "Linjie Li"
                    },
                    {
                        "name": "Zhengyuan Yang"
                    },
                    {
                        "name": "Lijuan Wang"
                    },
                    {
                        "name": "Alex Jinpeng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Alex Jinpeng Wang"
                },
                "author": "Alex Jinpeng Wang"
            },
            {
                "id": "http://arxiv.org/abs/2512.10563v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10563v1",
                "title": "NormCode: A Semi-Formal Language for Context-Isolated AI Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NormCode: A Semi-Formal Language for Context-Isolated AI Planning"
                },
                "updated": "2025-12-11T11:50:50Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    11,
                    50,
                    50,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10563v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10563v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Multistep workflows that chain large language model (LLM) calls suffer from context pollution: as information accumulates across steps, models hallucinate, confuse intermediate outputs, and lose track of task constraints. We present NormCode, a semiformal language for constructing plans of inferences, structured decompositions where each step operates in data isolation and receives only explicitly passed inputs, which eliminates crossstep contamination by design. NormCode enforces a strict separation between semantic operations (LLMdriven reasoning, nondeterministic) and syntactic operations (deterministic data restructuring), enabling precise cost and reliability tracing. The language exists in three isomorphic formats: .ncds for human authoring, .ncd for machine execution, and .ncn for human verification, supporting progressive formalization from sketch to production. We validate NormCode through two demonstrations: (1) a base X addition algorithm achieving 100 percent accuracy on arbitrary length inputs, and (2) self hosted execution of NormCode's own five phase compiler pipeline. The working orchestrator provides dependency driven scheduling, SQLite backed checkpointing, and loop management, making AI workflows auditable by design and addressing a critical need for transparency in high stakes domains such as legal reasoning, medical decision making, and financial analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multistep workflows that chain large language model (LLM) calls suffer from context pollution: as information accumulates across steps, models hallucinate, confuse intermediate outputs, and lose track of task constraints. We present NormCode, a semiformal language for constructing plans of inferences, structured decompositions where each step operates in data isolation and receives only explicitly passed inputs, which eliminates crossstep contamination by design. NormCode enforces a strict separation between semantic operations (LLMdriven reasoning, nondeterministic) and syntactic operations (deterministic data restructuring), enabling precise cost and reliability tracing. The language exists in three isomorphic formats: .ncds for human authoring, .ncd for machine execution, and .ncn for human verification, supporting progressive formalization from sketch to production. We validate NormCode through two demonstrations: (1) a base X addition algorithm achieving 100 percent accuracy on arbitrary length inputs, and (2) self hosted execution of NormCode's own five phase compiler pipeline. The working orchestrator provides dependency driven scheduling, SQLite backed checkpointing, and loop management, making AI workflows auditable by design and addressing a critical need for transparency in high stakes domains such as legal reasoning, medical decision making, and financial analysis."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T11:50:50Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    11,
                    50,
                    50,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Xin Guan"
                    }
                ],
                "author_detail": {
                    "name": "Xin Guan"
                },
                "author": "Xin Guan"
            },
            {
                "id": "http://arxiv.org/abs/2512.10561v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10561v1",
                "title": "Causal Reasoning Favors Encoders: On The Limits of Decoder-Only Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal Reasoning Favors Encoders: On The Limits of Decoder-Only Models"
                },
                "updated": "2025-12-11T11:46:48Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    11,
                    46,
                    48,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10561v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10561v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In context learning (ICL) underpins recent advances in large language models (LLMs), although its role and performance in causal reasoning remains unclear. Causal reasoning demands multihop composition and strict conjunctive control, and reliance on spurious lexical relations of the input could provide misleading results. We hypothesize that, due to their ability to project the input into a latent space, encoder and encoder decoder architectures are better suited for said multihop conjunctive reasoning versus decoder only models. To do this, we compare fine-tuned versions of all the aforementioned architectures with zero and few shot ICL in both natural language and non natural language scenarios. We find that ICL alone is insufficient for reliable causal reasoning, often overfocusing on irrelevant input features. In particular, decoder only models are noticeably brittle to distributional shifts, while finetuned encoder and encoder decoder models can generalize more robustly across our tests, including the non natural language split. Both architectures are only matched or surpassed by decoder only architectures at large scales. We conclude by noting that for cost effective, short horizon robust causal reasoning, encoder or encoder decoder architectures with targeted finetuning are preferable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In context learning (ICL) underpins recent advances in large language models (LLMs), although its role and performance in causal reasoning remains unclear. Causal reasoning demands multihop composition and strict conjunctive control, and reliance on spurious lexical relations of the input could provide misleading results. We hypothesize that, due to their ability to project the input into a latent space, encoder and encoder decoder architectures are better suited for said multihop conjunctive reasoning versus decoder only models. To do this, we compare fine-tuned versions of all the aforementioned architectures with zero and few shot ICL in both natural language and non natural language scenarios. We find that ICL alone is insufficient for reliable causal reasoning, often overfocusing on irrelevant input features. In particular, decoder only models are noticeably brittle to distributional shifts, while finetuned encoder and encoder decoder models can generalize more robustly across our tests, including the non natural language split. Both architectures are only matched or surpassed by decoder only architectures at large scales. We conclude by noting that for cost effective, short horizon robust causal reasoning, encoder or encoder decoder architectures with targeted finetuning are preferable."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T11:46:48Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    11,
                    46,
                    48,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Amartya Roy"
                    },
                    {
                        "name": "Elamparithy M"
                    },
                    {
                        "name": "Kripabandhu Ghosh"
                    },
                    {
                        "name": "Ponnurangam Kumaraguru"
                    },
                    {
                        "name": "Adrian de Wynter"
                    }
                ],
                "author_detail": {
                    "name": "Adrian de Wynter"
                },
                "author": "Adrian de Wynter"
            },
            {
                "id": "http://arxiv.org/abs/2512.09543v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.09543v2",
                "title": "SWEnergy: An Empirical Study on Energy Efficiency in Agentic Issue Resolution Frameworks with SLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SWEnergy: An Empirical Study on Energy Efficiency in Agentic Issue Resolution Frameworks with SLMs"
                },
                "updated": "2025-12-11T11:33:34Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    11,
                    33,
                    34,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.09543v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.09543v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Context. LLM-based autonomous agents in software engineering rely on large, proprietary models, limiting local deployment. This has spurred interest in Small Language Models (SLMs), but their practical effectiveness and efficiency within complex agentic frameworks for automated issue resolution remain poorly understood.\n  Goal. We investigate the performance, energy efficiency, and resource consumption of four leading agentic issue resolution frameworks when deliberately constrained to using SLMs. We aim to assess the viability of these systems for this task in resource-limited settings and characterize the resulting trade-offs.\n  Method. We conduct a controlled evaluation of four leading agentic frameworks (SWE-Agent, OpenHands, Mini SWE Agent, AutoCodeRover) using two SLMs (Gemma-3 4B, Qwen-3 1.7B) on the SWE-bench Verified Mini benchmark. On fixed hardware, we measure energy, duration, token usage, and memory over 150 runs per configuration.\n  Results. We find that framework architecture is the primary driver of energy consumption. The most energy-intensive framework, AutoCodeRover (Gemma), consumed 9.4x more energy on average than the least energy-intensive, OpenHands (Gemma). However, this energy is largely wasted. Task resolution rates were near-zero, demonstrating that current frameworks, when paired with SLMs, consume significant energy on unproductive reasoning loops. The SLM's limited reasoning was the bottleneck for success, but the framework's design was the bottleneck for efficiency.\n  Conclusions. Current agentic frameworks, designed for powerful LLMs, fail to operate efficiently with SLMs. We find that framework architecture is the primary driver of energy consumption, but this energy is largely wasted due to the SLMs' limited reasoning. Viable low-energy solutions require shifting from passive orchestration to architectures that actively manage SLM weaknesses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context. LLM-based autonomous agents in software engineering rely on large, proprietary models, limiting local deployment. This has spurred interest in Small Language Models (SLMs), but their practical effectiveness and efficiency within complex agentic frameworks for automated issue resolution remain poorly understood.\n  Goal. We investigate the performance, energy efficiency, and resource consumption of four leading agentic issue resolution frameworks when deliberately constrained to using SLMs. We aim to assess the viability of these systems for this task in resource-limited settings and characterize the resulting trade-offs.\n  Method. We conduct a controlled evaluation of four leading agentic frameworks (SWE-Agent, OpenHands, Mini SWE Agent, AutoCodeRover) using two SLMs (Gemma-3 4B, Qwen-3 1.7B) on the SWE-bench Verified Mini benchmark. On fixed hardware, we measure energy, duration, token usage, and memory over 150 runs per configuration.\n  Results. We find that framework architecture is the primary driver of energy consumption. The most energy-intensive framework, AutoCodeRover (Gemma), consumed 9.4x more energy on average than the least energy-intensive, OpenHands (Gemma). However, this energy is largely wasted. Task resolution rates were near-zero, demonstrating that current frameworks, when paired with SLMs, consume significant energy on unproductive reasoning loops. The SLM's limited reasoning was the bottleneck for success, but the framework's design was the bottleneck for efficiency.\n  Conclusions. Current agentic frameworks, designed for powerful LLMs, fail to operate efficiently with SLMs. We find that framework architecture is the primary driver of energy consumption, but this energy is largely wasted due to the SLMs' limited reasoning. Viable low-energy solutions require shifting from passive orchestration to architectures that actively manage SLM weaknesses."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-10T11:28:48Z",
                "published_parsed": [
                    2025,
                    12,
                    10,
                    11,
                    28,
                    48,
                    2,
                    344,
                    0
                ],
                "arxiv_comment": "8 pages, 5 figures, 1 table. Accepted to AGENT 2026 (ICSE 2026 workshop)",
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Arihant Tripathy"
                    },
                    {
                        "name": "Ch Pavan Harshit"
                    },
                    {
                        "name": "Karthik Vaidhyanathan"
                    }
                ],
                "author_detail": {
                    "name": "Karthik Vaidhyanathan"
                },
                "author": "Karthik Vaidhyanathan"
            },
            {
                "id": "http://arxiv.org/abs/2512.10551v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10551v1",
                "title": "LLM-Auction: Generative Auction towards LLM-Native Advertising",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Auction: Generative Auction towards LLM-Native Advertising"
                },
                "updated": "2025-12-11T11:31:20Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    11,
                    31,
                    20,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10551v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10551v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The rapid advancement of large language models (LLMs) necessitates novel monetization strategies, among which LLM-native advertising has emerged as a promising paradigm by naturally integrating advertisement within LLM-generated responses. However, this paradigm fundamentally shifts the auction object from discrete ad slots to the distribution over LLM outputs, posing new challenges for designing auction mechanisms. Existing mechanisms for LLM-native advertising adopt frameworks that decouple auction and generation, which either ignore externalities or require multiple LLM inferences for ad allocation, rendering them impractical for industrial scenarios. To address these challenges, we propose LLM-Auction, which to the best of our knowledge is the first learning-based generative auction mechanism that integrates auction and LLM generation for LLM-native advertising. By formulating the allocation optimization as a preference alignment problem between LLM outputs and the mechanism's objective which reflects both advertisers' expected value and user experience, we introduce Iterative Reward-Preference Optimization (IRPO) algorithm that alternately optimizes the reward model and the LLM. This approach enables the LLM to inherently model allocation externalities without any extra inference cost. We further identify the allocation monotonicity and continuity of LLM-Auction, which allows us to prove that a simple first-price payment rule exhibits favorable incentive properties. Additionally, we design an LLM-as-a-judge simulation environment to facilitate large-scale data construction and enable comprehensive quantitative evaluation of the mechanism's performance. Extensive quantitative and qualitative experiments demonstrate that LLM-Auction significantly outperforms existing baselines in allocation efficiency, while achieving the desired mechanism properties.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) necessitates novel monetization strategies, among which LLM-native advertising has emerged as a promising paradigm by naturally integrating advertisement within LLM-generated responses. However, this paradigm fundamentally shifts the auction object from discrete ad slots to the distribution over LLM outputs, posing new challenges for designing auction mechanisms. Existing mechanisms for LLM-native advertising adopt frameworks that decouple auction and generation, which either ignore externalities or require multiple LLM inferences for ad allocation, rendering them impractical for industrial scenarios. To address these challenges, we propose LLM-Auction, which to the best of our knowledge is the first learning-based generative auction mechanism that integrates auction and LLM generation for LLM-native advertising. By formulating the allocation optimization as a preference alignment problem between LLM outputs and the mechanism's objective which reflects both advertisers' expected value and user experience, we introduce Iterative Reward-Preference Optimization (IRPO) algorithm that alternately optimizes the reward model and the LLM. This approach enables the LLM to inherently model allocation externalities without any extra inference cost. We further identify the allocation monotonicity and continuity of LLM-Auction, which allows us to prove that a simple first-price payment rule exhibits favorable incentive properties. Additionally, we design an LLM-as-a-judge simulation environment to facilitate large-scale data construction and enable comprehensive quantitative evaluation of the mechanism's performance. Extensive quantitative and qualitative experiments demonstrate that LLM-Auction significantly outperforms existing baselines in allocation efficiency, while achieving the desired mechanism properties."
                },
                "tags": [
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T11:31:20Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    11,
                    31,
                    20,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.GT"
                },
                "authors": [
                    {
                        "name": "Chujie Zhao"
                    },
                    {
                        "name": "Qun Hu"
                    },
                    {
                        "name": "Shiping Song"
                    },
                    {
                        "name": "Dagui Chen"
                    },
                    {
                        "name": "Han Zhu"
                    },
                    {
                        "name": "Jian Xu"
                    },
                    {
                        "name": "Bo Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zheng"
                },
                "author": "Bo Zheng"
            },
            {
                "id": "http://arxiv.org/abs/2512.10547v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10547v1",
                "title": "Unlocking the Address Book: Dissecting the Sparse Semantic Structure of LLM Key-Value Caches via Sparse Autoencoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking the Address Book: Dissecting the Sparse Semantic Structure of LLM Key-Value Caches via Sparse Autoencoders"
                },
                "updated": "2025-12-11T11:23:50Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    11,
                    23,
                    50,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10547v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10547v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The Key-Value (KV) cache is the primary memory bottleneck in long-context Large Language Models, yet it is typically treated as an opaque numerical tensor. In this work, we propose \\textbf{STA-Attention}, a framework that utilizes Top-K Sparse Autoencoders (SAEs) to decompose the KV cache into interpretable ``semantic atoms.'' Unlike standard $L_1$-regularized SAEs, our Top-K approach eliminates shrinkage bias, preserving the precise dot-product geometry required for attention. Our analysis uncovers a fundamental \\textbf{Key-Value Asymmetry}: while Key vectors serve as highly sparse routers dominated by a ``Semantic Elbow,'' deep Value vectors carry dense content payloads requiring a larger budget. Based on this structure, we introduce a Dual-Budget Strategy that selectively preserves the most informative semantic components while filtering representational noise. Experiments on Yi-6B, Mistral-7B, Qwen2.5-32B, and others show that our semantic reconstructions maintain perplexity and zero-shot performance comparable to the original models, effectively bridging the gap between mechanistic interpretability and faithful attention modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Key-Value (KV) cache is the primary memory bottleneck in long-context Large Language Models, yet it is typically treated as an opaque numerical tensor. In this work, we propose \\textbf{STA-Attention}, a framework that utilizes Top-K Sparse Autoencoders (SAEs) to decompose the KV cache into interpretable ``semantic atoms.'' Unlike standard $L_1$-regularized SAEs, our Top-K approach eliminates shrinkage bias, preserving the precise dot-product geometry required for attention. Our analysis uncovers a fundamental \\textbf{Key-Value Asymmetry}: while Key vectors serve as highly sparse routers dominated by a ``Semantic Elbow,'' deep Value vectors carry dense content payloads requiring a larger budget. Based on this structure, we introduce a Dual-Budget Strategy that selectively preserves the most informative semantic components while filtering representational noise. Experiments on Yi-6B, Mistral-7B, Qwen2.5-32B, and others show that our semantic reconstructions maintain perplexity and zero-shot performance comparable to the original models, effectively bridging the gap between mechanistic interpretability and faithful attention modeling."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T11:23:50Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    11,
                    23,
                    50,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Qingsen Ma"
                    },
                    {
                        "name": "Dianyun Wang"
                    },
                    {
                        "name": "Jiaming Lyu"
                    },
                    {
                        "name": "Yaoye Wang"
                    },
                    {
                        "name": "Lechen Ning"
                    },
                    {
                        "name": "Sujie Zhu"
                    },
                    {
                        "name": "Zhenbo Xu"
                    },
                    {
                        "name": "Liuyu Xiang"
                    },
                    {
                        "name": "Huining Li"
                    },
                    {
                        "name": "Huijia Wu"
                    },
                    {
                        "name": "Zhaofeng He"
                    }
                ],
                "author_detail": {
                    "name": "Zhaofeng He"
                },
                "author": "Zhaofeng He"
            },
            {
                "id": "http://arxiv.org/abs/2512.10545v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10545v1",
                "title": "XDoGE: Multilingual Data Reweighting to Enhance Language Inclusivity in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XDoGE: Multilingual Data Reweighting to Enhance Language Inclusivity in LLMs"
                },
                "updated": "2025-12-11T11:22:53Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    11,
                    22,
                    53,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10545v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10545v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Current large language models (LLMs) are trained on massive amounts of text data, primarily from a few dominant languages. Studies suggest that this over-reliance on high-resource languages, such as English, hampers LLM performance in mid- and low-resource languages. To mitigate this problem, we propose to (i) optimize the language distribution by training a small proxy model within a domain-reweighing DoGE algorithm that we extend to XDoGE for a multilingual setup, and (ii) rescale the data and train a full-size model with the established language weights either from scratch or within a continual pre-training phase (CPT). We target six languages possessing a variety of geographic and intra- and inter-language-family relations, namely, English and Spanish (high-resource), Portuguese and Catalan (mid-resource), Galician and Basque (low-resource). We experiment with Salamandra-2b, which is a promising model for these languages. We investigate the effects of substantial data repetition on minor languages and under-sampling on dominant languages using the IberoBench framework for quantitative evaluation. Finally, we release a new promising IberianLLM-7B-Instruct model centering on Iberian languages and English that we pretrained from scratch and further improved using CPT with the XDoGE weights.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current large language models (LLMs) are trained on massive amounts of text data, primarily from a few dominant languages. Studies suggest that this over-reliance on high-resource languages, such as English, hampers LLM performance in mid- and low-resource languages. To mitigate this problem, we propose to (i) optimize the language distribution by training a small proxy model within a domain-reweighing DoGE algorithm that we extend to XDoGE for a multilingual setup, and (ii) rescale the data and train a full-size model with the established language weights either from scratch or within a continual pre-training phase (CPT). We target six languages possessing a variety of geographic and intra- and inter-language-family relations, namely, English and Spanish (high-resource), Portuguese and Catalan (mid-resource), Galician and Basque (low-resource). We experiment with Salamandra-2b, which is a promising model for these languages. We investigate the effects of substantial data repetition on minor languages and under-sampling on dominant languages using the IberoBench framework for quantitative evaluation. Finally, we release a new promising IberianLLM-7B-Instruct model centering on Iberian languages and English that we pretrained from scratch and further improved using CPT with the XDoGE weights."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T11:22:53Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    11,
                    22,
                    53,
                    3,
                    345,
                    0
                ],
                "arxiv_comment": "Accepted and presented at the LLMs4All workshop at the IEEE BigData 2025 Conference, Macau - December 8-11, 2025",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Iñaki Lacunza"
                    },
                    {
                        "name": "José Javier Saiz"
                    },
                    {
                        "name": "Alexander Shvets"
                    },
                    {
                        "name": "Aitor Gonzalez-Agirre"
                    },
                    {
                        "name": "Marta Villegas"
                    }
                ],
                "author_detail": {
                    "name": "Marta Villegas"
                },
                "author": "Marta Villegas"
            },
            {
                "id": "http://arxiv.org/abs/2512.00087v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.00087v2",
                "title": "Exploring Automated Recognition of Instructional Activity and Discourse from Multimodal Classroom Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Automated Recognition of Instructional Activity and Discourse from Multimodal Classroom Data"
                },
                "updated": "2025-12-11T11:15:19Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    11,
                    15,
                    19,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.00087v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.00087v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Observation of classroom interactions can provide concrete feedback to teachers, but current methods rely on manual annotation, which is resource-intensive and hard to scale. This work explores AI-driven analysis of classroom recordings, focusing on multimodal instructional activity and discourse recognition as a foundation for actionable feedback. Using a densely annotated dataset of 164 hours of video and 68 lesson transcripts, we design parallel, modality-specific pipelines. For video, we evaluate zero-shot multimodal LLMs, fine-tuned vision-language models, and self-supervised video transformers on 24 activity labels. For transcripts, we fine-tune a transformer-based classifier with contextualized inputs and compare it against prompting-based LLMs on 19 discourse labels. To handle class imbalance and multi-label complexity, we apply per-label thresholding, context windows, and imbalance-aware loss functions. The results show that fine-tuned models consistently outperform prompting-based approaches, achieving macro-F1 scores of 0.577 for video and 0.460 for transcripts. These results demonstrate the feasibility of automated classroom analysis and establish a foundation for scalable teacher feedback systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Observation of classroom interactions can provide concrete feedback to teachers, but current methods rely on manual annotation, which is resource-intensive and hard to scale. This work explores AI-driven analysis of classroom recordings, focusing on multimodal instructional activity and discourse recognition as a foundation for actionable feedback. Using a densely annotated dataset of 164 hours of video and 68 lesson transcripts, we design parallel, modality-specific pipelines. For video, we evaluate zero-shot multimodal LLMs, fine-tuned vision-language models, and self-supervised video transformers on 24 activity labels. For transcripts, we fine-tune a transformer-based classifier with contextualized inputs and compare it against prompting-based LLMs on 19 discourse labels. To handle class imbalance and multi-label complexity, we apply per-label thresholding, context windows, and imbalance-aware loss functions. The results show that fine-tuned models consistently outperform prompting-based approaches, achieving macro-F1 scores of 0.577 for video and 0.460 for transcripts. These results demonstrate the feasibility of automated classroom analysis and establish a foundation for scalable teacher feedback systems."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-26T11:57:22Z",
                "published_parsed": [
                    2025,
                    11,
                    26,
                    11,
                    57,
                    22,
                    2,
                    330,
                    0
                ],
                "arxiv_comment": "This article has been accepted for publication in the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) 2026",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Ivo Bueno"
                    },
                    {
                        "name": "Ruikun Hou"
                    },
                    {
                        "name": "Babette Bühler"
                    },
                    {
                        "name": "Tim Fütterer"
                    },
                    {
                        "name": "James Drimalla"
                    },
                    {
                        "name": "Jonathan Kyle Foster"
                    },
                    {
                        "name": "Peter Youngs"
                    },
                    {
                        "name": "Peter Gerjets"
                    },
                    {
                        "name": "Ulrich Trautwein"
                    },
                    {
                        "name": "Enkelejda Kasneci"
                    }
                ],
                "author_detail": {
                    "name": "Enkelejda Kasneci"
                },
                "author": "Enkelejda Kasneci"
            },
            {
                "id": "http://arxiv.org/abs/2407.03859v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2407.03859v3",
                "title": "Anthropocentric bias in language model evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anthropocentric bias in language model evaluation"
                },
                "updated": "2025-12-11T11:10:15Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    11,
                    10,
                    15,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2407.03859v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2407.03859v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1162/COLI.a.582",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Evaluating the cognitive capacities of large language models (LLMs) requires overcoming not only anthropomorphic but also anthropocentric biases. This article identifies two types of anthropocentric bias that have been neglected: overlooking how auxiliary factors can impede LLM performance despite competence (\"auxiliary oversight\"), and dismissing LLM mechanistic strategies that differ from those of humans as not genuinely competent (\"mechanistic chauvinism\"). Mitigating these biases necessitates an empirically-driven, iterative approach to mapping cognitive tasks to LLM-specific capacities and mechanisms, which can be done by supplementing carefully designed behavioral experiments with mechanistic studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the cognitive capacities of large language models (LLMs) requires overcoming not only anthropomorphic but also anthropocentric biases. This article identifies two types of anthropocentric bias that have been neglected: overlooking how auxiliary factors can impede LLM performance despite competence (\"auxiliary oversight\"), and dismissing LLM mechanistic strategies that differ from those of humans as not genuinely competent (\"mechanistic chauvinism\"). Mitigating these biases necessitates an empirically-driven, iterative approach to mapping cognitive tasks to LLM-specific capacities and mechanisms, which can be done by supplementing carefully designed behavioral experiments with mechanistic studies."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-07-04T11:44:28Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    11,
                    44,
                    28,
                    3,
                    186,
                    0
                ],
                "arxiv_comment": "Published in Computational Linguistics",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "arxiv_journal_ref": "Computational Linguistics, 1-10. (2025)",
                "authors": [
                    {
                        "name": "Raphaël Millière"
                    },
                    {
                        "name": "Charles Rathkopf"
                    }
                ],
                "author_detail": {
                    "name": "Charles Rathkopf"
                },
                "author": "Charles Rathkopf",
                "arxiv_doi": "10.1162/COLI.a.582"
            },
            {
                "id": "http://arxiv.org/abs/2512.10534v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10534v1",
                "title": "Achieving Olympia-Level Geometry Large Language Model Agent via Complexity Boosting Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Achieving Olympia-Level Geometry Large Language Model Agent via Complexity Boosting Reinforcement Learning"
                },
                "updated": "2025-12-11T11:05:04Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    11,
                    5,
                    4,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10534v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10534v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language model (LLM) agents exhibit strong mathematical problem-solving abilities and can even solve International Mathematical Olympiad (IMO) level problems with the assistance of formal proof systems. However, due to weak heuristics for auxiliary constructions, AI for geometry problem solving remains dominated by expert models such as AlphaGeometry 2, which rely heavily on large-scale data synthesis and search for both training and evaluation. In this work, we make the first attempt to build a medalist-level LLM agent for geometry and present InternGeometry. InternGeometry overcomes the heuristic limitations in geometry by iteratively proposing propositions and auxiliary constructions, verifying them with a symbolic engine, and reflecting on the engine's feedback to guide subsequent proposals. A dynamic memory mechanism enables InternGeometry to conduct more than two hundred interactions with the symbolic engine per problem. To further accelerate learning, we introduce Complexity-Boosting Reinforcement Learning (CBRL), which gradually increases the complexity of synthesized problems across training stages. Built on InternThinker-32B, InternGeometry solves 44 of 50 IMO geometry problems (2000-2024), exceeding the average gold medalist score (40.9), using only 13K training examples, just 0.004% of the data used by AlphaGeometry 2, demonstrating the potential of LLM agents on expert-level geometry tasks. InternGeometry can also propose novel auxiliary constructions for IMO problems that do not appear in human solutions. We will release the model, data, and symbolic engine to support future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) agents exhibit strong mathematical problem-solving abilities and can even solve International Mathematical Olympiad (IMO) level problems with the assistance of formal proof systems. However, due to weak heuristics for auxiliary constructions, AI for geometry problem solving remains dominated by expert models such as AlphaGeometry 2, which rely heavily on large-scale data synthesis and search for both training and evaluation. In this work, we make the first attempt to build a medalist-level LLM agent for geometry and present InternGeometry. InternGeometry overcomes the heuristic limitations in geometry by iteratively proposing propositions and auxiliary constructions, verifying them with a symbolic engine, and reflecting on the engine's feedback to guide subsequent proposals. A dynamic memory mechanism enables InternGeometry to conduct more than two hundred interactions with the symbolic engine per problem. To further accelerate learning, we introduce Complexity-Boosting Reinforcement Learning (CBRL), which gradually increases the complexity of synthesized problems across training stages. Built on InternThinker-32B, InternGeometry solves 44 of 50 IMO geometry problems (2000-2024), exceeding the average gold medalist score (40.9), using only 13K training examples, just 0.004% of the data used by AlphaGeometry 2, demonstrating the potential of LLM agents on expert-level geometry tasks. InternGeometry can also propose novel auxiliary constructions for IMO problems that do not appear in human solutions. We will release the model, data, and symbolic engine to support future research."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T11:05:04Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    11,
                    5,
                    4,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Haiteng Zhao"
                    },
                    {
                        "name": "Junhao Shen"
                    },
                    {
                        "name": "Yiming Zhang"
                    },
                    {
                        "name": "Songyang Gao"
                    },
                    {
                        "name": "Kuikun Liu"
                    },
                    {
                        "name": "Tianyou Ma"
                    },
                    {
                        "name": "Fan Zheng"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Wenwei Zhang"
                    },
                    {
                        "name": "Kai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Chen"
                },
                "author": "Kai Chen"
            },
            {
                "id": "http://arxiv.org/abs/2512.09830v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.09830v2",
                "title": "LLMs in Interpreting Legal Documents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs in Interpreting Legal Documents"
                },
                "updated": "2025-12-11T11:01:35Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    11,
                    1,
                    35,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.09830v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.09830v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This chapter explores the application of Large Language Models in the legal domain, showcasing their potential to optimise and augment traditional legal tasks by analysing possible use cases, such as assisting in interpreting statutes, contracts, and case law, enhancing clarity in legal summarisation, contract negotiation, and information retrieval. There are several challenges that can arise from the application of such technologies, such as algorithmic monoculture, hallucinations, and compliance with existing regulations, including the EU's AI Act and recent U.S. initiatives, alongside the emerging approaches in China. Furthermore, two different benchmarks are presented.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This chapter explores the application of Large Language Models in the legal domain, showcasing their potential to optimise and augment traditional legal tasks by analysing possible use cases, such as assisting in interpreting statutes, contracts, and case law, enhancing clarity in legal summarisation, contract negotiation, and information retrieval. There are several challenges that can arise from the application of such technologies, such as algorithmic monoculture, hallucinations, and compliance with existing regulations, including the EU's AI Act and recent U.S. initiatives, alongside the emerging approaches in China. Furthermore, two different benchmarks are presented."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-10T17:09:13Z",
                "published_parsed": [
                    2025,
                    12,
                    10,
                    17,
                    9,
                    13,
                    2,
                    344,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Simone Corbo"
                    }
                ],
                "author_detail": {
                    "name": "Simone Corbo"
                },
                "author": "Simone Corbo"
            },
            {
                "id": "http://arxiv.org/abs/2512.08844v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08844v2",
                "title": "A Methodology for Quantitative AI Risk Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Methodology for Quantitative AI Risk Modeling"
                },
                "updated": "2025-12-11T10:25:12Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    10,
                    25,
                    12,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08844v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08844v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Although general-purpose AI systems offer transformational opportunities in science and industry, they simultaneously raise critical concerns about safety, misuse, and potential loss of control. Despite these risks, methods for assessing and managing them remain underdeveloped. Effective risk management requires systematic modeling to characterize potential harms, as emphasized in frameworks such as the EU General-Purpose AI Code of Practice. This paper advances the risk modeling component of AI risk management by introducing a methodology that integrates scenario building with quantitative risk estimation, drawing on established approaches from other high-risk industries. Our methodology models risks through a six-step process: (1) defining risk scenarios, (2) decomposing them into quantifiable parameters, (3) quantifying baseline risk without AI models, (4) identifying key risk indicators such as benchmarks, (5) mapping these indicators to model parameters to estimate LLM uplift, and (6) aggregating individual parameters into risk estimates that enable concrete claims (e.g., X% probability of >\\$Y in annual cyber damages). We examine the choices that underlie our methodology throughout the article, with discussions of strengths, limitations, and implications for future research. Our methodology is designed to be applicable to key systemic AI risks, including cyber offense, biological weapon development, harmful manipulation, and loss-of-control, and is validated through extensive application in LLM-enabled cyber offense. Detailed empirical results and cyber-specific insights are presented in a companion paper.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although general-purpose AI systems offer transformational opportunities in science and industry, they simultaneously raise critical concerns about safety, misuse, and potential loss of control. Despite these risks, methods for assessing and managing them remain underdeveloped. Effective risk management requires systematic modeling to characterize potential harms, as emphasized in frameworks such as the EU General-Purpose AI Code of Practice. This paper advances the risk modeling component of AI risk management by introducing a methodology that integrates scenario building with quantitative risk estimation, drawing on established approaches from other high-risk industries. Our methodology models risks through a six-step process: (1) defining risk scenarios, (2) decomposing them into quantifiable parameters, (3) quantifying baseline risk without AI models, (4) identifying key risk indicators such as benchmarks, (5) mapping these indicators to model parameters to estimate LLM uplift, and (6) aggregating individual parameters into risk estimates that enable concrete claims (e.g., X% probability of >\\$Y in annual cyber damages). We examine the choices that underlie our methodology throughout the article, with discussions of strengths, limitations, and implications for future research. Our methodology is designed to be applicable to key systemic AI risks, including cyber offense, biological weapon development, harmful manipulation, and loss-of-control, and is validated through extensive application in LLM-enabled cyber offense. Detailed empirical results and cyber-specific insights are presented in a companion paper."
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T17:34:59Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    17,
                    34,
                    59,
                    1,
                    343,
                    0
                ],
                "arxiv_comment": "The only changes in v2 are some updates to a few arXiv URLs in the references",
                "arxiv_primary_category": {
                    "term": "cs.CY"
                },
                "authors": [
                    {
                        "name": "Malcolm Murray"
                    },
                    {
                        "name": "Steve Barrett"
                    },
                    {
                        "name": "Henry Papadatos"
                    },
                    {
                        "name": "Otter Quarks"
                    },
                    {
                        "name": "Matt Smith"
                    },
                    {
                        "name": "Alejandro Tlaie Boria"
                    },
                    {
                        "name": "Chloé Touzet"
                    },
                    {
                        "name": "Siméon Campos"
                    }
                ],
                "author_detail": {
                    "name": "Siméon Campos"
                },
                "author": "Siméon Campos"
            },
            {
                "id": "http://arxiv.org/abs/2506.01524v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.01524v2",
                "title": "V-VAE: A Variational Auto Encoding Framework Towards Fine-Grained Control over Human-Like Chat",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "V-VAE: A Variational Auto Encoding Framework Towards Fine-Grained Control over Human-Like Chat"
                },
                "updated": "2025-12-11T10:22:55Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    10,
                    22,
                    55,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.01524v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.01524v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "With the continued proliferation of Large Language Model (LLM) based chatbots, there is a growing demand for generating responses that are not only linguistically fluent but also consistently aligned with persona-specific traits in conversations. However, existing role-play and persona-based chat approaches rely heavily on static role descriptions, coarse-grained signal space, and low-quality synthetic data, which fail to capture dynamic fine-grained details in human-like chat. Human-like chat requires modeling subtle latent traits, such as emotional tone, situational awareness, and evolving personality, which are difficult to predefine and cannot be easily learned from synthetic or distillation-based data. To address these limitations, we propose a Verbal Variational Auto-Encoding (V-VAE) framework, containing a variational auto-encoding module and fine-grained control space which dynamically adapts dialogue behaviour based on fine-grained, interpretable latent variables across talking style, interaction patterns, and personal attributes. We also construct a high-quality dataset, HumanChatData, and benchmark HumanChatBench to address the scarcity of high-quality data in the human-like domain. Experiments show that LLMs based on V-VAE consistently outperform standard baselines on HumanChatBench and DialogBench, which further demonstrates the effectiveness of V-VAE and HumanChatData.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the continued proliferation of Large Language Model (LLM) based chatbots, there is a growing demand for generating responses that are not only linguistically fluent but also consistently aligned with persona-specific traits in conversations. However, existing role-play and persona-based chat approaches rely heavily on static role descriptions, coarse-grained signal space, and low-quality synthetic data, which fail to capture dynamic fine-grained details in human-like chat. Human-like chat requires modeling subtle latent traits, such as emotional tone, situational awareness, and evolving personality, which are difficult to predefine and cannot be easily learned from synthetic or distillation-based data. To address these limitations, we propose a Verbal Variational Auto-Encoding (V-VAE) framework, containing a variational auto-encoding module and fine-grained control space which dynamically adapts dialogue behaviour based on fine-grained, interpretable latent variables across talking style, interaction patterns, and personal attributes. We also construct a high-quality dataset, HumanChatData, and benchmark HumanChatBench to address the scarcity of high-quality data in the human-like domain. Experiments show that LLMs based on V-VAE consistently outperform standard baselines on HumanChatBench and DialogBench, which further demonstrates the effectiveness of V-VAE and HumanChatData."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-02T10:38:02Z",
                "published_parsed": [
                    2025,
                    6,
                    2,
                    10,
                    38,
                    2,
                    0,
                    153,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Qi Lin"
                    },
                    {
                        "name": "Weikai Xu"
                    },
                    {
                        "name": "Lisi Chen"
                    },
                    {
                        "name": "Bin Dai"
                    }
                ],
                "author_detail": {
                    "name": "Bin Dai"
                },
                "author": "Bin Dai"
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2512.10947v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10947v1",
                "title": "Towards Efficient and Effective Multi-Camera Encoding for End-to-End Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Efficient and Effective Multi-Camera Encoding for End-to-End Driving"
                },
                "updated": "2025-12-11T18:59:46Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    18,
                    59,
                    46,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10947v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10947v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We present Flex, an efficient and effective scene encoder that addresses the computational bottleneck of processing high-volume multi-camera data in end-to-end autonomous driving. Flex employs a small set of learnable scene tokens to jointly encode information from all image tokens across different cameras and timesteps. By design, our approach is geometry-agnostic, learning a compact scene representation directly from data without relying on the explicit 3D inductive biases, such as Bird-Eye-View (BEV), occupancy or tri-plane representations, which are common in prior work. This holistic encoding strategy aggressively compresses the visual input for the downstream Large Language Model (LLM) based policy model. Evaluated on a large-scale proprietary dataset of 20,000 driving hours, our Flex achieves 2.2x greater inference throughput while improving driving performance by a large margin compared to state-of-the-art methods. Furthermore, we show that these compact scene tokens develop an emergent capability for scene decomposition without any explicit supervision. Our findings challenge the prevailing assumption that 3D priors are necessary, demonstrating that a data-driven, joint encoding strategy offers a more scalable, efficient and effective path for future autonomous driving systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Flex, an efficient and effective scene encoder that addresses the computational bottleneck of processing high-volume multi-camera data in end-to-end autonomous driving. Flex employs a small set of learnable scene tokens to jointly encode information from all image tokens across different cameras and timesteps. By design, our approach is geometry-agnostic, learning a compact scene representation directly from data without relying on the explicit 3D inductive biases, such as Bird-Eye-View (BEV), occupancy or tri-plane representations, which are common in prior work. This holistic encoding strategy aggressively compresses the visual input for the downstream Large Language Model (LLM) based policy model. Evaluated on a large-scale proprietary dataset of 20,000 driving hours, our Flex achieves 2.2x greater inference throughput while improving driving performance by a large margin compared to state-of-the-art methods. Furthermore, we show that these compact scene tokens develop an emergent capability for scene decomposition without any explicit supervision. Our findings challenge the prevailing assumption that 3D priors are necessary, demonstrating that a data-driven, joint encoding strategy offers a more scalable, efficient and effective path for future autonomous driving systems."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T18:59:46Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    18,
                    59,
                    46,
                    3,
                    345,
                    0
                ],
                "arxiv_comment": "Project Page: https://jiawei-yang.github.io/Flex/",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Jiawei Yang"
                    },
                    {
                        "name": "Ziyu Chen"
                    },
                    {
                        "name": "Yurong You"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Yuxiao Chen"
                    },
                    {
                        "name": "Boyi Li"
                    },
                    {
                        "name": "Boris Ivanovic"
                    },
                    {
                        "name": "Marco Pavone"
                    },
                    {
                        "name": "Yue Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yue Wang"
                },
                "author": "Yue Wang"
            },
            {
                "id": "http://arxiv.org/abs/2512.10936v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10936v1",
                "title": "Empirical evaluation of the Frank-Wolfe methods for constructing white-box adversarial attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empirical evaluation of the Frank-Wolfe methods for constructing white-box adversarial attacks"
                },
                "updated": "2025-12-11T18:58:17Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    18,
                    58,
                    17,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10936v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10936v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The construction of adversarial attacks for neural networks appears to be a crucial challenge for their deployment in various services. To estimate the adversarial robustness of a neural network, a fast and efficient approach is needed to construct adversarial attacks. Since the formalization of adversarial attack construction involves solving a specific optimization problem, we consider the problem of constructing an efficient and effective adversarial attack from a numerical optimization perspective. Specifically, we suggest utilizing advanced projection-free methods, known as modified Frank-Wolfe methods, to construct white-box adversarial attacks on the given input data. We perform a theoretical and numerical evaluation of these methods and compare them with standard approaches based on projection operations or geometrical intuition. Numerical experiments are performed on the MNIST and CIFAR-10 datasets, utilizing a multiclass logistic regression model, the convolutional neural networks (CNNs), and the Vision Transformer (ViT).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The construction of adversarial attacks for neural networks appears to be a crucial challenge for their deployment in various services. To estimate the adversarial robustness of a neural network, a fast and efficient approach is needed to construct adversarial attacks. Since the formalization of adversarial attack construction involves solving a specific optimization problem, we consider the problem of constructing an efficient and effective adversarial attack from a numerical optimization perspective. Specifically, we suggest utilizing advanced projection-free methods, known as modified Frank-Wolfe methods, to construct white-box adversarial attacks on the given input data. We perform a theoretical and numerical evaluation of these methods and compare them with standard approaches based on projection operations or geometrical intuition. Numerical experiments are performed on the MNIST and CIFAR-10 datasets, utilizing a multiclass logistic regression model, the convolutional neural networks (CNNs), and the Vision Transformer (ViT)."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T18:58:17Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    18,
                    58,
                    17,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Kristina Korotkova"
                    },
                    {
                        "name": "Aleksandr Katrutsa"
                    }
                ],
                "author_detail": {
                    "name": "Aleksandr Katrutsa"
                },
                "author": "Aleksandr Katrutsa"
            },
            {
                "id": "http://arxiv.org/abs/2512.10931v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10931v1",
                "title": "Asynchronous Reasoning: Training-Free Interactive Thinking LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Asynchronous Reasoning: Training-Free Interactive Thinking LLMs"
                },
                "updated": "2025-12-11T18:57:02Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    18,
                    57,
                    2,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10931v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10931v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Many state-of-the-art LLMs are trained to think before giving their answer. Reasoning can greatly improve language model capabilities and safety, but it also makes them less interactive: given a new input, a model must stop thinking before it can respond. Real-world use cases such as voice-based or embedded assistants require an LLM agent to respond and adapt to additional information in real time, which is incompatible with sequential interactions. In contrast, humans can listen, think, and act asynchronously: we begin thinking about the problem while reading it and continue thinking while formulating the answer. In this work, we augment LLMs capable of reasoning to operate in a similar way without additional training. Our method uses the properties of rotary embeddings to enable LLMs built for sequential interactions to simultaneously think, listen, and generate outputs. We evaluate our approach on math, commonsense, and safety reasoning and find that it can generate accurate thinking-augmented answers in real time, reducing time to first non-thinking token from minutes to <= 5s. and the overall real-time delays by 6-11x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many state-of-the-art LLMs are trained to think before giving their answer. Reasoning can greatly improve language model capabilities and safety, but it also makes them less interactive: given a new input, a model must stop thinking before it can respond. Real-world use cases such as voice-based or embedded assistants require an LLM agent to respond and adapt to additional information in real time, which is incompatible with sequential interactions. In contrast, humans can listen, think, and act asynchronously: we begin thinking about the problem while reading it and continue thinking while formulating the answer. In this work, we augment LLMs capable of reasoning to operate in a similar way without additional training. Our method uses the properties of rotary embeddings to enable LLMs built for sequential interactions to simultaneously think, listen, and generate outputs. We evaluate our approach on math, commonsense, and safety reasoning and find that it can generate accurate thinking-augmented answers in real time, reducing time to first non-thinking token from minutes to <= 5s. and the overall real-time delays by 6-11x."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T18:57:02Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    18,
                    57,
                    2,
                    3,
                    345,
                    0
                ],
                "arxiv_comment": "Preprint, work in progress",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "George Yakushev"
                    },
                    {
                        "name": "Nataliia Babina"
                    },
                    {
                        "name": "Masoud Vahid Dastgerdi"
                    },
                    {
                        "name": "Vyacheslav Zhdanovskiy"
                    },
                    {
                        "name": "Alina Shutova"
                    },
                    {
                        "name": "Denis Kuznedelev"
                    }
                ],
                "author_detail": {
                    "name": "Denis Kuznedelev"
                },
                "author": "Denis Kuznedelev"
            },
            {
                "id": "http://arxiv.org/abs/2512.10927v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10927v1",
                "title": "FoundationMotion: Auto-Labeling and Reasoning about Spatial Movement in Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FoundationMotion: Auto-Labeling and Reasoning about Spatial Movement in Videos"
                },
                "updated": "2025-12-11T18:53:15Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    18,
                    53,
                    15,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10927v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10927v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Motion understanding is fundamental to physical reasoning, enabling models to infer dynamics and predict future states. However, state-of-the-art models still struggle on recent motion benchmarks, primarily due to the scarcity of large-scale, fine-grained motion datasets. Existing motion datasets are often constructed from costly manual annotation, severely limiting scalability. To address this challenge, we introduce FoundationMotion, a fully automated data curation pipeline that constructs large-scale motion datasets. Our approach first detects and tracks objects in videos to extract their trajectories, then leverages these trajectories and video frames with Large Language Models (LLMs) to generate fine-grained captions and diverse question-answer pairs about motion and spatial reasoning. Using datasets produced by this pipeline, we fine-tune open-source models including NVILA-Video-15B and Qwen2.5-7B, achieving substantial improvements in motion understanding without compromising performance on other tasks. Notably, our models outperform strong closed-source baselines like Gemini-2.5 Flash and large open-source models such as Qwen2.5-VL-72B across diverse motion understanding datasets and benchmarks. FoundationMotion thus provides a scalable solution for curating fine-grained motion datasets that enable effective fine-tuning of diverse models to enhance motion understanding and spatial reasoning capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motion understanding is fundamental to physical reasoning, enabling models to infer dynamics and predict future states. However, state-of-the-art models still struggle on recent motion benchmarks, primarily due to the scarcity of large-scale, fine-grained motion datasets. Existing motion datasets are often constructed from costly manual annotation, severely limiting scalability. To address this challenge, we introduce FoundationMotion, a fully automated data curation pipeline that constructs large-scale motion datasets. Our approach first detects and tracks objects in videos to extract their trajectories, then leverages these trajectories and video frames with Large Language Models (LLMs) to generate fine-grained captions and diverse question-answer pairs about motion and spatial reasoning. Using datasets produced by this pipeline, we fine-tune open-source models including NVILA-Video-15B and Qwen2.5-7B, achieving substantial improvements in motion understanding without compromising performance on other tasks. Notably, our models outperform strong closed-source baselines like Gemini-2.5 Flash and large open-source models such as Qwen2.5-VL-72B across diverse motion understanding datasets and benchmarks. FoundationMotion thus provides a scalable solution for curating fine-grained motion datasets that enable effective fine-tuning of diverse models to enhance motion understanding and spatial reasoning capabilities."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T18:53:15Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    18,
                    53,
                    15,
                    3,
                    345,
                    0
                ],
                "arxiv_comment": "Code is available at https://github.com/Wolfv0/FoundationMotion/tree/main",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Yulu Gan"
                    },
                    {
                        "name": "Ligeng Zhu"
                    },
                    {
                        "name": "Dandan Shan"
                    },
                    {
                        "name": "Baifeng Shi"
                    },
                    {
                        "name": "Hongxu Yin"
                    },
                    {
                        "name": "Boris Ivanovic"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Trevor Darrell"
                    },
                    {
                        "name": "Jitendra Malik"
                    },
                    {
                        "name": "Marco Pavone"
                    },
                    {
                        "name": "Boyi Li"
                    }
                ],
                "author_detail": {
                    "name": "Boyi Li"
                },
                "author": "Boyi Li"
            },
            {
                "id": "http://arxiv.org/abs/2512.06982v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.06982v2",
                "title": "LLM-Driven Composite Neural Architecture Search for Multi-Source RL State Encoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Driven Composite Neural Architecture Search for Multi-Source RL State Encoding"
                },
                "updated": "2025-12-11T18:52:44Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    18,
                    52,
                    44,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.06982v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.06982v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Designing state encoders for reinforcement learning (RL) with multiple information sources -- such as sensor measurements, time-series signals, image observations, and textual instructions -- remains underexplored and often requires manual design. We formalize this challenge as a problem of composite neural architecture search (NAS), where multiple source-specific modules and a fusion module are jointly optimized. Existing NAS methods overlook useful side information from the intermediate outputs of these modules -- such as their representation quality -- limiting sample efficiency in multi-source RL settings. To address this, we propose an LLM-driven NAS pipeline in which the LLM serves as a neural architecture design agent, leveraging language-model priors and intermediate-output signals to guide sample-efficient search for high-performing composite state encoders. On a mixed-autonomy traffic control task, our approach discovers higher-performing architectures with fewer candidate evaluations than traditional NAS baselines and the LLM-based GENIUS framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing state encoders for reinforcement learning (RL) with multiple information sources -- such as sensor measurements, time-series signals, image observations, and textual instructions -- remains underexplored and often requires manual design. We formalize this challenge as a problem of composite neural architecture search (NAS), where multiple source-specific modules and a fusion module are jointly optimized. Existing NAS methods overlook useful side information from the intermediate outputs of these modules -- such as their representation quality -- limiting sample efficiency in multi-source RL settings. To address this, we propose an LLM-driven NAS pipeline in which the LLM serves as a neural architecture design agent, leveraging language-model priors and intermediate-output signals to guide sample-efficient search for high-performing composite state encoders. On a mixed-autonomy traffic control task, our approach discovers higher-performing architectures with fewer candidate evaluations than traditional NAS baselines and the LLM-based GENIUS framework."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-07T20:25:07Z",
                "published_parsed": [
                    2025,
                    12,
                    7,
                    20,
                    25,
                    7,
                    6,
                    341,
                    0
                ],
                "arxiv_comment": "NeurIPS 2025 Workshop on Bridging Language, Agent, and World Models for Reasoning and Planning",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Yu Yu"
                    },
                    {
                        "name": "Qian Xie"
                    },
                    {
                        "name": "Nairen Cao"
                    },
                    {
                        "name": "Li Jin"
                    }
                ],
                "author_detail": {
                    "name": "Li Jin"
                },
                "author": "Li Jin"
            },
            {
                "id": "http://arxiv.org/abs/2504.20101v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2504.20101v4",
                "title": "PlanetServe: A Decentralized, Scalable, and Privacy-Preserving Overlay for Democratizing Large Language Model Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PlanetServe: A Decentralized, Scalable, and Privacy-Preserving Overlay for Democratizing Large Language Model Serving"
                },
                "updated": "2025-12-11T18:49:32Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    18,
                    49,
                    32,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2504.20101v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2504.20101v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "While significant progress has been made in research and development on open-source and cost-efficient large-language models (LLMs), serving scalability remains a critical challenge, particularly for small organizations and individuals seeking to deploy and test their LLM innovations. Inspired by peer-to-peer networks that leverage decentralized overlay nodes to increase throughput and availability, we propose GenTorrent, an LLM serving overlay that harnesses computing resources from decentralized contributors. We identify four key research problems inherent to enabling such a decentralized infrastructure: 1) overlay network organization; 2) LLM communication privacy; 3) overlay forwarding for resource efficiency; and 4) verification of serving quality. This work presents the first systematic study of these fundamental problems in the context of decentralized LLM serving. Evaluation results from a prototype implemented on a set of decentralized nodes demonstrate that GenTorrent achieves a latency reduction of over 50% compared to the baseline design without overlay forwarding. Furthermore, the security features introduce minimal overhead to serving latency and throughput. We believe this work pioneers a new direction for democratizing and scaling future AI serving capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While significant progress has been made in research and development on open-source and cost-efficient large-language models (LLMs), serving scalability remains a critical challenge, particularly for small organizations and individuals seeking to deploy and test their LLM innovations. Inspired by peer-to-peer networks that leverage decentralized overlay nodes to increase throughput and availability, we propose GenTorrent, an LLM serving overlay that harnesses computing resources from decentralized contributors. We identify four key research problems inherent to enabling such a decentralized infrastructure: 1) overlay network organization; 2) LLM communication privacy; 3) overlay forwarding for resource efficiency; and 4) verification of serving quality. This work presents the first systematic study of these fundamental problems in the context of decentralized LLM serving. Evaluation results from a prototype implemented on a set of decentralized nodes demonstrate that GenTorrent achieves a latency reduction of over 50% compared to the baseline design without overlay forwarding. Furthermore, the security features introduce minimal overhead to serving latency and throughput. We believe this work pioneers a new direction for democratizing and scaling future AI serving capabilities."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-04-27T01:08:25Z",
                "published_parsed": [
                    2025,
                    4,
                    27,
                    1,
                    8,
                    25,
                    6,
                    117,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Fei Fang"
                    },
                    {
                        "name": "Yifan Hua"
                    },
                    {
                        "name": "Shengze Wang"
                    },
                    {
                        "name": "Ruilin Zhou"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Chen Qian"
                    },
                    {
                        "name": "Xiaoxue Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoxue Zhang"
                },
                "author": "Xiaoxue Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2512.10922v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10922v1",
                "title": "SparseSwaps: Tractable LLM Pruning Mask Refinement at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseSwaps: Tractable LLM Pruning Mask Refinement at Scale"
                },
                "updated": "2025-12-11T18:47:48Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    18,
                    47,
                    48,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10922v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10922v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The resource requirements of Neural Networks can be significantly reduced through pruning -- the removal of seemingly less important parameters. However, with the rise of Large Language Models (LLMs), full retraining to recover pruning-induced performance degradation is often prohibitive and classical approaches such as global magnitude pruning are suboptimal on Transformer architectures. State-of-the-art methods hence solve a layer-wise mask selection problem, the problem of finding a pruning mask which minimizes the per-layer pruning error on a small set of calibration data. Exactly solving this problem to optimality using Integer Programming (IP) solvers is computationally infeasible due to its combinatorial nature and the size of the search space, and existing approaches therefore rely on approximations or heuristics. In this work, we demonstrate that the mask selection problem can be made drastically more tractable at LLM scale. To that end, we decouple the rows by enforcing equal sparsity levels per row. This allows us to derive optimal 1-swaps (exchanging one kept and one pruned weight) that can be computed efficiently using the Gram matrix of the calibration data. Using these observations, we propose a tractable and simple 1-swap algorithm that warm starts from any pruning mask, runs efficiently on GPUs at LLM scale, and is essentially hyperparameter-free. We demonstrate that our approach reduces per-layer pruning error by up to 60% over Wanda (Sun et al., 2023) and consistently improves perplexity and zero-shot accuracy across state-of-the-art GPT architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The resource requirements of Neural Networks can be significantly reduced through pruning -- the removal of seemingly less important parameters. However, with the rise of Large Language Models (LLMs), full retraining to recover pruning-induced performance degradation is often prohibitive and classical approaches such as global magnitude pruning are suboptimal on Transformer architectures. State-of-the-art methods hence solve a layer-wise mask selection problem, the problem of finding a pruning mask which minimizes the per-layer pruning error on a small set of calibration data. Exactly solving this problem to optimality using Integer Programming (IP) solvers is computationally infeasible due to its combinatorial nature and the size of the search space, and existing approaches therefore rely on approximations or heuristics. In this work, we demonstrate that the mask selection problem can be made drastically more tractable at LLM scale. To that end, we decouple the rows by enforcing equal sparsity levels per row. This allows us to derive optimal 1-swaps (exchanging one kept and one pruned weight) that can be computed efficiently using the Gram matrix of the calibration data. Using these observations, we propose a tractable and simple 1-swap algorithm that warm starts from any pruning mask, runs efficiently on GPUs at LLM scale, and is essentially hyperparameter-free. We demonstrate that our approach reduces per-layer pruning error by up to 60% over Wanda (Sun et al., 2023) and consistently improves perplexity and zero-shot accuracy across state-of-the-art GPT architectures."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T18:47:48Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    18,
                    47,
                    48,
                    3,
                    345,
                    0
                ],
                "arxiv_comment": "15 pages, 2 figures, 4 tables",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Max Zimmer"
                    },
                    {
                        "name": "Christophe Roux"
                    },
                    {
                        "name": "Moritz Wagner"
                    },
                    {
                        "name": "Deborah Hendrych"
                    },
                    {
                        "name": "Sebastian Pokutta"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Pokutta"
                },
                "author": "Sebastian Pokutta"
            },
            {
                "id": "http://arxiv.org/abs/2509.06120v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.06120v2",
                "title": "If generative AI is the answer, what is the question?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "If generative AI is the answer, what is the question?"
                },
                "updated": "2025-12-11T18:45:18Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    18,
                    45,
                    18,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.06120v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.06120v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Beginning with text and images, generative AI has expanded to audio, video, computer code, and molecules. Yet, if generative AI is the answer, what is the question? We explore the foundations of generation as a distinct machine learning task with connections to prediction, compression, and decision-making. We survey five major generative model families: autoregressive models, variational autoencoders, normalizing flows, generative adversarial networks, and diffusion models. We then introduce a probabilistic framework that emphasizes the distinction between density estimation and generation. We review a game-theoretic framework with a two-player adversary-learner setup to study generation. We discuss post-training modifications that prepare generative models for deployment. We end by highlighting some important topics in socially responsible generation such as privacy, detection of AI-generated content, and copyright and IP. We adopt a task-first framing of generation, focusing on what generation is as a machine learning problem, rather than only on how models implement it.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beginning with text and images, generative AI has expanded to audio, video, computer code, and molecules. Yet, if generative AI is the answer, what is the question? We explore the foundations of generation as a distinct machine learning task with connections to prediction, compression, and decision-making. We survey five major generative model families: autoregressive models, variational autoencoders, normalizing flows, generative adversarial networks, and diffusion models. We then introduce a probabilistic framework that emphasizes the distinction between density estimation and generation. We review a game-theoretic framework with a two-player adversary-learner setup to study generation. We discuss post-training modifications that prepare generative models for deployment. We end by highlighting some important topics in socially responsible generation such as privacy, detection of AI-generated content, and copyright and IP. We adopt a task-first framing of generation, focusing on what generation is as a machine learning problem, rather than only on how models implement it."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-07T16:07:45Z",
                "published_parsed": [
                    2025,
                    9,
                    7,
                    16,
                    7,
                    45,
                    6,
                    250,
                    0
                ],
                "arxiv_comment": "To appear as a book chapter in a Springer book titled \"Statistical Foundations and Applications of Artificial Intelligence, Machine Learning and Deep Learning\" and edited by S. Ejaz Ahmed, Pierre Alquier, Yi Li, Shuangge Ma",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Ambuj Tewari"
                    }
                ],
                "author_detail": {
                    "name": "Ambuj Tewari"
                },
                "author": "Ambuj Tewari"
            },
            {
                "id": "http://arxiv.org/abs/2512.10918v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10918v1",
                "title": "CompanionCast: A Multi-Agent Conversational AI Framework with Spatial Audio for Social Co-Viewing Experiences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CompanionCast: A Multi-Agent Conversational AI Framework with Spatial Audio for Social Co-Viewing Experiences"
                },
                "updated": "2025-12-11T18:44:44Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    18,
                    44,
                    44,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10918v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10918v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Social presence is central to the enjoyment of watching content together, yet modern media consumption is increasingly solitary. We investigate whether multi-agent conversational AI systems can recreate the dynamics of shared viewing experiences across diverse content types. We present CompanionCast, a general framework for orchestrating multiple role-specialized AI agents that respond to video content using multimodal inputs, speech synthesis, and spatial audio. Distinctly, CompanionCast integrates an LLM-as-a-Judge module that iteratively scores and refines conversations across five dimensions (relevance, authenticity, engagement, diversity, personality consistency). We validate this framework through sports viewing, a domain with rich dynamics and strong social traditions, where a pilot study with soccer fans suggests that multi-agent interaction improves perceived social presence compared to solo viewing. We contribute: (1) a generalizable framework for orchestrating multi-agent conversations around multimodal video content, (2) a novel evaluator-agent pipeline for conversation quality control, and (3) exploratory evidence of increased social presence in AI-mediated co-viewing. We discuss challenges and future directions for applying this approach to diverse viewing contexts including entertainment, education, and collaborative watching experiences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Social presence is central to the enjoyment of watching content together, yet modern media consumption is increasingly solitary. We investigate whether multi-agent conversational AI systems can recreate the dynamics of shared viewing experiences across diverse content types. We present CompanionCast, a general framework for orchestrating multiple role-specialized AI agents that respond to video content using multimodal inputs, speech synthesis, and spatial audio. Distinctly, CompanionCast integrates an LLM-as-a-Judge module that iteratively scores and refines conversations across five dimensions (relevance, authenticity, engagement, diversity, personality consistency). We validate this framework through sports viewing, a domain with rich dynamics and strong social traditions, where a pilot study with soccer fans suggests that multi-agent interaction improves perceived social presence compared to solo viewing. We contribute: (1) a generalizable framework for orchestrating multi-agent conversations around multimodal video content, (2) a novel evaluator-agent pipeline for conversation quality control, and (3) exploratory evidence of increased social presence in AI-mediated co-viewing. We discuss challenges and future directions for applying this approach to diverse viewing contexts including entertainment, education, and collaborative watching experiences."
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T18:44:44Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    18,
                    44,
                    44,
                    3,
                    345,
                    0
                ],
                "arxiv_comment": "11 pages",
                "arxiv_primary_category": {
                    "term": "cs.HC"
                },
                "authors": [
                    {
                        "name": "Yiyang Wang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Tica Lin"
                    },
                    {
                        "name": "Vishnu Raj"
                    },
                    {
                        "name": "Josh Kimball"
                    },
                    {
                        "name": "Alex Cabral"
                    },
                    {
                        "name": "Josiah Hester"
                    }
                ],
                "author_detail": {
                    "name": "Josiah Hester"
                },
                "author": "Josiah Hester"
            },
            {
                "id": "http://arxiv.org/abs/2512.10913v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10913v1",
                "title": "Reinforcement Learning in Financial Decision Making: A Systematic Review of Performance, Challenges, and Implementation Strategies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning in Financial Decision Making: A Systematic Review of Performance, Challenges, and Implementation Strategies"
                },
                "updated": "2025-12-11T18:42:19Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    18,
                    42,
                    19,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10913v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10913v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Reinforcement learning (RL) is an innovative approach to financial decision making, offering specialized solutions to complex investment problems where traditional methods fail. This review analyzes 167 articles from 2017--2025, focusing on market making, portfolio optimization, and algorithmic trading. It identifies key performance issues and challenges in RL for finance. Generally, RL offers advantages over traditional methods, particularly in market making. This study proposes a unified framework to address common concerns such as explainability, robustness, and deployment feasibility. Empirical evidence with synthetic data suggests that implementation quality and domain knowledge often outweigh algorithmic complexity. The study highlights the need for interpretable RL architectures for regulatory compliance, enhanced robustness in nonstationary environments, and standardized benchmarking protocols. Organizations should focus less on algorithm sophistication and more on market microstructure, regulatory constraints, and risk management in decision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) is an innovative approach to financial decision making, offering specialized solutions to complex investment problems where traditional methods fail. This review analyzes 167 articles from 2017--2025, focusing on market making, portfolio optimization, and algorithmic trading. It identifies key performance issues and challenges in RL for finance. Generally, RL offers advantages over traditional methods, particularly in market making. This study proposes a unified framework to address common concerns such as explainability, robustness, and deployment feasibility. Empirical evidence with synthetic data suggests that implementation quality and domain knowledge often outweigh algorithmic complexity. The study highlights the need for interpretable RL architectures for regulatory compliance, enhanced robustness in nonstationary environments, and standardized benchmarking protocols. Organizations should focus less on algorithm sophistication and more on market microstructure, regulatory constraints, and risk management in decision-making."
                },
                "tags": [
                    {
                        "term": "q-fin.CP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T18:42:19Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    18,
                    42,
                    19,
                    3,
                    345,
                    0
                ],
                "arxiv_comment": "Paper submitted to Management Science",
                "arxiv_primary_category": {
                    "term": "q-fin.CP"
                },
                "authors": [
                    {
                        "name": "Mohammad Rezoanul Hoque"
                    },
                    {
                        "name": "Md Meftahul Ferdaus"
                    },
                    {
                        "name": "M. Kabir Hassan"
                    }
                ],
                "author_detail": {
                    "name": "M. Kabir Hassan"
                },
                "author": "M. Kabir Hassan"
            },
            {
                "id": "http://arxiv.org/abs/2512.10903v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10903v1",
                "title": "Multi-Granular Node Pruning for Circuit Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Granular Node Pruning for Circuit Discovery"
                },
                "updated": "2025-12-11T18:32:15Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    18,
                    32,
                    15,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10903v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10903v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Circuit discovery aims to identify minimal subnetworks that are responsible for specific behaviors in large language models (LLMs). Existing approaches primarily rely on iterative edge pruning, which is computationally expensive and limited to coarse-grained units such as attention heads or MLP blocks, overlooking finer structures like individual neurons. We propose a node-level pruning framework for circuit discovery that addresses both scalability and granularity limitations. Our method introduces learnable masks across multiple levels of granularity, from entire blocks to individual neurons, within a unified optimization objective. Granularity-specific sparsity penalties guide the pruning process, allowing a comprehensive compression in a single fine-tuning run. Empirically, our approach identifies circuits that are smaller in nodes than those discovered by prior methods; moreover, we demonstrate that many neurons deemed important by coarse methods are actually irrelevant, while still maintaining task performance. Furthermore, our method has a significantly lower memory footprint, 5-10x, as it does not require keeping intermediate activations in the memory to work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Circuit discovery aims to identify minimal subnetworks that are responsible for specific behaviors in large language models (LLMs). Existing approaches primarily rely on iterative edge pruning, which is computationally expensive and limited to coarse-grained units such as attention heads or MLP blocks, overlooking finer structures like individual neurons. We propose a node-level pruning framework for circuit discovery that addresses both scalability and granularity limitations. Our method introduces learnable masks across multiple levels of granularity, from entire blocks to individual neurons, within a unified optimization objective. Granularity-specific sparsity penalties guide the pruning process, allowing a comprehensive compression in a single fine-tuning run. Empirically, our approach identifies circuits that are smaller in nodes than those discovered by prior methods; moreover, we demonstrate that many neurons deemed important by coarse methods are actually irrelevant, while still maintaining task performance. Furthermore, our method has a significantly lower memory footprint, 5-10x, as it does not require keeping intermediate activations in the memory to work."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T18:32:15Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    18,
                    32,
                    15,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Muhammad Umair Haider"
                    },
                    {
                        "name": "Hammad Rizwan"
                    },
                    {
                        "name": "Hassan Sajjad"
                    },
                    {
                        "name": "A. B. Siddique"
                    }
                ],
                "author_detail": {
                    "name": "A. B. Siddique"
                },
                "author": "A. B. Siddique"
            },
            {
                "id": "http://arxiv.org/abs/2512.10895v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10895v1",
                "title": "LLMs Can Assist with Proposal Selection at Large User Facilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs Can Assist with Proposal Selection at Large User Facilities"
                },
                "updated": "2025-12-11T18:23:56Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    18,
                    23,
                    56,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10895v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10895v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We explore how large language models (LLMs) can enhance the proposal selection process at large user facilities, offering a scalable, consistent, and cost-effective alternative to traditional human review. Proposal selection depends on assessing the relative strength among submitted proposals; however, traditional human scoring often suffers from weak inter-proposal correlations and is subject to reviewer bias and inconsistency. A pairwise preference-based approach is logically superior, providing a more rigorous and internally consistent basis for ranking, but its quadratic workload makes it impractical for human reviewers. We address this limitation using LLMs. Leveraging the uniquely well-curated proposals and publication records from three beamlines at the Spallation Neutron Source (SNS), Oak Ridge National Laboratory (ORNL), we show that the LLM rankings correlate strongly with the human rankings (Spearman $ρ\\simeq 0.2-0.8$, improving to $\\geq 0.5$ after 10\\% outlier removal). Moreover, LLM performance is no worse than that of human reviewers in identifying proposals with high publication potential, while costing over two orders of magnitude less. Beyond ranking, LLMs enable advanced analyses that are challenging for humans, such as quantitative assessment of proposal similarity via embedding models, which provides information crucial for review committees.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We explore how large language models (LLMs) can enhance the proposal selection process at large user facilities, offering a scalable, consistent, and cost-effective alternative to traditional human review. Proposal selection depends on assessing the relative strength among submitted proposals; however, traditional human scoring often suffers from weak inter-proposal correlations and is subject to reviewer bias and inconsistency. A pairwise preference-based approach is logically superior, providing a more rigorous and internally consistent basis for ranking, but its quadratic workload makes it impractical for human reviewers. We address this limitation using LLMs. Leveraging the uniquely well-curated proposals and publication records from three beamlines at the Spallation Neutron Source (SNS), Oak Ridge National Laboratory (ORNL), we show that the LLM rankings correlate strongly with the human rankings (Spearman $ρ\\simeq 0.2-0.8$, improving to $\\geq 0.5$ after 10\\% outlier removal). Moreover, LLM performance is no worse than that of human reviewers in identifying proposals with high publication potential, while costing over two orders of magnitude less. Beyond ranking, LLMs enable advanced analyses that are challenging for humans, such as quantitative assessment of proposal similarity via embedding models, which provides information crucial for review committees."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T18:23:56Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    18,
                    23,
                    56,
                    3,
                    345,
                    0
                ],
                "arxiv_comment": "9 pages, 8figures",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Lijie Ding"
                    },
                    {
                        "name": "Janell Thomson"
                    },
                    {
                        "name": "Jon Taylor"
                    },
                    {
                        "name": "Changwoo Do"
                    }
                ],
                "author_detail": {
                    "name": "Changwoo Do"
                },
                "author": "Changwoo Do"
            },
            {
                "id": "http://arxiv.org/abs/2506.18156v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.18156v3",
                "title": "AI Through the Human Lens: Investigating Cognitive Theories in Machine Psychology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Through the Human Lens: Investigating Cognitive Theories in Machine Psychology"
                },
                "updated": "2025-12-11T18:18:42Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    18,
                    18,
                    42,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.18156v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.18156v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We investigate whether Large Language Models (LLMs) exhibit human-like cognitive patterns under four established frameworks from psychology: Thematic Apperception Test (TAT), Framing Bias, Moral Foundations Theory (MFT), and Cognitive Dissonance. We evaluated several proprietary and open-source models using structured prompts and automated scoring. Our findings reveal that these models often produce coherent narratives, show susceptibility to positive framing, exhibit moral judgments aligned with Liberty/Oppression concerns, and demonstrate self-contradictions tempered by extensive rationalization. Such behaviors mirror human cognitive tendencies yet are shaped by their training data and alignment methods. We discuss the implications for AI transparency, ethical deployment, and future work that bridges cognitive psychology and AI safety",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate whether Large Language Models (LLMs) exhibit human-like cognitive patterns under four established frameworks from psychology: Thematic Apperception Test (TAT), Framing Bias, Moral Foundations Theory (MFT), and Cognitive Dissonance. We evaluated several proprietary and open-source models using structured prompts and automated scoring. Our findings reveal that these models often produce coherent narratives, show susceptibility to positive framing, exhibit moral judgments aligned with Liberty/Oppression concerns, and demonstrate self-contradictions tempered by extensive rationalization. Such behaviors mirror human cognitive tendencies yet are shaped by their training data and alignment methods. We discuss the implications for AI transparency, ethical deployment, and future work that bridges cognitive psychology and AI safety"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-22T19:58:19Z",
                "published_parsed": [
                    2025,
                    6,
                    22,
                    19,
                    58,
                    19,
                    6,
                    173,
                    0
                ],
                "arxiv_comment": "Accepted to IJCNLP-AACL 2025 Student Research Workshop",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Akash Kundu"
                    },
                    {
                        "name": "Rishika Goswami"
                    }
                ],
                "author_detail": {
                    "name": "Rishika Goswami"
                },
                "author": "Rishika Goswami"
            },
            {
                "id": "http://arxiv.org/abs/2512.00846v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.00846v2",
                "title": "AFRAgent : An Adaptive Feature Renormalization Based High Resolution Aware GUI agent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AFRAgent : An Adaptive Feature Renormalization Based High Resolution Aware GUI agent"
                },
                "updated": "2025-12-11T18:16:38Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    18,
                    16,
                    38,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.00846v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.00846v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "There is a growing demand for mobile user interface (UI) automation, driven by its broad applications across industries. With the advent of visual language models (VLMs), GUI automation has progressed from generating text-based instructions for humans to autonomously executing tasks, thus optimizing automation workflows. Recent approaches leverage VLMs for this problem due to their ability to 1) process on-screen content directly, 2) remain independent of device-specific APIs by utilizing human actions (e.g., clicks, typing), and 3) apply real-world contextual knowledge for task understanding. However, these models often have trouble accurately identifying widgets and determining actions due to limited spatial information in vision encoder features. Additionally, top-performing models are often large, requiring extensive training and resulting in inference delays. In this work, we introduce AFRAgent, an instruct-BLIP-based multimodal architecture that achieves superior performance in GUI automation while being less than one-fourth the size of its nearest competitor. To enhance image embeddings in the large language model (LLM) pipeline, we propose an adaptive feature renormalization-based (a token-level affine transformation) technique that effectively enriches low-resolution image embeddings and fuses high-resolution details. We evaluate AFRAgent on Meta-GUI and AITW benchmarks, establishing a new state-of-the-art baseline for smartphone automation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is a growing demand for mobile user interface (UI) automation, driven by its broad applications across industries. With the advent of visual language models (VLMs), GUI automation has progressed from generating text-based instructions for humans to autonomously executing tasks, thus optimizing automation workflows. Recent approaches leverage VLMs for this problem due to their ability to 1) process on-screen content directly, 2) remain independent of device-specific APIs by utilizing human actions (e.g., clicks, typing), and 3) apply real-world contextual knowledge for task understanding. However, these models often have trouble accurately identifying widgets and determining actions due to limited spatial information in vision encoder features. Additionally, top-performing models are often large, requiring extensive training and resulting in inference delays. In this work, we introduce AFRAgent, an instruct-BLIP-based multimodal architecture that achieves superior performance in GUI automation while being less than one-fourth the size of its nearest competitor. To enhance image embeddings in the large language model (LLM) pipeline, we propose an adaptive feature renormalization-based (a token-level affine transformation) technique that effectively enriches low-resolution image embeddings and fuses high-resolution details. We evaluate AFRAgent on Meta-GUI and AITW benchmarks, establishing a new state-of-the-art baseline for smartphone automation."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-30T11:32:54Z",
                "published_parsed": [
                    2025,
                    11,
                    30,
                    11,
                    32,
                    54,
                    6,
                    334,
                    0
                ],
                "arxiv_comment": "Accepted at WACV 2026 Conference",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Neeraj Anand"
                    },
                    {
                        "name": "Rishabh Jain"
                    },
                    {
                        "name": "Sohan Patnaik"
                    },
                    {
                        "name": "Balaji Krishnamurthy"
                    },
                    {
                        "name": "Mausoom Sarkar"
                    }
                ],
                "author_detail": {
                    "name": "Mausoom Sarkar"
                },
                "author": "Mausoom Sarkar"
            },
            {
                "id": "http://arxiv.org/abs/2512.10882v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10882v1",
                "title": "Computational emotion analysis with multimodal LLMs: Current evidence on an emerging methodological opportunity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational emotion analysis with multimodal LLMs: Current evidence on an emerging methodological opportunity"
                },
                "updated": "2025-12-11T18:11:46Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    18,
                    11,
                    46,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10882v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10882v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Emotions are central to politics and analyzing their role in political communication has a long tradition. As research increasingly leverages audio-visual materials to analyze the display of emotions, the emergence of multimodal generative AI promises great advances. However, we lack evidence about the effectiveness of multimodal AI in emotion analysis. This paper addresses this gap by evaluating current multimodal large language models (mLLMs) in video-based analysis of emotional arousal in two complementary data sets of human-labeled video recordings. I find that under ideal circumstances, mLLMs' emotional arousal ratings are highly reliable and show little to know indication of demographic bias. However, in recordings of speakers in real-world parliamentary debates, mLLMs' arousal ratings fail to deliver on this promise with potential negative consequences for downstream statistical inferences. This study therefore underscores the need for continued, thorough evaluation of emerging generative AI methods in political analysis and contributes a suitable replicable framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emotions are central to politics and analyzing their role in political communication has a long tradition. As research increasingly leverages audio-visual materials to analyze the display of emotions, the emergence of multimodal generative AI promises great advances. However, we lack evidence about the effectiveness of multimodal AI in emotion analysis. This paper addresses this gap by evaluating current multimodal large language models (mLLMs) in video-based analysis of emotional arousal in two complementary data sets of human-labeled video recordings. I find that under ideal circumstances, mLLMs' emotional arousal ratings are highly reliable and show little to know indication of demographic bias. However, in recordings of speakers in real-world parliamentary debates, mLLMs' arousal ratings fail to deliver on this promise with potential negative consequences for downstream statistical inferences. This study therefore underscores the need for continued, thorough evaluation of emerging generative AI methods in political analysis and contributes a suitable replicable framework."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T18:11:46Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    18,
                    11,
                    46,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Hauke Licht"
                    }
                ],
                "author_detail": {
                    "name": "Hauke Licht"
                },
                "author": "Hauke Licht"
            },
            {
                "id": "http://arxiv.org/abs/2512.10860v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10860v1",
                "title": "SWiT-4D: Sliding-Window Transformer for Lossless and Parameter-Free Temporal 4D Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SWiT-4D: Sliding-Window Transformer for Lossless and Parameter-Free Temporal 4D Generation"
                },
                "updated": "2025-12-11T17:54:31Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    17,
                    54,
                    31,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10860v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10860v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Despite significant progress in 4D content generation, the conversion of monocular videos into high-quality animated 3D assets with explicit 4D meshes remains considerably challenging. The scarcity of large-scale, naturally captured 4D mesh datasets further limits the ability to train generalizable video-to-4D models from scratch in a purely data-driven manner. Meanwhile, advances in image-to-3D generation, supported by extensive datasets, offer powerful prior models that can be leveraged. To better utilize these priors while minimizing reliance on 4D supervision, we introduce SWiT-4D, a Sliding-Window Transformer for lossless, parameter-free temporal 4D mesh generation. SWiT-4D integrates seamlessly with any Diffusion Transformer (DiT)-based image-to-3D generator, adding spatial-temporal modeling across video frames while preserving the original single-image forward process, enabling 4D mesh reconstruction from videos of arbitrary length. To recover global translation, we further introduce an optimization-based trajectory module tailored for static-camera monocular videos. SWiT-4D demonstrates strong data efficiency: with only a single short (<10s) video for fine-tuning, it achieves high-fidelity geometry and stable temporal consistency, indicating practical deployability under extremely limited 4D supervision. Comprehensive experiments on both in-domain zoo-test sets and challenging out-of-domain benchmarks (C4D, Objaverse, and in-the-wild videos) show that SWiT-4D consistently outperforms existing baselines in temporal smoothness. Project page: https://animotionlab.github.io/SWIT4D/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite significant progress in 4D content generation, the conversion of monocular videos into high-quality animated 3D assets with explicit 4D meshes remains considerably challenging. The scarcity of large-scale, naturally captured 4D mesh datasets further limits the ability to train generalizable video-to-4D models from scratch in a purely data-driven manner. Meanwhile, advances in image-to-3D generation, supported by extensive datasets, offer powerful prior models that can be leveraged. To better utilize these priors while minimizing reliance on 4D supervision, we introduce SWiT-4D, a Sliding-Window Transformer for lossless, parameter-free temporal 4D mesh generation. SWiT-4D integrates seamlessly with any Diffusion Transformer (DiT)-based image-to-3D generator, adding spatial-temporal modeling across video frames while preserving the original single-image forward process, enabling 4D mesh reconstruction from videos of arbitrary length. To recover global translation, we further introduce an optimization-based trajectory module tailored for static-camera monocular videos. SWiT-4D demonstrates strong data efficiency: with only a single short (<10s) video for fine-tuning, it achieves high-fidelity geometry and stable temporal consistency, indicating practical deployability under extremely limited 4D supervision. Comprehensive experiments on both in-domain zoo-test sets and challenging out-of-domain benchmarks (C4D, Objaverse, and in-the-wild videos) show that SWiT-4D consistently outperforms existing baselines in temporal smoothness. Project page: https://animotionlab.github.io/SWIT4D/"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T17:54:31Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    17,
                    54,
                    31,
                    3,
                    345,
                    0
                ],
                "arxiv_comment": "Project page: https://animotionlab.github.io/SWIT4D/",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Kehong Gong"
                    },
                    {
                        "name": "Zhengyu Wen"
                    },
                    {
                        "name": "Mingxi Xu"
                    },
                    {
                        "name": "Weixia He"
                    },
                    {
                        "name": "Qi Wang"
                    },
                    {
                        "name": "Ning Zhang"
                    },
                    {
                        "name": "Zhengyu Li"
                    },
                    {
                        "name": "Chenbin Li"
                    },
                    {
                        "name": "Dongze Lian"
                    },
                    {
                        "name": "Wei Zhao"
                    },
                    {
                        "name": "Xiaoyu He"
                    },
                    {
                        "name": "Mingyuan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Mingyuan Zhang"
                },
                "author": "Mingyuan Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2512.10858v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10858v1",
                "title": "Scaling Behavior of Discrete Diffusion Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Behavior of Discrete Diffusion Language Models"
                },
                "updated": "2025-12-11T17:54:10Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    17,
                    54,
                    10,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10858v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10858v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Modern LLM pre-training consumes vast amounts of compute and training data, making the scaling behavior, or scaling laws, of different models a key distinguishing factor. Discrete diffusion language models (DLMs) have been proposed as an alternative to autoregressive language models (ALMs). However, their scaling behavior has not yet been fully explored, with prior work suggesting that they require more data and compute to match the performance of ALMs.\n  We study the scaling behavior of DLMs on different noise types by smoothly interpolating between masked and uniform diffusion while paying close attention to crucial hyperparameters such as batch size and learning rate. Our experiments reveal that the scaling behavior of DLMs strongly depends on the noise type and is considerably different from ALMs. While all noise types converge to similar loss values in compute-bound scaling, we find that uniform diffusion requires more parameters and less data for compute-efficient training compared to masked diffusion, making them a promising candidate in data-bound settings. We scale our uniform diffusion model up to 10B parameters trained for $10^{22}$ FLOPs, confirming the predicted scaling behavior and making it the largest publicly known uniform diffusion model to date.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern LLM pre-training consumes vast amounts of compute and training data, making the scaling behavior, or scaling laws, of different models a key distinguishing factor. Discrete diffusion language models (DLMs) have been proposed as an alternative to autoregressive language models (ALMs). However, their scaling behavior has not yet been fully explored, with prior work suggesting that they require more data and compute to match the performance of ALMs.\n  We study the scaling behavior of DLMs on different noise types by smoothly interpolating between masked and uniform diffusion while paying close attention to crucial hyperparameters such as batch size and learning rate. Our experiments reveal that the scaling behavior of DLMs strongly depends on the noise type and is considerably different from ALMs. While all noise types converge to similar loss values in compute-bound scaling, we find that uniform diffusion requires more parameters and less data for compute-efficient training compared to masked diffusion, making them a promising candidate in data-bound settings. We scale our uniform diffusion model up to 10B parameters trained for $10^{22}$ FLOPs, confirming the predicted scaling behavior and making it the largest publicly known uniform diffusion model to date."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T17:54:10Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    17,
                    54,
                    10,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Dimitri von Rütte"
                    },
                    {
                        "name": "Janis Fluri"
                    },
                    {
                        "name": "Omead Pooladzandi"
                    },
                    {
                        "name": "Bernhard Schölkopf"
                    },
                    {
                        "name": "Thomas Hofmann"
                    },
                    {
                        "name": "Antonio Orvieto"
                    }
                ],
                "author_detail": {
                    "name": "Antonio Orvieto"
                },
                "author": "Antonio Orvieto"
            },
            {
                "id": "http://arxiv.org/abs/2506.17090v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.17090v3",
                "title": "Better Language Model Inversion by Compactly Representing Next-Token Distributions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Better Language Model Inversion by Compactly Representing Next-Token Distributions"
                },
                "updated": "2025-12-11T17:53:55Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    17,
                    53,
                    55,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.17090v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.17090v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Language model inversion seeks to recover hidden prompts using only language model outputs. This capability has implications for security and accountability in language model deployments, such as leaking private information from an API-protected language model's system message. We propose a new method -- prompt inversion from logprob sequences (PILS) -- that recovers hidden prompts by gleaning clues from the model's next-token probabilities over the course of multiple generation steps. Our method is enabled by a key insight: The vector-valued outputs of a language model occupy a low-dimensional subspace. This enables us to losslessly compress the full next-token probability distribution over multiple generation steps using a linear map, allowing more output information to be used for inversion. Our approach yields massive gains over previous state-of-the-art methods for recovering hidden prompts, achieving 2--3.5 times higher exact recovery rates across test sets, in one case increasing the recovery rate from 17% to 60%. Our method also exhibits surprisingly good generalization behavior; for instance, an inverter trained on 16 generations steps gets 5--27 points higher prompt recovery when we increase the number of steps to 32 at test time. Furthermore, we demonstrate strong performance of our method on the more challenging task of recovering hidden system messages. We also analyze the role of verbatim repetition in prompt recovery and propose a new method for cross-family model transfer for logit-based inverters. Our findings show that next-token probabilities are a considerably more vulnerable attack surface for inversion attacks than previously known.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language model inversion seeks to recover hidden prompts using only language model outputs. This capability has implications for security and accountability in language model deployments, such as leaking private information from an API-protected language model's system message. We propose a new method -- prompt inversion from logprob sequences (PILS) -- that recovers hidden prompts by gleaning clues from the model's next-token probabilities over the course of multiple generation steps. Our method is enabled by a key insight: The vector-valued outputs of a language model occupy a low-dimensional subspace. This enables us to losslessly compress the full next-token probability distribution over multiple generation steps using a linear map, allowing more output information to be used for inversion. Our approach yields massive gains over previous state-of-the-art methods for recovering hidden prompts, achieving 2--3.5 times higher exact recovery rates across test sets, in one case increasing the recovery rate from 17% to 60%. Our method also exhibits surprisingly good generalization behavior; for instance, an inverter trained on 16 generations steps gets 5--27 points higher prompt recovery when we increase the number of steps to 32 at test time. Furthermore, we demonstrate strong performance of our method on the more challenging task of recovering hidden system messages. We also analyze the role of verbatim repetition in prompt recovery and propose a new method for cross-family model transfer for logit-based inverters. Our findings show that next-token probabilities are a considerably more vulnerable attack surface for inversion attacks than previously known."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-20T15:53:51Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    15,
                    53,
                    51,
                    4,
                    171,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Murtaza Nazir"
                    },
                    {
                        "name": "Matthew Finlayson"
                    },
                    {
                        "name": "John X. Morris"
                    },
                    {
                        "name": "Xiang Ren"
                    },
                    {
                        "name": "Swabha Swayamdipta"
                    }
                ],
                "author_detail": {
                    "name": "Swabha Swayamdipta"
                },
                "author": "Swabha Swayamdipta"
            },
            {
                "id": "http://arxiv.org/abs/2512.10847v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10847v1",
                "title": "Large Language Models for Superconductor Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Superconductor Discovery"
                },
                "updated": "2025-12-11T17:32:38Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    17,
                    32,
                    38,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10847v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10847v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) offer new opportunities for automated data extraction and property prediction across materials science, yet their use in superconductivity research remains limited. Here we construct a large experimental database of 78,203 records, covering 19,058 unique compositions, extracted from scientific literature using an LLM-driven workflow. Each entry includes chemical composition, critical temperature, measurement pressure, structural descriptors, and critical fields. We fine-tune several open-source LLMs for three tasks: (i) classifying superconductors vs. non-superconductors, (ii) predicting the superconducting transition temperature directly from composition or structure-informed inputs, and (iii) inverse design of candidate compositions conditioned on target Tc. The fine-tuned LLMs achieve performance comparable to traditional feature-based models and in some cases exceed them, while substantially outperforming their base versions and capturing meaningful chemical and structural trends. The inverse-design model generates chemically plausible compositions, including 28% novel candidates not seen in training. Finally, applying the trained predictors to the GNoME database identifies unreported materials with predicted Tc > 10 K. Although unverified, these candidates illustrate how integrating an LLM-driven workflow can enable scalable hypothesis generation for superconductivity discovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) offer new opportunities for automated data extraction and property prediction across materials science, yet their use in superconductivity research remains limited. Here we construct a large experimental database of 78,203 records, covering 19,058 unique compositions, extracted from scientific literature using an LLM-driven workflow. Each entry includes chemical composition, critical temperature, measurement pressure, structural descriptors, and critical fields. We fine-tune several open-source LLMs for three tasks: (i) classifying superconductors vs. non-superconductors, (ii) predicting the superconducting transition temperature directly from composition or structure-informed inputs, and (iii) inverse design of candidate compositions conditioned on target Tc. The fine-tuned LLMs achieve performance comparable to traditional feature-based models and in some cases exceed them, while substantially outperforming their base versions and capturing meaningful chemical and structural trends. The inverse-design model generates chemically plausible compositions, including 28% novel candidates not seen in training. Finally, applying the trained predictors to the GNoME database identifies unreported materials with predicted Tc > 10 K. Although unverified, these candidates illustrate how integrating an LLM-driven workflow can enable scalable hypothesis generation for superconductivity discovery."
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T17:32:38Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    17,
                    32,
                    38,
                    3,
                    345,
                    0
                ],
                "arxiv_comment": "15 pages, 6 figures",
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci"
                },
                "authors": [
                    {
                        "name": "Suman Itani"
                    },
                    {
                        "name": "Yibo Zhang"
                    },
                    {
                        "name": "Ranjit Itani"
                    },
                    {
                        "name": "Jiadong Zang"
                    }
                ],
                "author_detail": {
                    "name": "Jiadong Zang"
                },
                "author": "Jiadong Zang"
            },
            {
                "id": "http://arxiv.org/abs/2512.03477v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.03477v2",
                "title": "Fairness-Aware Fine-Tuning of Vision-Language Models for Medical Glaucoma Diagnosis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fairness-Aware Fine-Tuning of Vision-Language Models for Medical Glaucoma Diagnosis"
                },
                "updated": "2025-12-11T17:17:07Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    17,
                    17,
                    7,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.03477v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.03477v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Vision-language models achieve expert-level performance on medical imaging tasks but exhibit significant diagnostic accuracy disparities across demographic groups. We introduce fairness-aware Low-Rank Adaptation for medical VLMs, combining parameter efficiency with explicit fairness optimization. Our key algorithmic contribution is a differentiable MaxAccGap loss that enables end-to-end optimization of accuracy parity across demographic groups. We propose three methods: FR-LoRA integrates MaxAccGap regularization into the training objective, GR-LoRA applies inverse frequency weighting to balance gradient contributions, and Hybrid-LoRA combines both mechanisms. Evaluated on 10,000 glaucoma fundus images, GR-LoRA reduces diagnostic accuracy disparities by 69% while maintaining 53.15% overall accuracy. Ablation studies reveal that strong regularization strength achieves optimal fairness with minimal accuracy trade-off, and race-specific optimization yields 60% disparity reduction. Our approach requires only 0.24% trainable parameters, enabling practical deployment of fair medical AI in resource-constrained healthcare settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models achieve expert-level performance on medical imaging tasks but exhibit significant diagnostic accuracy disparities across demographic groups. We introduce fairness-aware Low-Rank Adaptation for medical VLMs, combining parameter efficiency with explicit fairness optimization. Our key algorithmic contribution is a differentiable MaxAccGap loss that enables end-to-end optimization of accuracy parity across demographic groups. We propose three methods: FR-LoRA integrates MaxAccGap regularization into the training objective, GR-LoRA applies inverse frequency weighting to balance gradient contributions, and Hybrid-LoRA combines both mechanisms. Evaluated on 10,000 glaucoma fundus images, GR-LoRA reduces diagnostic accuracy disparities by 69% while maintaining 53.15% overall accuracy. Ablation studies reveal that strong regularization strength achieves optimal fairness with minimal accuracy trade-off, and race-specific optimization yields 60% disparity reduction. Our approach requires only 0.24% trainable parameters, enabling practical deployment of fair medical AI in resource-constrained healthcare settings."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-03T06:09:14Z",
                "published_parsed": [
                    2025,
                    12,
                    3,
                    6,
                    9,
                    14,
                    2,
                    337,
                    0
                ],
                "arxiv_comment": "10 pages, 3 tables",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Zijian Gu"
                    },
                    {
                        "name": "Yuxi Liu"
                    },
                    {
                        "name": "Zhenhao Zhang"
                    },
                    {
                        "name": "Song Wang"
                    }
                ],
                "author_detail": {
                    "name": "Song Wang"
                },
                "author": "Song Wang"
            },
            {
                "id": "http://arxiv.org/abs/2512.10805v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10805v1",
                "title": "Interpretable and Steerable Concept Bottleneck Sparse Autoencoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpretable and Steerable Concept Bottleneck Sparse Autoencoders"
                },
                "updated": "2025-12-11T16:48:07Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    16,
                    48,
                    7,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10805v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Sparse autoencoders (SAEs) promise a unified approach for mechanistic interpretability, concept discovery, and model steering in LLMs and LVLMs. However, realizing this potential requires that the learned features be both interpretable and steerable. To that end, we introduce two new computationally inexpensive interpretability and steerability metrics and conduct a systematic analysis on LVLMs. Our analysis uncovers two observations; (i) a majority of SAE neurons exhibit either low interpretability or low steerability or both, rendering them ineffective for downstream use; and (ii) due to the unsupervised nature of SAEs, user-desired concepts are often absent in the learned dictionary, thus limiting their practical utility. To address these limitations, we propose Concept Bottleneck Sparse Autoencoders (CB-SAE) - a novel post-hoc framework that prunes low-utility neurons and augments the latent space with a lightweight concept bottleneck aligned to a user-defined concept set. The resulting CB-SAE improves interpretability by +32.1% and steerability by +14.5% across LVLMs and image generation tasks. We will make our code and model weights available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse autoencoders (SAEs) promise a unified approach for mechanistic interpretability, concept discovery, and model steering in LLMs and LVLMs. However, realizing this potential requires that the learned features be both interpretable and steerable. To that end, we introduce two new computationally inexpensive interpretability and steerability metrics and conduct a systematic analysis on LVLMs. Our analysis uncovers two observations; (i) a majority of SAE neurons exhibit either low interpretability or low steerability or both, rendering them ineffective for downstream use; and (ii) due to the unsupervised nature of SAEs, user-desired concepts are often absent in the learned dictionary, thus limiting their practical utility. To address these limitations, we propose Concept Bottleneck Sparse Autoencoders (CB-SAE) - a novel post-hoc framework that prunes low-utility neurons and augments the latent space with a lightweight concept bottleneck aligned to a user-defined concept set. The resulting CB-SAE improves interpretability by +32.1% and steerability by +14.5% across LVLMs and image generation tasks. We will make our code and model weights available."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T16:48:07Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    16,
                    48,
                    7,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Akshay Kulkarni"
                    },
                    {
                        "name": "Tsui-Wei Weng"
                    },
                    {
                        "name": "Vivek Narayanaswamy"
                    },
                    {
                        "name": "Shusen Liu"
                    },
                    {
                        "name": "Wesam A. Sakla"
                    },
                    {
                        "name": "Kowshik Thopalli"
                    }
                ],
                "author_detail": {
                    "name": "Kowshik Thopalli"
                },
                "author": "Kowshik Thopalli"
            },
            {
                "id": "http://arxiv.org/abs/2401.11255v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2401.11255v2",
                "title": "Visualization Generation with Large Language Models: An Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visualization Generation with Large Language Models: An Evaluation"
                },
                "updated": "2025-12-11T16:41:18Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    16,
                    41,
                    18,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2401.11255v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2401.11255v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The frequent need for analysts to create visualizations to derive insights from data has driven extensive research into the generation of natural Language to Visualization (NL2VIS). While recent progress in large language models (LLMs) suggests their potential to effectively support NL2VIS tasks, existing studies lack a systematic investigation into the performance of different LLMs under various prompt strategies. This paper addresses this gap and contributes a crucial baseline evaluation of LLMs' capabilities in generating visualization specifications of NL2VIS tasks. Our evaluation utilizes the nvBench dataset, employing six representative LLMs and eight distinct prompt strategies to evaluate their performance in generating six target chart types using the Vega-Lite visualization specification. We assess model performance with multiple metrics, including vis accuracy, validity and legality. Our results reveal substantial performance disparities across prompt strategies, chart types, and LLMs. Furthermore, based on the evaluation results, we uncover several counterintuitive behaviors across these dimensions, and propose directions for enhancing the NL2VIS benchmark to better support future NL2VIS research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The frequent need for analysts to create visualizations to derive insights from data has driven extensive research into the generation of natural Language to Visualization (NL2VIS). While recent progress in large language models (LLMs) suggests their potential to effectively support NL2VIS tasks, existing studies lack a systematic investigation into the performance of different LLMs under various prompt strategies. This paper addresses this gap and contributes a crucial baseline evaluation of LLMs' capabilities in generating visualization specifications of NL2VIS tasks. Our evaluation utilizes the nvBench dataset, employing six representative LLMs and eight distinct prompt strategies to evaluate their performance in generating six target chart types using the Vega-Lite visualization specification. We assess model performance with multiple metrics, including vis accuracy, validity and legality. Our results reveal substantial performance disparities across prompt strategies, chart types, and LLMs. Furthermore, based on the evaluation results, we uncover several counterintuitive behaviors across these dimensions, and propose directions for enhancing the NL2VIS benchmark to better support future NL2VIS research."
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-01-20T15:28:22Z",
                "published_parsed": [
                    2024,
                    1,
                    20,
                    15,
                    28,
                    22,
                    5,
                    20,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC"
                },
                "authors": [
                    {
                        "name": "Xinyu Wang"
                    },
                    {
                        "name": "Chenwei Liang"
                    },
                    {
                        "name": "Shunyuan Zheng"
                    },
                    {
                        "name": "Jinyuan Liang"
                    },
                    {
                        "name": "Guozheng Li"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Chi Harold Liu"
                    }
                ],
                "author_detail": {
                    "name": "Chi Harold Liu"
                },
                "author": "Chi Harold Liu"
            },
            {
                "id": "http://arxiv.org/abs/2512.10793v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10793v1",
                "title": "LabelFusion: Learning to Fuse LLMs and Transformer Classifiers for Robust Text Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LabelFusion: Learning to Fuse LLMs and Transformer Classifiers for Robust Text Classification"
                },
                "updated": "2025-12-11T16:39:07Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    16,
                    39,
                    7,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10793v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10793v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "LabelFusion is a fusion ensemble for text classification that learns to combine a traditional transformer-based classifier (e.g., RoBERTa) with one or more Large Language Models (LLMs such as OpenAI GPT, Google Gemini, or DeepSeek) to deliver accurate and cost-aware predictions across multi-class and multi-label tasks. The package provides a simple high-level interface (AutoFusionClassifier) that trains the full pipeline end-to-end with minimal configuration, and a flexible API for advanced users. Under the hood, LabelFusion integrates vector signals from both sources by concatenating the ML backbone's embeddings with the LLM-derived per-class scores -- obtained through structured prompt-engineering strategies -- and feeds this joint representation into a compact multi-layer perceptron (FusionMLP) that produces the final prediction. This learned fusion approach captures complementary strengths of LLM reasoning and traditional transformer-based classifiers, yielding robust performance across domains -- achieving 92.4% accuracy on AG News and 92.3% on 10-class Reuters 21578 topic classification -- while enabling practical trade-offs between accuracy, latency, and cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LabelFusion is a fusion ensemble for text classification that learns to combine a traditional transformer-based classifier (e.g., RoBERTa) with one or more Large Language Models (LLMs such as OpenAI GPT, Google Gemini, or DeepSeek) to deliver accurate and cost-aware predictions across multi-class and multi-label tasks. The package provides a simple high-level interface (AutoFusionClassifier) that trains the full pipeline end-to-end with minimal configuration, and a flexible API for advanced users. Under the hood, LabelFusion integrates vector signals from both sources by concatenating the ML backbone's embeddings with the LLM-derived per-class scores -- obtained through structured prompt-engineering strategies -- and feeds this joint representation into a compact multi-layer perceptron (FusionMLP) that produces the final prediction. This learned fusion approach captures complementary strengths of LLM reasoning and traditional transformer-based classifiers, yielding robust performance across domains -- achieving 92.4% accuracy on AG News and 92.3% on 10-class Reuters 21578 topic classification -- while enabling practical trade-offs between accuracy, latency, and cost."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T16:39:07Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    16,
                    39,
                    7,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Michael Schlee"
                    },
                    {
                        "name": "Christoph Weisser"
                    },
                    {
                        "name": "Timo Kivimäki"
                    },
                    {
                        "name": "Melchizedek Mashiku"
                    },
                    {
                        "name": "Benjamin Saefken"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Saefken"
                },
                "author": "Benjamin Saefken"
            },
            {
                "id": "http://arxiv.org/abs/2508.15432v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.15432v3",
                "title": "SyGra: A Unified Graph-Based Framework for Scalable Generation, Quality Tagging, and Management of Synthetic Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SyGra: A Unified Graph-Based Framework for Scalable Generation, Quality Tagging, and Management of Synthetic Data"
                },
                "updated": "2025-12-11T16:38:00Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    16,
                    38,
                    0,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.15432v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.15432v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The advancement of large language models (LLMs) is critically dependent on the availability of high-quality datasets for Supervised Fine-Tuning (SFT), alignment tasks like Direct Preference Optimization (DPO), etc. In this work, we present a comprehensive synthetic data generation framework that facilitates scalable, configurable, and high-fidelity generation of synthetic data tailored for these training paradigms. Our approach employs a modular and configuration-based pipeline capable of modeling complex dialogue flows with minimal manual intervention. This framework uses a dual-stage quality tagging mechanism, combining heuristic rules and LLM-based evaluations, to automatically filter and score data extracted from OASST-formatted conversations, ensuring the curation of high-quality dialogue samples. The resulting datasets are structured under a flexible schema supporting both SFT and DPO use cases, enabling seamless integration into diverse training workflows. Together, these innovations offer a robust solution for generating and managing synthetic conversational data at scale, significantly reducing the overhead of data preparation in LLM training pipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advancement of large language models (LLMs) is critically dependent on the availability of high-quality datasets for Supervised Fine-Tuning (SFT), alignment tasks like Direct Preference Optimization (DPO), etc. In this work, we present a comprehensive synthetic data generation framework that facilitates scalable, configurable, and high-fidelity generation of synthetic data tailored for these training paradigms. Our approach employs a modular and configuration-based pipeline capable of modeling complex dialogue flows with minimal manual intervention. This framework uses a dual-stage quality tagging mechanism, combining heuristic rules and LLM-based evaluations, to automatically filter and score data extracted from OASST-formatted conversations, ensuring the curation of high-quality dialogue samples. The resulting datasets are structured under a flexible schema supporting both SFT and DPO use cases, enabling seamless integration into diverse training workflows. Together, these innovations offer a robust solution for generating and managing synthetic conversational data at scale, significantly reducing the overhead of data preparation in LLM training pipelines."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-21T10:35:41Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    10,
                    35,
                    41,
                    3,
                    233,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Bidyapati Pradhan"
                    },
                    {
                        "name": "Surajit Dasgupta"
                    },
                    {
                        "name": "Amit Kumar Saha"
                    },
                    {
                        "name": "Omkar Anustoop"
                    },
                    {
                        "name": "Sriram Puttagunta"
                    },
                    {
                        "name": "Vipul Mittal"
                    },
                    {
                        "name": "Gopal Sarda"
                    }
                ],
                "author_detail": {
                    "name": "Gopal Sarda"
                },
                "author": "Gopal Sarda"
            },
            {
                "id": "http://arxiv.org/abs/2512.10785v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10785v1",
                "title": "Developing and Evaluating a Large Language Model-Based Automated Feedback System Grounded in Evidence-Centered Design for Supporting Physics Problem Solving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing and Evaluating a Large Language Model-Based Automated Feedback System Grounded in Evidence-Centered Design for Supporting Physics Problem Solving"
                },
                "updated": "2025-12-11T16:29:38Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    16,
                    29,
                    38,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10785v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10785v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Generative AI offers new opportunities for individualized and adaptive learning, particularly through large language model (LLM)-based feedback systems. While LLMs can produce effective feedback for relatively straightforward conceptual tasks, delivering high-quality feedback for tasks that require advanced domain expertise, such as physics problem solving, remains a substantial challenge. This study presents the design of an LLM-based feedback system for physics problem solving grounded in evidence-centered design (ECD) and evaluates its performance within the German Physics Olympiad. Participants assessed the usefulness and accuracy of the generated feedback, which was generally perceived as useful and highly accurate. However, an in-depth analysis revealed that the feedback contained factual errors in 20% of cases; errors that often went unnoticed by the students. We discuss the risks associated with uncritical reliance on LLM-based feedback systems and outline potential directions for generating more adaptive and reliable LLM-based feedback in the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI offers new opportunities for individualized and adaptive learning, particularly through large language model (LLM)-based feedback systems. While LLMs can produce effective feedback for relatively straightforward conceptual tasks, delivering high-quality feedback for tasks that require advanced domain expertise, such as physics problem solving, remains a substantial challenge. This study presents the design of an LLM-based feedback system for physics problem solving grounded in evidence-centered design (ECD) and evaluates its performance within the German Physics Olympiad. Participants assessed the usefulness and accuracy of the generated feedback, which was generally perceived as useful and highly accurate. However, an in-depth analysis revealed that the feedback contained factual errors in 20% of cases; errors that often went unnoticed by the students. We discuss the risks associated with uncritical reliance on LLM-based feedback systems and outline potential directions for generating more adaptive and reliable LLM-based feedback in the future."
                },
                "tags": [
                    {
                        "term": "physics.ed-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T16:29:38Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    16,
                    29,
                    38,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "physics.ed-ph"
                },
                "authors": [
                    {
                        "name": "Holger Maus"
                    },
                    {
                        "name": "Paul Tschisgale"
                    },
                    {
                        "name": "Fabian Kieser"
                    },
                    {
                        "name": "Stefan Petersen"
                    },
                    {
                        "name": "Peter Wulff"
                    }
                ],
                "author_detail": {
                    "name": "Peter Wulff"
                },
                "author": "Peter Wulff"
            },
            {
                "id": "http://arxiv.org/abs/2512.10780v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10780v1",
                "title": "Script Gap: Evaluating LLM Triage on Indian Languages in Native vs Roman Scripts in a Real World Setting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Script Gap: Evaluating LLM Triage on Indian Languages in Native vs Roman Scripts in a Real World Setting"
                },
                "updated": "2025-12-11T16:15:42Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    16,
                    15,
                    42,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10780v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10780v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) are increasingly deployed in high-stakes clinical applications in India. In many such settings, speakers of Indian languages frequently communicate using romanized text rather than native scripts, yet existing research rarely evaluates this orthographic variation using real-world data. We investigate how romanization impacts the reliability of LLMs in a critical domain: maternal and newborn healthcare triage. We benchmark leading LLMs on a real-world dataset of user-generated queries spanning five Indian languages and Nepali. Our results reveal consistent degradation in performance for romanized messages, with F1 scores trailing those of native scripts by 5-12 points. At our partner maternal health organization in India, this gap could cause nearly 2 million excess errors in triage. Crucially, this performance gap by scripts is not due to a failure in clinical reasoning. We demonstrate that LLMs often correctly infer the semantic intent of romanized queries. Nevertheless, their final classification outputs remain brittle in the presence of orthographic noise in romanized inputs. Our findings highlight a critical safety blind spot in LLM-based health systems: models that appear to understand romanized input may still fail to act on it reliably.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed in high-stakes clinical applications in India. In many such settings, speakers of Indian languages frequently communicate using romanized text rather than native scripts, yet existing research rarely evaluates this orthographic variation using real-world data. We investigate how romanization impacts the reliability of LLMs in a critical domain: maternal and newborn healthcare triage. We benchmark leading LLMs on a real-world dataset of user-generated queries spanning five Indian languages and Nepali. Our results reveal consistent degradation in performance for romanized messages, with F1 scores trailing those of native scripts by 5-12 points. At our partner maternal health organization in India, this gap could cause nearly 2 million excess errors in triage. Crucially, this performance gap by scripts is not due to a failure in clinical reasoning. We demonstrate that LLMs often correctly infer the semantic intent of romanized queries. Nevertheless, their final classification outputs remain brittle in the presence of orthographic noise in romanized inputs. Our findings highlight a critical safety blind spot in LLM-based health systems: models that appear to understand romanized input may still fail to act on it reliably."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T16:15:42Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    16,
                    15,
                    42,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Manurag Khullar"
                    },
                    {
                        "name": "Utkarsh Desai"
                    },
                    {
                        "name": "Poorva Malviya"
                    },
                    {
                        "name": "Aman Dalmia"
                    },
                    {
                        "name": "Zheyuan Ryan Shi"
                    }
                ],
                "author_detail": {
                    "name": "Zheyuan Ryan Shi"
                },
                "author": "Zheyuan Ryan Shi"
            },
            {
                "id": "http://arxiv.org/abs/2512.10770v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10770v1",
                "title": "Template-Free Retrosynthesis with Graph-Prior Augmented Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Template-Free Retrosynthesis with Graph-Prior Augmented Transformers"
                },
                "updated": "2025-12-11T16:08:32Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    16,
                    8,
                    32,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10770v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10770v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Retrosynthesis reaction prediction seeks to infer plausible reactant molecules for a given product and is a central problem in computer-aided organic synthesis. Despite recent progress, many existing models still fall short of the accuracy and robustness required for practical deployment. This work studies a template-free, Transformer-based framework that eliminates reliance on handcrafted reaction templates or additional chemical rule engines. The model injects molecular graph information into the attention mechanism to jointly exploit \\SMILES\\ sequences and structural cues, and further applies a paired data augmentation strategy to enhance training diversity and scale. On the USPTO-50K benchmark, our proposed approach achieves state-of-the-art performance among template-free methods and substantially outperforming a vanilla Transformer baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrosynthesis reaction prediction seeks to infer plausible reactant molecules for a given product and is a central problem in computer-aided organic synthesis. Despite recent progress, many existing models still fall short of the accuracy and robustness required for practical deployment. This work studies a template-free, Transformer-based framework that eliminates reliance on handcrafted reaction templates or additional chemical rule engines. The model injects molecular graph information into the attention mechanism to jointly exploit \\SMILES\\ sequences and structural cues, and further applies a paired data augmentation strategy to enhance training diversity and scale. On the USPTO-50K benchmark, our proposed approach achieves state-of-the-art performance among template-free methods and substantially outperforming a vanilla Transformer baseline."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T16:08:32Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    16,
                    8,
                    32,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Youjun Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Youjun Zhao"
                },
                "author": "Youjun Zhao"
            },
            {
                "id": "http://arxiv.org/abs/2505.14932v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.14932v2",
                "title": "FOL-Traces: Verified First-Order Logic Reasoning Traces at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FOL-Traces: Verified First-Order Logic Reasoning Traces at Scale"
                },
                "updated": "2025-12-11T16:02:17Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    16,
                    2,
                    17,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.14932v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.14932v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Reasoning in language models is difficult to evaluate: natural-language traces are unverifiable, symbolic datasets too small, and most benchmarks conflate heuristics with inference. We present FOL-Traces, the first large-scale dataset of programmatically verified reasoning traces, enabling rigorous evaluation of structured logical inference. We also propose two challenging and comprehensive diagnostic tasks-masked operation prediction and step completion-that directly probe syntactic awareness and process fidelity. FOL-Traces serves as a scalable testbed for rigorously studying how models perform structured logical inference. Systematic experiments with 5 reasoning LLMs show that the dataset remains challenging: models only reach around 45.7% accuracy on masked operation prediction and around 27% on two-step completion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning in language models is difficult to evaluate: natural-language traces are unverifiable, symbolic datasets too small, and most benchmarks conflate heuristics with inference. We present FOL-Traces, the first large-scale dataset of programmatically verified reasoning traces, enabling rigorous evaluation of structured logical inference. We also propose two challenging and comprehensive diagnostic tasks-masked operation prediction and step completion-that directly probe syntactic awareness and process fidelity. FOL-Traces serves as a scalable testbed for rigorously studying how models perform structured logical inference. Systematic experiments with 5 reasoning LLMs show that the dataset remains challenging: models only reach around 45.7% accuracy on masked operation prediction and around 27% on two-step completion."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-20T21:38:28Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    21,
                    38,
                    28,
                    1,
                    140,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Isabelle Lee"
                    },
                    {
                        "name": "Sarah Liaw"
                    },
                    {
                        "name": "Dani Yogatama"
                    }
                ],
                "author_detail": {
                    "name": "Dani Yogatama"
                },
                "author": "Dani Yogatama"
            },
            {
                "id": "http://arxiv.org/abs/2512.10756v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10756v1",
                "title": "OPV: Outcome-based Process Verifier for Efficient Long Chain-of-Thought Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OPV: Outcome-based Process Verifier for Efficient Long Chain-of-Thought Verification"
                },
                "updated": "2025-12-11T15:47:38Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    15,
                    47,
                    38,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10756v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10756v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) have achieved significant progress in solving complex reasoning tasks by Reinforcement Learning with Verifiable Rewards (RLVR). This advancement is also inseparable from the oversight automated by reliable verifiers. However, current outcome-based verifiers (OVs) are unable to inspect the unreliable intermediate steps in the long reasoning chains of thought (CoTs). Meanwhile, current process-based verifiers (PVs) have difficulties in reliably detecting errors in the complex long CoTs, limited by the scarcity of high-quality annotations due to the prohibitive costs of human annotations. Therefore, we propose the Outcome-based Process Verifier (OPV), which verifies the rationale process of summarized outcomes from long CoTs to achieve both accurate and efficient verification and enable large-scale annotation. To empower the proposed verifier, we adopt an iterative active learning framework with expert annotations to progressively improve the verification capability of OPV with fewer annotation costs. Specifically, in each iteration, the most uncertain cases of the current best OPV are annotated and then subsequently used to train a new OPV through Rejection Fine-Tuning (RFT) and RLVR for the next round. Extensive experiments demonstrate OPV's superior performance and broad applicability. It achieves new state-of-the-art results on our held-out OPV-Bench, outperforming much larger open-source models such as Qwen3-Max-Preview with an F1 score of 83.1 compared to 76.3. Furthermore, OPV effectively detects false positives within synthetic dataset, closely align with expert assessment. When collaborating with policy models, OPV consistently yields performance gains, e.g., raising the accuracy of DeepSeek-R1-Distill-Qwen-32B from 55.2% to 73.3% on AIME2025 as the compute budget scales.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved significant progress in solving complex reasoning tasks by Reinforcement Learning with Verifiable Rewards (RLVR). This advancement is also inseparable from the oversight automated by reliable verifiers. However, current outcome-based verifiers (OVs) are unable to inspect the unreliable intermediate steps in the long reasoning chains of thought (CoTs). Meanwhile, current process-based verifiers (PVs) have difficulties in reliably detecting errors in the complex long CoTs, limited by the scarcity of high-quality annotations due to the prohibitive costs of human annotations. Therefore, we propose the Outcome-based Process Verifier (OPV), which verifies the rationale process of summarized outcomes from long CoTs to achieve both accurate and efficient verification and enable large-scale annotation. To empower the proposed verifier, we adopt an iterative active learning framework with expert annotations to progressively improve the verification capability of OPV with fewer annotation costs. Specifically, in each iteration, the most uncertain cases of the current best OPV are annotated and then subsequently used to train a new OPV through Rejection Fine-Tuning (RFT) and RLVR for the next round. Extensive experiments demonstrate OPV's superior performance and broad applicability. It achieves new state-of-the-art results on our held-out OPV-Bench, outperforming much larger open-source models such as Qwen3-Max-Preview with an F1 score of 83.1 compared to 76.3. Furthermore, OPV effectively detects false positives within synthetic dataset, closely align with expert assessment. When collaborating with policy models, OPV consistently yields performance gains, e.g., raising the accuracy of DeepSeek-R1-Distill-Qwen-32B from 55.2% to 73.3% on AIME2025 as the compute budget scales."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T15:47:38Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    15,
                    47,
                    38,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Zijian Wu"
                    },
                    {
                        "name": "Lingkai Kong"
                    },
                    {
                        "name": "Wenwei Zhang"
                    },
                    {
                        "name": "Songyang Gao"
                    },
                    {
                        "name": "Yuzhe Gu"
                    },
                    {
                        "name": "Zhongrui Cai"
                    },
                    {
                        "name": "Tianyou Ma"
                    },
                    {
                        "name": "Yuhong Liu"
                    },
                    {
                        "name": "Zhi Wang"
                    },
                    {
                        "name": "Runyuan Ma"
                    },
                    {
                        "name": "Guangyu Wang"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Kai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Chen"
                },
                "author": "Kai Chen"
            },
            {
                "id": "http://arxiv.org/abs/2512.10750v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10750v1",
                "title": "LDP: Parameter-Efficient Fine-Tuning of Multimodal LLM for Medical Report Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LDP: Parameter-Efficient Fine-Tuning of Multimodal LLM for Medical Report Generation"
                },
                "updated": "2025-12-11T15:43:33Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    15,
                    43,
                    33,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10750v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10750v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Colonoscopic polyp diagnosis is pivotal for early colorectal cancer detection, yet traditional automated reporting suffers from inconsistencies and hallucinations due to the scarcity of high-quality multimodal medical data. To bridge this gap, we propose LDP, a novel framework leveraging multimodal large language models (MLLMs) for professional polyp diagnosis report generation. Specifically, we curate MMEndo, a multimodal endoscopic dataset comprising expert-annotated colonoscopy image-text pairs. We fine-tune the Qwen2-VL-7B backbone using Parameter-Efficient Fine-Tuning (LoRA) and align it with clinical standards via Direct Preference Optimization (DPO). Extensive experiments show that our LDP outperforms existing baselines on both automated metrics and rigorous clinical expert evaluations (achieving a Physician Score of 7.2/10), significantly reducing training computational costs by 833x compared to full fine-tuning. The proposed solution offers a scalable, clinically viable path for primary healthcare, with additional validation on the IU-XRay dataset confirming its robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Colonoscopic polyp diagnosis is pivotal for early colorectal cancer detection, yet traditional automated reporting suffers from inconsistencies and hallucinations due to the scarcity of high-quality multimodal medical data. To bridge this gap, we propose LDP, a novel framework leveraging multimodal large language models (MLLMs) for professional polyp diagnosis report generation. Specifically, we curate MMEndo, a multimodal endoscopic dataset comprising expert-annotated colonoscopy image-text pairs. We fine-tune the Qwen2-VL-7B backbone using Parameter-Efficient Fine-Tuning (LoRA) and align it with clinical standards via Direct Preference Optimization (DPO). Extensive experiments show that our LDP outperforms existing baselines on both automated metrics and rigorous clinical expert evaluations (achieving a Physician Score of 7.2/10), significantly reducing training computational costs by 833x compared to full fine-tuning. The proposed solution offers a scalable, clinically viable path for primary healthcare, with additional validation on the IU-XRay dataset confirming its robustness."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T15:43:33Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    15,
                    43,
                    33,
                    3,
                    345,
                    0
                ],
                "arxiv_comment": "Work in progress",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Tianyu Zhou"
                    },
                    {
                        "name": "Junyi Tang"
                    },
                    {
                        "name": "Zehui Li"
                    },
                    {
                        "name": "Dahong Qian"
                    },
                    {
                        "name": "Suncheng Xiang"
                    }
                ],
                "author_detail": {
                    "name": "Suncheng Xiang"
                },
                "author": "Suncheng Xiang"
            },
            {
                "id": "http://arxiv.org/abs/2504.01951v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2504.01951v2",
                "title": "The LLM Wears Prada: Analysing Gender Bias and Stereotypes through Online Shopping Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The LLM Wears Prada: Analysing Gender Bias and Stereotypes through Online Shopping Data"
                },
                "updated": "2025-12-11T15:33:30Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    15,
                    33,
                    30,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2504.01951v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2504.01951v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "With the wide and cross-domain adoption of Large Language Models, it becomes crucial to assess to which extent the statistical correlations in training data, which underlie their impressive performance, hide subtle and potentially troubling biases. Gender bias in LLMs has been widely investigated from the perspectives of works, hobbies, and emotions typically associated with a specific gender. In this study, we introduce a novel perspective. We investigate whether LLMs can predict an individual's gender based solely on online shopping histories and whether these predictions are influenced by gender biases and stereotypes. Using a dataset of historical online purchases from users in the United States, we evaluate the ability of six LLMs to classify gender and we then analyze their reasoning and products-gender co-occurrences. Results indicate that while models can infer gender with moderate accuracy, their decisions are often rooted in stereotypical associations between product categories and gender. Furthermore, explicit instructions to avoid bias reduce the certainty of model predictions, but do not eliminate stereotypical patterns. Our findings highlight the persistent nature of gender biases in LLMs and emphasize the need for robust bias-mitigation strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the wide and cross-domain adoption of Large Language Models, it becomes crucial to assess to which extent the statistical correlations in training data, which underlie their impressive performance, hide subtle and potentially troubling biases. Gender bias in LLMs has been widely investigated from the perspectives of works, hobbies, and emotions typically associated with a specific gender. In this study, we introduce a novel perspective. We investigate whether LLMs can predict an individual's gender based solely on online shopping histories and whether these predictions are influenced by gender biases and stereotypes. Using a dataset of historical online purchases from users in the United States, we evaluate the ability of six LLMs to classify gender and we then analyze their reasoning and products-gender co-occurrences. Results indicate that while models can infer gender with moderate accuracy, their decisions are often rooted in stereotypical associations between product categories and gender. Furthermore, explicit instructions to avoid bias reduce the certainty of model predictions, but do not eliminate stereotypical patterns. Our findings highlight the persistent nature of gender biases in LLMs and emphasize the need for robust bias-mitigation strategies."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-04-02T17:56:08Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    17,
                    56,
                    8,
                    2,
                    92,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Massimiliano Luca"
                    },
                    {
                        "name": "Ciro Beneduce"
                    },
                    {
                        "name": "Bruno Lepri"
                    },
                    {
                        "name": "Jacopo Staiano"
                    }
                ],
                "author_detail": {
                    "name": "Jacopo Staiano"
                },
                "author": "Jacopo Staiano"
            },
            {
                "id": "http://arxiv.org/abs/2512.10741v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10741v1",
                "title": "TRIDENT: A Redundant Architecture for Caribbean-Accented Emergency Speech Triage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TRIDENT: A Redundant Architecture for Caribbean-Accented Emergency Speech Triage"
                },
                "updated": "2025-12-11T15:29:33Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    15,
                    29,
                    33,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10741v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10741v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Emergency speech recognition systems exhibit systematic performance degradation on non-standard English varieties, creating a critical gap in services for Caribbean populations. We present TRIDENT (Transcription and Routing Intelligence for Dispatcher-Empowered National Triage), a three-layer dispatcher-support architecture designed to structure emergency call inputs for human application of established triage protocols (the ESI for routine operations and START for mass casualty events), even when automatic speech recognition fails.\n  The system combines Caribbean-accent-tuned ASR, local entity extraction via large language models, and bio-acoustic distress detection to provide dispatchers with three complementary signals: transcription confidence, structured clinical entities, and vocal stress indicators. Our key insight is that low ASR confidence, rather than representing system failure, serves as a valuable queue prioritization signal -- particularly when combined with elevated vocal distress markers indicating a caller in crisis whose speech may have shifted toward basilectal registers. A complementary insight drives the entity extraction layer: trained responders and composed bystanders may report life-threatening emergencies without elevated vocal stress, requiring semantic analysis to capture clinical indicators that paralinguistic features miss.\n  We describe the architectural design, theoretical grounding in psycholinguistic research on stress-induced code-switching, and deployment considerations for offline operation during disaster scenarios. This work establishes a framework for accent-resilient emergency AI that ensures Caribbean voices receive equitable access to established national triage protocols. Empirical validation on Caribbean emergency calls remains future work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emergency speech recognition systems exhibit systematic performance degradation on non-standard English varieties, creating a critical gap in services for Caribbean populations. We present TRIDENT (Transcription and Routing Intelligence for Dispatcher-Empowered National Triage), a three-layer dispatcher-support architecture designed to structure emergency call inputs for human application of established triage protocols (the ESI for routine operations and START for mass casualty events), even when automatic speech recognition fails.\n  The system combines Caribbean-accent-tuned ASR, local entity extraction via large language models, and bio-acoustic distress detection to provide dispatchers with three complementary signals: transcription confidence, structured clinical entities, and vocal stress indicators. Our key insight is that low ASR confidence, rather than representing system failure, serves as a valuable queue prioritization signal -- particularly when combined with elevated vocal distress markers indicating a caller in crisis whose speech may have shifted toward basilectal registers. A complementary insight drives the entity extraction layer: trained responders and composed bystanders may report life-threatening emergencies without elevated vocal stress, requiring semantic analysis to capture clinical indicators that paralinguistic features miss.\n  We describe the architectural design, theoretical grounding in psycholinguistic research on stress-induced code-switching, and deployment considerations for offline operation during disaster scenarios. This work establishes a framework for accent-resilient emergency AI that ensures Caribbean voices receive equitable access to established national triage protocols. Empirical validation on Caribbean emergency calls remains future work."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T15:29:33Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    15,
                    29,
                    33,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Elroy Galbraith"
                    },
                    {
                        "name": "Chadwick Sutherland"
                    },
                    {
                        "name": "Donahue Morgan"
                    }
                ],
                "author_detail": {
                    "name": "Donahue Morgan"
                },
                "author": "Donahue Morgan"
            },
            {
                "id": "http://arxiv.org/abs/2512.10739v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10739v1",
                "title": "Long-horizon Reasoning Agent for Olympiad-Level Mathematical Problem Solving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-horizon Reasoning Agent for Olympiad-Level Mathematical Problem Solving"
                },
                "updated": "2025-12-11T15:26:28Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    15,
                    26,
                    28,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10739v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10739v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) have achieved significant progress in solving complex reasoning tasks by Reinforcement Learning with Verifiable Rewards (RLVR). This advancement is also inseparable from the oversight automated by reliable verifiers. However, current outcome-based verifiers (OVs) are unable to inspect the unreliable intermediate steps in the long reasoning chains of thought (CoTs). Meanwhile, current process-based verifiers (PVs) have difficulties in reliably detecting errors in the complex long CoTs, limited by the scarcity of high-quality annotations due to the prohibitive costs of human annotations. Therefore, we propose the \\textbf{O}utcome-based \\textbf{P}rocess \\textbf{V}erifier (OPV), which verifies the rationale process of summarized outcomes from long CoTs to achieve both accurate and efficient verification and enable large-scale annotation. To empower the proposed verifier, we adopt an iterative active learning framework with expert annotations to progressively improve the verification capability of OPV with fewer annotation costs. Specifically, in each iteration, the most uncertain cases of the current best OPV are annotated and then subsequently used to train a new OPV through Rejection Fine-Tuning (RFT) and RLVR for the next round. Extensive experiments demonstrate OPV's superior performance and broad applicability. It achieves new state-of-the-art results on our held-out \\textsc{\\thisbench}, outperforming much larger open-source models such as Qwen3-Max-Preview with an F1 score of 83.1 compared to 76.3. Furthermore, OPV effectively detects false positives within synthetic dataset, closely align with expert assessment. When collaborating with policy models, OPV consistently yields performance gains, e.g., raising the accuracy of DeepSeek-R1-Distill-Qwen-32B from 55.2\\% to 73.3\\% on AIME2025 as the compute budget scales.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved significant progress in solving complex reasoning tasks by Reinforcement Learning with Verifiable Rewards (RLVR). This advancement is also inseparable from the oversight automated by reliable verifiers. However, current outcome-based verifiers (OVs) are unable to inspect the unreliable intermediate steps in the long reasoning chains of thought (CoTs). Meanwhile, current process-based verifiers (PVs) have difficulties in reliably detecting errors in the complex long CoTs, limited by the scarcity of high-quality annotations due to the prohibitive costs of human annotations. Therefore, we propose the \\textbf{O}utcome-based \\textbf{P}rocess \\textbf{V}erifier (OPV), which verifies the rationale process of summarized outcomes from long CoTs to achieve both accurate and efficient verification and enable large-scale annotation. To empower the proposed verifier, we adopt an iterative active learning framework with expert annotations to progressively improve the verification capability of OPV with fewer annotation costs. Specifically, in each iteration, the most uncertain cases of the current best OPV are annotated and then subsequently used to train a new OPV through Rejection Fine-Tuning (RFT) and RLVR for the next round. Extensive experiments demonstrate OPV's superior performance and broad applicability. It achieves new state-of-the-art results on our held-out \\textsc{\\thisbench}, outperforming much larger open-source models such as Qwen3-Max-Preview with an F1 score of 83.1 compared to 76.3. Furthermore, OPV effectively detects false positives within synthetic dataset, closely align with expert assessment. When collaborating with policy models, OPV consistently yields performance gains, e.g., raising the accuracy of DeepSeek-R1-Distill-Qwen-32B from 55.2\\% to 73.3\\% on AIME2025 as the compute budget scales."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T15:26:28Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    15,
                    26,
                    28,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Songyang Gao"
                    },
                    {
                        "name": "Yuzhe Gu"
                    },
                    {
                        "name": "Zijian Wu"
                    },
                    {
                        "name": "Lingkai Kong"
                    },
                    {
                        "name": "Wenwei Zhang"
                    },
                    {
                        "name": "Zhongrui Cai"
                    },
                    {
                        "name": "Fan Zheng"
                    },
                    {
                        "name": "Tianyou Ma"
                    },
                    {
                        "name": "Junhao Shen"
                    },
                    {
                        "name": "Haiteng Zhao"
                    },
                    {
                        "name": "Duanyang Zhang"
                    },
                    {
                        "name": "Huilun Zhang"
                    },
                    {
                        "name": "Kuikun Liu"
                    },
                    {
                        "name": "Chengqi Lyu"
                    },
                    {
                        "name": "Yanhui Duan"
                    },
                    {
                        "name": "Chiyu Chen"
                    },
                    {
                        "name": "Ningsheng Ma"
                    },
                    {
                        "name": "Jianfei Gao"
                    },
                    {
                        "name": "Han Lyu"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Kai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Chen"
                },
                "author": "Kai Chen"
            },
            {
                "id": "http://arxiv.org/abs/2512.10734v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10734v1",
                "title": "Textual Data Bias Detection and Mitigation - An Extensible Pipeline with Experimental Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Textual Data Bias Detection and Mitigation - An Extensible Pipeline with Experimental Evaluation"
                },
                "updated": "2025-12-11T15:18:59Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    15,
                    18,
                    59,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10734v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10734v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Textual data used to train large language models (LLMs) exhibits multifaceted bias manifestations encompassing harmful language and skewed demographic distributions. Regulations such as the European AI Act require identifying and mitigating biases against protected groups in data, with the ultimate goal of preventing unfair model outputs. However, practical guidance and operationalization are lacking. We propose a comprehensive data bias detection and mitigation pipeline comprising four components that address two data bias types, namely representation bias and (explicit) stereotypes for a configurable sensitive attribute. First, we leverage LLM-generated word lists created based on quality criteria to detect relevant group labels. Second, representation bias is quantified using the Demographic Representation Score. Third, we detect and mitigate stereotypes using sociolinguistically informed filtering. Finally, we compensate representation bias through Grammar- and Context-Aware Counterfactual Data Augmentation. We conduct a two-fold evaluation using the examples of gender, religion and age. First, the effectiveness of each individual component on data debiasing is evaluated through human validation and baseline comparison. The findings demonstrate that we successfully reduce representation bias and (explicit) stereotypes in a text dataset. Second, the effect of data debiasing on model bias reduction is evaluated by bias benchmarking of several models (0.6B-8B parameters), fine-tuned on the debiased text dataset. This evaluation reveals that LLMs fine-tuned on debiased data do not consistently show improved performance on bias benchmarks, exposing critical gaps in current evaluation methodologies and highlighting the need for targeted data manipulation to address manifested model bias.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Textual data used to train large language models (LLMs) exhibits multifaceted bias manifestations encompassing harmful language and skewed demographic distributions. Regulations such as the European AI Act require identifying and mitigating biases against protected groups in data, with the ultimate goal of preventing unfair model outputs. However, practical guidance and operationalization are lacking. We propose a comprehensive data bias detection and mitigation pipeline comprising four components that address two data bias types, namely representation bias and (explicit) stereotypes for a configurable sensitive attribute. First, we leverage LLM-generated word lists created based on quality criteria to detect relevant group labels. Second, representation bias is quantified using the Demographic Representation Score. Third, we detect and mitigate stereotypes using sociolinguistically informed filtering. Finally, we compensate representation bias through Grammar- and Context-Aware Counterfactual Data Augmentation. We conduct a two-fold evaluation using the examples of gender, religion and age. First, the effectiveness of each individual component on data debiasing is evaluated through human validation and baseline comparison. The findings demonstrate that we successfully reduce representation bias and (explicit) stereotypes in a text dataset. Second, the effect of data debiasing on model bias reduction is evaluated by bias benchmarking of several models (0.6B-8B parameters), fine-tuned on the debiased text dataset. This evaluation reveals that LLMs fine-tuned on debiased data do not consistently show improved performance on bias benchmarks, exposing critical gaps in current evaluation methodologies and highlighting the need for targeted data manipulation to address manifested model bias."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T15:18:59Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    15,
                    18,
                    59,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Rebekka Görge"
                    },
                    {
                        "name": "Sujan Sai Gannamaneni"
                    },
                    {
                        "name": "Tabea Naeven"
                    },
                    {
                        "name": "Hammam Abdelwahab"
                    },
                    {
                        "name": "Héctor Allende-Cid"
                    },
                    {
                        "name": "Armin B. Cremers"
                    },
                    {
                        "name": "Lennard Helmer"
                    },
                    {
                        "name": "Michael Mock"
                    },
                    {
                        "name": "Anna Schmitz"
                    },
                    {
                        "name": "Songkai Xue"
                    },
                    {
                        "name": "Elif Yildirir"
                    },
                    {
                        "name": "Maximilian Poretschkin"
                    },
                    {
                        "name": "Stefan Wrobel"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Wrobel"
                },
                "author": "Stefan Wrobel"
            },
            {
                "id": "http://arxiv.org/abs/2512.05543v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05543v3",
                "title": "Are Bus-Mounted Edge Servers Feasible?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Bus-Mounted Edge Servers Feasible?"
                },
                "updated": "2025-12-11T14:58:22Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    14,
                    58,
                    22,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05543v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05543v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Placement of edge servers is the prerequisite of provisioning edge computing services for Internet of Vehicles (IoV). Fixed-site edge servers at Road Side Units (RSUs) or base stations are able to offer basic service coverage for end users, i.e., vehicles on road. However, the server locations and capacity are fixed after deployment, rendering their inefficiency in handling spationtemporal user dynamics. Mobile servers such as buses, on the other hand, have the potential of adding computation elasticity to such system. To this end, this paper studies the feasibility of bus-mounted edge servers based on real traces. First, we investigate the coverage of the buses and base stations using the Shanghai bus/taxi/Telecom datasets, which shows a great potential of bus-based edge servers as they cover a great portion of geographic area and demand points. Next, we build a mathematical model and design a simple greedy heuristic algorithm to select a limited number of buses that maximizes the coverage of demand points, i.e., with a limited purchase budget. We perform trace-driven simulations to verify the performance of the proposed bus selection algorithm. The results show that our approach effectively handles the dynamic user demand under realistic constraints such as server capacity and purchase quantity. Thus, we claim: bus-mounted edge servers for vehicular networks in urban areas are feasible, beneficial, and valuable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Placement of edge servers is the prerequisite of provisioning edge computing services for Internet of Vehicles (IoV). Fixed-site edge servers at Road Side Units (RSUs) or base stations are able to offer basic service coverage for end users, i.e., vehicles on road. However, the server locations and capacity are fixed after deployment, rendering their inefficiency in handling spationtemporal user dynamics. Mobile servers such as buses, on the other hand, have the potential of adding computation elasticity to such system. To this end, this paper studies the feasibility of bus-mounted edge servers based on real traces. First, we investigate the coverage of the buses and base stations using the Shanghai bus/taxi/Telecom datasets, which shows a great potential of bus-based edge servers as they cover a great portion of geographic area and demand points. Next, we build a mathematical model and design a simple greedy heuristic algorithm to select a limited number of buses that maximizes the coverage of demand points, i.e., with a limited purchase budget. We perform trace-driven simulations to verify the performance of the proposed bus selection algorithm. The results show that our approach effectively handles the dynamic user demand under realistic constraints such as server capacity and purchase quantity. Thus, we claim: bus-mounted edge servers for vehicular networks in urban areas are feasible, beneficial, and valuable."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T08:56:15Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    8,
                    56,
                    15,
                    4,
                    339,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Xuezhi Li"
                    },
                    {
                        "name": "Jiancong He"
                    },
                    {
                        "name": "Ming Xie"
                    },
                    {
                        "name": "Xuyang Chen"
                    },
                    {
                        "name": "Le Chang"
                    },
                    {
                        "name": "Li Jiang"
                    },
                    {
                        "name": "Gui Gui"
                    }
                ],
                "author_detail": {
                    "name": "Gui Gui"
                },
                "author": "Gui Gui"
            },
            {
                "id": "http://arxiv.org/abs/2512.10715v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10715v1",
                "title": "CheXmask-U: Quantifying uncertainty in landmark-based anatomical segmentation for X-ray images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CheXmask-U: Quantifying uncertainty in landmark-based anatomical segmentation for X-ray images"
                },
                "updated": "2025-12-11T14:50:23Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    14,
                    50,
                    23,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10715v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10715v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Uncertainty estimation is essential for the safe clinical deployment of medical image segmentation systems, enabling the identification of unreliable predictions and supporting human oversight. While prior work has largely focused on pixel-level uncertainty, landmark-based segmentation offers inherent topological guarantees yet remains underexplored from an uncertainty perspective. In this work, we study uncertainty estimation for anatomical landmark-based segmentation on chest X-rays. Inspired by hybrid neural network architectures that combine standard image convolutional encoders with graph-based generative decoders, and leveraging their variational latent space, we derive two complementary measures: (i) latent uncertainty, captured directly from the learned distribution parameters, and (ii) predictive uncertainty, obtained by generating multiple stochastic output predictions from latent samples. Through controlled corruption experiments we show that both uncertainty measures increase with perturbation severity, reflecting both global and local degradation. We demonstrate that these uncertainty signals can identify unreliable predictions by comparing with manual ground-truth, and support out-of-distribution detection on the CheXmask dataset. More importantly, we release CheXmask-U (huggingface.co/datasets/mcosarinsky/CheXmask-U), a large scale dataset of 657,566 chest X-ray landmark segmentations with per-node uncertainty estimates, enabling researchers to account for spatial variations in segmentation quality when using these anatomical masks. Our findings establish uncertainty estimation as a promising direction to enhance robustness and safe deployment of landmark-based anatomical segmentation methods in chest X-ray. A fully working interactive demo of the method is available at huggingface.co/spaces/matiasky/CheXmask-U and the source code at github.com/mcosarinsky/CheXmask-U.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty estimation is essential for the safe clinical deployment of medical image segmentation systems, enabling the identification of unreliable predictions and supporting human oversight. While prior work has largely focused on pixel-level uncertainty, landmark-based segmentation offers inherent topological guarantees yet remains underexplored from an uncertainty perspective. In this work, we study uncertainty estimation for anatomical landmark-based segmentation on chest X-rays. Inspired by hybrid neural network architectures that combine standard image convolutional encoders with graph-based generative decoders, and leveraging their variational latent space, we derive two complementary measures: (i) latent uncertainty, captured directly from the learned distribution parameters, and (ii) predictive uncertainty, obtained by generating multiple stochastic output predictions from latent samples. Through controlled corruption experiments we show that both uncertainty measures increase with perturbation severity, reflecting both global and local degradation. We demonstrate that these uncertainty signals can identify unreliable predictions by comparing with manual ground-truth, and support out-of-distribution detection on the CheXmask dataset. More importantly, we release CheXmask-U (huggingface.co/datasets/mcosarinsky/CheXmask-U), a large scale dataset of 657,566 chest X-ray landmark segmentations with per-node uncertainty estimates, enabling researchers to account for spatial variations in segmentation quality when using these anatomical masks. Our findings establish uncertainty estimation as a promising direction to enhance robustness and safe deployment of landmark-based anatomical segmentation methods in chest X-ray. A fully working interactive demo of the method is available at huggingface.co/spaces/matiasky/CheXmask-U and the source code at github.com/mcosarinsky/CheXmask-U."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T14:50:23Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    14,
                    50,
                    23,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Matias Cosarinsky"
                    },
                    {
                        "name": "Nicolas Gaggion"
                    },
                    {
                        "name": "Rodrigo Echeveste"
                    },
                    {
                        "name": "Enzo Ferrante"
                    }
                ],
                "author_detail": {
                    "name": "Enzo Ferrante"
                },
                "author": "Enzo Ferrante"
            },
            {
                "id": "http://arxiv.org/abs/2512.10713v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10713v1",
                "title": "PACIFIC: a framework for generating benchmarks to check Precise Automatically Checked Instruction Following In Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PACIFIC: a framework for generating benchmarks to check Precise Automatically Checked Instruction Following In Code"
                },
                "updated": "2025-12-11T14:49:56Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    14,
                    49,
                    56,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10713v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10713v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Model (LLM)-based code assistants have emerged as a powerful application of generative AI, demonstrating impressive capabilities in code generation and comprehension. A key requirement for these systems is their ability to accurately follow user instructions. We present Precise Automatically Checked Instruction Following In Code (PACIFIC), a novel framework designed to automatically generate benchmarks that rigorously assess sequential instruction-following and code dry-running capabilities in LLMs, while allowing control over benchmark difficulty. PACIFIC produces benchmark variants with clearly defined expected outputs, enabling straightforward and reliable evaluation through simple output comparisons. In contrast to existing approaches that often rely on tool usage or agentic behavior, our work isolates and evaluates the LLM's intrinsic ability to reason through code behavior step-by-step without execution (dry running) and to follow instructions. Furthermore, our framework mitigates training data contamination by facilitating effortless generation of novel benchmark variations. We validate our framework by generating a suite of benchmarks spanning a range of difficulty levels and evaluating multiple state-of-the-art LLMs. Our results demonstrate that PACIFIC can produce increasingly challenging benchmarks that effectively differentiate instruction-following and dry running capabilities, even among advanced models. Overall, our framework offers a scalable, contamination-resilient methodology for assessing core competencies of LLMs in code-related tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM)-based code assistants have emerged as a powerful application of generative AI, demonstrating impressive capabilities in code generation and comprehension. A key requirement for these systems is their ability to accurately follow user instructions. We present Precise Automatically Checked Instruction Following In Code (PACIFIC), a novel framework designed to automatically generate benchmarks that rigorously assess sequential instruction-following and code dry-running capabilities in LLMs, while allowing control over benchmark difficulty. PACIFIC produces benchmark variants with clearly defined expected outputs, enabling straightforward and reliable evaluation through simple output comparisons. In contrast to existing approaches that often rely on tool usage or agentic behavior, our work isolates and evaluates the LLM's intrinsic ability to reason through code behavior step-by-step without execution (dry running) and to follow instructions. Furthermore, our framework mitigates training data contamination by facilitating effortless generation of novel benchmark variations. We validate our framework by generating a suite of benchmarks spanning a range of difficulty levels and evaluating multiple state-of-the-art LLMs. Our results demonstrate that PACIFIC can produce increasingly challenging benchmarks that effectively differentiate instruction-following and dry running capabilities, even among advanced models. Overall, our framework offers a scalable, contamination-resilient methodology for assessing core competencies of LLMs in code-related tasks."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T14:49:56Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    14,
                    49,
                    56,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Itay Dreyfuss"
                    },
                    {
                        "name": "Antonio Abu Nassar"
                    },
                    {
                        "name": "Samuel Ackerman"
                    },
                    {
                        "name": "Axel Ben David"
                    },
                    {
                        "name": "Rami Katan"
                    },
                    {
                        "name": "Orna Raz"
                    },
                    {
                        "name": "Marcel Zalmanovici"
                    }
                ],
                "author_detail": {
                    "name": "Marcel Zalmanovici"
                },
                "author": "Marcel Zalmanovici"
            },
            {
                "id": "http://arxiv.org/abs/2510.21737v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.21737v2",
                "title": "From Factoid Questions to Data Product Requests: Benchmarking Data Product Discovery over Tables and Text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Factoid Questions to Data Product Requests: Benchmarking Data Product Discovery over Tables and Text"
                },
                "updated": "2025-12-11T14:47:03Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    14,
                    47,
                    3,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.21737v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.21737v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Data products are reusable, self-contained assets designed for specific business use cases. Automating their discovery and generation is of great industry interest, as it enables discovery in large data lakes and supports analytical Data Product Requests (DPRs). Currently, there is no benchmark established specifically for data product discovery. Existing datasets focus on answering single factoid questions over individual tables rather than collecting multiple data assets for broader, coherent products. To address this gap, we introduce DPBench, the first user-request-driven data product benchmark over hybrid table-text corpora. Our framework systematically repurposes existing table-text QA datasets by clustering related tables and passages into coherent data products, generating professional-level analytical requests that span both data sources, and validating benchmark quality through multi-LLM evaluation. DPBench preserves full provenance while producing actionable, analyst-like data product requests. Baseline experiments with hybrid retrieval methods establish the feasibility of DPR evaluation, reveal current limitations, and point to new opportunities for automatic data product discovery research.\n  Code and datasets are available at: https://anonymous.4open.science/r/data-product-benchmark-BBA7/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data products are reusable, self-contained assets designed for specific business use cases. Automating their discovery and generation is of great industry interest, as it enables discovery in large data lakes and supports analytical Data Product Requests (DPRs). Currently, there is no benchmark established specifically for data product discovery. Existing datasets focus on answering single factoid questions over individual tables rather than collecting multiple data assets for broader, coherent products. To address this gap, we introduce DPBench, the first user-request-driven data product benchmark over hybrid table-text corpora. Our framework systematically repurposes existing table-text QA datasets by clustering related tables and passages into coherent data products, generating professional-level analytical requests that span both data sources, and validating benchmark quality through multi-LLM evaluation. DPBench preserves full provenance while producing actionable, analyst-like data product requests. Baseline experiments with hybrid retrieval methods establish the feasibility of DPR evaluation, reveal current limitations, and point to new opportunities for automatic data product discovery research.\n  Code and datasets are available at: https://anonymous.4open.science/r/data-product-benchmark-BBA7/"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-30T23:07:36Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    23,
                    7,
                    36,
                    1,
                    273,
                    0
                ],
                "arxiv_comment": "9 pages, 1 figure, 2 tables",
                "arxiv_primary_category": {
                    "term": "cs.IR"
                },
                "authors": [
                    {
                        "name": "Liangliang Zhang"
                    },
                    {
                        "name": "Nandana Mihindukulasooriya"
                    },
                    {
                        "name": "Niharika S. D'Souza"
                    },
                    {
                        "name": "Sola Shirai"
                    },
                    {
                        "name": "Sarthak Dash"
                    },
                    {
                        "name": "Yao Ma"
                    },
                    {
                        "name": "Horst Samulowitz"
                    }
                ],
                "author_detail": {
                    "name": "Horst Samulowitz"
                },
                "author": "Horst Samulowitz"
            },
            {
                "id": "http://arxiv.org/abs/2512.10698v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10698v1",
                "title": "How to Brake? Ethical Emergency Braking with Deep Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to Brake? Ethical Emergency Braking with Deep Reinforcement Learning"
                },
                "updated": "2025-12-11T14:40:33Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    14,
                    40,
                    33,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10698v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10698v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Connected and automated vehicles (CAVs) have the potential to enhance driving safety, for example by enabling safe vehicle following and more efficient traffic scheduling. For such future deployments, safety requirements should be addressed, where the primary such are avoidance of vehicle collisions and substantial mitigating of harm when collisions are unavoidable. However, conservative worst-case-based control strategies come at the price of reduced flexibility and may compromise overall performance. In light of this, we investigate how Deep Reinforcement Learning (DRL) can be leveraged to improve safety in multi-vehicle-following scenarios involving emergency braking. Specifically, we investigate how DRL with vehicle-to-vehicle communication can be used to ethically select an emergency breaking profile in scenarios where overall, or collective, three-vehicle harm reduction or collision avoidance shall be obtained instead of single-vehicle such. As an algorithm, we provide a hybrid approach that combines DRL with a previously published method based on analytical expressions for selecting optimal constant deceleration. By combining DRL with the previous method, the proposed hybrid approach increases the reliability compared to standalone DRL, while achieving superior performance in terms of overall harm reduction and collision avoidance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Connected and automated vehicles (CAVs) have the potential to enhance driving safety, for example by enabling safe vehicle following and more efficient traffic scheduling. For such future deployments, safety requirements should be addressed, where the primary such are avoidance of vehicle collisions and substantial mitigating of harm when collisions are unavoidable. However, conservative worst-case-based control strategies come at the price of reduced flexibility and may compromise overall performance. In light of this, we investigate how Deep Reinforcement Learning (DRL) can be leveraged to improve safety in multi-vehicle-following scenarios involving emergency braking. Specifically, we investigate how DRL with vehicle-to-vehicle communication can be used to ethically select an emergency breaking profile in scenarios where overall, or collective, three-vehicle harm reduction or collision avoidance shall be obtained instead of single-vehicle such. As an algorithm, we provide a hybrid approach that combines DRL with a previously published method based on analytical expressions for selecting optimal constant deceleration. By combining DRL with the previous method, the proposed hybrid approach increases the reliability compared to standalone DRL, while achieving superior performance in terms of overall harm reduction and collision avoidance."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T14:40:33Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    14,
                    40,
                    33,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Jianbo Wang"
                    },
                    {
                        "name": "Galina Sidorenko"
                    },
                    {
                        "name": "Johan Thunberg"
                    }
                ],
                "author_detail": {
                    "name": "Johan Thunberg"
                },
                "author": "Johan Thunberg"
            },
            {
                "id": "http://arxiv.org/abs/2512.10696v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10696v1",
                "title": "Remember Me, Refine Me: A Dynamic Procedural Memory Framework for Experience-Driven Agent Evolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Remember Me, Refine Me: A Dynamic Procedural Memory Framework for Experience-Driven Agent Evolution"
                },
                "updated": "2025-12-11T14:40:01Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    14,
                    40,
                    1,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10696v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10696v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Procedural memory enables large language model (LLM) agents to internalize \"how-to\" knowledge, theoretically reducing redundant trial-and-error. However, existing frameworks predominantly suffer from a \"passive accumulation\" paradigm, treating memory as a static append-only archive. To bridge the gap between static storage and dynamic reasoning, we propose $\\textbf{ReMe}$ ($\\textit{Remember Me, Refine Me}$), a comprehensive framework for experience-driven agent evolution. ReMe innovates across the memory lifecycle via three mechanisms: 1) $\\textit{multi-faceted distillation}$, which extracts fine-grained experiences by recognizing success patterns, analyzing failure triggers and generating comparative insights; 2) $\\textit{context-adaptive reuse}$, which tailors historical insights to new contexts via scenario-aware indexing; and 3) $\\textit{utility-based refinement}$, which autonomously adds valid memories and prunes outdated ones to maintain a compact, high-quality experience pool. Extensive experiments on BFCL-V3 and AppWorld demonstrate that ReMe establishes a new state-of-the-art in agent memory system. Crucially, we observe a significant memory-scaling effect: Qwen3-8B equipped with ReMe outperforms larger, memoryless Qwen3-14B, suggesting that self-evolving memory provides a computation-efficient pathway for lifelong learning. We release our code and the $\\texttt{reme.library}$ dataset to facilitate further research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Procedural memory enables large language model (LLM) agents to internalize \"how-to\" knowledge, theoretically reducing redundant trial-and-error. However, existing frameworks predominantly suffer from a \"passive accumulation\" paradigm, treating memory as a static append-only archive. To bridge the gap between static storage and dynamic reasoning, we propose $\\textbf{ReMe}$ ($\\textit{Remember Me, Refine Me}$), a comprehensive framework for experience-driven agent evolution. ReMe innovates across the memory lifecycle via three mechanisms: 1) $\\textit{multi-faceted distillation}$, which extracts fine-grained experiences by recognizing success patterns, analyzing failure triggers and generating comparative insights; 2) $\\textit{context-adaptive reuse}$, which tailors historical insights to new contexts via scenario-aware indexing; and 3) $\\textit{utility-based refinement}$, which autonomously adds valid memories and prunes outdated ones to maintain a compact, high-quality experience pool. Extensive experiments on BFCL-V3 and AppWorld demonstrate that ReMe establishes a new state-of-the-art in agent memory system. Crucially, we observe a significant memory-scaling effect: Qwen3-8B equipped with ReMe outperforms larger, memoryless Qwen3-14B, suggesting that self-evolving memory provides a computation-efficient pathway for lifelong learning. We release our code and the $\\texttt{reme.library}$ dataset to facilitate further research."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T14:40:01Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    14,
                    40,
                    1,
                    3,
                    345,
                    0
                ],
                "arxiv_comment": "16 pages, 9 figures, 9 tables",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Zouying Cao"
                    },
                    {
                        "name": "Jiaji Deng"
                    },
                    {
                        "name": "Li Yu"
                    },
                    {
                        "name": "Weikang Zhou"
                    },
                    {
                        "name": "Zhaoyang Liu"
                    },
                    {
                        "name": "Bolin Ding"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao"
            },
            {
                "id": "http://arxiv.org/abs/2512.10687v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10687v1",
                "title": "Challenges of Evaluating LLM Safety for User Welfare",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Challenges of Evaluating LLM Safety for User Welfare"
                },
                "updated": "2025-12-11T14:34:40Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    14,
                    34,
                    40,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10687v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10687v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Safety evaluations of large language models (LLMs) typically focus on universal risks like dangerous capabilities or undesirable propensities. However, millions use LLMs for personal advice on high-stakes topics like finance and health, where harms are context-dependent rather than universal. While frameworks like the OECD's AI classification recognize the need to assess individual risks, user-welfare safety evaluations remain underdeveloped. We argue that developing such evaluations is non-trivial due to fundamental questions about accounting for user context in evaluation design. In this exploratory study, we evaluated advice on finance and health from GPT-5, Claude Sonnet 4, and Gemini 2.5 Pro across user profiles of varying vulnerability. First, we demonstrate that evaluators must have access to rich user context: identical LLM responses were rated significantly safer by context-blind evaluators than by those aware of user circumstances, with safety scores for high-vulnerability users dropping from safe (5/7) to somewhat unsafe (3/7). One might assume this gap could be addressed by creating realistic user prompts containing key contextual information. However, our second study challenges this: we rerun the evaluation on prompts containing context users report they would disclose, finding no significant improvement. Our work establishes that effective user-welfare safety evaluation requires evaluators to assess responses against diverse user profiles, as realistic user context disclosure alone proves insufficient, particularly for vulnerable populations. By demonstrating a methodology for context-aware evaluation, this study provides both a starting point for such assessments and foundational evidence that evaluating individual welfare demands approaches distinct from existing universal-risk frameworks. We publish our code and dataset to aid future developments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safety evaluations of large language models (LLMs) typically focus on universal risks like dangerous capabilities or undesirable propensities. However, millions use LLMs for personal advice on high-stakes topics like finance and health, where harms are context-dependent rather than universal. While frameworks like the OECD's AI classification recognize the need to assess individual risks, user-welfare safety evaluations remain underdeveloped. We argue that developing such evaluations is non-trivial due to fundamental questions about accounting for user context in evaluation design. In this exploratory study, we evaluated advice on finance and health from GPT-5, Claude Sonnet 4, and Gemini 2.5 Pro across user profiles of varying vulnerability. First, we demonstrate that evaluators must have access to rich user context: identical LLM responses were rated significantly safer by context-blind evaluators than by those aware of user circumstances, with safety scores for high-vulnerability users dropping from safe (5/7) to somewhat unsafe (3/7). One might assume this gap could be addressed by creating realistic user prompts containing key contextual information. However, our second study challenges this: we rerun the evaluation on prompts containing context users report they would disclose, finding no significant improvement. Our work establishes that effective user-welfare safety evaluation requires evaluators to assess responses against diverse user profiles, as realistic user context disclosure alone proves insufficient, particularly for vulnerable populations. By demonstrating a methodology for context-aware evaluation, this study provides both a starting point for such assessments and foundational evidence that evaluating individual welfare demands approaches distinct from existing universal-risk frameworks. We publish our code and dataset to aid future developments."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T14:34:40Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    14,
                    34,
                    40,
                    3,
                    345,
                    0
                ],
                "arxiv_comment": "Paper accepted at IASEAI'26; please cite that peer-reviewed version instead",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Manon Kempermann"
                    },
                    {
                        "name": "Sai Suresh Macharla Vasu"
                    },
                    {
                        "name": "Mahalakshmi Raveenthiran"
                    },
                    {
                        "name": "Theo Farrell"
                    },
                    {
                        "name": "Ingmar Weber"
                    }
                ],
                "author_detail": {
                    "name": "Ingmar Weber"
                },
                "author": "Ingmar Weber"
            },
            {
                "id": "http://arxiv.org/abs/2512.05414v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05414v3",
                "title": "LMSpell: Neural Spell Checking for Low-Resource Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LMSpell: Neural Spell Checking for Low-Resource Languages"
                },
                "updated": "2025-12-11T14:22:42Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    14,
                    22,
                    42,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05414v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05414v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Spell correction is still a challenging problem for low-resource languages (LRLs). While pretrained language models (PLMs) have been employed for spell correction, their use is still limited to a handful of languages, and there has been no proper comparison across PLMs. We present the first empirical study on the effectiveness of PLMs for spell correction, which includes LRLs. We find that Large Language Models (LLMs) outperform their counterparts (encoder-based and encoder-decoder) when the fine-tuning dataset is large. This observation holds even in languages for which the LLM is not pre-trained. We release LMSpell, an easy- to use spell correction toolkit across PLMs. It includes an evaluation function that compensates for the hallucination of LLMs. Further, we present a case study with Sinhala to shed light on the plight of spell correction for LRLs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spell correction is still a challenging problem for low-resource languages (LRLs). While pretrained language models (PLMs) have been employed for spell correction, their use is still limited to a handful of languages, and there has been no proper comparison across PLMs. We present the first empirical study on the effectiveness of PLMs for spell correction, which includes LRLs. We find that Large Language Models (LLMs) outperform their counterparts (encoder-based and encoder-decoder) when the fine-tuning dataset is large. This observation holds even in languages for which the LLM is not pre-trained. We release LMSpell, an easy- to use spell correction toolkit across PLMs. It includes an evaluation function that compensates for the hallucination of LLMs. Further, we present a case study with Sinhala to shed light on the plight of spell correction for LRLs."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T04:14:09Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    4,
                    14,
                    9,
                    4,
                    339,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Akesh Gunathilake"
                    },
                    {
                        "name": "Nadil Karunarathna"
                    },
                    {
                        "name": "Tharusha Bandaranayake"
                    },
                    {
                        "name": "Nisansa de Silva"
                    },
                    {
                        "name": "Surangika Ranathunga"
                    }
                ],
                "author_detail": {
                    "name": "Surangika Ranathunga"
                },
                "author": "Surangika Ranathunga"
            },
            {
                "id": "http://arxiv.org/abs/2512.10674v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10674v1",
                "title": "Geo6DPose: Fast Zero-Shot 6D Object Pose Estimation via Geometry-Filtered Feature Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geo6DPose: Fast Zero-Shot 6D Object Pose Estimation via Geometry-Filtered Feature Matching"
                },
                "updated": "2025-12-11T14:20:17Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    14,
                    20,
                    17,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10674v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10674v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent progress in zero-shot 6D object pose estimation has been driven largely by large-scale models and cloud-based inference. However, these approaches often introduce high latency, elevated energy consumption, and deployment risks related to connectivity, cost, and data governance; factors that conflict with the practical constraints of real-world robotics, where compute is limited and on-device inference is frequently required. We introduce Geo6DPose, a lightweight, fully local, and training-free pipeline for zero-shot 6D pose estimation that trades model scale for geometric reliability. Our method combines foundation model visual features with a geometric filtering strategy: Similarity maps are computed between onboarded template DINO descriptors and scene patches, and mutual correspondences are established by projecting scene patch centers to 3D and template descriptors to the object model coordinate system. Final poses are recovered via correspondence-driven RANSAC and ranked using a weighted geometric alignment metric that jointly accounts for reprojection consistency and spatial support, improving robustness to noise, clutter, and partial visibility. Geo6DPose achieves sub-second inference on a single commodity GPU while matching the average recall of significantly larger zero-shot baselines (53.7 AR, 1.08 FPS). It requires no training, fine-tuning, or network access, and remains compatible with evolving foundation backbones, advancing practical, fully local 6D perception for robotic deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent progress in zero-shot 6D object pose estimation has been driven largely by large-scale models and cloud-based inference. However, these approaches often introduce high latency, elevated energy consumption, and deployment risks related to connectivity, cost, and data governance; factors that conflict with the practical constraints of real-world robotics, where compute is limited and on-device inference is frequently required. We introduce Geo6DPose, a lightweight, fully local, and training-free pipeline for zero-shot 6D pose estimation that trades model scale for geometric reliability. Our method combines foundation model visual features with a geometric filtering strategy: Similarity maps are computed between onboarded template DINO descriptors and scene patches, and mutual correspondences are established by projecting scene patch centers to 3D and template descriptors to the object model coordinate system. Final poses are recovered via correspondence-driven RANSAC and ranked using a weighted geometric alignment metric that jointly accounts for reprojection consistency and spatial support, improving robustness to noise, clutter, and partial visibility. Geo6DPose achieves sub-second inference on a single commodity GPU while matching the average recall of significantly larger zero-shot baselines (53.7 AR, 1.08 FPS). It requires no training, fine-tuning, or network access, and remains compatible with evolving foundation backbones, advancing practical, fully local 6D perception for robotic deployment."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T14:20:17Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    14,
                    20,
                    17,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Javier Villena Toro"
                    },
                    {
                        "name": "Mehdi Tarkian"
                    }
                ],
                "author_detail": {
                    "name": "Mehdi Tarkian"
                },
                "author": "Mehdi Tarkian"
            },
            {
                "id": "http://arxiv.org/abs/2512.10665v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10665v1",
                "title": "On the Dynamics of Multi-Agent LLM Communities Driven by Value Diversity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Dynamics of Multi-Agent LLM Communities Driven by Value Diversity"
                },
                "updated": "2025-12-11T14:13:53Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    14,
                    13,
                    53,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10665v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10665v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As Large Language Models (LLM) based multi-agent systems become increasingly prevalent, the collective behaviors, e.g., collective intelligence, of such artificial communities have drawn growing attention. This work aims to answer a fundamental question: How does diversity of values shape the collective behavior of AI communities? Using naturalistic value elicitation grounded in the prevalent Schwartz's Theory of Basic Human Values, we constructed multi-agent simulations where communities with varying numbers of agents engaged in open-ended interactions and constitution formation. The results show that value diversity enhances value stability, fosters emergent behaviors, and brings more creative principles developed by the agents themselves without external guidance. However, these effects also show diminishing returns: extreme heterogeneity induces instability. This work positions value diversity as a new axis of future AI capability, bridging AI ability and sociological studies of institutional emergence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLM) based multi-agent systems become increasingly prevalent, the collective behaviors, e.g., collective intelligence, of such artificial communities have drawn growing attention. This work aims to answer a fundamental question: How does diversity of values shape the collective behavior of AI communities? Using naturalistic value elicitation grounded in the prevalent Schwartz's Theory of Basic Human Values, we constructed multi-agent simulations where communities with varying numbers of agents engaged in open-ended interactions and constitution formation. The results show that value diversity enhances value stability, fosters emergent behaviors, and brings more creative principles developed by the agents themselves without external guidance. However, these effects also show diminishing returns: extreme heterogeneity induces instability. This work positions value diversity as a new axis of future AI capability, bridging AI ability and sociological studies of institutional emergence."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T14:13:53Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    14,
                    13,
                    53,
                    3,
                    345,
                    0
                ],
                "arxiv_comment": "Working Paper",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Muhua Huang"
                    },
                    {
                        "name": "Qinlin Zhao"
                    },
                    {
                        "name": "Xiaoyuan Yi"
                    },
                    {
                        "name": "Xing Xie"
                    }
                ],
                "author_detail": {
                    "name": "Xing Xie"
                },
                "author": "Xing Xie"
            },
            {
                "id": "http://arxiv.org/abs/2508.08139v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.08139v2",
                "title": "Can LLMs Detect Their Confabulations? Estimating Reliability in Uncertainty-Aware Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Detect Their Confabulations? Estimating Reliability in Uncertainty-Aware Language Models"
                },
                "updated": "2025-12-11T13:49:54Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    13,
                    49,
                    54,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.08139v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.08139v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) are prone to generating fluent but incorrect content, known as confabulation, which poses increasing risks in multi-turn or agentic applications where outputs may be reused as context. In this work, we investigate how in-context information influences model behavior and whether LLMs can identify their unreliable responses. We propose a reliability estimation that leverages token-level uncertainty to guide the aggregation of internal model representations. Specifically, we compute aleatoric and epistemic uncertainty from output logits to identify salient tokens and aggregate their hidden states into compact representations for response-level reliability prediction. Through controlled experiments on open QA benchmarks, we find that correct in-context information improves both answer accuracy and model confidence, while misleading context often induces confidently incorrect responses, revealing a misalignment between uncertainty and correctness. Our probing-based method captures these shifts in model behavior and improves the detection of unreliable outputs across multiple open-source LLMs. These results underscore the limitations of direct uncertainty signals and highlight the potential of uncertainty-guided probing for reliability-aware generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are prone to generating fluent but incorrect content, known as confabulation, which poses increasing risks in multi-turn or agentic applications where outputs may be reused as context. In this work, we investigate how in-context information influences model behavior and whether LLMs can identify their unreliable responses. We propose a reliability estimation that leverages token-level uncertainty to guide the aggregation of internal model representations. Specifically, we compute aleatoric and epistemic uncertainty from output logits to identify salient tokens and aggregate their hidden states into compact representations for response-level reliability prediction. Through controlled experiments on open QA benchmarks, we find that correct in-context information improves both answer accuracy and model confidence, while misleading context often induces confidently incorrect responses, revealing a misalignment between uncertainty and correctness. Our probing-based method captures these shifts in model behavior and improves the detection of unreliable outputs across multiple open-source LLMs. These results underscore the limitations of direct uncertainty signals and highlight the potential of uncertainty-guided probing for reliability-aware generation."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-11T16:12:36Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    16,
                    12,
                    36,
                    0,
                    223,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Tianyi Zhou"
                    },
                    {
                        "name": "Johanne Medina"
                    },
                    {
                        "name": "Sanjay Chawla"
                    }
                ],
                "author_detail": {
                    "name": "Sanjay Chawla"
                },
                "author": "Sanjay Chawla"
            },
            {
                "id": "http://arxiv.org/abs/2510.08158v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.08158v2",
                "title": "Beyond Over-Refusal: Scenario-Based Diagnostics and Post-Hoc Mitigation for Exaggerated Refusals in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Over-Refusal: Scenario-Based Diagnostics and Post-Hoc Mitigation for Exaggerated Refusals in LLMs"
                },
                "updated": "2025-12-11T13:48:33Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    13,
                    48,
                    33,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.08158v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.08158v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) frequently produce false refusals, declining benign requests that contain terms resembling unsafe queries. We address this challenge by introducing two comprehensive benchmarks: the Exaggerated Safety Benchmark (XSB) for single-turn prompts, annotated with \"Focus\" keywords that identify refusal-inducing triggers, and the Multi-turn Scenario-based Exaggerated Safety Benchmark (MS-XSB), which systematically evaluates refusal calibration in realistic, context-rich dialog settings. Our benchmarks reveal that exaggerated refusals persist across diverse recent LLMs and are especially pronounced in complex, multi-turn scenarios. To mitigate these failures, we leverage post-hoc explanation methods to identify refusal triggers and deploy three lightweight, model-agnostic approaches, ignore-word instructions, prompt rephrasing, and attention steering, at inference time, all without retraining or parameter access. Experiments on four instruction-tuned Llama models demonstrate that these strategies substantially improve compliance on safe prompts while maintaining robust safety protections. Our findings establish a reproducible framework for diagnosing and mitigating exaggerated refusals, highlighting practical pathways to safer and more helpful LLM deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) frequently produce false refusals, declining benign requests that contain terms resembling unsafe queries. We address this challenge by introducing two comprehensive benchmarks: the Exaggerated Safety Benchmark (XSB) for single-turn prompts, annotated with \"Focus\" keywords that identify refusal-inducing triggers, and the Multi-turn Scenario-based Exaggerated Safety Benchmark (MS-XSB), which systematically evaluates refusal calibration in realistic, context-rich dialog settings. Our benchmarks reveal that exaggerated refusals persist across diverse recent LLMs and are especially pronounced in complex, multi-turn scenarios. To mitigate these failures, we leverage post-hoc explanation methods to identify refusal triggers and deploy three lightweight, model-agnostic approaches, ignore-word instructions, prompt rephrasing, and attention steering, at inference time, all without retraining or parameter access. Experiments on four instruction-tuned Llama models demonstrate that these strategies substantially improve compliance on safe prompts while maintaining robust safety protections. Our findings establish a reproducible framework for diagnosing and mitigating exaggerated refusals, highlighting practical pathways to safer and more helpful LLM deployments."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-09T12:38:16Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    12,
                    38,
                    16,
                    3,
                    282,
                    0
                ],
                "arxiv_comment": "Errors in the paper",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Shuzhou Yuan"
                    },
                    {
                        "name": "Ercong Nie"
                    },
                    {
                        "name": "Yinuo Sun"
                    },
                    {
                        "name": "Chenxuan Zhao"
                    },
                    {
                        "name": "William LaCroix"
                    },
                    {
                        "name": "Michael Färber"
                    }
                ],
                "author_detail": {
                    "name": "Michael Färber"
                },
                "author": "Michael Färber"
            },
            {
                "id": "http://arxiv.org/abs/2512.10630v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10630v1",
                "title": "From Data Scarcity to Data Care: Reimagining Language Technologies for Serbian and other Low-Resource Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Data Scarcity to Data Care: Reimagining Language Technologies for Serbian and other Low-Resource Languages"
                },
                "updated": "2025-12-11T13:29:25Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    13,
                    29,
                    25,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10630v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10630v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models are commonly trained on dominant languages like English, and their representation of low resource languages typically reflects cultural and linguistic biases present in the source language materials. Using the Serbian language as a case, this study examines the structural, historical, and sociotechnical factors shaping language technology development for low resource languages in the AI age. Drawing on semi structured interviews with ten scholars and practitioners, including linguists, digital humanists, and AI developers, it traces challenges rooted in historical destruction of Serbian textual heritage, intensified by contemporary issues that drive reductive, engineering first approaches prioritizing functionality over linguistic nuance. These include superficial transliteration, reliance on English-trained models, data bias, and dataset curation lacking cultural specificity. To address these challenges, the study proposes Data Care, a framework grounded in CARE principles (Collective Benefit, Authority to Control, Responsibility, and Ethics), that reframes bias mitigation from a post hoc technical fix to an integral component of corpus design, annotation, and governance, and positions Data Care as a replicable model for building inclusive, sustainable, and culturally grounded language technologies in contexts where traditional LLM development reproduces existing power imbalances and cultural blind spots.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models are commonly trained on dominant languages like English, and their representation of low resource languages typically reflects cultural and linguistic biases present in the source language materials. Using the Serbian language as a case, this study examines the structural, historical, and sociotechnical factors shaping language technology development for low resource languages in the AI age. Drawing on semi structured interviews with ten scholars and practitioners, including linguists, digital humanists, and AI developers, it traces challenges rooted in historical destruction of Serbian textual heritage, intensified by contemporary issues that drive reductive, engineering first approaches prioritizing functionality over linguistic nuance. These include superficial transliteration, reliance on English-trained models, data bias, and dataset curation lacking cultural specificity. To address these challenges, the study proposes Data Care, a framework grounded in CARE principles (Collective Benefit, Authority to Control, Responsibility, and Ethics), that reframes bias mitigation from a post hoc technical fix to an integral component of corpus design, annotation, and governance, and positions Data Care as a replicable model for building inclusive, sustainable, and culturally grounded language technologies in contexts where traditional LLM development reproduces existing power imbalances and cultural blind spots."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T13:29:25Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    13,
                    29,
                    25,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Smiljana Antonijevic Ubois"
                    }
                ],
                "author_detail": {
                    "name": "Smiljana Antonijevic Ubois"
                },
                "author": "Smiljana Antonijevic Ubois"
            },
            {
                "id": "http://arxiv.org/abs/2512.10628v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10628v1",
                "title": "K-Track: Kalman-Enhanced Tracking for Accelerating Deep Point Trackers on Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "K-Track: Kalman-Enhanced Tracking for Accelerating Deep Point Trackers on Edge Devices"
                },
                "updated": "2025-12-11T13:26:58Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    13,
                    26,
                    58,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10628v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10628v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Point tracking in video sequences is a foundational capability for real-world computer vision applications, including robotics, autonomous systems, augmented reality, and video analysis. While recent deep learning-based trackers achieve state-of-the-art accuracy on challenging benchmarks, their reliance on per-frame GPU inference poses a major barrier to deployment on resource-constrained edge devices, where compute, power, and connectivity are limited. We introduce K-Track (Kalman-enhanced Tracking), a general-purpose, tracker-agnostic acceleration framework designed to bridge this deployment gap. K-Track reduces inference cost by combining sparse deep learning keyframe updates with lightweight Kalman filtering for intermediate frame prediction, using principled Bayesian uncertainty propagation to maintain temporal coherence. This hybrid strategy enables 5-10X speedup while retaining over 85% of the original trackers' accuracy. We evaluate K-Track across multiple state-of-the-art point trackers and demonstrate real-time performance on edge platforms such as the NVIDIA Jetson Nano and RTX Titan. By preserving accuracy while dramatically lowering computational requirements, K-Track provides a practical path toward deploying high-quality point tracking in real-world, resource-limited settings, closing the gap between modern tracking algorithms and deployable vision systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Point tracking in video sequences is a foundational capability for real-world computer vision applications, including robotics, autonomous systems, augmented reality, and video analysis. While recent deep learning-based trackers achieve state-of-the-art accuracy on challenging benchmarks, their reliance on per-frame GPU inference poses a major barrier to deployment on resource-constrained edge devices, where compute, power, and connectivity are limited. We introduce K-Track (Kalman-enhanced Tracking), a general-purpose, tracker-agnostic acceleration framework designed to bridge this deployment gap. K-Track reduces inference cost by combining sparse deep learning keyframe updates with lightweight Kalman filtering for intermediate frame prediction, using principled Bayesian uncertainty propagation to maintain temporal coherence. This hybrid strategy enables 5-10X speedup while retaining over 85% of the original trackers' accuracy. We evaluate K-Track across multiple state-of-the-art point trackers and demonstrate real-time performance on edge platforms such as the NVIDIA Jetson Nano and RTX Titan. By preserving accuracy while dramatically lowering computational requirements, K-Track provides a practical path toward deploying high-quality point tracking in real-world, resource-limited settings, closing the gap between modern tracking algorithms and deployable vision systems."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T13:26:58Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    13,
                    26,
                    58,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Bishoy Galoaa"
                    },
                    {
                        "name": "Pau Closas"
                    },
                    {
                        "name": "Sarah Ostadabbas"
                    }
                ],
                "author_detail": {
                    "name": "Sarah Ostadabbas"
                },
                "author": "Sarah Ostadabbas"
            },
            {
                "id": "http://arxiv.org/abs/2512.09443v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.09443v2",
                "title": "Advancing Mathematical Research via Human-AI Interactive Theorem Proving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Mathematical Research via Human-AI Interactive Theorem Proving"
                },
                "updated": "2025-12-11T13:10:50Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    13,
                    10,
                    50,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.09443v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.09443v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We investigate how large language models can be used as research tools in scientific computing while preserving mathematical rigor. We propose a human-in-the-loop workflow for interactive theorem proving and discovery with LLMs. Human experts retain control over problem formulation and admissible assumptions, while the model searches for proofs or contradictions, proposes candidate properties and theorems, and helps construct structures and parameters that satisfy explicit constraints, supported by numerical experiments and simple verification checks. Experts treat these outputs as raw material, further refine them, and organize the results into precise statements and rigorous proofs. We instantiate this workflow in a case study on the connection between manifold optimization and Grover's quantum search algorithm, where the pipeline helps identify invariant subspaces, explore Grover-compatible retractions, and obtain convergence guarantees for the retraction-based gradient method. The framework provides a practical template for integrating large language models into frontier mathematical research, enabling faster exploration of proof space and algorithm design while maintaining transparent reasoning responsibilities. Although illustrated on manifold optimization problems in quantum computing, the principles extend to other core areas of scientific computing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate how large language models can be used as research tools in scientific computing while preserving mathematical rigor. We propose a human-in-the-loop workflow for interactive theorem proving and discovery with LLMs. Human experts retain control over problem formulation and admissible assumptions, while the model searches for proofs or contradictions, proposes candidate properties and theorems, and helps construct structures and parameters that satisfy explicit constraints, supported by numerical experiments and simple verification checks. Experts treat these outputs as raw material, further refine them, and organize the results into precise statements and rigorous proofs. We instantiate this workflow in a case study on the connection between manifold optimization and Grover's quantum search algorithm, where the pipeline helps identify invariant subspaces, explore Grover-compatible retractions, and obtain convergence guarantees for the retraction-based gradient method. The framework provides a practical template for integrating large language models into frontier mathematical research, enabling faster exploration of proof space and algorithm design while maintaining transparent reasoning responsibilities. Although illustrated on manifold optimization problems in quantum computing, the principles extend to other core areas of scientific computing."
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-10T09:16:27Z",
                "published_parsed": [
                    2025,
                    12,
                    10,
                    9,
                    16,
                    27,
                    2,
                    344,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC"
                },
                "authors": [
                    {
                        "name": "Chenyi Li"
                    },
                    {
                        "name": "Zhijian Lai"
                    },
                    {
                        "name": "Dong An"
                    },
                    {
                        "name": "Jiang Hu"
                    },
                    {
                        "name": "Zaiwen Wen"
                    }
                ],
                "author_detail": {
                    "name": "Zaiwen Wen"
                },
                "author": "Zaiwen Wen"
            },
            {
                "id": "http://arxiv.org/abs/2512.10611v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10611v1",
                "title": "Phythesis: Physics-Guided Evolutionary Scene Synthesis for Energy-Efficient Data Center Design via LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phythesis: Physics-Guided Evolutionary Scene Synthesis for Energy-Efficient Data Center Design via LLMs"
                },
                "updated": "2025-12-11T13:04:44Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    13,
                    4,
                    44,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10611v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10611v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Data center (DC) infrastructure serves as the backbone to support the escalating demand for computing capacity. Traditional design methodologies that blend human expertise with specialized simulation tools scale poorly with the increasing system complexity. Recent studies adopt generative artificial intelligence to design plausible human-centric indoor layouts. However, they do not consider the underlying physics, making them unsuitable for the DC design that sets quantifiable operational objectives and strict physical constraints. To bridge the gap, we propose Phythesis, a novel framework that synergizes large language models (LLMs) and physics-guided evolutionary optimization to automate simulation-ready (SimReady) scene synthesis for energy-efficient DC design. Phythesis employs an iterative bi-level optimization architecture, where (i) the LLM-driven optimization level generates physically plausible three-dimensional layouts and self-criticizes them to refine the scene topology, and (ii) the physics-informed optimization level identifies the optimal asset parameters and selects the best asset combination. Experiments on three generation scales show that Phythesis achieves 57.3% generation success rate increase and 11.5% power usage effectiveness (PUE) improvement, compared with the vanilla LLM-based solution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data center (DC) infrastructure serves as the backbone to support the escalating demand for computing capacity. Traditional design methodologies that blend human expertise with specialized simulation tools scale poorly with the increasing system complexity. Recent studies adopt generative artificial intelligence to design plausible human-centric indoor layouts. However, they do not consider the underlying physics, making them unsuitable for the DC design that sets quantifiable operational objectives and strict physical constraints. To bridge the gap, we propose Phythesis, a novel framework that synergizes large language models (LLMs) and physics-guided evolutionary optimization to automate simulation-ready (SimReady) scene synthesis for energy-efficient DC design. Phythesis employs an iterative bi-level optimization architecture, where (i) the LLM-driven optimization level generates physically plausible three-dimensional layouts and self-criticizes them to refine the scene topology, and (ii) the physics-informed optimization level identifies the optimal asset parameters and selects the best asset combination. Experiments on three generation scales show that Phythesis achieves 57.3% generation success rate increase and 11.5% power usage effectiveness (PUE) improvement, compared with the vanilla LLM-based solution."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T13:04:44Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    13,
                    4,
                    44,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Minghao LI"
                    },
                    {
                        "name": "Ruihang Wang"
                    },
                    {
                        "name": "Rui Tan"
                    },
                    {
                        "name": "Yonggang Wen"
                    }
                ],
                "author_detail": {
                    "name": "Yonggang Wen"
                },
                "author": "Yonggang Wen"
            },
            {
                "id": "http://arxiv.org/abs/2512.10610v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10610v1",
                "title": "Thinking While Driving: A Concurrent Framework for Real-Time, LLM-Based Adaptive Routing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thinking While Driving: A Concurrent Framework for Real-Time, LLM-Based Adaptive Routing"
                },
                "updated": "2025-12-11T13:04:37Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    13,
                    4,
                    37,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10610v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10610v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We present Thinking While Driving, a concurrent routing framework that integrates LLMs into a graph-based traffic environment. Unlike approaches that require agents to stop and deliberate, our system enables LLM-based route planning while agents are moving, significantly reducing intersection wait times. Under high traffic, agents average just 0.75 seconds of decision latency. To coordinate many agents in real-time, we implement a non-blocking asynchronous architecture using Unity coroutines and a dedicated request manager. The environment is a weighted undirected graph with live congestion metrics, updated continuously by the agents to enable shared perception. Our results show LLM-driven agents can dynamically adapt to traffic, reroute around congestion, and exhibit behaviors beyond static pathfinding, all while maintaining real-time performance. This work provides a reproducible framework for future research in adaptive routing and multi-agent cooperation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Thinking While Driving, a concurrent routing framework that integrates LLMs into a graph-based traffic environment. Unlike approaches that require agents to stop and deliberate, our system enables LLM-based route planning while agents are moving, significantly reducing intersection wait times. Under high traffic, agents average just 0.75 seconds of decision latency. To coordinate many agents in real-time, we implement a non-blocking asynchronous architecture using Unity coroutines and a dedicated request manager. The environment is a weighted undirected graph with live congestion metrics, updated continuously by the agents to enable shared perception. Our results show LLM-driven agents can dynamically adapt to traffic, reroute around congestion, and exhibit behaviors beyond static pathfinding, all while maintaining real-time performance. This work provides a reproducible framework for future research in adaptive routing and multi-agent cooperation."
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T13:04:37Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    13,
                    4,
                    37,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA"
                },
                "authors": [
                    {
                        "name": "Xiaopei Tan"
                    },
                    {
                        "name": "Muyang Fan"
                    }
                ],
                "author_detail": {
                    "name": "Muyang Fan"
                },
                "author": "Muyang Fan"
            },
            {
                "id": "http://arxiv.org/abs/2512.10608v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10608v1",
                "title": "Robust Multi-Disease Retinal Classification via Xception-Based Transfer Learning and W-Net Vessel Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Multi-Disease Retinal Classification via Xception-Based Transfer Learning and W-Net Vessel Segmentation"
                },
                "updated": "2025-12-11T13:03:03Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    13,
                    3,
                    3,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10608v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10608v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In recent years, the incidence of vision-threatening eye diseases has risen dramatically, necessitating scalable and accurate screening solutions. This paper presents a comprehensive study on deep learning architectures for the automated diagnosis of ocular conditions. To mitigate the \"black-box\" limitations of standard convolutional neural networks (CNNs), we implement a pipeline that combines deep feature extraction with interpretable image processing modules. Specifically, we focus on high-fidelity retinal vessel segmentation as an auxiliary task to guide the classification process. By grounding the model's predictions in clinically relevant morphological features, we aim to bridge the gap between algorithmic output and expert medical validation, thereby reducing false positives and improving deployment viability in clinical settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the incidence of vision-threatening eye diseases has risen dramatically, necessitating scalable and accurate screening solutions. This paper presents a comprehensive study on deep learning architectures for the automated diagnosis of ocular conditions. To mitigate the \"black-box\" limitations of standard convolutional neural networks (CNNs), we implement a pipeline that combines deep feature extraction with interpretable image processing modules. Specifically, we focus on high-fidelity retinal vessel segmentation as an auxiliary task to guide the classification process. By grounding the model's predictions in clinically relevant morphological features, we aim to bridge the gap between algorithmic output and expert medical validation, thereby reducing false positives and improving deployment viability in clinical settings."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T13:03:03Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    13,
                    3,
                    3,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Mohammad Sadegh Gholizadeh"
                    },
                    {
                        "name": "Amir Arsalan Rezapour"
                    }
                ],
                "author_detail": {
                    "name": "Amir Arsalan Rezapour"
                },
                "author": "Amir Arsalan Rezapour"
            },
            {
                "id": "http://arxiv.org/abs/2512.10605v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10605v1",
                "title": "LEO-RobotAgent: A General-purpose Robotic Agent for Language-driven Embodied Operator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LEO-RobotAgent: A General-purpose Robotic Agent for Language-driven Embodied Operator"
                },
                "updated": "2025-12-11T12:58:36Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    12,
                    58,
                    36,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10605v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10605v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We propose LEO-RobotAgent, a general-purpose language-driven intelligent agent framework for robots. Under this framework, LLMs can operate different types of robots to complete unpredictable complex tasks across various scenarios. This framework features strong generalization, robustness, and efficiency. The application-level system built around it can fully enhance bidirectional human-robot intent understanding and lower the threshold for human-robot interaction. Regarding robot task planning, the vast majority of existing studies focus on the application of large models in single-task scenarios and for single robot types. These algorithms often have complex structures and lack generalizability. Thus, the proposed LEO-RobotAgent framework is designed with a streamlined structure as much as possible, enabling large models to independently think, plan, and act within this clear framework. We provide a modular and easily registrable toolset, allowing large models to flexibly call various tools to meet different requirements. Meanwhile, the framework incorporates a human-robot interaction mechanism, enabling the algorithm to collaborate with humans like a partner. Experiments have verified that this framework can be easily adapted to mainstream robot platforms including unmanned aerial vehicles (UAVs), robotic arms, and wheeled robot, and efficiently execute a variety of carefully designed tasks with different complexity levels. Our code is available at https://github.com/LegendLeoChen/LEO-RobotAgent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose LEO-RobotAgent, a general-purpose language-driven intelligent agent framework for robots. Under this framework, LLMs can operate different types of robots to complete unpredictable complex tasks across various scenarios. This framework features strong generalization, robustness, and efficiency. The application-level system built around it can fully enhance bidirectional human-robot intent understanding and lower the threshold for human-robot interaction. Regarding robot task planning, the vast majority of existing studies focus on the application of large models in single-task scenarios and for single robot types. These algorithms often have complex structures and lack generalizability. Thus, the proposed LEO-RobotAgent framework is designed with a streamlined structure as much as possible, enabling large models to independently think, plan, and act within this clear framework. We provide a modular and easily registrable toolset, allowing large models to flexibly call various tools to meet different requirements. Meanwhile, the framework incorporates a human-robot interaction mechanism, enabling the algorithm to collaborate with humans like a partner. Experiments have verified that this framework can be easily adapted to mainstream robot platforms including unmanned aerial vehicles (UAVs), robotic arms, and wheeled robot, and efficiently execute a variety of carefully designed tasks with different complexity levels. Our code is available at https://github.com/LegendLeoChen/LEO-RobotAgent."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T12:58:36Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    12,
                    58,
                    36,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Lihuang Chen"
                    },
                    {
                        "name": "Xiangyu Luo"
                    },
                    {
                        "name": "Jun Meng"
                    }
                ],
                "author_detail": {
                    "name": "Jun Meng"
                },
                "author": "Jun Meng"
            },
            {
                "id": "http://arxiv.org/abs/2512.10602v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10602v1",
                "title": "Uncertainty-Preserving QBNNs: Multi-Level Quantization of SVI-Based Bayesian Neural Networks for Image Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty-Preserving QBNNs: Multi-Level Quantization of SVI-Based Bayesian Neural Networks for Image Classification"
                },
                "updated": "2025-12-11T12:51:42Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    12,
                    51,
                    42,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10602v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10602v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Bayesian Neural Networks (BNNs) provide principled uncertainty quantification but suffer from substantial computational and memory overhead compared to deterministic networks. While quantization techniques have successfully reduced resource requirements in standard deep learning models, their application to probabilistic models remains largely unexplored. We introduce a systematic multi-level quantization framework for Stochastic Variational Inference based BNNs that distinguishes between three quantization strategies: Variational Parameter Quantization (VPQ), Sampled Parameter Quantization (SPQ), and Joint Quantization (JQ). Our logarithmic quantization for variance parameters, and specialized activation functions to preserve the distributional structure are essential for calibrated uncertainty estimation. Through comprehensive experiments on Dirty-MNIST, we demonstrate that BNNs can be quantized down to 4-bit precision while maintaining both classification accuracy and uncertainty disentanglement. At 4 bits, Joint Quantization achieves up to 8x memory reduction compared to floating-point implementations with minimal degradation in epistemic and aleatoric uncertainty estimation. These results enable deployment of BNNs on resource-constrained edge devices and provide design guidelines for future analog \"Bayesian Machines\" operating at inherently low precision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Neural Networks (BNNs) provide principled uncertainty quantification but suffer from substantial computational and memory overhead compared to deterministic networks. While quantization techniques have successfully reduced resource requirements in standard deep learning models, their application to probabilistic models remains largely unexplored. We introduce a systematic multi-level quantization framework for Stochastic Variational Inference based BNNs that distinguishes between three quantization strategies: Variational Parameter Quantization (VPQ), Sampled Parameter Quantization (SPQ), and Joint Quantization (JQ). Our logarithmic quantization for variance parameters, and specialized activation functions to preserve the distributional structure are essential for calibrated uncertainty estimation. Through comprehensive experiments on Dirty-MNIST, we demonstrate that BNNs can be quantized down to 4-bit precision while maintaining both classification accuracy and uncertainty disentanglement. At 4 bits, Joint Quantization achieves up to 8x memory reduction compared to floating-point implementations with minimal degradation in epistemic and aleatoric uncertainty estimation. These results enable deployment of BNNs on resource-constrained edge devices and provide design guidelines for future analog \"Bayesian Machines\" operating at inherently low precision."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T12:51:42Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    12,
                    51,
                    42,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Hendrik Borras"
                    },
                    {
                        "name": "Yong Wu"
                    },
                    {
                        "name": "Bernhard Klein"
                    },
                    {
                        "name": "Holger Fröning"
                    }
                ],
                "author_detail": {
                    "name": "Holger Fröning"
                },
                "author": "Holger Fröning"
            },
            {
                "id": "http://arxiv.org/abs/2506.05191v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.05191v2",
                "title": "MokA: Multimodal Low-Rank Adaptation for MLLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MokA: Multimodal Low-Rank Adaptation for MLLMs"
                },
                "updated": "2025-12-11T12:37:26Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    12,
                    37,
                    26,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.05191v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.05191v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In this paper, we reveal that most current efficient multimodal fine-tuning methods are hindered by a key limitation: they are directly borrowed from LLMs, often neglecting the intrinsic differences of multimodal scenarios and even affecting the full utilization of all modalities. Inspired by our empirical observation, we argue that unimodal adaptation and cross-modal adaptation are two essential parts for the effective fine-tuning of MLLMs. From this perspective, we propose Multimodal low-rank Adaptation (MokA), a multimodal-aware efficient fine-tuning strategy that takes multimodal characteristics into consideration. It compresses unimodal information by modality-specific parameters while explicitly enhancing cross-modal interaction, ensuring both unimodal and cross-modal adaptation. Extensive experiments cover three representative multimodal scenarios (audio-visual-text, visual-text, and speech-text), and multiple LLM backbones (LLaMA2/3, Qwen2, Qwen2.5-VL, etc). Consistent improvements indicate the efficacy and versatility of the proposed method. Ablation studies and efficiency evaluation are also conducted to fully asses our method. Overall, we think MokA provides a more targeted solution for efficient adaptation of MLLMs, paving the way for further exploration. The project page is at https://gewu-lab.github.io/MokA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we reveal that most current efficient multimodal fine-tuning methods are hindered by a key limitation: they are directly borrowed from LLMs, often neglecting the intrinsic differences of multimodal scenarios and even affecting the full utilization of all modalities. Inspired by our empirical observation, we argue that unimodal adaptation and cross-modal adaptation are two essential parts for the effective fine-tuning of MLLMs. From this perspective, we propose Multimodal low-rank Adaptation (MokA), a multimodal-aware efficient fine-tuning strategy that takes multimodal characteristics into consideration. It compresses unimodal information by modality-specific parameters while explicitly enhancing cross-modal interaction, ensuring both unimodal and cross-modal adaptation. Extensive experiments cover three representative multimodal scenarios (audio-visual-text, visual-text, and speech-text), and multiple LLM backbones (LLaMA2/3, Qwen2, Qwen2.5-VL, etc). Consistent improvements indicate the efficacy and versatility of the proposed method. Ablation studies and efficiency evaluation are also conducted to fully asses our method. Overall, we think MokA provides a more targeted solution for efficient adaptation of MLLMs, paving the way for further exploration. The project page is at https://gewu-lab.github.io/MokA."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-05T16:04:08Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    16,
                    4,
                    8,
                    3,
                    156,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Yake Wei"
                    },
                    {
                        "name": "Yu Miao"
                    },
                    {
                        "name": "Dongzhan Zhou"
                    },
                    {
                        "name": "Di Hu"
                    }
                ],
                "author_detail": {
                    "name": "Di Hu"
                },
                "author": "Di Hu"
            },
            {
                "id": "http://arxiv.org/abs/2512.10576v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10576v1",
                "title": "ESS: An Offload-Centric Latent-Cache Management Architecture for DeepSeek-V3.2-Exp",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ESS: An Offload-Centric Latent-Cache Management Architecture for DeepSeek-V3.2-Exp"
                },
                "updated": "2025-12-11T12:06:00Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    12,
                    6,
                    0,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10576v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10576v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "DeepSeek-V3.2-Exp introduces a sparse attention mechanism that significantly reduces inference latency in long-context scenarios. Although the overall throughput has improved greatly, the Decode-stage of PD disaggregation remains to be a major bottleneck. This bottleneck primarily stems from the conflict between linear growth of Latent-Cache with sequence length and the limited GPU memory capacity, which constrains the feasible batch-size and thereby suppresses Decode-stage throughput.\n  To address this challenge, we propose ESS (Extended Sparse Server), an offload-centric system design tailored for DeepSeek-V3.2-Exp. ESS selectively offloads Latent-Cache to CPU memory while preserving latency-critical components on GPU. By freeing up GPU memory, ESS effectively decoupling batch-size scaling from GPU memory constraints. This design significantly improves Decode-stage throughput, thereby reducing deployment costs in real-world settings.\n  Our high-fidelity simulations show that ESS delivers 69.4\\% throughput improvement at 32K context length and up to 123\\% throughput improvement at 128K, demonstrating its effectiveness for large-context inference workloads. These results highlight ESS as a practical and scalable solution for long-context LLM serving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepSeek-V3.2-Exp introduces a sparse attention mechanism that significantly reduces inference latency in long-context scenarios. Although the overall throughput has improved greatly, the Decode-stage of PD disaggregation remains to be a major bottleneck. This bottleneck primarily stems from the conflict between linear growth of Latent-Cache with sequence length and the limited GPU memory capacity, which constrains the feasible batch-size and thereby suppresses Decode-stage throughput.\n  To address this challenge, we propose ESS (Extended Sparse Server), an offload-centric system design tailored for DeepSeek-V3.2-Exp. ESS selectively offloads Latent-Cache to CPU memory while preserving latency-critical components on GPU. By freeing up GPU memory, ESS effectively decoupling batch-size scaling from GPU memory constraints. This design significantly improves Decode-stage throughput, thereby reducing deployment costs in real-world settings.\n  Our high-fidelity simulations show that ESS delivers 69.4\\% throughput improvement at 32K context length and up to 123\\% throughput improvement at 128K, demonstrating its effectiveness for large-context inference workloads. These results highlight ESS as a practical and scalable solution for long-context LLM serving."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T12:06:00Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    12,
                    6,
                    0,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Xinhang Chen"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Jiahuan He"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Jianming Zhang"
                    },
                    {
                        "name": "Wenlong Zhou"
                    },
                    {
                        "name": "Xiao Li"
                    },
                    {
                        "name": "Pai Zeng"
                    },
                    {
                        "name": "Shiyong Li"
                    },
                    {
                        "name": "Yuanpan Qian"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Zhaogeng Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhaogeng Li"
                },
                "author": "Zhaogeng Li"
            },
            {
                "id": "http://arxiv.org/abs/2512.10575v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10575v1",
                "title": "RoleRMBench & RoleRM: Towards Reward Modeling for Profile-Based Role Play in Dialogue Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoleRMBench & RoleRM: Towards Reward Modeling for Profile-Based Role Play in Dialogue Systems"
                },
                "updated": "2025-12-11T12:04:46Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    12,
                    4,
                    46,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10575v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10575v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Reward modeling has become a cornerstone of aligning large language models (LLMs) with human preferences. Yet, when extended to subjective and open-ended domains such as role play, existing reward models exhibit severe degradation, struggling to capture nuanced and persona-grounded human judgments. To address this gap, we introduce RoleRMBench, the first systematic benchmark for reward modeling in role-playing dialogue, covering seven fine-grained capabilities from narrative management to role consistency and engagement. Evaluation on RoleRMBench reveals large and consistent gaps between general-purpose reward models and human judgment, particularly in narrative and stylistic dimensions. We further propose RoleRM, a reward model trained with Continuous Implicit Preferences (CIP), which reformulates subjective evaluation as continuous consistent pairwise supervision under multiple structuring strategies. Comprehensive experiments show that RoleRM surpasses strong open- and closed-source reward models by over 24% on average, demonstrating substantial gains in narrative coherence and stylistic fidelity. Our findings highlight the importance of continuous preference representation and annotation consistency, establishing a foundation for subjective alignment in human-centered dialogue systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward modeling has become a cornerstone of aligning large language models (LLMs) with human preferences. Yet, when extended to subjective and open-ended domains such as role play, existing reward models exhibit severe degradation, struggling to capture nuanced and persona-grounded human judgments. To address this gap, we introduce RoleRMBench, the first systematic benchmark for reward modeling in role-playing dialogue, covering seven fine-grained capabilities from narrative management to role consistency and engagement. Evaluation on RoleRMBench reveals large and consistent gaps between general-purpose reward models and human judgment, particularly in narrative and stylistic dimensions. We further propose RoleRM, a reward model trained with Continuous Implicit Preferences (CIP), which reformulates subjective evaluation as continuous consistent pairwise supervision under multiple structuring strategies. Comprehensive experiments show that RoleRM surpasses strong open- and closed-source reward models by over 24% on average, demonstrating substantial gains in narrative coherence and stylistic fidelity. Our findings highlight the importance of continuous preference representation and annotation consistency, establishing a foundation for subjective alignment in human-centered dialogue systems."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T12:04:46Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    12,
                    4,
                    46,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Hang Ding"
                    },
                    {
                        "name": "Qiming Feng"
                    },
                    {
                        "name": "Dongqi Liu"
                    },
                    {
                        "name": "Qi Zhao"
                    },
                    {
                        "name": "Tao Yao"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Dongsheng Chen"
                    },
                    {
                        "name": "Jian Li"
                    },
                    {
                        "name": "Zhenye Gan"
                    },
                    {
                        "name": "Jiangning Zhang"
                    },
                    {
                        "name": "Chengjie Wang"
                    },
                    {
                        "name": "Yabiao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yabiao Wang"
                },
                "author": "Yabiao Wang"
            },
            {
                "id": "http://arxiv.org/abs/2512.05647v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.05647v2",
                "title": "A Greek Government Decisions Dataset for Public-Sector Analysis and Insight",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Greek Government Decisions Dataset for Public-Sector Analysis and Insight"
                },
                "updated": "2025-12-11T11:58:25Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    11,
                    58,
                    25,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.05647v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.05647v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We introduce an open, machine-readable corpus of Greek government decisions sourced from the national transparency platform Diavgeia. The resource comprises 1 million decisions, featuring and high-quality raw text extracted from PDFs. It is released with raw extracted text in Markdown format, alongside a fully reproducible extraction pipeline. Beyond the core dataset, we conduct qualitative analyses to explore boilerplate patterns and design a retrieval-augmented generation (RAG) task by formulating a set of representative questions, creating high-quality answers, and evaluating a baseline RAG system on its ability to retrieve and reason over public decisions. This evaluation demonstrates the potential of large-scale public-sector corpora to support advanced information access and transparency through structured retrieval and reasoning over governmental documents, and highlights how such a RAG pipeline could simulate a chat-based assistant capable of interactively answering questions about public decisions. Due to its scale, quality, and domain coverage, the corpus can also serve as high-value pre-training or fine-tuning material for new Language Models (LMs) and Large Language Models (LLMs) respectively, including specialized models for legal and governmental domains, and as a foundation for novel approaches in domain adaptation, knowledge-grounded generation, and explainable AI. Finally, we discuss limitations, outline future directions, and make both the data and the code accessible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce an open, machine-readable corpus of Greek government decisions sourced from the national transparency platform Diavgeia. The resource comprises 1 million decisions, featuring and high-quality raw text extracted from PDFs. It is released with raw extracted text in Markdown format, alongside a fully reproducible extraction pipeline. Beyond the core dataset, we conduct qualitative analyses to explore boilerplate patterns and design a retrieval-augmented generation (RAG) task by formulating a set of representative questions, creating high-quality answers, and evaluating a baseline RAG system on its ability to retrieve and reason over public decisions. This evaluation demonstrates the potential of large-scale public-sector corpora to support advanced information access and transparency through structured retrieval and reasoning over governmental documents, and highlights how such a RAG pipeline could simulate a chat-based assistant capable of interactively answering questions about public decisions. Due to its scale, quality, and domain coverage, the corpus can also serve as high-value pre-training or fine-tuning material for new Language Models (LMs) and Large Language Models (LLMs) respectively, including specialized models for legal and governmental domains, and as a foundation for novel approaches in domain adaptation, knowledge-grounded generation, and explainable AI. Finally, we discuss limitations, outline future directions, and make both the data and the code accessible."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-05T11:47:33Z",
                "published_parsed": [
                    2025,
                    12,
                    5,
                    11,
                    47,
                    33,
                    4,
                    339,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Giorgos Antoniou"
                    },
                    {
                        "name": "Giorgos Filandrianos"
                    },
                    {
                        "name": "Aggelos Vlachos"
                    },
                    {
                        "name": "Giorgos Stamou"
                    },
                    {
                        "name": "Lampros Kollimenos"
                    },
                    {
                        "name": "Konstantinos Skianis"
                    },
                    {
                        "name": "Michalis Vazirgiannis"
                    }
                ],
                "author_detail": {
                    "name": "Michalis Vazirgiannis"
                },
                "author": "Michalis Vazirgiannis"
            },
            {
                "id": "http://arxiv.org/abs/2512.02899v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.02899v2",
                "title": "Glance: Accelerating Diffusion Models with 1 Sample",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Glance: Accelerating Diffusion Models with 1 Sample"
                },
                "updated": "2025-12-11T11:53:22Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    11,
                    53,
                    22,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.02899v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.02899v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion models have achieved remarkable success in image generation, yet their deployment remains constrained by the heavy computational cost and the need for numerous inference steps. Previous efforts on fewer-step distillation attempt to skip redundant steps by training compact student models, yet they often suffer from heavy retraining costs and degraded generalization. In this work, we take a different perspective: we accelerate smartly, not evenly, applying smaller speedups to early semantic stages and larger ones to later redundant phases. We instantiate this phase-aware strategy with two experts that specialize in slow and fast denoising phases. Surprisingly, instead of investing massive effort in retraining student models, we find that simply equipping the base model with lightweight LoRA adapters achieves both efficient acceleration and strong generalization. We refer to these two adapters as Slow-LoRA and Fast-LoRA. Through extensive experiments, our method achieves up to 5 acceleration over the base model while maintaining comparable visual quality across diverse benchmarks. Remarkably, the LoRA experts are trained with only 1 samples on a single V100 within one hour, yet the resulting models generalize strongly on unseen prompts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have achieved remarkable success in image generation, yet their deployment remains constrained by the heavy computational cost and the need for numerous inference steps. Previous efforts on fewer-step distillation attempt to skip redundant steps by training compact student models, yet they often suffer from heavy retraining costs and degraded generalization. In this work, we take a different perspective: we accelerate smartly, not evenly, applying smaller speedups to early semantic stages and larger ones to later redundant phases. We instantiate this phase-aware strategy with two experts that specialize in slow and fast denoising phases. Surprisingly, instead of investing massive effort in retraining student models, we find that simply equipping the base model with lightweight LoRA adapters achieves both efficient acceleration and strong generalization. We refer to these two adapters as Slow-LoRA and Fast-LoRA. Through extensive experiments, our method achieves up to 5 acceleration over the base model while maintaining comparable visual quality across diverse benchmarks. Remarkably, the LoRA experts are trained with only 1 samples on a single V100 within one hour, yet the resulting models generalize strongly on unseen prompts."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-02T16:05:21Z",
                "published_parsed": [
                    2025,
                    12,
                    2,
                    16,
                    5,
                    21,
                    1,
                    336,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Zhuobai Dong"
                    },
                    {
                        "name": "Rui Zhao"
                    },
                    {
                        "name": "Songjie Wu"
                    },
                    {
                        "name": "Junchao Yi"
                    },
                    {
                        "name": "Linjie Li"
                    },
                    {
                        "name": "Zhengyuan Yang"
                    },
                    {
                        "name": "Lijuan Wang"
                    },
                    {
                        "name": "Alex Jinpeng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Alex Jinpeng Wang"
                },
                "author": "Alex Jinpeng Wang"
            },
            {
                "id": "http://arxiv.org/abs/2512.10563v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10563v1",
                "title": "NormCode: A Semi-Formal Language for Context-Isolated AI Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NormCode: A Semi-Formal Language for Context-Isolated AI Planning"
                },
                "updated": "2025-12-11T11:50:50Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    11,
                    50,
                    50,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10563v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10563v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Multistep workflows that chain large language model (LLM) calls suffer from context pollution: as information accumulates across steps, models hallucinate, confuse intermediate outputs, and lose track of task constraints. We present NormCode, a semiformal language for constructing plans of inferences, structured decompositions where each step operates in data isolation and receives only explicitly passed inputs, which eliminates crossstep contamination by design. NormCode enforces a strict separation between semantic operations (LLMdriven reasoning, nondeterministic) and syntactic operations (deterministic data restructuring), enabling precise cost and reliability tracing. The language exists in three isomorphic formats: .ncds for human authoring, .ncd for machine execution, and .ncn for human verification, supporting progressive formalization from sketch to production. We validate NormCode through two demonstrations: (1) a base X addition algorithm achieving 100 percent accuracy on arbitrary length inputs, and (2) self hosted execution of NormCode's own five phase compiler pipeline. The working orchestrator provides dependency driven scheduling, SQLite backed checkpointing, and loop management, making AI workflows auditable by design and addressing a critical need for transparency in high stakes domains such as legal reasoning, medical decision making, and financial analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multistep workflows that chain large language model (LLM) calls suffer from context pollution: as information accumulates across steps, models hallucinate, confuse intermediate outputs, and lose track of task constraints. We present NormCode, a semiformal language for constructing plans of inferences, structured decompositions where each step operates in data isolation and receives only explicitly passed inputs, which eliminates crossstep contamination by design. NormCode enforces a strict separation between semantic operations (LLMdriven reasoning, nondeterministic) and syntactic operations (deterministic data restructuring), enabling precise cost and reliability tracing. The language exists in three isomorphic formats: .ncds for human authoring, .ncd for machine execution, and .ncn for human verification, supporting progressive formalization from sketch to production. We validate NormCode through two demonstrations: (1) a base X addition algorithm achieving 100 percent accuracy on arbitrary length inputs, and (2) self hosted execution of NormCode's own five phase compiler pipeline. The working orchestrator provides dependency driven scheduling, SQLite backed checkpointing, and loop management, making AI workflows auditable by design and addressing a critical need for transparency in high stakes domains such as legal reasoning, medical decision making, and financial analysis."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T11:50:50Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    11,
                    50,
                    50,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Xin Guan"
                    }
                ],
                "author_detail": {
                    "name": "Xin Guan"
                },
                "author": "Xin Guan"
            },
            {
                "id": "http://arxiv.org/abs/2512.10561v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10561v1",
                "title": "Causal Reasoning Favors Encoders: On The Limits of Decoder-Only Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal Reasoning Favors Encoders: On The Limits of Decoder-Only Models"
                },
                "updated": "2025-12-11T11:46:48Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    11,
                    46,
                    48,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10561v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10561v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In context learning (ICL) underpins recent advances in large language models (LLMs), although its role and performance in causal reasoning remains unclear. Causal reasoning demands multihop composition and strict conjunctive control, and reliance on spurious lexical relations of the input could provide misleading results. We hypothesize that, due to their ability to project the input into a latent space, encoder and encoder decoder architectures are better suited for said multihop conjunctive reasoning versus decoder only models. To do this, we compare fine-tuned versions of all the aforementioned architectures with zero and few shot ICL in both natural language and non natural language scenarios. We find that ICL alone is insufficient for reliable causal reasoning, often overfocusing on irrelevant input features. In particular, decoder only models are noticeably brittle to distributional shifts, while finetuned encoder and encoder decoder models can generalize more robustly across our tests, including the non natural language split. Both architectures are only matched or surpassed by decoder only architectures at large scales. We conclude by noting that for cost effective, short horizon robust causal reasoning, encoder or encoder decoder architectures with targeted finetuning are preferable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In context learning (ICL) underpins recent advances in large language models (LLMs), although its role and performance in causal reasoning remains unclear. Causal reasoning demands multihop composition and strict conjunctive control, and reliance on spurious lexical relations of the input could provide misleading results. We hypothesize that, due to their ability to project the input into a latent space, encoder and encoder decoder architectures are better suited for said multihop conjunctive reasoning versus decoder only models. To do this, we compare fine-tuned versions of all the aforementioned architectures with zero and few shot ICL in both natural language and non natural language scenarios. We find that ICL alone is insufficient for reliable causal reasoning, often overfocusing on irrelevant input features. In particular, decoder only models are noticeably brittle to distributional shifts, while finetuned encoder and encoder decoder models can generalize more robustly across our tests, including the non natural language split. Both architectures are only matched or surpassed by decoder only architectures at large scales. We conclude by noting that for cost effective, short horizon robust causal reasoning, encoder or encoder decoder architectures with targeted finetuning are preferable."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T11:46:48Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    11,
                    46,
                    48,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Amartya Roy"
                    },
                    {
                        "name": "Elamparithy M"
                    },
                    {
                        "name": "Kripabandhu Ghosh"
                    },
                    {
                        "name": "Ponnurangam Kumaraguru"
                    },
                    {
                        "name": "Adrian de Wynter"
                    }
                ],
                "author_detail": {
                    "name": "Adrian de Wynter"
                },
                "author": "Adrian de Wynter"
            },
            {
                "id": "http://arxiv.org/abs/2512.09543v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.09543v2",
                "title": "SWEnergy: An Empirical Study on Energy Efficiency in Agentic Issue Resolution Frameworks with SLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SWEnergy: An Empirical Study on Energy Efficiency in Agentic Issue Resolution Frameworks with SLMs"
                },
                "updated": "2025-12-11T11:33:34Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    11,
                    33,
                    34,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.09543v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.09543v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Context. LLM-based autonomous agents in software engineering rely on large, proprietary models, limiting local deployment. This has spurred interest in Small Language Models (SLMs), but their practical effectiveness and efficiency within complex agentic frameworks for automated issue resolution remain poorly understood.\n  Goal. We investigate the performance, energy efficiency, and resource consumption of four leading agentic issue resolution frameworks when deliberately constrained to using SLMs. We aim to assess the viability of these systems for this task in resource-limited settings and characterize the resulting trade-offs.\n  Method. We conduct a controlled evaluation of four leading agentic frameworks (SWE-Agent, OpenHands, Mini SWE Agent, AutoCodeRover) using two SLMs (Gemma-3 4B, Qwen-3 1.7B) on the SWE-bench Verified Mini benchmark. On fixed hardware, we measure energy, duration, token usage, and memory over 150 runs per configuration.\n  Results. We find that framework architecture is the primary driver of energy consumption. The most energy-intensive framework, AutoCodeRover (Gemma), consumed 9.4x more energy on average than the least energy-intensive, OpenHands (Gemma). However, this energy is largely wasted. Task resolution rates were near-zero, demonstrating that current frameworks, when paired with SLMs, consume significant energy on unproductive reasoning loops. The SLM's limited reasoning was the bottleneck for success, but the framework's design was the bottleneck for efficiency.\n  Conclusions. Current agentic frameworks, designed for powerful LLMs, fail to operate efficiently with SLMs. We find that framework architecture is the primary driver of energy consumption, but this energy is largely wasted due to the SLMs' limited reasoning. Viable low-energy solutions require shifting from passive orchestration to architectures that actively manage SLM weaknesses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context. LLM-based autonomous agents in software engineering rely on large, proprietary models, limiting local deployment. This has spurred interest in Small Language Models (SLMs), but their practical effectiveness and efficiency within complex agentic frameworks for automated issue resolution remain poorly understood.\n  Goal. We investigate the performance, energy efficiency, and resource consumption of four leading agentic issue resolution frameworks when deliberately constrained to using SLMs. We aim to assess the viability of these systems for this task in resource-limited settings and characterize the resulting trade-offs.\n  Method. We conduct a controlled evaluation of four leading agentic frameworks (SWE-Agent, OpenHands, Mini SWE Agent, AutoCodeRover) using two SLMs (Gemma-3 4B, Qwen-3 1.7B) on the SWE-bench Verified Mini benchmark. On fixed hardware, we measure energy, duration, token usage, and memory over 150 runs per configuration.\n  Results. We find that framework architecture is the primary driver of energy consumption. The most energy-intensive framework, AutoCodeRover (Gemma), consumed 9.4x more energy on average than the least energy-intensive, OpenHands (Gemma). However, this energy is largely wasted. Task resolution rates were near-zero, demonstrating that current frameworks, when paired with SLMs, consume significant energy on unproductive reasoning loops. The SLM's limited reasoning was the bottleneck for success, but the framework's design was the bottleneck for efficiency.\n  Conclusions. Current agentic frameworks, designed for powerful LLMs, fail to operate efficiently with SLMs. We find that framework architecture is the primary driver of energy consumption, but this energy is largely wasted due to the SLMs' limited reasoning. Viable low-energy solutions require shifting from passive orchestration to architectures that actively manage SLM weaknesses."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-10T11:28:48Z",
                "published_parsed": [
                    2025,
                    12,
                    10,
                    11,
                    28,
                    48,
                    2,
                    344,
                    0
                ],
                "arxiv_comment": "8 pages, 5 figures, 1 table. Accepted to AGENT 2026 (ICSE 2026 workshop)",
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Arihant Tripathy"
                    },
                    {
                        "name": "Ch Pavan Harshit"
                    },
                    {
                        "name": "Karthik Vaidhyanathan"
                    }
                ],
                "author_detail": {
                    "name": "Karthik Vaidhyanathan"
                },
                "author": "Karthik Vaidhyanathan"
            },
            {
                "id": "http://arxiv.org/abs/2512.10551v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10551v1",
                "title": "LLM-Auction: Generative Auction towards LLM-Native Advertising",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Auction: Generative Auction towards LLM-Native Advertising"
                },
                "updated": "2025-12-11T11:31:20Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    11,
                    31,
                    20,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10551v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10551v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The rapid advancement of large language models (LLMs) necessitates novel monetization strategies, among which LLM-native advertising has emerged as a promising paradigm by naturally integrating advertisement within LLM-generated responses. However, this paradigm fundamentally shifts the auction object from discrete ad slots to the distribution over LLM outputs, posing new challenges for designing auction mechanisms. Existing mechanisms for LLM-native advertising adopt frameworks that decouple auction and generation, which either ignore externalities or require multiple LLM inferences for ad allocation, rendering them impractical for industrial scenarios. To address these challenges, we propose LLM-Auction, which to the best of our knowledge is the first learning-based generative auction mechanism that integrates auction and LLM generation for LLM-native advertising. By formulating the allocation optimization as a preference alignment problem between LLM outputs and the mechanism's objective which reflects both advertisers' expected value and user experience, we introduce Iterative Reward-Preference Optimization (IRPO) algorithm that alternately optimizes the reward model and the LLM. This approach enables the LLM to inherently model allocation externalities without any extra inference cost. We further identify the allocation monotonicity and continuity of LLM-Auction, which allows us to prove that a simple first-price payment rule exhibits favorable incentive properties. Additionally, we design an LLM-as-a-judge simulation environment to facilitate large-scale data construction and enable comprehensive quantitative evaluation of the mechanism's performance. Extensive quantitative and qualitative experiments demonstrate that LLM-Auction significantly outperforms existing baselines in allocation efficiency, while achieving the desired mechanism properties.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) necessitates novel monetization strategies, among which LLM-native advertising has emerged as a promising paradigm by naturally integrating advertisement within LLM-generated responses. However, this paradigm fundamentally shifts the auction object from discrete ad slots to the distribution over LLM outputs, posing new challenges for designing auction mechanisms. Existing mechanisms for LLM-native advertising adopt frameworks that decouple auction and generation, which either ignore externalities or require multiple LLM inferences for ad allocation, rendering them impractical for industrial scenarios. To address these challenges, we propose LLM-Auction, which to the best of our knowledge is the first learning-based generative auction mechanism that integrates auction and LLM generation for LLM-native advertising. By formulating the allocation optimization as a preference alignment problem between LLM outputs and the mechanism's objective which reflects both advertisers' expected value and user experience, we introduce Iterative Reward-Preference Optimization (IRPO) algorithm that alternately optimizes the reward model and the LLM. This approach enables the LLM to inherently model allocation externalities without any extra inference cost. We further identify the allocation monotonicity and continuity of LLM-Auction, which allows us to prove that a simple first-price payment rule exhibits favorable incentive properties. Additionally, we design an LLM-as-a-judge simulation environment to facilitate large-scale data construction and enable comprehensive quantitative evaluation of the mechanism's performance. Extensive quantitative and qualitative experiments demonstrate that LLM-Auction significantly outperforms existing baselines in allocation efficiency, while achieving the desired mechanism properties."
                },
                "tags": [
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T11:31:20Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    11,
                    31,
                    20,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.GT"
                },
                "authors": [
                    {
                        "name": "Chujie Zhao"
                    },
                    {
                        "name": "Qun Hu"
                    },
                    {
                        "name": "Shiping Song"
                    },
                    {
                        "name": "Dagui Chen"
                    },
                    {
                        "name": "Han Zhu"
                    },
                    {
                        "name": "Jian Xu"
                    },
                    {
                        "name": "Bo Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zheng"
                },
                "author": "Bo Zheng"
            },
            {
                "id": "http://arxiv.org/abs/2512.10547v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10547v1",
                "title": "Unlocking the Address Book: Dissecting the Sparse Semantic Structure of LLM Key-Value Caches via Sparse Autoencoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking the Address Book: Dissecting the Sparse Semantic Structure of LLM Key-Value Caches via Sparse Autoencoders"
                },
                "updated": "2025-12-11T11:23:50Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    11,
                    23,
                    50,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10547v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10547v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The Key-Value (KV) cache is the primary memory bottleneck in long-context Large Language Models, yet it is typically treated as an opaque numerical tensor. In this work, we propose \\textbf{STA-Attention}, a framework that utilizes Top-K Sparse Autoencoders (SAEs) to decompose the KV cache into interpretable ``semantic atoms.'' Unlike standard $L_1$-regularized SAEs, our Top-K approach eliminates shrinkage bias, preserving the precise dot-product geometry required for attention. Our analysis uncovers a fundamental \\textbf{Key-Value Asymmetry}: while Key vectors serve as highly sparse routers dominated by a ``Semantic Elbow,'' deep Value vectors carry dense content payloads requiring a larger budget. Based on this structure, we introduce a Dual-Budget Strategy that selectively preserves the most informative semantic components while filtering representational noise. Experiments on Yi-6B, Mistral-7B, Qwen2.5-32B, and others show that our semantic reconstructions maintain perplexity and zero-shot performance comparable to the original models, effectively bridging the gap between mechanistic interpretability and faithful attention modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Key-Value (KV) cache is the primary memory bottleneck in long-context Large Language Models, yet it is typically treated as an opaque numerical tensor. In this work, we propose \\textbf{STA-Attention}, a framework that utilizes Top-K Sparse Autoencoders (SAEs) to decompose the KV cache into interpretable ``semantic atoms.'' Unlike standard $L_1$-regularized SAEs, our Top-K approach eliminates shrinkage bias, preserving the precise dot-product geometry required for attention. Our analysis uncovers a fundamental \\textbf{Key-Value Asymmetry}: while Key vectors serve as highly sparse routers dominated by a ``Semantic Elbow,'' deep Value vectors carry dense content payloads requiring a larger budget. Based on this structure, we introduce a Dual-Budget Strategy that selectively preserves the most informative semantic components while filtering representational noise. Experiments on Yi-6B, Mistral-7B, Qwen2.5-32B, and others show that our semantic reconstructions maintain perplexity and zero-shot performance comparable to the original models, effectively bridging the gap between mechanistic interpretability and faithful attention modeling."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T11:23:50Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    11,
                    23,
                    50,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Qingsen Ma"
                    },
                    {
                        "name": "Dianyun Wang"
                    },
                    {
                        "name": "Jiaming Lyu"
                    },
                    {
                        "name": "Yaoye Wang"
                    },
                    {
                        "name": "Lechen Ning"
                    },
                    {
                        "name": "Sujie Zhu"
                    },
                    {
                        "name": "Zhenbo Xu"
                    },
                    {
                        "name": "Liuyu Xiang"
                    },
                    {
                        "name": "Huining Li"
                    },
                    {
                        "name": "Huijia Wu"
                    },
                    {
                        "name": "Zhaofeng He"
                    }
                ],
                "author_detail": {
                    "name": "Zhaofeng He"
                },
                "author": "Zhaofeng He"
            },
            {
                "id": "http://arxiv.org/abs/2512.10545v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10545v1",
                "title": "XDoGE: Multilingual Data Reweighting to Enhance Language Inclusivity in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XDoGE: Multilingual Data Reweighting to Enhance Language Inclusivity in LLMs"
                },
                "updated": "2025-12-11T11:22:53Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    11,
                    22,
                    53,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10545v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10545v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Current large language models (LLMs) are trained on massive amounts of text data, primarily from a few dominant languages. Studies suggest that this over-reliance on high-resource languages, such as English, hampers LLM performance in mid- and low-resource languages. To mitigate this problem, we propose to (i) optimize the language distribution by training a small proxy model within a domain-reweighing DoGE algorithm that we extend to XDoGE for a multilingual setup, and (ii) rescale the data and train a full-size model with the established language weights either from scratch or within a continual pre-training phase (CPT). We target six languages possessing a variety of geographic and intra- and inter-language-family relations, namely, English and Spanish (high-resource), Portuguese and Catalan (mid-resource), Galician and Basque (low-resource). We experiment with Salamandra-2b, which is a promising model for these languages. We investigate the effects of substantial data repetition on minor languages and under-sampling on dominant languages using the IberoBench framework for quantitative evaluation. Finally, we release a new promising IberianLLM-7B-Instruct model centering on Iberian languages and English that we pretrained from scratch and further improved using CPT with the XDoGE weights.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current large language models (LLMs) are trained on massive amounts of text data, primarily from a few dominant languages. Studies suggest that this over-reliance on high-resource languages, such as English, hampers LLM performance in mid- and low-resource languages. To mitigate this problem, we propose to (i) optimize the language distribution by training a small proxy model within a domain-reweighing DoGE algorithm that we extend to XDoGE for a multilingual setup, and (ii) rescale the data and train a full-size model with the established language weights either from scratch or within a continual pre-training phase (CPT). We target six languages possessing a variety of geographic and intra- and inter-language-family relations, namely, English and Spanish (high-resource), Portuguese and Catalan (mid-resource), Galician and Basque (low-resource). We experiment with Salamandra-2b, which is a promising model for these languages. We investigate the effects of substantial data repetition on minor languages and under-sampling on dominant languages using the IberoBench framework for quantitative evaluation. Finally, we release a new promising IberianLLM-7B-Instruct model centering on Iberian languages and English that we pretrained from scratch and further improved using CPT with the XDoGE weights."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T11:22:53Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    11,
                    22,
                    53,
                    3,
                    345,
                    0
                ],
                "arxiv_comment": "Accepted and presented at the LLMs4All workshop at the IEEE BigData 2025 Conference, Macau - December 8-11, 2025",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Iñaki Lacunza"
                    },
                    {
                        "name": "José Javier Saiz"
                    },
                    {
                        "name": "Alexander Shvets"
                    },
                    {
                        "name": "Aitor Gonzalez-Agirre"
                    },
                    {
                        "name": "Marta Villegas"
                    }
                ],
                "author_detail": {
                    "name": "Marta Villegas"
                },
                "author": "Marta Villegas"
            },
            {
                "id": "http://arxiv.org/abs/2505.10537v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.10537v3",
                "title": "LibIQ: Toward Real-Time Spectrum Classification in O-RAN dApps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LibIQ: Toward Real-Time Spectrum Classification in O-RAN dApps"
                },
                "updated": "2025-12-11T11:18:01Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    11,
                    18,
                    1,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.10537v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.10537v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1109/MedComNet65822.2025.11100289",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "The O-RAN architecture is transforming cellular networks by adopting RAN softwarization and disaggregation concepts to enable data-driven monitoring and control of the network. Such management is enabled by RICs, which facilitate near-real-time and non-real-time network control through xApps and rApps. However, they face limitations, including latency overhead in data exchange between the RAN and RIC, restricting real-time monitoring, and the inability to access user plain data due to privacy and security constraints, hindering use cases like beamforming and spectrum classification. In this paper, we leverage the dApps concept to enable real-time RF spectrum classification with LibIQ, a novel library for RF signals that facilitates efficient spectrum monitoring and signal classification by providing functionalities to read I/Q samples as time-series, create datasets and visualize time-series data through plots and spectrograms. Thanks to LibIQ, I/Q samples can be efficiently processed to detect external RF signals, which are subsequently classified using a CNN inside the library. To achieve accurate spectrum analysis, we created an extensive dataset of time-series-based I/Q samples, representing distinct signal types captured using a custom dApp running on a 5G deployment over the Colosseum network emulator and an OTA testbed. We evaluate our model by deploying LibIQ in heterogeneous scenarios with varying center frequencies, time windows, and external RF signals. In real-time analysis, the model classifies the processed I/Q samples, achieving an average accuracy of approximately 97.8% in identifying signal types across all scenarios. We pledge to release both LibIQ and the dataset created as a publicly available framework upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The O-RAN architecture is transforming cellular networks by adopting RAN softwarization and disaggregation concepts to enable data-driven monitoring and control of the network. Such management is enabled by RICs, which facilitate near-real-time and non-real-time network control through xApps and rApps. However, they face limitations, including latency overhead in data exchange between the RAN and RIC, restricting real-time monitoring, and the inability to access user plain data due to privacy and security constraints, hindering use cases like beamforming and spectrum classification. In this paper, we leverage the dApps concept to enable real-time RF spectrum classification with LibIQ, a novel library for RF signals that facilitates efficient spectrum monitoring and signal classification by providing functionalities to read I/Q samples as time-series, create datasets and visualize time-series data through plots and spectrograms. Thanks to LibIQ, I/Q samples can be efficiently processed to detect external RF signals, which are subsequently classified using a CNN inside the library. To achieve accurate spectrum analysis, we created an extensive dataset of time-series-based I/Q samples, representing distinct signal types captured using a custom dApp running on a 5G deployment over the Colosseum network emulator and an OTA testbed. We evaluate our model by deploying LibIQ in heterogeneous scenarios with varying center frequencies, time windows, and external RF signals. In real-time analysis, the model classifies the processed I/Q samples, achieving an average accuracy of approximately 97.8% in identifying signal types across all scenarios. We pledge to release both LibIQ and the dataset created as a publicly available framework upon acceptance."
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-15T17:47:30Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    17,
                    47,
                    30,
                    3,
                    135,
                    0
                ],
                "arxiv_comment": "6 pages, 5 figures, 2 tables",
                "arxiv_primary_category": {
                    "term": "cs.NI"
                },
                "arxiv_journal_ref": "2025 23rd Mediterranean Communication and Computer Networking Conference (MedComNet), Cagliari, Italy, 2025, pp. 1-6",
                "authors": [
                    {
                        "name": "Filippo Olimpieri"
                    },
                    {
                        "name": "Noemi Giustini"
                    },
                    {
                        "name": "Andrea Lacava"
                    },
                    {
                        "name": "Salvatore D'Oro"
                    },
                    {
                        "name": "Tommaso Melodia"
                    },
                    {
                        "name": "Francesca Cuomo"
                    }
                ],
                "author_detail": {
                    "name": "Francesca Cuomo"
                },
                "author": "Francesca Cuomo",
                "arxiv_doi": "10.1109/MedComNet65822.2025.11100289"
            },
            {
                "id": "http://arxiv.org/abs/2512.00087v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.00087v2",
                "title": "Exploring Automated Recognition of Instructional Activity and Discourse from Multimodal Classroom Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Automated Recognition of Instructional Activity and Discourse from Multimodal Classroom Data"
                },
                "updated": "2025-12-11T11:15:19Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    11,
                    15,
                    19,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.00087v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.00087v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Observation of classroom interactions can provide concrete feedback to teachers, but current methods rely on manual annotation, which is resource-intensive and hard to scale. This work explores AI-driven analysis of classroom recordings, focusing on multimodal instructional activity and discourse recognition as a foundation for actionable feedback. Using a densely annotated dataset of 164 hours of video and 68 lesson transcripts, we design parallel, modality-specific pipelines. For video, we evaluate zero-shot multimodal LLMs, fine-tuned vision-language models, and self-supervised video transformers on 24 activity labels. For transcripts, we fine-tune a transformer-based classifier with contextualized inputs and compare it against prompting-based LLMs on 19 discourse labels. To handle class imbalance and multi-label complexity, we apply per-label thresholding, context windows, and imbalance-aware loss functions. The results show that fine-tuned models consistently outperform prompting-based approaches, achieving macro-F1 scores of 0.577 for video and 0.460 for transcripts. These results demonstrate the feasibility of automated classroom analysis and establish a foundation for scalable teacher feedback systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Observation of classroom interactions can provide concrete feedback to teachers, but current methods rely on manual annotation, which is resource-intensive and hard to scale. This work explores AI-driven analysis of classroom recordings, focusing on multimodal instructional activity and discourse recognition as a foundation for actionable feedback. Using a densely annotated dataset of 164 hours of video and 68 lesson transcripts, we design parallel, modality-specific pipelines. For video, we evaluate zero-shot multimodal LLMs, fine-tuned vision-language models, and self-supervised video transformers on 24 activity labels. For transcripts, we fine-tune a transformer-based classifier with contextualized inputs and compare it against prompting-based LLMs on 19 discourse labels. To handle class imbalance and multi-label complexity, we apply per-label thresholding, context windows, and imbalance-aware loss functions. The results show that fine-tuned models consistently outperform prompting-based approaches, achieving macro-F1 scores of 0.577 for video and 0.460 for transcripts. These results demonstrate the feasibility of automated classroom analysis and establish a foundation for scalable teacher feedback systems."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-26T11:57:22Z",
                "published_parsed": [
                    2025,
                    11,
                    26,
                    11,
                    57,
                    22,
                    2,
                    330,
                    0
                ],
                "arxiv_comment": "This article has been accepted for publication in the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) 2026",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Ivo Bueno"
                    },
                    {
                        "name": "Ruikun Hou"
                    },
                    {
                        "name": "Babette Bühler"
                    },
                    {
                        "name": "Tim Fütterer"
                    },
                    {
                        "name": "James Drimalla"
                    },
                    {
                        "name": "Jonathan Kyle Foster"
                    },
                    {
                        "name": "Peter Youngs"
                    },
                    {
                        "name": "Peter Gerjets"
                    },
                    {
                        "name": "Ulrich Trautwein"
                    },
                    {
                        "name": "Enkelejda Kasneci"
                    }
                ],
                "author_detail": {
                    "name": "Enkelejda Kasneci"
                },
                "author": "Enkelejda Kasneci"
            },
            {
                "id": "http://arxiv.org/abs/2512.10538v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10538v1",
                "title": "Near Ultraviolet Transient Explorer (NUTEx): A CubeSat-Based NUV Imaging Payload for Transient Sky Surveys",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Near Ultraviolet Transient Explorer (NUTEx): A CubeSat-Based NUV Imaging Payload for Transient Sky Surveys"
                },
                "updated": "2025-12-11T11:14:54Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    11,
                    14,
                    54,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10538v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10538v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.3389/fspas.2025.1670437",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "The Near Ultraviolet Transient Explorer (NUTEx) is a CubeSat-based near-ultraviolet (NUV) imaging payload designed for transient sky surveys and is currently under development. CubeSats are compact and cost-effective satellite platforms that have emerged as versatile tools for scientific exploration and technology demonstrations in space. NUTEx is an imaging telescope operating in the 200-300 nm wavelength range, intended for deployment on a micro-satellite bus. The optical system is based on a Ritchey Chretien (RC) telescope configuration, featuring a 146 mm primary mirror. The detector is a photon-counting microchannel plate (MCP) device with a solar-blind photocathode, paired with an in-house developed readout unit. The instrument has a wide field of view (FoV) of 4 deg, a peak effective area of approximately 18 sq cm at 260 nm, and can reach a sensitivity of 21 AB magnitude (SNR = 5) in a 200 second exposure. The primary scientific objective of NUTEx is to monitor the night sky for transient phenomena, such as supernova remnants, flaring M-dwarf stars, and other short-timescale events. The payload is currently scheduled for launch in Q2 2026. This paper presents the NUTEx instrument design, outlines its scientific goals and capabilities, and provides an overview of the electronics and mechanical subsystems, including structural analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Near Ultraviolet Transient Explorer (NUTEx) is a CubeSat-based near-ultraviolet (NUV) imaging payload designed for transient sky surveys and is currently under development. CubeSats are compact and cost-effective satellite platforms that have emerged as versatile tools for scientific exploration and technology demonstrations in space. NUTEx is an imaging telescope operating in the 200-300 nm wavelength range, intended for deployment on a micro-satellite bus. The optical system is based on a Ritchey Chretien (RC) telescope configuration, featuring a 146 mm primary mirror. The detector is a photon-counting microchannel plate (MCP) device with a solar-blind photocathode, paired with an in-house developed readout unit. The instrument has a wide field of view (FoV) of 4 deg, a peak effective area of approximately 18 sq cm at 260 nm, and can reach a sensitivity of 21 AB magnitude (SNR = 5) in a 200 second exposure. The primary scientific objective of NUTEx is to monitor the night sky for transient phenomena, such as supernova remnants, flaring M-dwarf stars, and other short-timescale events. The payload is currently scheduled for launch in Q2 2026. This paper presents the NUTEx instrument design, outlines its scientific goals and capabilities, and provides an overview of the electronics and mechanical subsystems, including structural analysis."
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T11:14:54Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    11,
                    14,
                    54,
                    3,
                    345,
                    0
                ],
                "arxiv_comment": "10 figures, 6 tables",
                "arxiv_primary_category": {
                    "term": "astro-ph.IM"
                },
                "authors": [
                    {
                        "name": "Shubham Ghatul"
                    },
                    {
                        "name": "Rekhesh Mohan"
                    },
                    {
                        "name": "Jayant Murthy"
                    },
                    {
                        "name": "Margarita Safonova"
                    },
                    {
                        "name": "Praveen Kumar"
                    },
                    {
                        "name": "Maheswar Gopinathan"
                    },
                    {
                        "name": "Shubhangi Jain"
                    },
                    {
                        "name": "Mahesh Babu S"
                    }
                ],
                "author_detail": {
                    "name": "Mahesh Babu S"
                },
                "author": "Mahesh Babu S",
                "arxiv_doi": "10.3389/fspas.2025.1670437"
            },
            {
                "id": "http://arxiv.org/abs/2407.03859v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2407.03859v3",
                "title": "Anthropocentric bias in language model evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anthropocentric bias in language model evaluation"
                },
                "updated": "2025-12-11T11:10:15Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    11,
                    10,
                    15,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2407.03859v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2407.03859v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1162/COLI.a.582",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Evaluating the cognitive capacities of large language models (LLMs) requires overcoming not only anthropomorphic but also anthropocentric biases. This article identifies two types of anthropocentric bias that have been neglected: overlooking how auxiliary factors can impede LLM performance despite competence (\"auxiliary oversight\"), and dismissing LLM mechanistic strategies that differ from those of humans as not genuinely competent (\"mechanistic chauvinism\"). Mitigating these biases necessitates an empirically-driven, iterative approach to mapping cognitive tasks to LLM-specific capacities and mechanisms, which can be done by supplementing carefully designed behavioral experiments with mechanistic studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the cognitive capacities of large language models (LLMs) requires overcoming not only anthropomorphic but also anthropocentric biases. This article identifies two types of anthropocentric bias that have been neglected: overlooking how auxiliary factors can impede LLM performance despite competence (\"auxiliary oversight\"), and dismissing LLM mechanistic strategies that differ from those of humans as not genuinely competent (\"mechanistic chauvinism\"). Mitigating these biases necessitates an empirically-driven, iterative approach to mapping cognitive tasks to LLM-specific capacities and mechanisms, which can be done by supplementing carefully designed behavioral experiments with mechanistic studies."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-07-04T11:44:28Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    11,
                    44,
                    28,
                    3,
                    186,
                    0
                ],
                "arxiv_comment": "Published in Computational Linguistics",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "arxiv_journal_ref": "Computational Linguistics, 1-10. (2025)",
                "authors": [
                    {
                        "name": "Raphaël Millière"
                    },
                    {
                        "name": "Charles Rathkopf"
                    }
                ],
                "author_detail": {
                    "name": "Charles Rathkopf"
                },
                "author": "Charles Rathkopf",
                "arxiv_doi": "10.1162/COLI.a.582"
            },
            {
                "id": "http://arxiv.org/abs/2512.10534v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10534v1",
                "title": "Achieving Olympia-Level Geometry Large Language Model Agent via Complexity Boosting Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Achieving Olympia-Level Geometry Large Language Model Agent via Complexity Boosting Reinforcement Learning"
                },
                "updated": "2025-12-11T11:05:04Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    11,
                    5,
                    4,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10534v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10534v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language model (LLM) agents exhibit strong mathematical problem-solving abilities and can even solve International Mathematical Olympiad (IMO) level problems with the assistance of formal proof systems. However, due to weak heuristics for auxiliary constructions, AI for geometry problem solving remains dominated by expert models such as AlphaGeometry 2, which rely heavily on large-scale data synthesis and search for both training and evaluation. In this work, we make the first attempt to build a medalist-level LLM agent for geometry and present InternGeometry. InternGeometry overcomes the heuristic limitations in geometry by iteratively proposing propositions and auxiliary constructions, verifying them with a symbolic engine, and reflecting on the engine's feedback to guide subsequent proposals. A dynamic memory mechanism enables InternGeometry to conduct more than two hundred interactions with the symbolic engine per problem. To further accelerate learning, we introduce Complexity-Boosting Reinforcement Learning (CBRL), which gradually increases the complexity of synthesized problems across training stages. Built on InternThinker-32B, InternGeometry solves 44 of 50 IMO geometry problems (2000-2024), exceeding the average gold medalist score (40.9), using only 13K training examples, just 0.004% of the data used by AlphaGeometry 2, demonstrating the potential of LLM agents on expert-level geometry tasks. InternGeometry can also propose novel auxiliary constructions for IMO problems that do not appear in human solutions. We will release the model, data, and symbolic engine to support future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) agents exhibit strong mathematical problem-solving abilities and can even solve International Mathematical Olympiad (IMO) level problems with the assistance of formal proof systems. However, due to weak heuristics for auxiliary constructions, AI for geometry problem solving remains dominated by expert models such as AlphaGeometry 2, which rely heavily on large-scale data synthesis and search for both training and evaluation. In this work, we make the first attempt to build a medalist-level LLM agent for geometry and present InternGeometry. InternGeometry overcomes the heuristic limitations in geometry by iteratively proposing propositions and auxiliary constructions, verifying them with a symbolic engine, and reflecting on the engine's feedback to guide subsequent proposals. A dynamic memory mechanism enables InternGeometry to conduct more than two hundred interactions with the symbolic engine per problem. To further accelerate learning, we introduce Complexity-Boosting Reinforcement Learning (CBRL), which gradually increases the complexity of synthesized problems across training stages. Built on InternThinker-32B, InternGeometry solves 44 of 50 IMO geometry problems (2000-2024), exceeding the average gold medalist score (40.9), using only 13K training examples, just 0.004% of the data used by AlphaGeometry 2, demonstrating the potential of LLM agents on expert-level geometry tasks. InternGeometry can also propose novel auxiliary constructions for IMO problems that do not appear in human solutions. We will release the model, data, and symbolic engine to support future research."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T11:05:04Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    11,
                    5,
                    4,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Haiteng Zhao"
                    },
                    {
                        "name": "Junhao Shen"
                    },
                    {
                        "name": "Yiming Zhang"
                    },
                    {
                        "name": "Songyang Gao"
                    },
                    {
                        "name": "Kuikun Liu"
                    },
                    {
                        "name": "Tianyou Ma"
                    },
                    {
                        "name": "Fan Zheng"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Wenwei Zhang"
                    },
                    {
                        "name": "Kai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Chen"
                },
                "author": "Kai Chen"
            },
            {
                "id": "http://arxiv.org/abs/2512.09830v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.09830v2",
                "title": "LLMs in Interpreting Legal Documents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs in Interpreting Legal Documents"
                },
                "updated": "2025-12-11T11:01:35Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    11,
                    1,
                    35,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.09830v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.09830v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This chapter explores the application of Large Language Models in the legal domain, showcasing their potential to optimise and augment traditional legal tasks by analysing possible use cases, such as assisting in interpreting statutes, contracts, and case law, enhancing clarity in legal summarisation, contract negotiation, and information retrieval. There are several challenges that can arise from the application of such technologies, such as algorithmic monoculture, hallucinations, and compliance with existing regulations, including the EU's AI Act and recent U.S. initiatives, alongside the emerging approaches in China. Furthermore, two different benchmarks are presented.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This chapter explores the application of Large Language Models in the legal domain, showcasing their potential to optimise and augment traditional legal tasks by analysing possible use cases, such as assisting in interpreting statutes, contracts, and case law, enhancing clarity in legal summarisation, contract negotiation, and information retrieval. There are several challenges that can arise from the application of such technologies, such as algorithmic monoculture, hallucinations, and compliance with existing regulations, including the EU's AI Act and recent U.S. initiatives, alongside the emerging approaches in China. Furthermore, two different benchmarks are presented."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-10T17:09:13Z",
                "published_parsed": [
                    2025,
                    12,
                    10,
                    17,
                    9,
                    13,
                    2,
                    344,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Simone Corbo"
                    }
                ],
                "author_detail": {
                    "name": "Simone Corbo"
                },
                "author": "Simone Corbo"
            },
            {
                "id": "http://arxiv.org/abs/2512.10522v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10522v1",
                "title": "Disentangled and Distilled Encoder for Out-of-Distribution Reasoning with Rademacher Guarantees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disentangled and Distilled Encoder for Out-of-Distribution Reasoning with Rademacher Guarantees"
                },
                "updated": "2025-12-11T10:47:38Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    10,
                    47,
                    38,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10522v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10522v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recently, the disentangled latent space of a variational autoencoder (VAE) has been used to reason about multi-label out-of-distribution (OOD) test samples that are derived from different distributions than training samples. Disentangled latent space means having one-to-many maps between latent dimensions and generative factors or important characteristics of an image. This paper proposes a disentangled distilled encoder (DDE) framework to decrease the OOD reasoner size for deployment on resource-constrained devices while preserving disentanglement. DDE formalizes student-teacher distillation for model compression as a constrained optimization problem while preserving disentanglement with disentanglement constraints. Theoretical guarantees for disentanglement during distillation based on Rademacher complexity are established. The approach is evaluated empirically by deploying the compressed model on an NVIDIA",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, the disentangled latent space of a variational autoencoder (VAE) has been used to reason about multi-label out-of-distribution (OOD) test samples that are derived from different distributions than training samples. Disentangled latent space means having one-to-many maps between latent dimensions and generative factors or important characteristics of an image. This paper proposes a disentangled distilled encoder (DDE) framework to decrease the OOD reasoner size for deployment on resource-constrained devices while preserving disentanglement. DDE formalizes student-teacher distillation for model compression as a constrained optimization problem while preserving disentanglement with disentanglement constraints. Theoretical guarantees for disentanglement during distillation based on Rademacher complexity are established. The approach is evaluated empirically by deploying the compressed model on an NVIDIA"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T10:47:38Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    10,
                    47,
                    38,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Zahra Rahiminasab"
                    },
                    {
                        "name": "Michael Yuhas"
                    },
                    {
                        "name": "Arvind Easwaran"
                    }
                ],
                "author_detail": {
                    "name": "Arvind Easwaran"
                },
                "author": "Arvind Easwaran"
            },
            {
                "id": "http://arxiv.org/abs/2512.08844v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08844v2",
                "title": "A Methodology for Quantitative AI Risk Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Methodology for Quantitative AI Risk Modeling"
                },
                "updated": "2025-12-11T10:25:12Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    10,
                    25,
                    12,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08844v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08844v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Although general-purpose AI systems offer transformational opportunities in science and industry, they simultaneously raise critical concerns about safety, misuse, and potential loss of control. Despite these risks, methods for assessing and managing them remain underdeveloped. Effective risk management requires systematic modeling to characterize potential harms, as emphasized in frameworks such as the EU General-Purpose AI Code of Practice. This paper advances the risk modeling component of AI risk management by introducing a methodology that integrates scenario building with quantitative risk estimation, drawing on established approaches from other high-risk industries. Our methodology models risks through a six-step process: (1) defining risk scenarios, (2) decomposing them into quantifiable parameters, (3) quantifying baseline risk without AI models, (4) identifying key risk indicators such as benchmarks, (5) mapping these indicators to model parameters to estimate LLM uplift, and (6) aggregating individual parameters into risk estimates that enable concrete claims (e.g., X% probability of >\\$Y in annual cyber damages). We examine the choices that underlie our methodology throughout the article, with discussions of strengths, limitations, and implications for future research. Our methodology is designed to be applicable to key systemic AI risks, including cyber offense, biological weapon development, harmful manipulation, and loss-of-control, and is validated through extensive application in LLM-enabled cyber offense. Detailed empirical results and cyber-specific insights are presented in a companion paper.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although general-purpose AI systems offer transformational opportunities in science and industry, they simultaneously raise critical concerns about safety, misuse, and potential loss of control. Despite these risks, methods for assessing and managing them remain underdeveloped. Effective risk management requires systematic modeling to characterize potential harms, as emphasized in frameworks such as the EU General-Purpose AI Code of Practice. This paper advances the risk modeling component of AI risk management by introducing a methodology that integrates scenario building with quantitative risk estimation, drawing on established approaches from other high-risk industries. Our methodology models risks through a six-step process: (1) defining risk scenarios, (2) decomposing them into quantifiable parameters, (3) quantifying baseline risk without AI models, (4) identifying key risk indicators such as benchmarks, (5) mapping these indicators to model parameters to estimate LLM uplift, and (6) aggregating individual parameters into risk estimates that enable concrete claims (e.g., X% probability of >\\$Y in annual cyber damages). We examine the choices that underlie our methodology throughout the article, with discussions of strengths, limitations, and implications for future research. Our methodology is designed to be applicable to key systemic AI risks, including cyber offense, biological weapon development, harmful manipulation, and loss-of-control, and is validated through extensive application in LLM-enabled cyber offense. Detailed empirical results and cyber-specific insights are presented in a companion paper."
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T17:34:59Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    17,
                    34,
                    59,
                    1,
                    343,
                    0
                ],
                "arxiv_comment": "The only changes in v2 are some updates to a few arXiv URLs in the references",
                "arxiv_primary_category": {
                    "term": "cs.CY"
                },
                "authors": [
                    {
                        "name": "Malcolm Murray"
                    },
                    {
                        "name": "Steve Barrett"
                    },
                    {
                        "name": "Henry Papadatos"
                    },
                    {
                        "name": "Otter Quarks"
                    },
                    {
                        "name": "Matt Smith"
                    },
                    {
                        "name": "Alejandro Tlaie Boria"
                    },
                    {
                        "name": "Chloé Touzet"
                    },
                    {
                        "name": "Siméon Campos"
                    }
                ],
                "author_detail": {
                    "name": "Siméon Campos"
                },
                "author": "Siméon Campos"
            },
            {
                "id": "http://arxiv.org/abs/2506.01524v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.01524v2",
                "title": "V-VAE: A Variational Auto Encoding Framework Towards Fine-Grained Control over Human-Like Chat",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "V-VAE: A Variational Auto Encoding Framework Towards Fine-Grained Control over Human-Like Chat"
                },
                "updated": "2025-12-11T10:22:55Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    10,
                    22,
                    55,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.01524v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.01524v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "With the continued proliferation of Large Language Model (LLM) based chatbots, there is a growing demand for generating responses that are not only linguistically fluent but also consistently aligned with persona-specific traits in conversations. However, existing role-play and persona-based chat approaches rely heavily on static role descriptions, coarse-grained signal space, and low-quality synthetic data, which fail to capture dynamic fine-grained details in human-like chat. Human-like chat requires modeling subtle latent traits, such as emotional tone, situational awareness, and evolving personality, which are difficult to predefine and cannot be easily learned from synthetic or distillation-based data. To address these limitations, we propose a Verbal Variational Auto-Encoding (V-VAE) framework, containing a variational auto-encoding module and fine-grained control space which dynamically adapts dialogue behaviour based on fine-grained, interpretable latent variables across talking style, interaction patterns, and personal attributes. We also construct a high-quality dataset, HumanChatData, and benchmark HumanChatBench to address the scarcity of high-quality data in the human-like domain. Experiments show that LLMs based on V-VAE consistently outperform standard baselines on HumanChatBench and DialogBench, which further demonstrates the effectiveness of V-VAE and HumanChatData.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the continued proliferation of Large Language Model (LLM) based chatbots, there is a growing demand for generating responses that are not only linguistically fluent but also consistently aligned with persona-specific traits in conversations. However, existing role-play and persona-based chat approaches rely heavily on static role descriptions, coarse-grained signal space, and low-quality synthetic data, which fail to capture dynamic fine-grained details in human-like chat. Human-like chat requires modeling subtle latent traits, such as emotional tone, situational awareness, and evolving personality, which are difficult to predefine and cannot be easily learned from synthetic or distillation-based data. To address these limitations, we propose a Verbal Variational Auto-Encoding (V-VAE) framework, containing a variational auto-encoding module and fine-grained control space which dynamically adapts dialogue behaviour based on fine-grained, interpretable latent variables across talking style, interaction patterns, and personal attributes. We also construct a high-quality dataset, HumanChatData, and benchmark HumanChatBench to address the scarcity of high-quality data in the human-like domain. Experiments show that LLMs based on V-VAE consistently outperform standard baselines on HumanChatBench and DialogBench, which further demonstrates the effectiveness of V-VAE and HumanChatData."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-02T10:38:02Z",
                "published_parsed": [
                    2025,
                    6,
                    2,
                    10,
                    38,
                    2,
                    0,
                    153,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Qi Lin"
                    },
                    {
                        "name": "Weikai Xu"
                    },
                    {
                        "name": "Lisi Chen"
                    },
                    {
                        "name": "Bin Dai"
                    }
                ],
                "author_detail": {
                    "name": "Bin Dai"
                },
                "author": "Bin Dai"
            },
            {
                "id": "http://arxiv.org/abs/2512.10501v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10501v1",
                "title": "Zero-shot 3D Map Generation with LLM Agents: A Dual-Agent Architecture for Procedural Content Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-shot 3D Map Generation with LLM Agents: A Dual-Agent Architecture for Procedural Content Generation"
                },
                "updated": "2025-12-11T10:22:02Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    10,
                    22,
                    2,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10501v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10501v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Procedural Content Generation (PCG) offers scalable methods for algorithmically creating complex, customizable worlds. However, controlling these pipelines requires the precise configuration of opaque technical parameters. We propose a training-free architecture that utilizes LLM agents for zero-shot PCG parameter configuration. While Large Language Models (LLMs) promise a natural language interface for PCG tools, off-the-shelf models often fail to bridge the semantic gap between abstract user instructions and strict parameter specifications. Our system pairs an Actor agent with a Critic agent, enabling an iterative workflow where the system autonomously reasons over tool parameters and refines configurations to progressively align with human design preferences. We validate this approach on the generation of various 3D maps, establishing a new benchmark for instruction-following in PCG. Experiments demonstrate that our approach outperforms single-agent baselines, producing diverse and structurally valid environments from natural language descriptions. These results demonstrate that off-the-shelf LLMs can be effectively repurposed as generalized agents for arbitrary PCG tools. By shifting the burden from model training to architectural reasoning, our method offers a scalable framework for mastering complex software without task-specific fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Procedural Content Generation (PCG) offers scalable methods for algorithmically creating complex, customizable worlds. However, controlling these pipelines requires the precise configuration of opaque technical parameters. We propose a training-free architecture that utilizes LLM agents for zero-shot PCG parameter configuration. While Large Language Models (LLMs) promise a natural language interface for PCG tools, off-the-shelf models often fail to bridge the semantic gap between abstract user instructions and strict parameter specifications. Our system pairs an Actor agent with a Critic agent, enabling an iterative workflow where the system autonomously reasons over tool parameters and refines configurations to progressively align with human design preferences. We validate this approach on the generation of various 3D maps, establishing a new benchmark for instruction-following in PCG. Experiments demonstrate that our approach outperforms single-agent baselines, producing diverse and structurally valid environments from natural language descriptions. These results demonstrate that off-the-shelf LLMs can be effectively repurposed as generalized agents for arbitrary PCG tools. By shifting the burden from model training to architectural reasoning, our method offers a scalable framework for mastering complex software without task-specific fine-tuning."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T10:22:02Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    10,
                    22,
                    2,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Lim Chien Her"
                    },
                    {
                        "name": "Ming Yan"
                    },
                    {
                        "name": "Yunshu Bai"
                    },
                    {
                        "name": "Ruihao Li"
                    },
                    {
                        "name": "Hao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Zhang"
                },
                "author": "Hao Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2512.10493v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10493v1",
                "title": "Decoding Human-LLM Collaboration in Coding: An Empirical Study of Multi-Turn Conversations in the Wild",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoding Human-LLM Collaboration in Coding: An Empirical Study of Multi-Turn Conversations in the Wild"
                },
                "updated": "2025-12-11T10:14:42Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    10,
                    14,
                    42,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10493v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10493v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) are increasingly acting as dynamic conversational interfaces, supporting multi-turn interactions that mimic human-like conversation and facilitate complex tasks like coding. While datasets such as LMSYS-Chat-1M and WildChat capture real-world user-LLM conversations, few studies systematically explore the mechanisms of human-LLM collaboration in coding scenarios. What tortuous paths do users experience during the interaction process? How well do the LLMs follow instructions? Are users satisfied? In this paper, we conduct an empirical analysis on human-LLM coding collaboration using LMSYS-Chat-1M and WildChat datasets to explore the human-LLM collaboration mechanism, LLMs' instruction following ability, and human satisfaction. This study yields interesting findings: 1) Task types shape interaction patterns(linear, star and tree), with code quality optimization favoring linear patterns, design-driven tasks leaning toward tree structures, and queries preferring star patterns; 2) Bug fixing and code refactoring pose greater challenges to LLMs' instruction following, with non-compliance rates notably higher than in information querying; 3) Code quality optimization and requirements-driven development tasks show lower user satisfaction, whereas structured knowledge queries and algorithm designs yield higher levels. These insights offer recommendations for improving LLM interfaces and user satisfaction in coding collaborations, while highlighting avenues for future research on adaptive dialogue systems. We believe this work broadens understanding of human-LLM synergies and supports more effective AI-assisted development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly acting as dynamic conversational interfaces, supporting multi-turn interactions that mimic human-like conversation and facilitate complex tasks like coding. While datasets such as LMSYS-Chat-1M and WildChat capture real-world user-LLM conversations, few studies systematically explore the mechanisms of human-LLM collaboration in coding scenarios. What tortuous paths do users experience during the interaction process? How well do the LLMs follow instructions? Are users satisfied? In this paper, we conduct an empirical analysis on human-LLM coding collaboration using LMSYS-Chat-1M and WildChat datasets to explore the human-LLM collaboration mechanism, LLMs' instruction following ability, and human satisfaction. This study yields interesting findings: 1) Task types shape interaction patterns(linear, star and tree), with code quality optimization favoring linear patterns, design-driven tasks leaning toward tree structures, and queries preferring star patterns; 2) Bug fixing and code refactoring pose greater challenges to LLMs' instruction following, with non-compliance rates notably higher than in information querying; 3) Code quality optimization and requirements-driven development tasks show lower user satisfaction, whereas structured knowledge queries and algorithm designs yield higher levels. These insights offer recommendations for improving LLM interfaces and user satisfaction in coding collaborations, while highlighting avenues for future research on adaptive dialogue systems. We believe this work broadens understanding of human-LLM synergies and supports more effective AI-assisted development."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T10:14:42Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    10,
                    14,
                    42,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Binquan Zhang"
                    },
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Haoyuan Zhang"
                    },
                    {
                        "name": "Fang Liu"
                    },
                    {
                        "name": "Song Wang"
                    },
                    {
                        "name": "Bo Shen"
                    },
                    {
                        "name": "An Fu"
                    },
                    {
                        "name": "Lin Shi"
                    }
                ],
                "author_detail": {
                    "name": "Lin Shi"
                },
                "author": "Lin Shi"
            },
            {
                "id": "http://arxiv.org/abs/2512.10487v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10487v1",
                "title": "LLM-Assisted AHP for Explainable Cyber Range Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Assisted AHP for Explainable Cyber Range Evaluation"
                },
                "updated": "2025-12-11T10:07:15Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    10,
                    7,
                    15,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10487v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10487v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Cyber Ranges (CRs) have emerged as prominent platforms for cybersecurity training and education, especially for Critical Infrastructure (CI) sectors that face rising cyber threats. One way to address these threats is through hands-on exercises that bridge IT and OT domains to improve defensive readiness. However, consistently evaluating whether a CR platform is suitable and effective remains a challenge. This paper proposes an evaluation framework for CRs, emphasizing mission-critical settings by using a multi-criteria decision-making approach. We define a set of evaluation criteria that capture technical fidelity, training and assessment capabilities, scalability, usability, and other relevant factors. To weight and aggregate these criteria, we employ the Analytic Hierarchy Process (AHP), supported by a simulated panel of multidisciplinary experts implemented through a Large Language Model (LLM). This LLM-assisted expert reasoning enables consistent and reproducible pairwise comparisons across criteria without requiring direct expert convening. The framework's output equals quantitative scores that facilitate objective comparison of CR platforms and highlight areas for improvement. Overall, this work lays the foundation for a standardized and explainable evaluation methodology to guide both providers and end-users of CRs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cyber Ranges (CRs) have emerged as prominent platforms for cybersecurity training and education, especially for Critical Infrastructure (CI) sectors that face rising cyber threats. One way to address these threats is through hands-on exercises that bridge IT and OT domains to improve defensive readiness. However, consistently evaluating whether a CR platform is suitable and effective remains a challenge. This paper proposes an evaluation framework for CRs, emphasizing mission-critical settings by using a multi-criteria decision-making approach. We define a set of evaluation criteria that capture technical fidelity, training and assessment capabilities, scalability, usability, and other relevant factors. To weight and aggregate these criteria, we employ the Analytic Hierarchy Process (AHP), supported by a simulated panel of multidisciplinary experts implemented through a Large Language Model (LLM). This LLM-assisted expert reasoning enables consistent and reproducible pairwise comparisons across criteria without requiring direct expert convening. The framework's output equals quantitative scores that facilitate objective comparison of CR platforms and highlight areas for improvement. Overall, this work lays the foundation for a standardized and explainable evaluation methodology to guide both providers and end-users of CRs."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T10:07:15Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    10,
                    7,
                    15,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Vyron Kampourakis"
                    },
                    {
                        "name": "Georgios Kavallieratos"
                    },
                    {
                        "name": "Georgios Spathoulas"
                    },
                    {
                        "name": "Vasileios Gkioulos"
                    },
                    {
                        "name": "Sokratis Katsikas"
                    }
                ],
                "author_detail": {
                    "name": "Sokratis Katsikas"
                },
                "author": "Sokratis Katsikas"
            },
            {
                "id": "http://arxiv.org/abs/2512.10485v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10485v1",
                "title": "From Lab to Reality: A Practical Evaluation of Deep Learning Models and LLMs for Vulnerability Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Lab to Reality: A Practical Evaluation of Deep Learning Models and LLMs for Vulnerability Detection"
                },
                "updated": "2025-12-11T10:04:54Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    10,
                    4,
                    54,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10485v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10485v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Vulnerability detection methods based on deep learning (DL) have shown strong performance on benchmark datasets, yet their real-world effectiveness remains underexplored. Recent work suggests that both graph neural network (GNN)-based and transformer-based models, including large language models (LLMs), yield promising results when evaluated on curated benchmark datasets. These datasets are typically characterized by consistent data distributions and heuristic or partially noisy labels. In this study, we systematically evaluate two representative DL models-ReVeal and LineVul-across four representative datasets: Juliet, Devign, BigVul, and ICVul. Each model is trained independently on each respective dataset, and their code representations are analyzed using t-SNE to uncover vulnerability related patterns. To assess realistic applicability, we deploy these models along with four pretrained LLMs, Claude 3.5 Sonnet, GPT-o3-mini, GPT-4o, and GPT-5 on a curated dataset, VentiVul, comprising 20 recently (May 2025) fixed vulnerabilities from the Linux kernel. Our experiments reveal that current models struggle to distinguish vulnerable from non-vulnerable code in representation space and generalize poorly across datasets with differing distributions. When evaluated on VentiVul, our newly constructed time-wise out-of-distribution dataset, performance drops sharply, with most models failing to detect vulnerabilities reliably. These results expose a persistent gap between academic benchmarks and real-world deployment, emphasizing the value of our deployment-oriented evaluation framework and the need for more robust code representations and higher-quality datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vulnerability detection methods based on deep learning (DL) have shown strong performance on benchmark datasets, yet their real-world effectiveness remains underexplored. Recent work suggests that both graph neural network (GNN)-based and transformer-based models, including large language models (LLMs), yield promising results when evaluated on curated benchmark datasets. These datasets are typically characterized by consistent data distributions and heuristic or partially noisy labels. In this study, we systematically evaluate two representative DL models-ReVeal and LineVul-across four representative datasets: Juliet, Devign, BigVul, and ICVul. Each model is trained independently on each respective dataset, and their code representations are analyzed using t-SNE to uncover vulnerability related patterns. To assess realistic applicability, we deploy these models along with four pretrained LLMs, Claude 3.5 Sonnet, GPT-o3-mini, GPT-4o, and GPT-5 on a curated dataset, VentiVul, comprising 20 recently (May 2025) fixed vulnerabilities from the Linux kernel. Our experiments reveal that current models struggle to distinguish vulnerable from non-vulnerable code in representation space and generalize poorly across datasets with differing distributions. When evaluated on VentiVul, our newly constructed time-wise out-of-distribution dataset, performance drops sharply, with most models failing to detect vulnerabilities reliably. These results expose a persistent gap between academic benchmarks and real-world deployment, emphasizing the value of our deployment-oriented evaluation framework and the need for more robust code representations and higher-quality datasets."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T10:04:54Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    10,
                    4,
                    54,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Chaomeng Lu"
                    },
                    {
                        "name": "Bert Lagaisse"
                    }
                ],
                "author_detail": {
                    "name": "Bert Lagaisse"
                },
                "author": "Bert Lagaisse"
            },
            {
                "id": "http://arxiv.org/abs/2411.08464v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2411.08464v3",
                "title": "A Generation Framework with Strict Constraints for Crystal Materials Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Generation Framework with Strict Constraints for Crystal Materials Design"
                },
                "updated": "2025-12-11T09:59:41Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    9,
                    59,
                    41,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2411.08464v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2411.08464v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The design of crystal materials plays a critical role in areas such as new energy development, biomedical engineering, and semiconductors. Recent advances in data-driven methods have enabled the generation of diverse crystal structures. However, most existing approaches still rely on random sampling without strict constraints, requiring multiple post-processing steps to identify stable candidates with the desired physical and chemical properties. In this work, we present a new constrained generation framework that takes multiple constraints as input and enables the generation of crystal structures with specific chemical and properties. In this framework, intermediate constraints, such as symmetry information and composition ratio, are generated by a constraint generator based on large language models (LLMs), which considers the target properties. These constraints are then used by a subsequent crystal structure generator to ensure that the structure generation process is under control. Our method generates crystal structures with a probability of meeting the target properties that is more than twice that of existing approaches. Furthermore, nearly 100% of the generated crystals strictly adhere to predefined chemical composition, eliminating the risks of supply chain during production.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The design of crystal materials plays a critical role in areas such as new energy development, biomedical engineering, and semiconductors. Recent advances in data-driven methods have enabled the generation of diverse crystal structures. However, most existing approaches still rely on random sampling without strict constraints, requiring multiple post-processing steps to identify stable candidates with the desired physical and chemical properties. In this work, we present a new constrained generation framework that takes multiple constraints as input and enables the generation of crystal structures with specific chemical and properties. In this framework, intermediate constraints, such as symmetry information and composition ratio, are generated by a constraint generator based on large language models (LLMs), which considers the target properties. These constraints are then used by a subsequent crystal structure generator to ensure that the structure generation process is under control. Our method generates crystal structures with a probability of meeting the target properties that is more than twice that of existing approaches. Furthermore, nearly 100% of the generated crystals strictly adhere to predefined chemical composition, eliminating the risks of supply chain during production."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-11-13T09:36:50Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    9,
                    36,
                    50,
                    2,
                    318,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Chao Huang"
                    },
                    {
                        "name": "Jiahui Chen"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Chunyan Chen"
                    },
                    {
                        "name": "Renjie Su"
                    },
                    {
                        "name": "Shiyu Du"
                    }
                ],
                "author_detail": {
                    "name": "Shiyu Du"
                },
                "author": "Shiyu Du"
            },
            {
                "id": "http://arxiv.org/abs/2512.08864v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08864v2",
                "title": "Toward Quantitative Modeling of Cybersecurity Risks Due to AI Misuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward Quantitative Modeling of Cybersecurity Risks Due to AI Misuse"
                },
                "updated": "2025-12-11T09:54:50Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    9,
                    54,
                    50,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08864v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08864v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Advanced AI systems offer substantial benefits but also introduce risks. In 2025, AI-enabled cyber offense has emerged as a concrete example. This technical report applies a quantitative risk modeling methodology (described in full in a companion paper) to this domain. We develop nine detailed cyber risk models that allow analyzing AI uplift as a function of AI benchmark performance. Each model decomposes attacks into steps using the MITRE ATT&CK framework and estimates how AI affects the number of attackers, attack frequency, probability of success, and resulting harm to determine different types of uplift. To produce these estimates with associated uncertainty, we employ both human experts, via a Delphi study, as well as LLM-based simulated experts, both mapping benchmark scores (from Cybench and BountyBench) to risk model factors. Individual estimates are aggregated through Monte Carlo simulation. The results indicate systematic uplift in attack efficacy, speed, and target reach, with different mechanisms of uplift across risk models. We aim for our quantitative risk modeling to fulfill several aims: to help cybersecurity teams prioritize mitigations, AI evaluators design benchmarks, AI developers make more informed deployment decisions, and policymakers obtain information to set risk thresholds. Similar goals drove the shift from qualitative to quantitative assessment over time in other high-risk industries, such as nuclear power. We propose this methodology and initial application attempt as a step in that direction for AI risk management. While our estimates carry significant uncertainty, publishing detailed quantified results can enable experts to pinpoint exactly where they disagree. This helps to collectively refine estimates, something that cannot be done with qualitative assessments alone.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced AI systems offer substantial benefits but also introduce risks. In 2025, AI-enabled cyber offense has emerged as a concrete example. This technical report applies a quantitative risk modeling methodology (described in full in a companion paper) to this domain. We develop nine detailed cyber risk models that allow analyzing AI uplift as a function of AI benchmark performance. Each model decomposes attacks into steps using the MITRE ATT&CK framework and estimates how AI affects the number of attackers, attack frequency, probability of success, and resulting harm to determine different types of uplift. To produce these estimates with associated uncertainty, we employ both human experts, via a Delphi study, as well as LLM-based simulated experts, both mapping benchmark scores (from Cybench and BountyBench) to risk model factors. Individual estimates are aggregated through Monte Carlo simulation. The results indicate systematic uplift in attack efficacy, speed, and target reach, with different mechanisms of uplift across risk models. We aim for our quantitative risk modeling to fulfill several aims: to help cybersecurity teams prioritize mitigations, AI evaluators design benchmarks, AI developers make more informed deployment decisions, and policymakers obtain information to set risk thresholds. Similar goals drove the shift from qualitative to quantitative assessment over time in other high-risk industries, such as nuclear power. We propose this methodology and initial application attempt as a step in that direction for AI risk management. While our estimates carry significant uncertainty, publishing detailed quantified results can enable experts to pinpoint exactly where they disagree. This helps to collectively refine estimates, something that cannot be done with qualitative assessments alone."
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T17:54:17Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    17,
                    54,
                    17,
                    1,
                    343,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY"
                },
                "authors": [
                    {
                        "name": "Steve Barrett"
                    },
                    {
                        "name": "Malcolm Murray"
                    },
                    {
                        "name": "Otter Quarks"
                    },
                    {
                        "name": "Matthew Smith"
                    },
                    {
                        "name": "Jakub Kryś"
                    },
                    {
                        "name": "Siméon Campos"
                    },
                    {
                        "name": "Alejandro Tlaie Boria"
                    },
                    {
                        "name": "Chloé Touzet"
                    },
                    {
                        "name": "Sevan Hayrapet"
                    },
                    {
                        "name": "Fred Heiding"
                    },
                    {
                        "name": "Omer Nevo"
                    },
                    {
                        "name": "Adam Swanda"
                    },
                    {
                        "name": "Jair Aguirre"
                    },
                    {
                        "name": "Asher Brass Gershovich"
                    },
                    {
                        "name": "Eric Clay"
                    },
                    {
                        "name": "Ryan Fetterman"
                    },
                    {
                        "name": "Mario Fritz"
                    },
                    {
                        "name": "Marc Juarez"
                    },
                    {
                        "name": "Vasilios Mavroudis"
                    },
                    {
                        "name": "Henry Papadatos"
                    }
                ],
                "author_detail": {
                    "name": "Henry Papadatos"
                },
                "author": "Henry Papadatos"
            },
            {
                "id": "http://arxiv.org/abs/2512.10470v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10470v1",
                "title": "Stealth and Evasion in Rogue AP Attacks: An Analysis of Modern Detection and Bypass Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stealth and Evasion in Rogue AP Attacks: An Analysis of Modern Detection and Bypass Techniques"
                },
                "updated": "2025-12-11T09:45:48Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    9,
                    45,
                    48,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10470v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10470v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Wireless networks act as the backbone of modern digital connectivity, making them a primary target for cyber adversaries. Rogue Access Point attacks, specifically the Evil Twin variant, enable attackers to clone legitimate wireless network identifiers to deceive users into connecting. Once a connection is established, the adversary can intercept traffic and harvest sensitive credentials. While modern defensive architectures often employ Network Intrusion Detection Systems (NIDS) to identify malicious activity, the effectiveness of these systems against Layer 2 wireless threats remains a subject of critical inquiry. This project aimed to design a stealth-capable Rogue AP and evaluate its detectability against Suricata, an open-source NIDS/IPS. The methodology initially focused on a hardware-based deployment using Raspberry Pi platforms but transitioned to a virtualized environment due to severe system compatibility issues. Using Wifipumpkin3, the research team successfully deployed a captive portal that harvested user credentials from connected devices. However, the Suricata NIDS failed to flag the attack, highlighting a significant blind spot in traditional intrusion detection regarding wireless management frame attacks. This paper details the construction of the attack, the evasion techniques employed, and the limitations of current NIDS solutions in detecting localized wireless threats",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wireless networks act as the backbone of modern digital connectivity, making them a primary target for cyber adversaries. Rogue Access Point attacks, specifically the Evil Twin variant, enable attackers to clone legitimate wireless network identifiers to deceive users into connecting. Once a connection is established, the adversary can intercept traffic and harvest sensitive credentials. While modern defensive architectures often employ Network Intrusion Detection Systems (NIDS) to identify malicious activity, the effectiveness of these systems against Layer 2 wireless threats remains a subject of critical inquiry. This project aimed to design a stealth-capable Rogue AP and evaluate its detectability against Suricata, an open-source NIDS/IPS. The methodology initially focused on a hardware-based deployment using Raspberry Pi platforms but transitioned to a virtualized environment due to severe system compatibility issues. Using Wifipumpkin3, the research team successfully deployed a captive portal that harvested user credentials from connected devices. However, the Suricata NIDS failed to flag the attack, highlighting a significant blind spot in traditional intrusion detection regarding wireless management frame attacks. This paper details the construction of the attack, the evasion techniques employed, and the limitations of current NIDS solutions in detecting localized wireless threats"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T09:45:48Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    9,
                    45,
                    48,
                    3,
                    345,
                    0
                ],
                "arxiv_comment": "5 pages, 3 figures, experimental paper",
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Kaleb Bacztub"
                    },
                    {
                        "name": "Braden Vester"
                    },
                    {
                        "name": "Matteo Hodge"
                    },
                    {
                        "name": "Liulseged Abate"
                    }
                ],
                "author_detail": {
                    "name": "Liulseged Abate"
                },
                "author": "Liulseged Abate"
            },
            {
                "id": "http://arxiv.org/abs/2509.17552v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.17552v3",
                "title": "Can LLMs Reason Over Non-Text Modalities in a Training-Free Manner? A Case Study with In-Context Representation Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Reason Over Non-Text Modalities in a Training-Free Manner? A Case Study with In-Context Representation Learning"
                },
                "updated": "2025-12-11T09:40:22Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    9,
                    40,
                    22,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.17552v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.17552v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The remarkable performance of Large Language Models (LLMs) can be enhanced with test-time computation, which relies on external tools and even other deep learning models. However, existing approaches for integrating non-text modality representations into LLMs typically require additional costly supervised training, restricting on-the-fly adaptation to new domains and modalities. In this work, we explore the feasibility of integrating representations from non-text foundational models (FMs) into text-based LLMs in a training-free manner. We propose In-Context Representation Learning (ICRL) as a proof-of-concept to allow LLMs to adaptively utilize non-text modality representations with few-shot learning. Unlike traditional in-context learning, which incorporates text-label pairs, ICRL replaces text inputs with FM representations, enabling the LLM to perform multi-modal inference without fine-tuning. We evaluate ICRL on a suite of tasks in the molecular domain, investigating three core research questions: (i) how to map FM representations into LLMs in a training-free manner, (ii) what factors influence ICRL performance, and (iii) what mechanisms underlie the effectiveness of ICRL. To the best of our knowledge, ICRL is the first training-free framework for integrating non-text modality representations into text-based LLMs, presenting a promising direction for adaptable, multi-modal generalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The remarkable performance of Large Language Models (LLMs) can be enhanced with test-time computation, which relies on external tools and even other deep learning models. However, existing approaches for integrating non-text modality representations into LLMs typically require additional costly supervised training, restricting on-the-fly adaptation to new domains and modalities. In this work, we explore the feasibility of integrating representations from non-text foundational models (FMs) into text-based LLMs in a training-free manner. We propose In-Context Representation Learning (ICRL) as a proof-of-concept to allow LLMs to adaptively utilize non-text modality representations with few-shot learning. Unlike traditional in-context learning, which incorporates text-label pairs, ICRL replaces text inputs with FM representations, enabling the LLM to perform multi-modal inference without fine-tuning. We evaluate ICRL on a suite of tasks in the molecular domain, investigating three core research questions: (i) how to map FM representations into LLMs in a training-free manner, (ii) what factors influence ICRL performance, and (iii) what mechanisms underlie the effectiveness of ICRL. To the best of our knowledge, ICRL is the first training-free framework for integrating non-text modality representations into text-based LLMs, presenting a promising direction for adaptable, multi-modal generalization."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-22T09:16:34Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    9,
                    16,
                    34,
                    0,
                    265,
                    0
                ],
                "arxiv_comment": "NeurIPS 2025",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Tianle Zhang"
                    },
                    {
                        "name": "Wanlong Fang"
                    },
                    {
                        "name": "Jonathan Woo"
                    },
                    {
                        "name": "Paridhi Latawa"
                    },
                    {
                        "name": "Deepak A. Subramanian"
                    },
                    {
                        "name": "Alvin Chan"
                    }
                ],
                "author_detail": {
                    "name": "Alvin Chan"
                },
                "author": "Alvin Chan"
            },
            {
                "id": "http://arxiv.org/abs/2506.02671v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.02671v2",
                "title": "Test-Time Distillation for Continual Model Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-Time Distillation for Continual Model Adaptation"
                },
                "updated": "2025-12-11T09:34:01Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    9,
                    34,
                    1,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.02671v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.02671v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Deep neural networks often suffer performance degradation upon deployment due to distribution shifts. Continual Test-Time Adaptation (CTTA) aims to address this issue in an unsupervised manner, yet existing methods, which rely on self-supervision, are prone to an inherent self-referential feedback loop that amplifies initial prediction errors, leading to model drift. We revisit this limitation and propose Test-Time Distillation (TTD), which reframes adaptation as a distillation process guided by a frozen Vision-Language Model (VLM) as an external signal. While promising, we find that direct distillation is fraught with two pitfalls: the Generalist Trap, where the VLM's broad but non-specialized knowledge leads to suboptimal performance on specific tasks and shifts, and the Entropy Bias, where naive model fusion techniques based on entropy fail due to the disparate calibration of heterogeneous models. These pitfalls motivate our insight: the key is to build a robust supervisory signal and leverage it to guide the target model toward stable adaptation. Hence, we present CoDiRe, a Continual Distillation and Rectification framework for TTD. CoDiRe first constructs a robust blended teacher by dynamically fusing the predictions of the VLM and the target model. Critically, it circumvents the Entropy Bias by leveraging Maximum Softmax Probability (MSP) as a more reliable confidence metric for weighting each model's expertise. Then applies an Optimal Transport based rectification to further align predictions with the blended teacher, enabling continuous and stable adaptation. Extensive experiments show that CoDiRe outperforms state-of-the-art baselines, exceeding CoTTA by 10.55% while using only 48% of its time cost on ImageNet-C.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep neural networks often suffer performance degradation upon deployment due to distribution shifts. Continual Test-Time Adaptation (CTTA) aims to address this issue in an unsupervised manner, yet existing methods, which rely on self-supervision, are prone to an inherent self-referential feedback loop that amplifies initial prediction errors, leading to model drift. We revisit this limitation and propose Test-Time Distillation (TTD), which reframes adaptation as a distillation process guided by a frozen Vision-Language Model (VLM) as an external signal. While promising, we find that direct distillation is fraught with two pitfalls: the Generalist Trap, where the VLM's broad but non-specialized knowledge leads to suboptimal performance on specific tasks and shifts, and the Entropy Bias, where naive model fusion techniques based on entropy fail due to the disparate calibration of heterogeneous models. These pitfalls motivate our insight: the key is to build a robust supervisory signal and leverage it to guide the target model toward stable adaptation. Hence, we present CoDiRe, a Continual Distillation and Rectification framework for TTD. CoDiRe first constructs a robust blended teacher by dynamically fusing the predictions of the VLM and the target model. Critically, it circumvents the Entropy Bias by leveraging Maximum Softmax Probability (MSP) as a more reliable confidence metric for weighting each model's expertise. Then applies an Optimal Transport based rectification to further align predictions with the blended teacher, enabling continuous and stable adaptation. Extensive experiments show that CoDiRe outperforms state-of-the-art baselines, exceeding CoTTA by 10.55% while using only 48% of its time cost on ImageNet-C."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-03T09:16:51Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    9,
                    16,
                    51,
                    1,
                    154,
                    0
                ],
                "arxiv_comment": "11 pages, 6 figures",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Xiao Chen"
                    },
                    {
                        "name": "Jiazhen Huang"
                    },
                    {
                        "name": "Zhiming Liu"
                    },
                    {
                        "name": "Qinting Jiang"
                    },
                    {
                        "name": "Fanding Huang"
                    },
                    {
                        "name": "Jingyan Jiang"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang"
            },
            {
                "id": "http://arxiv.org/abs/2502.00791v5",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2502.00791v5",
                "title": "Vision-centric Token Compression in Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-centric Token Compression in Large Language Model"
                },
                "updated": "2025-12-11T09:27:00Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    9,
                    27,
                    0,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2502.00791v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2502.00791v5",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Real-world applications are stretching context windows to hundreds of thousand of tokens while Large Language Models (LLMs) swell from billions to trillions of parameters. This dual expansion send compute and memory costs skyrocketing, making token compression indispensable. We introduce Vision Centric Token Compression (Vist), a slow-fast compression framework that mirrors human reading: the fast path renders distant tokens into images, letting a frozen, lightweight vision encoder skim the low-salience context; the slow path feeds the proximal window into the LLM for fine-grained reasoning. A Probability-Informed Visual Enhancement (PVE) objective masks high-frequency tokens during training, steering the Resampler to concentrate on semantically rich regions-just as skilled reader gloss over function words. On eleven in-context learning benchmarks, Vist achieves the same accuracy with 2.3 times fewer tokens, cutting FLOPs by 16% and memory by 50%. This method delivers remarkable results, outperforming the strongest text encoder-based compression method CEPE by 7.6% on average over benchmarks like TriviaQA, NQ, PopQA, NLUI, and CLIN, setting a new standard for token efficiency in LLMs. The project is at https://github.com/CSU-JPG/VIST.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-world applications are stretching context windows to hundreds of thousand of tokens while Large Language Models (LLMs) swell from billions to trillions of parameters. This dual expansion send compute and memory costs skyrocketing, making token compression indispensable. We introduce Vision Centric Token Compression (Vist), a slow-fast compression framework that mirrors human reading: the fast path renders distant tokens into images, letting a frozen, lightweight vision encoder skim the low-salience context; the slow path feeds the proximal window into the LLM for fine-grained reasoning. A Probability-Informed Visual Enhancement (PVE) objective masks high-frequency tokens during training, steering the Resampler to concentrate on semantically rich regions-just as skilled reader gloss over function words. On eleven in-context learning benchmarks, Vist achieves the same accuracy with 2.3 times fewer tokens, cutting FLOPs by 16% and memory by 50%. This method delivers remarkable results, outperforming the strongest text encoder-based compression method CEPE by 7.6% on average over benchmarks like TriviaQA, NQ, PopQA, NLUI, and CLIN, setting a new standard for token efficiency in LLMs. The project is at https://github.com/CSU-JPG/VIST."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-02-02T13:10:06Z",
                "published_parsed": [
                    2025,
                    2,
                    2,
                    13,
                    10,
                    6,
                    6,
                    33,
                    0
                ],
                "arxiv_comment": "NeurIPS 2025 spotlight",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Ling Xing"
                    },
                    {
                        "name": "Alex Jinpeng Wang"
                    },
                    {
                        "name": "Rui Yan"
                    },
                    {
                        "name": "Xiangbo Shu"
                    },
                    {
                        "name": "Jinhui Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jinhui Tang"
                },
                "author": "Jinhui Tang"
            },
            {
                "id": "http://arxiv.org/abs/2512.10453v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10453v1",
                "title": "Grammaticality Judgments in Humans and Language Models: Revisiting Generative Grammar with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grammaticality Judgments in Humans and Language Models: Revisiting Generative Grammar with LLMs"
                },
                "updated": "2025-12-11T09:17:35Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    9,
                    17,
                    35,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10453v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10453v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "What counts as evidence for syntactic structure? In traditional generative grammar, systematic contrasts in grammaticality such as subject-auxiliary inversion and the licensing of parasitic gaps are taken as evidence for an internal, hierarchical grammar. In this paper, we test whether large language models (LLMs), trained only on surface forms, reproduce these contrasts in ways that imply an underlying structural representation.\n  We focus on two classic constructions: subject-auxiliary inversion (testing recognition of the subject boundary) and parasitic gap licensing (testing abstract dependency structure). We evaluate models including GPT-4 and LLaMA-3 using prompts eliciting acceptability ratings. Results show that LLMs reliably distinguish between grammatical and ungrammatical variants in both constructions, and as such support that they are sensitive to structure and not just linear order. Structural generalizations, distinct from cognitive knowledge, emerge from predictive training on surface forms, suggesting functional sensitivity to syntax without explicit encoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What counts as evidence for syntactic structure? In traditional generative grammar, systematic contrasts in grammaticality such as subject-auxiliary inversion and the licensing of parasitic gaps are taken as evidence for an internal, hierarchical grammar. In this paper, we test whether large language models (LLMs), trained only on surface forms, reproduce these contrasts in ways that imply an underlying structural representation.\n  We focus on two classic constructions: subject-auxiliary inversion (testing recognition of the subject boundary) and parasitic gap licensing (testing abstract dependency structure). We evaluate models including GPT-4 and LLaMA-3 using prompts eliciting acceptability ratings. Results show that LLMs reliably distinguish between grammatical and ungrammatical variants in both constructions, and as such support that they are sensitive to structure and not just linear order. Structural generalizations, distinct from cognitive knowledge, emerge from predictive training on surface forms, suggesting functional sensitivity to syntax without explicit encoding."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T09:17:35Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    9,
                    17,
                    35,
                    3,
                    345,
                    0
                ],
                "arxiv_comment": "2 figures",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Lars G. B. Johnsen"
                    }
                ],
                "author_detail": {
                    "name": "Lars G. B. Johnsen"
                },
                "author": "Lars G. B. Johnsen"
            },
            {
                "id": "http://arxiv.org/abs/2512.10449v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10449v1",
                "title": "When Reject Turns into Accept: Quantifying the Vulnerability of LLM-Based Scientific Reviewers to Indirect Prompt Injection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Reject Turns into Accept: Quantifying the Vulnerability of LLM-Based Scientific Reviewers to Indirect Prompt Injection"
                },
                "updated": "2025-12-11T09:13:36Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    9,
                    13,
                    36,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10449v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10449v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The landscape of scientific peer review is rapidly evolving with the integration of Large Language Models (LLMs). This shift is driven by two parallel trends: the widespread individual adoption of LLMs by reviewers to manage workload (the \"Lazy Reviewer\" hypothesis) and the formal institutional deployment of AI-powered assessment systems by conferences like AAAI and Stanford's Agents4Science. This study investigates the robustness of these \"LLM-as-a-Judge\" systems (both illicit and sanctioned) to adversarial PDF manipulation. Unlike general jailbreaks, we focus on a distinct incentive: flipping \"Reject\" decisions to \"Accept,\" for which we develop a novel evaluation metric which we term as WAVS (Weighted Adversarial Vulnerability Score). We curated a dataset of 200 scientific papers and adapted 15 domain-specific attack strategies to this task, evaluating them across 13 Language Models, including GPT-5, Claude Haiku, and DeepSeek. Our results demonstrate that obfuscation strategies like \"Maximum Mark Magyk\" successfully manipulate scores, achieving alarming decision flip rates even in large-scale models. We will release our complete dataset and injection framework to facilitate more research on this topic.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The landscape of scientific peer review is rapidly evolving with the integration of Large Language Models (LLMs). This shift is driven by two parallel trends: the widespread individual adoption of LLMs by reviewers to manage workload (the \"Lazy Reviewer\" hypothesis) and the formal institutional deployment of AI-powered assessment systems by conferences like AAAI and Stanford's Agents4Science. This study investigates the robustness of these \"LLM-as-a-Judge\" systems (both illicit and sanctioned) to adversarial PDF manipulation. Unlike general jailbreaks, we focus on a distinct incentive: flipping \"Reject\" decisions to \"Accept,\" for which we develop a novel evaluation metric which we term as WAVS (Weighted Adversarial Vulnerability Score). We curated a dataset of 200 scientific papers and adapted 15 domain-specific attack strategies to this task, evaluating them across 13 Language Models, including GPT-5, Claude Haiku, and DeepSeek. Our results demonstrate that obfuscation strategies like \"Maximum Mark Magyk\" successfully manipulate scores, achieving alarming decision flip rates even in large-scale models. We will release our complete dataset and injection framework to facilitate more research on this topic."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T09:13:36Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    9,
                    13,
                    36,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Devanshu Sahoo"
                    },
                    {
                        "name": "Manish Prasad"
                    },
                    {
                        "name": "Vasudev Majhi"
                    },
                    {
                        "name": "Jahnvi Singh"
                    },
                    {
                        "name": "Vinay Chamola"
                    },
                    {
                        "name": "Yash Sinha"
                    },
                    {
                        "name": "Murari Mandal"
                    },
                    {
                        "name": "Dhruv Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Dhruv Kumar"
                },
                "author": "Dhruv Kumar"
            },
            {
                "id": "http://arxiv.org/abs/2512.10441v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10441v1",
                "title": "Decoding Student Minds: Leveraging Conversational Agents for Psychological and Learning Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoding Student Minds: Leveraging Conversational Agents for Psychological and Learning Analysis"
                },
                "updated": "2025-12-11T09:06:45Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    9,
                    6,
                    45,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10441v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10441v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This paper presents a psychologically-aware conversational agent designed to enhance both learning performance and emotional well-being in educational settings. The system combines Large Language Models (LLMs), a knowledge graph-enhanced BERT (KG-BERT), and a bidirectional Long Short-Term Memory (LSTM) with attention to classify students' cognitive and affective states in real time. Unlike prior chatbots limited to either tutoring or affective support, our approach leverages multimodal data-including textual semantics, prosodic speech features, and temporal behavioral trends-to infer engagement, stress, and conceptual understanding. A pilot study with university students demonstrated improved motivation, reduced stress, and moderate academic gains compared to baseline methods. These results underline the promise of integrating semantic reasoning, multimodal fusion, and temporal modeling to support adaptive, student-centered educational interventions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a psychologically-aware conversational agent designed to enhance both learning performance and emotional well-being in educational settings. The system combines Large Language Models (LLMs), a knowledge graph-enhanced BERT (KG-BERT), and a bidirectional Long Short-Term Memory (LSTM) with attention to classify students' cognitive and affective states in real time. Unlike prior chatbots limited to either tutoring or affective support, our approach leverages multimodal data-including textual semantics, prosodic speech features, and temporal behavioral trends-to infer engagement, stress, and conceptual understanding. A pilot study with university students demonstrated improved motivation, reduced stress, and moderate academic gains compared to baseline methods. These results underline the promise of integrating semantic reasoning, multimodal fusion, and temporal modeling to support adaptive, student-centered educational interventions."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T09:06:45Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    9,
                    6,
                    45,
                    3,
                    345,
                    0
                ],
                "arxiv_comment": "This manuscript is currently under peer review in Expert Systems with Applications",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Nour El Houda Ben Chaabene"
                    },
                    {
                        "name": "Hamza Hammami"
                    },
                    {
                        "name": "Laid Kahloul"
                    }
                ],
                "author_detail": {
                    "name": "Laid Kahloul"
                },
                "author": "Laid Kahloul"
            },
            {
                "id": "http://arxiv.org/abs/2512.10440v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10440v1",
                "title": "Enhancing Next-Generation Language Models with Knowledge Graphs: Extending Claude, Mistral IA, and GPT-4 via KG-BERT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Next-Generation Language Models with Knowledge Graphs: Extending Claude, Mistral IA, and GPT-4 via KG-BERT"
                },
                "updated": "2025-12-11T09:02:45Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    9,
                    2,
                    45,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10440v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10440v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) like Claude, Mistral IA, and GPT-4 excel in NLP but lack structured knowledge, leading to factual inconsistencies. We address this by integrating Knowledge Graphs (KGs) via KG-BERT to enhance grounding and reasoning. Experiments show significant gains in knowledge-intensive tasks such as question answering and entity linking. This approach improves factual reliability and enables more context-aware next-generation LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) like Claude, Mistral IA, and GPT-4 excel in NLP but lack structured knowledge, leading to factual inconsistencies. We address this by integrating Knowledge Graphs (KGs) via KG-BERT to enhance grounding and reasoning. Experiments show significant gains in knowledge-intensive tasks such as question answering and entity linking. This approach improves factual reliability and enables more context-aware next-generation LLMs."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T09:02:45Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    9,
                    2,
                    45,
                    3,
                    345,
                    0
                ],
                "arxiv_comment": "This paper was accepted and scheduled for inclusion in the ICALT 2025 proceedings but was ultimately not published due to absence from the conference presentation. It appears in the official program booklet. Conference: 2025 IEEE International Conference on Advanced Learning Technologies (ICALT)",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Nour El Houda Ben Chaabene"
                    },
                    {
                        "name": "Hamza Hammami"
                    }
                ],
                "author_detail": {
                    "name": "Hamza Hammami"
                },
                "author": "Hamza Hammami"
            },
            {
                "id": "http://arxiv.org/abs/2505.14381v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.14381v2",
                "title": "SCAN: Semantic Document Layout Analysis for Textual and Visual Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCAN: Semantic Document Layout Analysis for Textual and Visual Retrieval-Augmented Generation"
                },
                "updated": "2025-12-11T08:51:09Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    8,
                    51,
                    9,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.14381v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.14381v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "With the increasing adoption of Large Language Models (LLMs) and Vision-Language Models (VLMs), rich document analysis technologies for applications like Retrieval-Augmented Generation (RAG) and visual RAG are gaining significant attention. Recent research indicates that using VLMs yields better RAG performance, but processing rich documents remains a challenge since a single page contains large amounts of information. In this paper, we present SCAN (SemantiC Document Layout ANalysis), a novel approach that enhances both textual and visual Retrieval-Augmented Generation (RAG) systems that work with visually rich documents. It is a VLM-friendly approach that identifies document components with appropriate semantic granularity, balancing context preservation with processing efficiency. SCAN uses a coarse-grained semantic approach that divides documents into coherent regions covering contiguous components. We trained the SCAN model by fine-tuning object detection models on an annotated dataset. Our experimental results across English and Japanese datasets demonstrate that applying SCAN improves end-to-end textual RAG performance by up to 9.4 points and visual RAG performance by up to 10.4 points, outperforming conventional approaches and even commercial document processing solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the increasing adoption of Large Language Models (LLMs) and Vision-Language Models (VLMs), rich document analysis technologies for applications like Retrieval-Augmented Generation (RAG) and visual RAG are gaining significant attention. Recent research indicates that using VLMs yields better RAG performance, but processing rich documents remains a challenge since a single page contains large amounts of information. In this paper, we present SCAN (SemantiC Document Layout ANalysis), a novel approach that enhances both textual and visual Retrieval-Augmented Generation (RAG) systems that work with visually rich documents. It is a VLM-friendly approach that identifies document components with appropriate semantic granularity, balancing context preservation with processing efficiency. SCAN uses a coarse-grained semantic approach that divides documents into coherent regions covering contiguous components. We trained the SCAN model by fine-tuning object detection models on an annotated dataset. Our experimental results across English and Japanese datasets demonstrate that applying SCAN improves end-to-end textual RAG performance by up to 9.4 points and visual RAG performance by up to 10.4 points, outperforming conventional approaches and even commercial document processing solutions."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-20T14:03:24Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    3,
                    24,
                    1,
                    140,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Yuyang Dong"
                    },
                    {
                        "name": "Nobuhiro Ueda"
                    },
                    {
                        "name": "Krisztián Boros"
                    },
                    {
                        "name": "Daiki Ito"
                    },
                    {
                        "name": "Takuya Sera"
                    },
                    {
                        "name": "Masafumi Oyamada"
                    }
                ],
                "author_detail": {
                    "name": "Masafumi Oyamada"
                },
                "author": "Masafumi Oyamada"
            },
            {
                "id": "http://arxiv.org/abs/2512.08609v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.08609v2",
                "title": "CogMCTS: A Novel Cognitive-Guided Monte Carlo Tree Search Framework for Iterative Heuristic Evolution with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CogMCTS: A Novel Cognitive-Guided Monte Carlo Tree Search Framework for Iterative Heuristic Evolution with Large Language Models"
                },
                "updated": "2025-12-11T08:46:55Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    8,
                    46,
                    55,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.08609v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.08609v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Automatic Heuristic Design (AHD) is an effective framework for solving complex optimization problems. The development of large language models (LLMs) enables the automated generation of heuristics. Existing LLM-based evolutionary methods rely on population strategies and are prone to local optima. Integrating LLMs with Monte Carlo Tree Search (MCTS) improves the trade-off between exploration and exploitation, but multi-round cognitive integration remains limited and search diversity is constrained. To overcome these limitations, this paper proposes a novel cognitive-guided MCTS framework (CogMCTS). CogMCTS tightly integrates the cognitive guidance mechanism of LLMs with MCTS to achieve efficient automated heuristic optimization. The framework employs multi-round cognitive feedback to incorporate historical experience, node information, and negative outcomes, dynamically improving heuristic generation. Dual-track node expansion combined with elite heuristic management balances the exploration of diverse heuristics and the exploitation of high-quality experience. In addition, strategic mutation modifies the heuristic forms and parameters to further enhance the diversity of the solution and the overall optimization performance. The experimental results indicate that CogMCTS outperforms existing LLM-based AHD methods in stability, efficiency, and solution quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Heuristic Design (AHD) is an effective framework for solving complex optimization problems. The development of large language models (LLMs) enables the automated generation of heuristics. Existing LLM-based evolutionary methods rely on population strategies and are prone to local optima. Integrating LLMs with Monte Carlo Tree Search (MCTS) improves the trade-off between exploration and exploitation, but multi-round cognitive integration remains limited and search diversity is constrained. To overcome these limitations, this paper proposes a novel cognitive-guided MCTS framework (CogMCTS). CogMCTS tightly integrates the cognitive guidance mechanism of LLMs with MCTS to achieve efficient automated heuristic optimization. The framework employs multi-round cognitive feedback to incorporate historical experience, node information, and negative outcomes, dynamically improving heuristic generation. Dual-track node expansion combined with elite heuristic management balances the exploration of diverse heuristics and the exploitation of high-quality experience. In addition, strategic mutation modifies the heuristic forms and parameters to further enhance the diversity of the solution and the overall optimization performance. The experimental results indicate that CogMCTS outperforms existing LLM-based AHD methods in stability, efficiency, and solution quality."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-09T13:54:18Z",
                "published_parsed": [
                    2025,
                    12,
                    9,
                    13,
                    54,
                    18,
                    1,
                    343,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Xiaoyu Zhang"
                    },
                    {
                        "name": "Chaoxu Mu"
                    }
                ],
                "author_detail": {
                    "name": "Chaoxu Mu"
                },
                "author": "Chaoxu Mu"
            },
            {
                "id": "http://arxiv.org/abs/2512.10430v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10430v1",
                "title": "T-pro 2.0: An Efficient Russian Hybrid-Reasoning Model and Playground",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "T-pro 2.0: An Efficient Russian Hybrid-Reasoning Model and Playground"
                },
                "updated": "2025-12-11T08:40:10Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    8,
                    40,
                    10,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10430v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10430v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We introduce T-pro 2.0, an open-weight Russian LLM for hybrid reasoning and efficient inference. The model supports direct answering and reasoning-trace generation, using a Cyrillic-dense tokenizer and an adapted EAGLE speculative-decoding pipeline to reduce latency. To enable reproducible and extensible research, we release the model weights, the T-Wix 500k instruction corpus, the T-Math reasoning benchmark, and the EAGLE weights on Hugging Face. These resources allow users to study Russian-language reasoning and to extend or adapt both the model and the inference pipeline. A public web demo exposes reasoning and non-reasoning modes and illustrates the speedups achieved by our inference stack across domains. T-pro 2.0 thus serves as an accessible open system for building and evaluating efficient, practical Russian LLM applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce T-pro 2.0, an open-weight Russian LLM for hybrid reasoning and efficient inference. The model supports direct answering and reasoning-trace generation, using a Cyrillic-dense tokenizer and an adapted EAGLE speculative-decoding pipeline to reduce latency. To enable reproducible and extensible research, we release the model weights, the T-Wix 500k instruction corpus, the T-Math reasoning benchmark, and the EAGLE weights on Hugging Face. These resources allow users to study Russian-language reasoning and to extend or adapt both the model and the inference pipeline. A public web demo exposes reasoning and non-reasoning modes and illustrates the speedups achieved by our inference stack across domains. T-pro 2.0 thus serves as an accessible open system for building and evaluating efficient, practical Russian LLM applications."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T08:40:10Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    8,
                    40,
                    10,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Dmitrii Stoianov"
                    },
                    {
                        "name": "Danil Taranets"
                    },
                    {
                        "name": "Olga Tsymboi"
                    },
                    {
                        "name": "Ramil Latypov"
                    },
                    {
                        "name": "Almaz Dautov"
                    },
                    {
                        "name": "Vladislav Kruglikov"
                    },
                    {
                        "name": "Nikita Surkov"
                    },
                    {
                        "name": "German Abramov"
                    },
                    {
                        "name": "Pavel Gein"
                    },
                    {
                        "name": "Dmitry Abulkhanov"
                    },
                    {
                        "name": "Mikhail Gashkov"
                    },
                    {
                        "name": "Viktor Zelenkovskiy"
                    },
                    {
                        "name": "Artem Batalov"
                    },
                    {
                        "name": "Aleksandr Medvedev"
                    },
                    {
                        "name": "Anatolii Potapov"
                    }
                ],
                "author_detail": {
                    "name": "Anatolii Potapov"
                },
                "author": "Anatolii Potapov"
            },
            {
                "id": "http://arxiv.org/abs/2512.10422v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10422v1",
                "title": "Cooperative Retrieval-Augmented Generation for Question Answering: Mutual Information Exchange and Ranking by Contrasting Layers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cooperative Retrieval-Augmented Generation for Question Answering: Mutual Information Exchange and Ranking by Contrasting Layers"
                },
                "updated": "2025-12-11T08:35:17Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    8,
                    35,
                    17,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10422v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10422v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Since large language models (LLMs) have a tendency to generate factually inaccurate output, retrieval-augmented generation (RAG) has gained significant attention as a key means to mitigate this downside of harnessing only LLMs. However, existing RAG methods for simple and multi-hop question answering (QA) are still prone to incorrect retrievals and hallucinations. To address these limitations, we propose CoopRAG, a novel RAG framework for the question answering task in which a retriever and an LLM work cooperatively with each other by exchanging informative knowledge, and the earlier and later layers of the retriever model work cooperatively with each other to accurately rank the retrieved documents relevant to a given query. In this framework, we (i) unroll a question into sub-questions and a reasoning chain in which uncertain positions are masked, (ii) retrieve the documents relevant to the question augmented with the sub-questions and the reasoning chain, (iii) rerank the documents by contrasting layers of the retriever, and (iv) reconstruct the reasoning chain by filling the masked positions via the LLM. Our experiments demonstrate that CoopRAG consistently outperforms state-of-the-art QA methods on three multi-hop QA datasets as well as a simple QA dataset in terms of both the retrieval and QA performances. Our code is available.\\footnote{https://github.com/meaningful96/CoopRAG}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since large language models (LLMs) have a tendency to generate factually inaccurate output, retrieval-augmented generation (RAG) has gained significant attention as a key means to mitigate this downside of harnessing only LLMs. However, existing RAG methods for simple and multi-hop question answering (QA) are still prone to incorrect retrievals and hallucinations. To address these limitations, we propose CoopRAG, a novel RAG framework for the question answering task in which a retriever and an LLM work cooperatively with each other by exchanging informative knowledge, and the earlier and later layers of the retriever model work cooperatively with each other to accurately rank the retrieved documents relevant to a given query. In this framework, we (i) unroll a question into sub-questions and a reasoning chain in which uncertain positions are masked, (ii) retrieve the documents relevant to the question augmented with the sub-questions and the reasoning chain, (iii) rerank the documents by contrasting layers of the retriever, and (iv) reconstruct the reasoning chain by filling the masked positions via the LLM. Our experiments demonstrate that CoopRAG consistently outperforms state-of-the-art QA methods on three multi-hop QA datasets as well as a simple QA dataset in terms of both the retrieval and QA performances. Our code is available.\\footnote{https://github.com/meaningful96/CoopRAG}"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T08:35:17Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    8,
                    35,
                    17,
                    3,
                    345,
                    0
                ],
                "arxiv_comment": "Accepted to NeurIPS 2025",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Youmin Ko"
                    },
                    {
                        "name": "Sungjong Seo"
                    },
                    {
                        "name": "Hyunjoon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Hyunjoon Kim"
                },
                "author": "Hyunjoon Kim"
            },
            {
                "id": "http://arxiv.org/abs/2509.22258v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.22258v2",
                "title": "Beyond Classification Accuracy: Neural-MedBench and the Need for Deeper Reasoning Benchmarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Classification Accuracy: Neural-MedBench and the Need for Deeper Reasoning Benchmarks"
                },
                "updated": "2025-12-11T08:31:33Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    8,
                    31,
                    33,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.22258v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.22258v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent advances in vision-language models (VLMs) have achieved remarkable performance on standard medical benchmarks, yet their true clinical reasoning ability remains unclear. Existing datasets predominantly emphasize classification accuracy, creating an evaluation illusion in which models appear proficient while still failing at high-stakes diagnostic reasoning. We introduce Neural-MedBench, a compact yet reasoning-intensive benchmark specifically designed to probe the limits of multimodal clinical reasoning in neurology. Neural-MedBench integrates multi-sequence MRI scans, structured electronic health records, and clinical notes, and encompasses three core task families: differential diagnosis, lesion recognition, and rationale generation. To ensure reliable evaluation, we develop a hybrid scoring pipeline that combines LLM-based graders, clinician validation, and semantic similarity metrics. Through systematic evaluation of state-of-the-art VLMs, including GPT-4o, Claude-4, and MedGemma, we observe a sharp performance drop compared to conventional datasets. Error analysis shows that reasoning failures, rather than perceptual errors, dominate model shortcomings. Our findings highlight the necessity of a Two-Axis Evaluation Framework: breadth-oriented large datasets for statistical generalization, and depth-oriented, compact benchmarks such as Neural-MedBench for reasoning fidelity. We release Neural-MedBench at https://neuromedbench.github.io/ as an open and extensible diagnostic testbed, which guides the expansion of future benchmarks and enables rigorous yet cost-effective assessment of clinically trustworthy AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in vision-language models (VLMs) have achieved remarkable performance on standard medical benchmarks, yet their true clinical reasoning ability remains unclear. Existing datasets predominantly emphasize classification accuracy, creating an evaluation illusion in which models appear proficient while still failing at high-stakes diagnostic reasoning. We introduce Neural-MedBench, a compact yet reasoning-intensive benchmark specifically designed to probe the limits of multimodal clinical reasoning in neurology. Neural-MedBench integrates multi-sequence MRI scans, structured electronic health records, and clinical notes, and encompasses three core task families: differential diagnosis, lesion recognition, and rationale generation. To ensure reliable evaluation, we develop a hybrid scoring pipeline that combines LLM-based graders, clinician validation, and semantic similarity metrics. Through systematic evaluation of state-of-the-art VLMs, including GPT-4o, Claude-4, and MedGemma, we observe a sharp performance drop compared to conventional datasets. Error analysis shows that reasoning failures, rather than perceptual errors, dominate model shortcomings. Our findings highlight the necessity of a Two-Axis Evaluation Framework: breadth-oriented large datasets for statistical generalization, and depth-oriented, compact benchmarks such as Neural-MedBench for reasoning fidelity. We release Neural-MedBench at https://neuromedbench.github.io/ as an open and extensible diagnostic testbed, which guides the expansion of future benchmarks and enables rigorous yet cost-effective assessment of clinically trustworthy AI."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-26T12:20:01Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    12,
                    20,
                    1,
                    4,
                    269,
                    0
                ],
                "arxiv_comment": "23 pages, 12 figures",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Miao Jing"
                    },
                    {
                        "name": "Mengting Jia"
                    },
                    {
                        "name": "Junling Lin"
                    },
                    {
                        "name": "Zhongxia Shen"
                    },
                    {
                        "name": "Huan Gao"
                    },
                    {
                        "name": "Mingkun Xu"
                    },
                    {
                        "name": "Shangyang Li"
                    }
                ],
                "author_detail": {
                    "name": "Shangyang Li"
                },
                "author": "Shangyang Li"
            },
            {
                "id": "http://arxiv.org/abs/2512.10415v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10415v1",
                "title": "How to Trick Your AI TA: A Systematic Study of Academic Jailbreaking in LLM Code Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to Trick Your AI TA: A Systematic Study of Academic Jailbreaking in LLM Code Evaluation"
                },
                "updated": "2025-12-11T08:28:33Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    8,
                    28,
                    33,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10415v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10415v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The use of Large Language Models (LLMs) as automatic judges for code evaluation is becoming increasingly prevalent in academic environments. But their reliability can be compromised by students who may employ adversarial prompting strategies in order to induce misgrading and secure undeserved academic advantages. In this paper, we present the first large-scale study of jailbreaking LLM-based automated code evaluators in academic context. Our contributions are: (i) We systematically adapt 20+ jailbreaking strategies for jailbreaking AI code evaluators in the academic context, defining a new class of attacks termed academic jailbreaking. (ii) We release a poisoned dataset of 25K adversarial student submissions, specifically designed for the academic code-evaluation setting, sourced from diverse real-world coursework and paired with rubrics and human-graded references, and (iii) In order to capture the multidimensional impact of academic jailbreaking, we systematically adapt and define three jailbreaking metrics (Jailbreak Success Rate, Score Inflation, and Harmfulness). (iv) We comprehensively evalulate the academic jailbreaking attacks using six LLMs. We find that these models exhibit significant vulnerability, particularly to persuasive and role-play-based attacks (up to 97% JSR). Our adversarial dataset and benchmark suite lay the groundwork for next-generation robust LLM-based evaluators in academic code assessment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of Large Language Models (LLMs) as automatic judges for code evaluation is becoming increasingly prevalent in academic environments. But their reliability can be compromised by students who may employ adversarial prompting strategies in order to induce misgrading and secure undeserved academic advantages. In this paper, we present the first large-scale study of jailbreaking LLM-based automated code evaluators in academic context. Our contributions are: (i) We systematically adapt 20+ jailbreaking strategies for jailbreaking AI code evaluators in the academic context, defining a new class of attacks termed academic jailbreaking. (ii) We release a poisoned dataset of 25K adversarial student submissions, specifically designed for the academic code-evaluation setting, sourced from diverse real-world coursework and paired with rubrics and human-graded references, and (iii) In order to capture the multidimensional impact of academic jailbreaking, we systematically adapt and define three jailbreaking metrics (Jailbreak Success Rate, Score Inflation, and Harmfulness). (iv) We comprehensively evalulate the academic jailbreaking attacks using six LLMs. We find that these models exhibit significant vulnerability, particularly to persuasive and role-play-based attacks (up to 97% JSR). Our adversarial dataset and benchmark suite lay the groundwork for next-generation robust LLM-based evaluators in academic code assessment."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T08:28:33Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    8,
                    28,
                    33,
                    3,
                    345,
                    0
                ],
                "arxiv_comment": "Under Review",
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Devanshu Sahoo"
                    },
                    {
                        "name": "Vasudev Majhi"
                    },
                    {
                        "name": "Arjun Neekhra"
                    },
                    {
                        "name": "Yash Sinha"
                    },
                    {
                        "name": "Murari Mandal"
                    },
                    {
                        "name": "Dhruv Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Dhruv Kumar"
                },
                "author": "Dhruv Kumar"
            },
            {
                "id": "http://arxiv.org/abs/2512.10411v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10411v1",
                "title": "Sliding Window Attention Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sliding Window Attention Adaptation"
                },
                "updated": "2025-12-11T08:21:24Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    8,
                    21,
                    24,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10411v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10411v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The self-attention mechanism in Transformer-based Large Language Models (LLMs) scales quadratically with input length, making long-context inference expensive. Sliding window attention (SWA) reduces this cost to linear complexity, but naively enabling complete SWA at inference-time for models pretrained with full attention (FA) causes severe long-context performance degradation due to training-inference mismatch. This makes us wonder: Can FA-pretrained LLMs be well adapted to SWA without pretraining? We investigate this by proposing Sliding Window Attention Adaptation (SWAA), a set of practical recipes that combine five methods for better adaptation: (1) applying SWA only during prefilling; (2) preserving \"sink\" tokens; (3) interleaving FA/SWA layers; (4) chain-of-thought (CoT); and (5) fine-tuning. Our experiments show that SWA adaptation is feasible while non-trivial: no single method suffices, yet specific synergistic combinations effectively recover the original long-context performance. We further analyze the performance-efficiency trade-offs of different SWAA configurations and provide recommended recipes for diverse scenarios. Our code is available at https://github.com/yuyijiong/sliding-window-attention-adaptation",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The self-attention mechanism in Transformer-based Large Language Models (LLMs) scales quadratically with input length, making long-context inference expensive. Sliding window attention (SWA) reduces this cost to linear complexity, but naively enabling complete SWA at inference-time for models pretrained with full attention (FA) causes severe long-context performance degradation due to training-inference mismatch. This makes us wonder: Can FA-pretrained LLMs be well adapted to SWA without pretraining? We investigate this by proposing Sliding Window Attention Adaptation (SWAA), a set of practical recipes that combine five methods for better adaptation: (1) applying SWA only during prefilling; (2) preserving \"sink\" tokens; (3) interleaving FA/SWA layers; (4) chain-of-thought (CoT); and (5) fine-tuning. Our experiments show that SWA adaptation is feasible while non-trivial: no single method suffices, yet specific synergistic combinations effectively recover the original long-context performance. We further analyze the performance-efficiency trade-offs of different SWAA configurations and provide recommended recipes for diverse scenarios. Our code is available at https://github.com/yuyijiong/sliding-window-attention-adaptation"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T08:21:24Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    8,
                    21,
                    24,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Yijiong Yu"
                    },
                    {
                        "name": "Jiale Liu"
                    },
                    {
                        "name": "Qingyun Wu"
                    },
                    {
                        "name": "Huazheng Wang"
                    },
                    {
                        "name": "Ji Pei"
                    }
                ],
                "author_detail": {
                    "name": "Ji Pei"
                },
                "author": "Ji Pei"
            },
            {
                "id": "http://arxiv.org/abs/2512.10403v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10403v1",
                "title": "BRACE: A Benchmark for Robust Audio Caption Quality Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BRACE: A Benchmark for Robust Audio Caption Quality Evaluation"
                },
                "updated": "2025-12-11T08:09:24Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    8,
                    9,
                    24,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10403v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10403v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Automatic audio captioning is essential for audio understanding, enabling applications such as accessibility and content indexing. However, evaluating the quality of audio captions remains a major challenge, especially in reference-free settings where high-quality ground-truth captions are unavailable. While CLAPScore is currently the most widely used reference-free Audio Caption Evaluation Metric(ACEM), its robustness under diverse conditions has not been systematically validated.\n  To address this gap, we introduce BRACE, a new benchmark designed to evaluate audio caption alignment quality in a reference-free setting. BRACE is primarily designed for assessing ACEMs, and can also be extended to measure the modality alignment abilities of Large Audio Language Model(LALM). BRACE consists of two sub-benchmarks: BRACE-Main for fine-grained caption comparison and BRACE-Hallucination for detecting subtle hallucinated content. We construct these datasets through high-quality filtering, LLM-based corruption, and human annotation.\n  Given the widespread adoption of CLAPScore as a reference-free ACEM and the increasing application of LALMs in audio-language tasks, we evaluate both approaches using the BRACE benchmark, testing CLAPScore across various CLAP model variants and assessing multiple LALMs.\n  Notably, even the best-performing CLAP-based ACEM achieves only a 70.01 F1-score on the BRACE-Main benchmark, while the best LALM reaches just 63.19.\n  By revealing the limitations of CLAP models and LALMs, our BRACE benchmark offers valuable insights into the direction of future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic audio captioning is essential for audio understanding, enabling applications such as accessibility and content indexing. However, evaluating the quality of audio captions remains a major challenge, especially in reference-free settings where high-quality ground-truth captions are unavailable. While CLAPScore is currently the most widely used reference-free Audio Caption Evaluation Metric(ACEM), its robustness under diverse conditions has not been systematically validated.\n  To address this gap, we introduce BRACE, a new benchmark designed to evaluate audio caption alignment quality in a reference-free setting. BRACE is primarily designed for assessing ACEMs, and can also be extended to measure the modality alignment abilities of Large Audio Language Model(LALM). BRACE consists of two sub-benchmarks: BRACE-Main for fine-grained caption comparison and BRACE-Hallucination for detecting subtle hallucinated content. We construct these datasets through high-quality filtering, LLM-based corruption, and human annotation.\n  Given the widespread adoption of CLAPScore as a reference-free ACEM and the increasing application of LALMs in audio-language tasks, we evaluate both approaches using the BRACE benchmark, testing CLAPScore across various CLAP model variants and assessing multiple LALMs.\n  Notably, even the best-performing CLAP-based ACEM achieves only a 70.01 F1-score on the BRACE-Main benchmark, while the best LALM reaches just 63.19.\n  By revealing the limitations of CLAP models and LALMs, our BRACE benchmark offers valuable insights into the direction of future research."
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T08:09:24Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    8,
                    9,
                    24,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD"
                },
                "authors": [
                    {
                        "name": "Tianyu Guo"
                    },
                    {
                        "name": "Hongyu Chen"
                    },
                    {
                        "name": "Hao Liang"
                    },
                    {
                        "name": "Meiyi Qiang"
                    },
                    {
                        "name": "Bohan Zeng"
                    },
                    {
                        "name": "Linzhuang Sun"
                    },
                    {
                        "name": "Bin Cui"
                    },
                    {
                        "name": "Wentao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wentao Zhang"
                },
                "author": "Wentao Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2511.21203v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.21203v2",
                "title": "Transformer Driven Visual Servoing and Dual Arm Impedance Control for Fabric Texture Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer Driven Visual Servoing and Dual Arm Impedance Control for Fabric Texture Matching"
                },
                "updated": "2025-12-11T08:08:12Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    8,
                    8,
                    12,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.21203v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.21203v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In this paper, we propose a method to align and place a fabric piece on top of another using a dual-arm manipulator and a grayscale camera, so that their surface textures are accurately matched. We propose a novel control scheme that combines Transformer-driven visual servoing with dualarm impedance control. This approach enables the system to simultaneously control the pose of the fabric piece and place it onto the underlying one while applying tension to keep the fabric piece flat. Our transformer-based network incorporates pretrained backbones and a newly introduced Difference Extraction Attention Module (DEAM), which significantly enhances pose difference prediction accuracy. Trained entirely on synthetic images generated using rendering software, the network enables zero-shot deployment in real-world scenarios without requiring prior training on specific fabric textures. Real-world experiments demonstrate that the proposed system accurately aligns fabric pieces with different textures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose a method to align and place a fabric piece on top of another using a dual-arm manipulator and a grayscale camera, so that their surface textures are accurately matched. We propose a novel control scheme that combines Transformer-driven visual servoing with dualarm impedance control. This approach enables the system to simultaneously control the pose of the fabric piece and place it onto the underlying one while applying tension to keep the fabric piece flat. Our transformer-based network incorporates pretrained backbones and a newly introduced Difference Extraction Attention Module (DEAM), which significantly enhances pose difference prediction accuracy. Trained entirely on synthetic images generated using rendering software, the network enables zero-shot deployment in real-world scenarios without requiring prior training on specific fabric textures. Real-world experiments demonstrate that the proposed system accurately aligns fabric pieces with different textures."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-26T09:33:03Z",
                "published_parsed": [
                    2025,
                    11,
                    26,
                    9,
                    33,
                    3,
                    2,
                    330,
                    0
                ],
                "arxiv_comment": "8 pages, 11 figures. Accepted to IEEE Robotics and Automation Letters (RA-L)",
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Fuyuki Tokuda"
                    },
                    {
                        "name": "Akira Seino"
                    },
                    {
                        "name": "Akinari Kobayashi"
                    },
                    {
                        "name": "Kai Tang"
                    },
                    {
                        "name": "Kazuhiro Kosuge"
                    }
                ],
                "author_detail": {
                    "name": "Kazuhiro Kosuge"
                },
                "author": "Kazuhiro Kosuge"
            },
            {
                "id": "http://arxiv.org/abs/2512.10398v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10398v1",
                "title": "Confucius Code Agent: An Open-sourced AI Software Engineer at Industrial Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Confucius Code Agent: An Open-sourced AI Software Engineer at Industrial Scale"
                },
                "updated": "2025-12-11T08:05:58Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    8,
                    5,
                    58,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10398v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10398v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Real-world AI software engineering demands coding agents that can reason over massive repositories, maintain durable memory across and within long sessions, and robustly coordinate complex toolchains at test time. Existing open-source coding agents provide transparency but frequently fall short when pushed to these industrial-scale workloads, while proprietary coding agents offer strong practical performance but limited extensibility, interpretability, and controllability. We present the Confucius Code Agent (CCA), an open-sourced AI software engineer that can operate at an industrial scale. CCA is built atop the Confucius SDK, an open-sourced agent development platform designed around three complementary perspectives: Agent Experience (AX), User Experience (UX), and Developer Experience (DX). The SDK introduces a unified orchestrator with hierarchical working memory for long-context reasoning, a persistent note-taking system for cross-session continual learning, and a modular extension module for robust tool use. Moreover, a meta-agent automates the synthesis, evaluation, and refinement of agent configurations through a build-test-improve loop, enabling rapid agent development on new tasks, environments, and tool stacks. Instantiated on Confucius SDK with these mechanisms, CCA delivers strong performance on real-world software engineering tasks. On SWE-Bench-Pro, CCA achieves a state-of-the-art Resolve@1 performance of 54.3%, substantially improving over prior coding agents. Together, the Confucius SDK and CCA provide a transparent, extensible, and reproducible foundation for AI agents, bridge gaps between research prototypes and production-grade systems, and support agent development and deployment at industrial scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-world AI software engineering demands coding agents that can reason over massive repositories, maintain durable memory across and within long sessions, and robustly coordinate complex toolchains at test time. Existing open-source coding agents provide transparency but frequently fall short when pushed to these industrial-scale workloads, while proprietary coding agents offer strong practical performance but limited extensibility, interpretability, and controllability. We present the Confucius Code Agent (CCA), an open-sourced AI software engineer that can operate at an industrial scale. CCA is built atop the Confucius SDK, an open-sourced agent development platform designed around three complementary perspectives: Agent Experience (AX), User Experience (UX), and Developer Experience (DX). The SDK introduces a unified orchestrator with hierarchical working memory for long-context reasoning, a persistent note-taking system for cross-session continual learning, and a modular extension module for robust tool use. Moreover, a meta-agent automates the synthesis, evaluation, and refinement of agent configurations through a build-test-improve loop, enabling rapid agent development on new tasks, environments, and tool stacks. Instantiated on Confucius SDK with these mechanisms, CCA delivers strong performance on real-world software engineering tasks. On SWE-Bench-Pro, CCA achieves a state-of-the-art Resolve@1 performance of 54.3%, substantially improving over prior coding agents. Together, the Confucius SDK and CCA provide a transparent, extensible, and reproducible foundation for AI agents, bridge gaps between research prototypes and production-grade systems, and support agent development and deployment at industrial scale."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T08:05:58Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    8,
                    5,
                    58,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Zhaodong Wang"
                    },
                    {
                        "name": "Zhenting Qi"
                    },
                    {
                        "name": "Sherman Wong"
                    },
                    {
                        "name": "Nathan Hu"
                    },
                    {
                        "name": "Samuel Lin"
                    },
                    {
                        "name": "Jun Ge"
                    },
                    {
                        "name": "Erwin Gao"
                    },
                    {
                        "name": "Yining Yang"
                    },
                    {
                        "name": "Ben Maurer"
                    },
                    {
                        "name": "Wenlin Chen"
                    },
                    {
                        "name": "David Recordon"
                    },
                    {
                        "name": "Yilun Du"
                    },
                    {
                        "name": "Minlan Yu"
                    },
                    {
                        "name": "Ying Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ying Zhang"
                },
                "author": "Ying Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2412.21051v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2412.21051v4",
                "title": "Toward Intelligent and Secure Cloud: Large Language Model Empowered Proactive Defense",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward Intelligent and Secure Cloud: Large Language Model Empowered Proactive Defense"
                },
                "updated": "2025-12-11T08:02:12Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    8,
                    2,
                    12,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2412.21051v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2412.21051v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The rapid evolution of cloud computing technologies and the increasing number of cloud applications have provided numerous benefits in our daily lives. However, the diversity and complexity of different components pose a significant challenge to cloud security, especially when dealing with sophisticated and advanced cyberattacks such as Denial of Service (DoS). Recent advancements in the large language models (LLMs) offer promising solutions for security intelligence. By exploiting the powerful capabilities in language understanding, data analysis, task inference, action planning, and code generation, we present LLM-PD, a novel defense architecture that proactively mitigates various DoS threats in cloud networks. LLM-PD can efficiently make decisions through comprehensive data analysis and sequential reasoning, as well as dynamically create and deploy actionable defense mechanisms. Furthermore, it can flexibly self-evolve based on experience learned from previous interactions and adapt to new attack scenarios without additional training. Our case study on three distinct DoS attacks demonstrates its remarkable ability in terms of defense effectiveness and efficiency when compared with other existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of cloud computing technologies and the increasing number of cloud applications have provided numerous benefits in our daily lives. However, the diversity and complexity of different components pose a significant challenge to cloud security, especially when dealing with sophisticated and advanced cyberattacks such as Denial of Service (DoS). Recent advancements in the large language models (LLMs) offer promising solutions for security intelligence. By exploiting the powerful capabilities in language understanding, data analysis, task inference, action planning, and code generation, we present LLM-PD, a novel defense architecture that proactively mitigates various DoS threats in cloud networks. LLM-PD can efficiently make decisions through comprehensive data analysis and sequential reasoning, as well as dynamically create and deploy actionable defense mechanisms. Furthermore, it can flexibly self-evolve based on experience learned from previous interactions and adapt to new attack scenarios without additional training. Our case study on three distinct DoS attacks demonstrates its remarkable ability in terms of defense effectiveness and efficiency when compared with other existing methods."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-12-30T16:09:28Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    16,
                    9,
                    28,
                    0,
                    365,
                    0
                ],
                "arxiv_comment": "7 pages; Accepted by IEEE Communications Magazine",
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Yuyang Zhou"
                    },
                    {
                        "name": "Guang Cheng"
                    },
                    {
                        "name": "Kang Du"
                    },
                    {
                        "name": "Zihan Chen"
                    },
                    {
                        "name": "Yuyu Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yuyu Zhao"
                },
                "author": "Yuyu Zhao"
            },
            {
                "id": "http://arxiv.org/abs/2512.10394v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.10394v1",
                "title": "RoboNeuron: A Modular Framework Linking Foundation Models and ROS for Embodied AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoboNeuron: A Modular Framework Linking Foundation Models and ROS for Embodied AI"
                },
                "updated": "2025-12-11T07:58:19Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    7,
                    58,
                    19,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.10394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.10394v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Current embodied AI systems face severe engineering impediments, primarily characterized by poor cross-scenario adaptability, rigid inter-module coupling, and fragmented inference acceleration. To overcome these limitations, we propose RoboNeuron, a universal deployment framework for embodied intelligence. RoboNeuron is the first framework to deeply integrate the cognitive capabilities of Large Language Models (LLMs) and Vision-Language-Action (VLA) models with the real-time execution backbone of the Robot Operating System (ROS). We utilize the Model Context Protocol (MCP) as a semantic bridge, enabling the LLM to dynamically orchestrate underlying robotic tools. The framework establishes a highly modular architecture that strictly decouples sensing, reasoning, and control by leveraging ROS's unified communication interfaces. Crucially, we introduce an automated tool to translate ROS messages into callable MCP functions, significantly streamlining development. RoboNeuron significantly enhances cross-scenario adaptability and component flexibility, while establishing a systematic platform for horizontal performance benchmarking, laying a robust foundation for scalable real-world embodied applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current embodied AI systems face severe engineering impediments, primarily characterized by poor cross-scenario adaptability, rigid inter-module coupling, and fragmented inference acceleration. To overcome these limitations, we propose RoboNeuron, a universal deployment framework for embodied intelligence. RoboNeuron is the first framework to deeply integrate the cognitive capabilities of Large Language Models (LLMs) and Vision-Language-Action (VLA) models with the real-time execution backbone of the Robot Operating System (ROS). We utilize the Model Context Protocol (MCP) as a semantic bridge, enabling the LLM to dynamically orchestrate underlying robotic tools. The framework establishes a highly modular architecture that strictly decouples sensing, reasoning, and control by leveraging ROS's unified communication interfaces. Crucially, we introduce an automated tool to translate ROS messages into callable MCP functions, significantly streamlining development. RoboNeuron significantly enhances cross-scenario adaptability and component flexibility, while establishing a systematic platform for horizontal performance benchmarking, laying a robust foundation for scalable real-world embodied applications."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-11T07:58:19Z",
                "published_parsed": [
                    2025,
                    12,
                    11,
                    7,
                    58,
                    19,
                    3,
                    345,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Weifan Guan"
                    },
                    {
                        "name": "Huasen Xi"
                    },
                    {
                        "name": "Chenxiao Zhang"
                    },
                    {
                        "name": "Aosheng Li"
                    },
                    {
                        "name": "Qinghao Hu"
                    },
                    {
                        "name": "Jian Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Jian Cheng"
                },
                "author": "Jian Cheng"
            }
        ]
    }
]